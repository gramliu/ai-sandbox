{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb torch transformers sentence_transformers\n",
    "%pip install llama-index llama-index-llms-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gramliu/Projects/ai-sandbox/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "paper_collection = chroma_client.get_or_create_collection(name=\"papers\")\n",
    "note_collection = chroma_client.get_or_create_collection(name=\"notes\")\n",
    "\n",
    "QueryData = namedtuple(\"QueryData\", [\"query\", \"source\", \"query_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "ids = []\n",
    "for fn in os.listdir(\"data/synthetic/strong\"):\n",
    "  with open(f\"data/synthetic/strong/{fn}\") as f:\n",
    "    content = json.load(f)\n",
    "    text = content[\"input\"]\n",
    "    papers.append(text)\n",
    "    ids.append(fn[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gramliu/Projects/ai-sandbox/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = 'Alibaba-NLP/gte-base-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "def generate_embeddings(texts: List[str]) -> Tensor:\n",
    "    # Tokenize the input texts\n",
    "    batch_dict = tokenizer(texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = outputs.last_hidden_state[:, 0]\n",
    "    \n",
    "    # (Optionally) normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing papers 0 to 10\n",
      "Processing papers 10 to 20\n",
      "Processing papers 20 to 30\n",
      "Processing papers 30 to 40\n",
      "Processing papers 40 to 50\n",
      "Processing papers 50 to 60\n",
      "Processing papers 60 to 70\n"
     ]
    }
   ],
   "source": [
    "# Process papers in batches of 10\n",
    "batch_size = 10\n",
    "for batch in range(0, len(papers), batch_size):\n",
    "    print(f\"Processing papers {batch} to {batch+batch_size}\")\n",
    "    batch_papers = papers[batch:batch+batch_size]\n",
    "    batch_embeddings = generate_embeddings(batch_papers).tolist()\n",
    "    batch_ids = ids[batch:batch+batch_size]\n",
    "    paper_collection.add(\n",
    "        documents=batch_papers,\n",
    "        embeddings=batch_embeddings,\n",
    "        ids=batch_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries\n",
    "queries: List[QueryData] = []\n",
    "for ref_type in [\"weak\", \"strong\", \"weakv2\"]:\n",
    "  for fn in os.listdir(f\"data/synthetic/{ref_type}\"):\n",
    "    with open(f\"data/synthetic/{ref_type}/{fn}\") as f:\n",
    "      content = json.load(f)\n",
    "      query = content[\"output\"]\n",
    "      source = fn[:-5]\n",
    "      queries.append(QueryData(query, source, query_type=ref_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:39<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 accuracy\n",
      "Accuracy: 197/210 (93.81%)\n",
      "Scores: {'weak': 98.57142857142858, 'strong': 97.14285714285714, 'weakv2': 85.71428571428571}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "scores = {}\n",
    "for query_data in tqdm.tqdm(queries):\n",
    "  query = query_data.query\n",
    "  source = query_data.source\n",
    "  query_type = query_data.query_type\n",
    "  query_embedding = generate_embeddings([query]).tolist()\n",
    "  results = paper_collection.query(query_embeddings=query_embedding, n_results=3)\n",
    "  ids = results[\"ids\"][0]\n",
    "  if source in ids:\n",
    "    correct += 1\n",
    "    scores[query_type] = scores.get(query_type, 0) + 1\n",
    "batch_size = 70\n",
    "print(\"Top 3 accuracy\")\n",
    "print(f\"Accuracy: {correct}/{len(queries)} ({correct / len(queries) * 100:.2f}%)\")\n",
    "score_percentage = {k: v / batch_size * 100 for k, v in scores.items()}\n",
    "print(f\"Scores: {score_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:39<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy\n",
      "Accuracy: 168/210 (80.00%)\n",
      "Scores: {'weak': 87.14285714285714, 'strong': 90.0, 'weakv2': 62.857142857142854}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "scores = {}\n",
    "for query_data in tqdm.tqdm(queries):\n",
    "  query = query_data.query\n",
    "  source = query_data.source\n",
    "  query_type = query_data.query_type\n",
    "  query_embedding = generate_embeddings([query]).tolist()\n",
    "  results = paper_collection.query(query_embeddings=query_embedding, n_results=1)\n",
    "  ids = results[\"ids\"][0]\n",
    "  if source in ids:\n",
    "    correct += 1\n",
    "    scores[query_type] = scores.get(query_type, 0) + 1\n",
    "batch_size = 70\n",
    "print(\"Top 1 accuracy\")\n",
    "print(f\"Accuracy: {correct}/{len(queries)} ({correct / len(queries) * 100:.2f}%)\")\n",
    "score_percentage = {k: v / batch_size * 100 for k, v in scores.items()}\n",
    "print(f\"Scores: {score_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Alibaba-NLP/gte-large-en-v1.5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataset, model, top_k=5, verbose=False):\n",
    "  corpus = dataset.corpus\n",
    "  queries = dataset.queries\n",
    "  relevant_docs = dataset.relevant_docs\n",
    "\n",
    "  nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
    "  index = VectorStoreIndex(\n",
    "      nodes, embed_model=embed_model, show_progress=True\n",
    "  )\n",
    "  retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "  eval_results = []\n",
    "  for query_id, query in tqdm(queries.items()):\n",
    "      retrieved_nodes = retriever.retrieve(query)\n",
    "      retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
    "      expected_id = relevant_docs[query_id][0]\n",
    "      is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
    "\n",
    "      eval_result = {\n",
    "          \"is_hit\": is_hit,\n",
    "          \"retrieved\": retrieved_ids,\n",
    "          \"expected\": expected_id,\n",
    "          \"query\": query_id,\n",
    "      }\n",
    "      eval_results.append(eval_result)\n",
    "  return eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
