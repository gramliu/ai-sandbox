{
  "text": "Summary:\n\nThis paper evaluates the performance of state-of-the-art multimodal foundation models, GPT-4o and Gemini 1.5 Pro, on in-context learning (ICL) with a large number of demonstrating examples (many-shot ICL). Key findings:\n\n- Providing multimodal models with many demonstrating examples (up to ~2,000) leads to substantial performance improvements compared to few-shot (<100 examples) ICL across 10 datasets spanning multiple domains and tasks.\n\n- Gemini 1.5 Pro generally exhibits log-linear performance improvements as the number of demonstrating examples increases, while GPT-4o shows less stable improvements.\n\n- Gemini 1.5 Pro demonstrates higher ICL data efficiency than GPT-4o on most datasets, meaning it learns more from each additional demonstrating example.\n\n- Batching multiple queries in a single request can achieve similar or better performance than single query requests in a many-shot setting, while drastically reducing per-query latency and cost.\n\n- Batching queries can also lead to substantial performance improvements in the zero-shot setting, due to benefits from domain calibration, class calibration, and self-generated demonstrations.\n\nKeywords: multimodal foundation models, in-context learning, many-shot learning, batch querying, data efficiency\n\nExample Questions:\n1. How does the performance of Gemini 1.5 Pro and GPT-4o compare when scaling the number of demonstrating examples from few-shot to many-shot in-context learning?\n2. What techniques were used to ensure the safety and responsibility of the multimodal models during open-ended interactions?\n3. How could the ability to run highly capable multimodal models like phi-3-mini directly on consumer devices change the development and deployment of AI assistants in the future?\n4. What are some potential beneficial applications of a model like phi-3-mini that can perform advanced language tasks while preserving user privacy by running fully on-device?",
  "metadata": {
    "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
    "abstract": "  Large language models are well-known to be effective at few-shot in-context\nlearning (ICL). Recent advancements in multimodal foundation models have\nenabled unprecedentedly long context windows, presenting an opportunity to\nexplore their capability to perform ICL with many more demonstrating examples.\nIn this work, we evaluate the performance of multimodal foundation models\nscaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro\nacross 10 datasets spanning multiple domains (natural imagery, medical imagery,\nremote sensing, and molecular imagery) and tasks (multi-class, multi-label, and\nfine-grained classification). We observe that many-shot ICL, including up to\nalmost 2,000 multimodal demonstrating examples, leads to substantial\nimprovements compared to few-shot (&lt;100 examples) ICL across all of the\ndatasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly\nup to the maximum number of tested examples on many datasets. Given the high\ninference costs associated with the long prompts required for many-shot ICL, we\nalso explore the impact of batching multiple queries in a single API call. We\nshow that batching up to 50 queries can lead to performance improvements under\nzero-shot and many-shot ICL, with substantial gains in the zero-shot setting on\nmultiple datasets, while drastically reducing per-query cost and latency.\nFinally, we measure ICL data efficiency of the models, or the rate at which the\nmodels learn from more demonstrating examples. We find that while GPT-4o and\nGemini 1.5 Pro achieve similar zero-shot performance across the datasets,\nGemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most\ndatasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .\n",
    "published": "2024-05-16T04:02:43Z"
  }
}