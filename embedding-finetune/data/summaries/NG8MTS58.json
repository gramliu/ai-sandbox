{
  "text": "Summary:\n\nThis paper introduces Kolmogorov-Arnold Networks (KANs) as a promising alternative to Multi-Layer Perceptrons (MLPs) for function approximation and PDE solving. The key innovation in KANs is that they have learnable activation functions on the edges (weights) instead of fixed activation functions on the nodes (neurons) like in MLPs.\n\nThe main highlights are:\n\n- KANs can achieve comparable or better accuracy than much larger MLPs, while being significantly more parameter-efficient. This is enabled by the Kolmogorov-Arnold representation theorem, which allows high-dimensional functions to be decomposed into compositions of 1D functions.\n\n- Theoretically and empirically, KANs exhibit faster neural scaling laws (test loss ‚àù N^-4) compared to MLPs.\n\n- KANs are highly interpretable - the activation functions can be visualized and interactively modified by users. This allows KANs to be used as \"collaborators\" to help scientists (re)discover mathematical and physical laws.\n\n- KANs show promising results on a variety of tasks including data fitting, PDE solving, and continual learning, outperforming MLPs.\n\nKeywords: Kolmogorov-Arnold networks, interpretable machine learning, neural scaling laws, symbolic regression, physics-informed neural networks\n\nExample Questions:\n\nQ: How do KANs differ from MLPs in terms of their architecture and training?\nA: KANs have learnable activation functions on the edges (weights) instead of fixed activation functions on the nodes (neurons) like in MLPs. This allows KANs to decompose high-dimensional functions into compositions of 1D functions.\n\nQ: What are the key advantages of KANs over MLPs in terms of accuracy and interpretability?\nA: KANs can achieve better accuracy than much larger MLPs while being more parameter-efficient, due to the Kolmogorov-Arnold representation. KANs are also highly interpretable, with the activation functions being easily visualized and modified by users.\n\nQ: How can KANs be used to help scientists (re)discover mathematical and physical laws?\nA: The interpretability of KANs allows them to be used as \"collaborators\" with scientists. The activation functions and computation graphs of KANs can be inspected to uncover the underlying mathematical structure of the problem, facilitating scientific discovery.\n\nQ: What are some potential applications of highly capable yet compact language models like phi-3-mini that can run locally on consumer devices?\nA: Some potential applications include privacy-preserving mobile AI assistants, embedded NLP systems for domains like healthcare where privacy is critical, and democratization of advanced language AI capabilities to developers/researchers without access to massive computing resources.",
  "metadata": {
    "title": "KAN: Kolmogorov-Arnold Networks",
    "abstract": "  Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.\n",
    "published": "2024-04-30T17:58:29Z"
  }
}