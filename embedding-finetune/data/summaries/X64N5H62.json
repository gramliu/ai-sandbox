{
  "text": "This paper introduces Direct Preference Optimization (DPO), a new algorithm for training language models to align with human preferences without the need for reinforcement learning. The key insights are:\n\n1. DPO leverages a change of variables to express the reward function in terms of the optimal policy, rather than directly modeling the reward. This allows DPO to optimize the policy directly with a simple classification loss, avoiding the need for reinforcement learning.\n\n2. Theoretically, DPO can represent any reward function that is consistent with the Plackett-Luce family of preference models (which includes the Bradley-Terry model). This is done by parameterizing the reward as the log ratio of the policy to a reference policy.\n\n3. Empirically, DPO performs as well as or better than existing RLHF methods like PPO on tasks like sentiment control, summarization, and dialogue, while being much simpler to implement and train.\n\nKey Findings:\n- DPO achieves the best reward-KL tradeoff compared to PPO and other baselines in a controlled sentiment generation task.\n- On summarization and dialogue tasks, DPO matches or exceeds the performance of PPO and other strong baselines.\n- DPO is more robust to changes in sampling temperature compared to PPO.\n- DPO is able to generalize to out-of-distribution inputs as well as PPO.\n\nPotential Applications:\n- Training capable and aligned language models from human preferences, without the complexity of reinforcement learning.\n- Extending DPO to train generative models in other modalities beyond language.\n- Using DPO as a building block for more advanced preference learning algorithms.\n\nExample Questions:\nQ: How does DPO's parameterization of the reward function allow it to optimize the policy directly without reinforcement learning?\nA: DPO reparameterizes the reward function in terms of the log ratio of the policy to a reference policy. This allows DPO to express the Plackett-Luce preference model directly in terms of the policy, enabling optimization with a simple classification loss rather than reinforcement learning.\n\nQ: What theoretical guarantees does DPO provide in terms of the class of representable reward functions?\nA: DPO can represent any reward function that is consistent with the Plackett-Luce family of preference models, which includes the commonly used Bradley-Terry model. This is because the reparameterization allows DPO to select a unique reward function within each equivalence class of reward functions.\n\nQ: How does DPO's performance compare to existing RLHF methods like PPO across different language tasks?\nA: Empirically, DPO matches or exceeds the performance of PPO on tasks like sentiment control, summarization, and dialogue, while being much simpler to implement and train. DPO also exhibits greater robustness to changes in sampling temperature compared to PPO.",
  "metadata": {
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model",
    "abstract": "  While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.\n",
    "published": "2023-05-29T17:57:46Z"
  }
}