{
  "text": "This paper introduces Emu, a quality-tuned latent diffusion model for high-quality text-to-image generation. The key insights are:\n\n1. Quality-tuning, which involves fine-tuning a pre-trained model on a small set of carefully curated high-quality images, can significantly improve the visual appeal of generated images without compromising the generality of visual concepts.\n\n2. The quality of the fine-tuning dataset is much more important than the quantity. The authors show that as little as 100-2000 carefully selected high-quality images can lead to substantial improvements in visual appeal compared to the pre-trained model.\n\n3. Quality-tuning is a generic approach that can be applied to improve the performance of various text-to-image model architectures, including latent diffusion, pixel diffusion, and masked generative transformer models.\n\n4. Compared to the state-of-the-art SDXLv1.0 model, the quality-tuned Emu model is preferred 68.4% and 71.3% of the time on visual appeal on the PartiPrompts and Open User Input benchmark, respectively.\n\nKey contributions:\n- Introduction of the Emu model, a quality-tuned latent diffusion model that outperforms state-of-the-art on visual appeal.\n- Insights on the importance of high-quality fine-tuning data for aesthetic alignment of text-to-image models.\n- Demonstration that quality-tuning is a generic approach applicable to various model architectures.\n\nExample questions:\nQ: How does the visual appeal of images generated by Emu compare to the pre-trained model and the state-of-the-art SDXLv1.0 model?\nA: Emu significantly outperforms both the pre-trained model and SDXLv1.0 on visual appeal, being preferred 82.9% and 68.4% of the time respectively on the evaluation prompts.\n\nQ: What is the key insight behind the quality-tuning approach used to train Emu?\nA: The key insight is that a surprisingly small number (100-2000) of carefully curated high-quality images can have a significant impact on improving the visual appeal of generated images, without compromising the generality of visual concepts the model can depict.\n\nQ: How does the quality-tuning approach generalize beyond latent diffusion models?\nA: The authors show that the quality-tuning approach is generic and can also improve the performance of pixel diffusion and masked generative transformer models on both visual appeal and text faithfulness.",
  "metadata": {
    "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a\n  Haystack",
    "abstract": "  Training text-to-image models with web scale image-text pairs enables the\ngeneration of a wide range of visual concepts from text. However, these\npre-trained models often face challenges when it comes to generating highly\naesthetic images. This creates the need for aesthetic alignment post\npre-training. In this paper, we propose quality-tuning to effectively guide a\npre-trained model to exclusively generate highly visually appealing images,\nwhile maintaining generality across visual concepts. Our key insight is that\nsupervised fine-tuning with a set of surprisingly small but extremely visually\nappealing images can significantly improve the generation quality. We pre-train\na latent diffusion model on $1.1$ billion image-text pairs and fine-tune it\nwith only a few thousand carefully selected high-quality images. The resulting\nmodel, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only\ncounterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred\n$68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts\nand our Open User Input benchmark based on the real-world usage of\ntext-to-image models. In addition, we show that quality-tuning is a generic\napproach that is also effective for other architectures, including pixel\ndiffusion and masked generative transformer models.\n",
    "published": "2023-09-27T17:30:19Z"
  }
}