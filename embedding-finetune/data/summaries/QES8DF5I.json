{
  "text": "Summary:\n\nThis paper introduces Low-Rank Adaptation (LoRA), a parameter-efficient technique for adapting large pre-trained language models to downstream tasks. The key idea is to freeze the pre-trained model weights and inject trainable low-rank decomposition matrices into the model, greatly reducing the number of trainable parameters compared to full fine-tuning.\n\nKey Findings:\n- LoRA can reduce the number of trainable parameters by up to 10,000x compared to full fine-tuning on GPT-3 175B, while matching or exceeding the performance of full fine-tuning.\n- LoRA does not introduce any additional inference latency, unlike adapter-based methods.\n- Empirical analysis shows the update matrices learned by LoRA have a very low intrinsic rank, suggesting the changes needed for downstream adaptation can be well-captured by a compact low-rank representation.\n- LoRA can be combined with other efficient adaptation methods like prefix tuning for further improvements.\n\nKeywords:\n- Parameter-efficient adaptation\n- Low-rank matrix factorization\n- Transformer language models\n- GPT-3\n\nExample Questions:\nQ: How does LoRA compare to full fine-tuning in terms of the number of trainable parameters and computational efficiency?\nA: LoRA can reduce the number of trainable parameters by up to 10,000x compared to full fine-tuning on GPT-3 175B, while also providing a 25% speedup during training.\n\nQ: What are the key advantages of LoRA over adapter-based methods for efficient model adaptation?\nA: Unlike adapter layers, LoRA does not introduce any additional inference latency, as the trainable matrices can be merged with the frozen pre-trained weights during deployment.\n\nQ: How does the rank of the update matrices learned by LoRA relate to the intrinsic dimensionality of the changes needed for downstream adaptation?\nA: The empirical analysis shows the update matrices have a very low intrinsic rank, suggesting the changes needed for downstream tasks can be well-captured by a compact low-rank representation.\n\nQ: How can LoRA be combined with other efficient adaptation methods like prefix tuning?\nA: The paper shows that combining LoRA with prefix-embedding tuning can provide further performance improvements on some tasks, demonstrating the orthogonality of these approaches.",
  "metadata": {
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "abstract": "  An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.\n",
    "published": "2021-06-17T17:37:18Z"
  }
}