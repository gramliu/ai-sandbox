{
  "text": "Summary:\n\nThis paper introduces the Retrieval-Augmented Generation Benchmark (RGB), a new evaluation framework for assessing the capabilities of large language models (LLMs) in utilizing external knowledge through retrieval. The benchmark evaluates four key abilities required for effective retrieval-augmented generation:\n\n1. Noise Robustness: The ability to extract useful information from noisy documents that are relevant to the question but do not contain the answer.\n\n2. Negative Rejection: The ability to recognize when the required knowledge is not present in the retrieved documents and decline to answer.\n\n3. Information Integration: The ability to integrate information from multiple documents to answer complex questions.\n\n4. Counterfactual Robustness: The ability to identify and correct factual errors in the retrieved documents.\n\nThe authors evaluate six state-of-the-art LLMs, including ChatGPT, ChatGLM, and Vicuna, on the RGB benchmark. The results show that while LLMs exhibit some level of noise robustness, they struggle significantly with negative rejection, information integration, and counterfactual robustness. The authors provide detailed error analysis and discuss the key challenges that need to be addressed to effectively apply retrieval-augmented generation to LLMs.\n\nKeywords: Retrieval-Augmented Generation, Large Language Models, Benchmark, Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness\n\nExample Questions:\n\nQ: How does the noise robustness of different LLMs compare when evaluated on the RGB benchmark?\nA: The results show that as the noise ratio in the external documents increases, the performance of LLMs on the noise robustness testbed decreases significantly. For example, the accuracy of ChatGPT drops from 96.33% to 76% when the noise ratio reaches 80%.\n\nQ: What are the key challenges that LLMs face in effectively integrating information from multiple documents to answer complex questions?\nA: The authors identify three main types of errors in the information integration testbed: merging errors (where the model combines answers from different sub-questions), ignoring errors (where the model only answers one sub-question), and misalignment errors (where the model associates the wrong documents with a sub-question). These results suggest that LLMs struggle to comprehend and reason about complex, multi-faceted questions.\n\nQ: How well do LLMs perform in identifying and correcting factual errors in the retrieved documents, and what are the implications for the practical use of retrieval-augmented generation?\nA: The results on the counterfactual robustness testbed show that even when LLMs possess the relevant internal knowledge to answer a question correctly, they tend to trust and prioritize the information in the retrieved documents, even if those documents contain factual errors. This highlights a significant challenge in applying retrieval-augmented generation in real-world scenarios where the internet contains abundant misinformation.",
  "metadata": {
    "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
    "abstract": "  Retrieval-Augmented Generation (RAG) is a promising approach for mitigating\nthe hallucination of large language models (LLMs). However, existing research\nlacks rigorous evaluation of the impact of retrieval-augmented generation on\ndifferent large language models, which make it challenging to identify the\npotential bottlenecks in the capabilities of RAG for different LLMs. In this\npaper, we systematically investigate the impact of Retrieval-Augmented\nGeneration on large language models. We analyze the performance of different\nlarge language models in 4 fundamental abilities required for RAG, including\nnoise robustness, negative rejection, information integration, and\ncounterfactual robustness. To this end, we establish Retrieval-Augmented\nGeneration Benchmark (RGB), a new corpus for RAG evaluation in both English and\nChinese. RGB divides the instances within the benchmark into 4 separate\ntestbeds based on the aforementioned fundamental abilities required to resolve\nthe case. Then we evaluate 6 representative LLMs on RGB to diagnose the\nchallenges of current LLMs when applying RAG. Evaluation reveals that while\nLLMs exhibit a certain degree of noise robustness, they still struggle\nsignificantly in terms of negative rejection, information integration, and\ndealing with false information. The aforementioned assessment outcomes indicate\nthat there is still a considerable journey ahead to effectively apply RAG to\nLLMs.\n",
    "published": "2023-09-04T08:28:44Z"
  }
}