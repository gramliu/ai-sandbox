{
  "text": "Summary:\n\nMixtral 8x7B is a sparse mixture-of-experts (SMoE) language model that outperforms or matches the performance of larger models like Llama 2 70B and GPT-3.5 across a wide range of benchmarks, while using significantly fewer active parameters during inference.\n\nKey Findings:\n- Mixtral 8x7B has 47B total parameters but only uses 13B active parameters per token, allowing for faster inference and higher throughput.\n- Mixtral outperforms or matches Llama 2 70B on metrics like MMLU, HellaSwag, ARC Challenge, MBPP, and GSM-8K. It is particularly strong on mathematics and code generation tasks.\n- Mixtral also demonstrates superior performance on multilingual benchmarks compared to Llama 2.\n- Mixtral can effectively handle long-range dependencies, achieving 100% accuracy on a passkey retrieval task regardless of context length.\n- Compared to Llama 2, Mixtral exhibits less bias and more positive sentiment on bias benchmarks like BBQ and BOLD.\n- Mixtral 8x7B Instruct, a fine-tuned version for following instructions, outperforms GPT-3.5 Turbo, Claude-2.1, and Gemini Pro on human evaluation benchmarks.\n\nKeywords: sparse mixture-of-experts, language model, benchmarks, multilingual, long-range dependencies, bias, instruction following\n\nExample Questions:\nQ: How does the performance of Mixtral 8x7B compare to larger language models like Llama 2 70B and GPT-3.5 across different types of tasks?\nQ: What architectural innovations enable Mixtral to achieve high performance while using significantly fewer active parameters than its competitors?\nQ: How did the authors address safety and bias concerns in the development of Mixtral?\nQ: What are some potential applications of a highly capable language model like Mixtral that can run efficiently on consumer devices?",
  "metadata": {
    "title": "Mixtral of Experts",
    "abstract": "  We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.\n",
    "published": "2024-01-08T18:47:34Z"
  }
}