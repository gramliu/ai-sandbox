{
  "text": "Here is a summary of the key points from the paper:\n\n**Overview**\n- The paper presents GTE, a general-purpose text embedding model trained using multi-stage contrastive learning.\n- GTE aims to develop a unified text embedding model that can perform well across a variety of NLP and code-related tasks.\n- The key innovation is the use of a large-scale, diverse dataset for both unsupervised pre-training and supervised fine-tuning.\n\n**Approach**\n- The model uses a dual-encoder architecture with mean pooling on top of a deep Transformer encoder.\n- For unsupervised pre-training, the authors collect ~800M text pairs from diverse web sources like web pages, academic papers, social media, etc.\n- For supervised fine-tuning, the authors use ~3M text triples from tasks like web search, open-domain QA, NLI, fact verification, etc.\n- The authors use an improved contrastive loss function that enlarges the negative sample pool.\n\n**Evaluation**\n- GTE outperforms or matches the performance of much larger models on zero-shot text classification, unsupervised text retrieval, and the Massive Text Embedding Benchmark.\n- Without any language-specific fine-tuning, GTE also significantly outperforms state-of-the-art code retrievers of similar size on the CodeSearchNet benchmark.\n\n**Key Findings**\n- Scaling the amount of pre-training data and model size leads to consistent performance improvements.\n- The multi-stage contrastive learning approach, combining unsupervised pre-training and supervised fine-tuning, is crucial for achieving high-quality text embeddings.\n- GTE demonstrates the power of data-driven approaches in developing versatile text representation models.\n\nKeywords: text embeddings, contrastive learning, multi-task learning, code retrieval\n\nExample Questions:\nQ: How does the performance of GTE compare to state-of-the-art large language models on standard NLP benchmarks like MMLU and MT-Bench?\nQ: What techniques did the authors use to ensure GTE behaves in a safe and responsible manner during open-ended interactions?\nQ: How might the ability to run a highly capable text embedding model like GTE directly on consumer devices impact the development and deployment of AI assistants in the future?",
  "metadata": {
    "title": "Towards General Text Embeddings with Multi-stage Contrastive Learning",
    "abstract": "  We present GTE, a general-purpose text embedding model trained with\nmulti-stage contrastive learning. In line with recent advancements in unifying\nvarious NLP tasks into a single format, we train a unified text embedding model\nby employing contrastive learning over a diverse mixture of datasets from\nmultiple sources. By significantly increasing the number of training data\nduring both unsupervised pre-training and supervised fine-tuning stages, we\nachieve substantial performance gains over existing embedding models. Notably,\neven with a relatively modest parameter count of 110M, GTE$_\\text{base}$\noutperforms the black-box embedding API provided by OpenAI and even surpasses\n10x larger text embedding models on the massive text embedding benchmark.\nFurthermore, without additional fine-tuning on each programming language\nindividually, our model outperforms previous best code retrievers of similar\nsize by treating code as text. In summary, our model achieves impressive\nresults by effectively harnessing multi-stage contrastive learning, offering a\npowerful and efficient text embedding model with broad applicability across\nvarious NLP and code-related tasks.\n",
    "published": "2023-08-07T03:52:59Z"
  }
}