{
  "text": "Summary:\n\nThe paper introduces StructLM, a series of large language models (7B to 34B parameters) trained to excel at Structured Knowledge Grounding (SKG) tasks. Key points:\n\n- Motivation: LLMs struggle with SKG tasks compared to specialized models, despite their strong performance on plain text. StructLM aims to build a generalist model for diverse SKG tasks.\n\n- Dataset: The authors curated a 1.1M example dataset covering 18 SKG tasks across structured data types like tables, databases, and knowledge graphs. This dataset was used for instruction-based finetuning.\n\n- Results: StructLM outperforms specialized SKG models on 16 out of 18 held-in tasks, and establishes new state-of-the-art on 8 tasks. It also shows strong zero-shot generalization to 6 novel held-out SKG tasks, outperforming other generalist models like Flan-UL2 and TableLlama.\n\n- Ablations: The authors find that code-pretraining is most beneficial for SKG performance, and that including general instruction-following data helps preserve generalization. They also observe diminishing returns from scaling model size.\n\nKeywords: Structured Knowledge Grounding, Large Language Models, Instruction Tuning, Generalization\n\nExample Questions:\nQ: How does the performance of StructLM compare to specialized SKG models and other generalist LLMs like Flan-UL2 and TableLlama?\nA: StructLM outperforms specialized SKG models on 16 out of 18 held-in tasks, and establishes new state-of-the-art on 8 tasks. It also significantly outperforms Flan-UL2 and TableLlama on the 6 held-out SKG tasks, by 10% and 35% on average respectively.\n\nQ: What factors were found to be most important for StructLM's strong SKG performance?\nA: The authors found that code-pretraining was the most beneficial pretraining regime for SKG tasks, outperforming math-pretraining and the base Llama2 model. They also observed that including general instruction-following data helped preserve StructLM's generalization ability on the held-out tasks.",
  "metadata": {
    "title": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding",
    "abstract": "  Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.\n",
    "published": "2024-02-26T15:47:01Z"
  }
}