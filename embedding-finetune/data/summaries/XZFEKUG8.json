{
  "text": "Key Findings and Highlights:\n\n- The paper introduces Weight Averaged Reward Models (WARM), a novel strategy for designing reliable and robust reward models (RMs) to mitigate reward hacking in reinforcement learning from human feedback (RLHF).\n\n- WARM fine-tunes multiple RMs with diverse hyperparameters and then averages their weights, leveraging the linear mode connectivity property of fine-tuned weights. This makes WARM efficient (single model at inference) while improving reliability under distribution shifts and robustness to label noise/inconsistencies compared to standard prediction ensembling.\n\n- Theoretically, the paper shows that weight averaging selects the predictive mechanisms that are invariant across fine-tuning runs, reducing memorization of corrupted labels and enhancing generalization.\n\n- Experiments on summarization tasks demonstrate that WARM outperforms individual RMs and prediction ensembling, both in best-of-N sampling and RL fine-tuning. For example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy trained with a single RM.\n\nKeywords:\n- Reward modeling\n- Reinforcement learning from human feedback (RLHF)\n- Reward hacking\n- Out-of-distribution generalization\n- Label noise\n- Weight averaging\n\nExample Questions:\nQ: How does WARM improve the reliability of reward models under distribution shifts compared to standard prediction ensembling?\nA: WARM leverages the linear mode connectivity property of fine-tuned weights to efficiently approximate the benefits of prediction ensembling, while reducing the memory and inference overhead.\n\nQ: Why does weight averaging improve robustness to label noise and inconsistencies in the preference dataset compared to prediction ensembling?\nA: The paper shows theoretically that weight averaging selects the predictive mechanisms that are invariant across fine-tuning runs, reducing the model's reliance on corrupted or inconsistent labels.\n\nQ: How could the ability to run highly capable reward models like WARM directly on consumer devices impact the development and deployment of AI assistants in the future?\nA: (No definitive answer provided, as the paper does not speculate on this specific application.)\n\nQ: What are some potential beneficial applications of a compact, reliable and robust reward model like WARM beyond reinforcement learning from human feedback?\nA: WARM could enable privacy-preserving mobile AI assistants, embedded NLP systems for sensitive domains like healthcare, and democratization of advanced language AI capabilities to developers/researchers without access to massive computing resources.",
  "metadata": {
    "title": "WARM: On the Benefits of Weight Averaged Reward Models",
    "abstract": "  Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.\n",
    "published": "2024-01-22T18:27:08Z"
  }
}