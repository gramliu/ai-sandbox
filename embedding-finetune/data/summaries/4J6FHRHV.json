{
  "text": "Summary:\n\nThis paper introduces NExT-GPT, an end-to-end general-purpose any-to-any multimodal large language model (MM-LLM) that can perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. \n\nKey Highlights:\n- NExT-GPT connects an LLM with multimodal adaptors and diffusion decoders, enabling it to handle diverse modalities.\n- It leverages existing high-performance encoders and decoders, requiring only 1% parameter updates for effective semantic alignment.\n- The paper introduces \"modality-switching instruction tuning\" (MosIT) and a high-quality dataset to empower NExT-GPT with complex cross-modal understanding and generation.\n- Experiments show NExT-GPT achieves state-of-the-art or competitive performance on various text-to-X, X-to-text, and text-conditioned modal editing tasks.\n\nKeywords: multimodal language model, any-to-any generation, modality-switching instruction tuning, cross-modal understanding and generation\n\nExample Questions:\nQ: How does NExT-GPT's architecture differ from previous multimodal language models that only handle input-side multimodal understanding?\nA: NExT-GPT is designed as an end-to-end system that can not only perceive multimodal inputs but also generate outputs in arbitrary combinations of modalities, unlike previous MM-LLMs that were limited to input-side multimodal understanding.\n\nQ: What are the key techniques used in NExT-GPT to enable efficient training and expansion to more modalities?\nA: NExT-GPT leverages existing high-performance encoders and decoders, and only requires updating 1% of the parameters (the input/output projection layers) for effective semantic alignment, which benefits low-cost training and facilitates expansion to more modalities.\n\nQ: How does the \"modality-switching instruction tuning\" (MosIT) dataset and technique help improve NExT-GPT's cross-modal understanding and generation capabilities?\nA: The MosIT dataset and tuning process equip NExT-GPT with sophisticated cross-modal semantic understanding and content generation abilities by exposing it to complex, multi-turn dialogues involving diverse modality combinations and switches.\n\nQ: How could a highly capable any-to-any multimodal language model like NExT-GPT be applied in real-world scenarios to enhance human-AI interaction and collaboration? (no_answer)",
  "metadata": {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "abstract": "  While recently Multimodal Large Language Models (MM-LLMs) have made exciting\nstrides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities.\nAs we humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in\narbitrary combinations of text, images, videos, and audio. By leveraging the\nexisting well-trained highly-performing encoders and decoders, NExT-GPT is\ntuned with only a small amount of parameter (1%) of certain projection layers,\nwhich not only benefits low-cost training and also facilitates convenient\nexpansion to more potential modalities. Moreover, we introduce a\nmodality-switching instruction tuning (MosIT) and manually curate a\nhigh-quality dataset for MosIT, based on which NExT-GPT is empowered with\ncomplex cross-modal semantic understanding and content generation. Overall, our\nresearch showcases the promising possibility of building an AI agent capable of\nmodeling universal modalities, paving the way for more human-like AI research\nin the community. Project page: https://next-gpt.github.io/\n",
    "published": "2023-09-11T15:02:25Z"
  }
}