{
  "text": "Summary:\n\nThis paper introduces a novel approach to fine-tuning language models to be more factual, without relying on expensive human labeling. The key innovations are:\n\n1. Leveraging recent advances in automated factuality evaluation, including reference-based methods that measure consistency with external knowledge bases, as well as a novel reference-free approach that uses the model's own confidence as a proxy for truthfulness.\n\n2. Using these automated factuality scores to construct preference datasets, where responses with higher truthfulness scores are labeled as preferred. This preference data is then used to fine-tune the language model using the Direct Preference Optimization (DPO) algorithm.\n\nThe authors show that fine-tuning Llama-2 models with this factuality-aware preference learning significantly reduces the number of factual errors (hallucinations) in both biography generation and medical question-answering tasks, outperforming RLHF and decoding-based factuality improvement methods. Importantly, the reference-free confidence-based approach also provides strong factuality improvements, without requiring access to external knowledge sources.\n\nKeywords: factuality, language model fine-tuning, preference learning, automated factuality evaluation\n\nExample Questions:\nQ: How does the factuality tuning approach introduced in this paper differ from previous work on improving language model factuality?\nA: The key innovations are the use of automated factuality scoring, both reference-based and reference-free, to construct preference datasets for fine-tuning, rather than relying on expensive human labeling.\n\nQ: What are the main advantages of the reference-free, confidence-based factuality scoring approach compared to reference-based methods?\nA: The reference-free approach avoids the need for retrieving and aligning to external knowledge sources, making it more scalable and applicable in domains where high-quality reference texts are not available.\n\nQ: How well do the factuality improvements from this approach transfer to language models fine-tuned for open-ended dialogue, like Llama-2-Chat?\nA: The paper shows that factuality tuning can be composed with RLHF to further improve the factual accuracy of chat models, suggesting the techniques are complementary.\n\nQ: What are some potential future research directions building on this work to further improve language model factuality? (no_answer)",
  "metadata": {
    "title": "Fine-tuning Language Models for Factuality",
    "abstract": "  The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.\n",
    "published": "2023-11-14T18:59:15Z"
  }
}