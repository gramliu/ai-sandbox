{
  "text": "Summary:\n\nKey Findings:\n- Training large language models to predict multiple future tokens at once (multi-token prediction) results in higher sample efficiency and improved downstream capabilities compared to standard next-token prediction.\n- Multi-token prediction models consistently outperform next-token models on generative benchmarks like coding, solving 12% more problems on HumanEval and 17% more on MBPP.\n- Multi-token prediction improves the development of induction and algorithmic reasoning capabilities, especially for smaller model sizes.\n- Models trained with multi-token prediction can be up to 3 times faster at inference through self-speculative decoding.\n\nKeywords:\n- Multi-token prediction\n- Sample efficiency\n- Generative benchmarks\n- Induction capability\n- Algorithmic reasoning\n- Self-speculative decoding\n- Large language models\n\nExample Questions:\nQ: How does the performance of multi-token prediction models compare to next-token models on coding benchmarks like HumanEval and MBPP?\nA: Multi-token prediction models significantly outperform next-token models on these coding benchmarks, solving 12% more problems on HumanEval and 17% more on MBPP.\n\nQ: What are some of the key benefits of training language models with multi-token prediction losses?\nA: Key benefits include improved sample efficiency, better development of induction and algorithmic reasoning capabilities, and up to 3x faster inference speeds through self-speculative decoding.\n\nQ: How does multi-token prediction impact the model's ability to learn and utilize \"choice points\" in the text during training and generation?\nA: Multi-token prediction assigns higher implicit weights to \"choice points\" - tokens that are more consequential for the continuation of the text. This helps the model focus on making the right decisions at these critical junctures.\n\nQ: How could the ability to run highly capable language models like phi-3-mini directly on consumer devices impact the development and deployment of AI assistants in the future?\nA: The ability to run advanced language models on-device could enable the development of privacy-preserving mobile AI assistants, embedded NLP systems for sensitive domains, and democratization of powerful language AI capabilities to a wider range of developers and researchers.",
  "metadata": {
    "title": "Better &amp; Faster Large Language Models via Multi-token Prediction",
    "abstract": "  Large language models such as GPT and Llama are trained with a next-token\nprediction loss. In this work, we suggest that training language models to\npredict multiple future tokens at once results in higher sample efficiency.\nMore specifically, at each position in the training corpus, we ask the model to\npredict the following n tokens using n independent output heads, operating on\ntop of a shared model trunk. Considering multi-token prediction as an auxiliary\ntraining task, we measure improved downstream capabilities with no overhead in\ntraining time for both code and natural language models. The method is\nincreasingly useful for larger model sizes, and keeps its appeal when training\nfor multiple epochs. Gains are especially pronounced on generative benchmarks\nlike coding, where our models consistently outperform strong baselines by\nseveral percentage points. Our 13B parameter models solves 12 % more problems\non HumanEval and 17 % more on MBPP than comparable next-token models.\nExperiments on small algorithmic tasks demonstrate that multi-token prediction\nis favorable for the development of induction heads and algorithmic reasoning\ncapabilities. As an additional benefit, models trained with 4-token prediction\nare up to 3 times faster at inference, even with large batch sizes.\n",
    "published": "2024-04-30T17:33:57Z"
  }
}