{
  "text": "Summary:\n\nThis paper introduces OPRO (Optimization by PROmpting), a novel approach to leverage large language models (LLMs) as optimizers. The key idea is to describe the optimization task in natural language, and then have the LLM iteratively generate new solutions based on the prompt containing the previous solutions and their scores.\n\nThe paper first showcases OPRO on linear regression and traveling salesman problems, demonstrating that LLMs can optimize these tasks simply through prompting, sometimes matching or surpassing specialized algorithms. \n\nThe main application of OPRO is prompt optimization, where the goal is to find prompts that maximize the task accuracy on natural language benchmarks. The authors show that OPRO can significantly outperform human-designed prompts, improving accuracy by up to 8% on GSM8K and up to 50% on Big-Bench Hard tasks. This is achieved by having the LLM leverage the optimization trajectory, where past prompts and their scores are used to guide the generation of new, higher-performing prompts.\n\nThe paper also provides detailed ablation studies on the key components of the meta-prompt design, and analyzes the potential overfitting issue in prompt optimization. Comparisons to concurrent work on prompt optimization, such as EvoPrompt, are also presented.\n\nOverall, this work demonstrates the potential of using LLMs as general-purpose optimizers, beyond their traditional applications in language tasks. The ability to optimize prompts through natural language interaction opens up new possibilities for making advanced AI capabilities more accessible.\n\nKeywords: large language models, optimization, prompt engineering, prompt optimization\n\nExample Questions:\nQ: How does OPRO leverage the optimization trajectory to guide the LLM in generating better prompts over time?\nA: OPRO includes the past prompts and their scores in the meta-prompt, allowing the LLM to identify patterns in high-performing prompts and build upon them to generate new, improved prompts. This is in contrast to approaches that only consider a single prompt at a time.\n\nQ: What are some of the key design choices in the meta-prompt that were found to be important for the effectiveness of OPRO in prompt optimization?\nA: The ablation studies show that factors like the order of previous prompts, the inclusion of accuracy scores, and the number of exemplars in the meta-prompt can significantly impact the optimization performance. For example, presenting the prompts in ascending order of scores, and including the accuracy scores, were found to be important for the LLM to effectively exploit the optimization trajectory.\n\nQ: How does OPRO's approach to prompt optimization differ from concurrent work like EvoPrompt, and what are the advantages of OPRO's approach?\nA: Unlike EvoPrompt, which relies on explicit instructions for mutation and crossover of prompts, OPRO allows the LLM to directly generate new prompts based on the optimization trajectory. This gives the LLM more flexibility to discover patterns and generate prompts that improve upon the past ones, without being constrained by predefined operations. OPRO also provides richer information in the meta-prompt, such as exemplars and accuracy scores, which was shown to be beneficial for the LLM's optimization performance.",
  "metadata": {
    "title": "Large Language Models as Optimizers",
    "abstract": "  Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to our main application in prompt optimization,\nwhere the goal is to find instructions that maximize the task accuracy. With a\nvariety of LLMs, we demonstrate that the best prompts optimized by OPRO\noutperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on\nBig-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.\n",
    "published": "2023-09-07T00:07:15Z"
  }
}