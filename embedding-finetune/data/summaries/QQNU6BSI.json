{
  "text": "Summary:\n\nThe paper introduces StreamingLLM, an efficient framework that enables large language models (LLMs) trained with a finite attention window to perform stable and efficient language modeling on text of infinite length without fine-tuning. The key insights are:\n\n1. LLMs exhibit an \"attention sink\" phenomenon, where they disproportionately focus on the initial tokens in a sequence, even if those tokens are not semantically important. This causes the model's performance to collapse when the initial tokens are evicted from the attention cache.\n\n2. StreamingLLM addresses this by keeping a small number of initial tokens (e.g. 4) as \"attention sinks\" in the attention cache, along with the most recent tokens. This allows the model to maintain stable performance even on extremely long texts.\n\n3. The paper also shows that pre-training LLMs with a dedicated learnable \"sink token\" at the start of each sequence can further improve the model's streaming performance, eliminating the need for multiple initial tokens as attention sinks.\n\nKey results:\n- StreamingLLM enables Llama-2, MPT, Falcon, and Pythia models to reliably model up to 4 million tokens, outperforming baselines.\n- StreamingLLM achieves up to 22.2x speedup over the sliding window recomputation baseline.\n- Pre-training with a dedicated sink token improves streaming performance compared to vanilla models.\n\nThe paper highlights the importance of addressing the attention sink phenomenon to enable efficient deployment of LLMs in streaming applications like multi-round dialogue systems. The techniques proposed can be broadly applied to autoregressive Transformer-based models.\n\nExample Questions:\nQ: How does the attention sink phenomenon in LLMs lead to their performance collapse on long texts?\nA: LLMs disproportionately focus attention on the initial tokens in a sequence, even if those tokens are not semantically important. When the initial tokens are evicted from the attention cache as the text length exceeds the training window, a large portion of the denominator in the softmax attention computation is removed, causing a significant shift in the attention distribution and leading to performance collapse.\n\nQ: How does StreamingLLM address the attention sink issue to enable stable performance on long texts?\nA: StreamingLLM keeps a small number of initial tokens (e.g. 4) as \"attention sinks\" in the attention cache, along with the most recent tokens. This anchors the attention computation and prevents the performance collapse that occurs when the initial tokens are evicted in standard window attention approaches.\n\nQ: How can pre-training LLMs with a dedicated learnable \"sink token\" further improve their streaming performance?\nA: By including a learnable sink token at the start of each training sequence, the model learns to direct the unnecessary attention scores to this dedicated token, rather than inappropriately using the actual content tokens as attention sinks. This eliminates the need for multiple initial tokens as attention sinks during inference, further enhancing the model's streaming capabilities.",
  "metadata": {
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "abstract": "  Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na \"sink\" even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n",
    "published": "2023-09-29T17:59:56Z"
  }
}