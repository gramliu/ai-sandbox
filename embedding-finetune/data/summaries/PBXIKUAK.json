{
  "text": "Summary:\nThis technical report introduces phi-3-mini, a compact 3.8 billion parameter language model that achieves performance rivaling much larger models like Mixtral 8x7B and GPT-3.5 on academic benchmarks and internal testing, while being small enough to run locally on a modern smartphone. The key innovation is in the training data, which consists of heavily filtered web data and synthetic data, similar to the approach used for phi-2. The model is also aligned for robustness, safety, and chat format. Initial scaling results with 7B and 14B parameter models (phi-3-small and phi-3-medium) show significant further performance gains.\n\nDespite its small size, phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench. The model's main limitation is factual knowledge due to capacity constraints, but this can be mitigated by augmenting it with a search engine. Safety and responsibility were key focuses, with the model undergoing safety alignment, red-teaming, and automated testing. However, challenges remain around factual inaccuracies, bias, inappropriate content, and safety issues that still need to be fully addressed.\n\nKeywords: compact language models, filtered training data, on-device inference, model scaling, responsible AI\n\nExample Questions:\nQ: How does the performance of phi-3-mini compare to state-of-the-art large language models when evaluated on standard academic NLP benchmarks?\nQ: What techniques were used to ensure phi-3-mini behaves in a safe, robust, and responsible manner during open-ended interactions with humans?\nQ: How might the ability to run highly capable language models like phi-3-mini directly on consumer devices change the way AI assistants are developed and deployed in the future?\nQ: What are some potential beneficial applications of a model like phi-3-mini that can perform advanced language tasks while preserving user privacy by running fully on-device?",
  "metadata": {
    "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\n  Phone",
    "abstract": "  We introduce phi-3-mini, a 3.8 billion parameter language model trained on\n3.3 trillion tokens, whose overall performance, as measured by both academic\nbenchmarks and internal testing, rivals that of models such as Mixtral 8x7B and\nGPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite\nbeing small enough to be deployed on a phone. The innovation lies entirely in\nour dataset for training, a scaled-up version of the one used for phi-2,\ncomposed of heavily filtered web data and synthetic data. The model is also\nfurther aligned for robustness, safety, and chat format. We also provide some\ninitial parameter-scaling results with a 7B and 14B models trained for 4.8T\ntokens, called phi-3-small and phi-3-medium, both significantly more capable\nthan phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on\nMT-bench).\n",
    "published": "2024-04-22T14:32:33Z"
  }
}