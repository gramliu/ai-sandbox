{
  "input": "<reference id=\"8MF5D2QS\">\n<metadata>\n{\n  \"title\": \"Corrective Retrieval Augmented Generation\",\n  \"abstract\": \"  Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.\\n\",\n  \"published\": \"2024-01-29T04:36:39Z\"\n}\n</metadata>\n<text>\nHere is a summary of the key points from the paper:\n\nHighlights:\n- The paper introduces Corrective Retrieval Augmented Generation (CRAG), a method to improve the robustness of retrieval-augmented generation (RAG) approaches.\n- CRAG uses a lightweight retrieval evaluator to assess the quality of retrieved documents and trigger different knowledge retrieval actions (Correct, Incorrect, Ambiguous).\n- For Incorrect retrievals, CRAG leverages large-scale web searches to complement the limited corpus.\n- CRAG also uses a decompose-then-recompose algorithm to selectively focus on key information in retrieved documents.\n- Experiments show CRAG can significantly improve the performance of standard RAG and state-of-the-art Self-RAG across short-form and long-form generation tasks.\n\nKeywords:\n- Retrieval-augmented generation (RAG)\n- Hallucination\n- Retrieval evaluator\n- Knowledge refinement\n- Web search\n\nExample Questions:\nQ: How does the performance of phi-3-mini compare to state-of-the-art large language models when evaluated on standard academic NLP benchmarks?\nA: Phi-3-mini achieves performance rivaling much larger models like Mixtral 8x7B and GPT-3.5 on academic benchmarks, demonstrating the power of carefully curated training data to enable high quality in compact models.\n\nQ: What techniques were used to ensure phi-3-mini behaves in a safe, robust, and responsible manner during open-ended interactions with humans?\nA: The phi-3-mini model underwent an iterative process of safety alignment, red-teaming, and extensive automated testing to mitigate issues around factual inaccuracies, bias, inappropriate content, and safety, though challenges still remain in fully solving these problems.\n</text>\n</reference>",
  "output": "<summary>\nThe paper explores the field of enhancing the reliability and accuracy of text generation by large language models through the integration of external information retrieval systems. It addresses the issue of hallucinations in generated texts by evaluating the quality of retrieved documents and adjusting the retrieval process accordingly. The approach includes leveraging large-scale web searches to supplement limited corpora and employing algorithms to filter and focus on relevant information. This method aims to improve the robustness and performance of text generation tasks by ensuring that the retrieved information is accurate and relevant.\n</summary>"
}