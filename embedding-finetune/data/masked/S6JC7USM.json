{
  "input": "<reference id=\"S6JC7USM\">\n<metadata>\n{\n  \"title\": \"DSPy: Compiling Declarative Language Model Calls into Self-Improving\\n  Pipelines\",\n  \"abstract\": \"  The ML community is rapidly exploring techniques for prompting language\\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\\n\\\"prompt templates\\\", i.e. lengthy strings discovered via trial and error. Toward\\na more systematic approach for developing and optimizing LM pipelines, we\\nintroduce DSPy, a programming model that abstracts LM pipelines as text\\ntransformation graphs, i.e. imperative computational graphs where LMs are\\ninvoked through declarative modules. DSPy modules are parameterized, meaning\\nthey can learn (by creating and collecting demonstrations) how to apply\\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\\nmetric. We conduct two case studies, showing that succinct DSPy programs can\\nexpress and optimize sophisticated LM pipelines that reason about math word\\nproblems, tackle multi-hop retrieval, answer complex questions, and control\\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\\nprompting (generally by over 25% and 65%, respectively) and pipelines with\\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\\nof that, DSPy programs compiled to open and relatively small LMs like\\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\\nhttps://github.com/stanfordnlp/dspy\\n\",\n  \"published\": \"2023-10-05T17:37:25Z\"\n}\n</metadata>\n<text>\nThis paper introduces DSPy, a new programming model for designing AI systems using pipelines of pretrained language models (LMs) and other tools. The key contributions are:\n\n1. Signatures: DSPy abstracts LM prompting using natural language typed signatures, which specify the input/output behavior of a text transformation task rather than the specific prompting instructions.\n\n2. Modules: DSPy provides parameterized modules that encapsulate common prompting techniques (e.g., Chain of Thought, ReAct) in a generic way, allowing them to be composed into arbitrary pipelines.\n\n3. Teleprompters: DSPy introduces \"teleprompters\" - optimizers that can automatically generate effective prompts and demonstrations to improve the quality of DSPy programs, without relying on manual prompt engineering.\n\nThe paper demonstrates the effectiveness of this approach through two case studies: math word problems (GSM8K) and multi-hop question answering (HotPotQA). Key findings include:\n\n- DSPy programs can outperform systems using hand-crafted prompts, often by a large margin (25-65% on GSM8K), by automatically bootstrapping effective demonstrations.\n- DSPy allows smaller LMs like Llama2-13b-chat to be competitive with larger proprietary models like GPT-3.5 on these tasks.\n- The modular and optimizable nature of DSPy programs enables rapid exploration of complex pipelines that would be difficult to construct and tune manually.\n\nOverall, DSPy represents a shift away from artful prompt engineering towards a more systematic, programmable approach to leveraging LMs for complex tasks. The paper's evaluations suggest this programming model can significantly improve the quality and efficiency of LM-powered systems.\n\nKey questions that could be asked about the paper and how it could be applied:\n\nQ: How does the modular and optimizable nature of DSPy programs enable the rapid exploration of complex pipelines?\nA: DSPy's teleprompters can automatically optimize the prompts and demonstrations used by each module in a pipeline, allowing researchers to quickly iterate on and refine sophisticated multi-stage systems. This contrasts with manual prompt engineering, which becomes increasingly difficult as pipelines grow more complex.\n\nQ: What are some potential applications of a highly capable language model like phi-3-mini that can run locally on a mobile device?\nA: Potential applications include privacy-preserving mobile AI assistants, embedded NLP systems for domains like healthcare where privacy is critical, and democratizing advanced language AI capabilities to developers/researchers without access to massive computing resources.\n\nQ: How does DSPy's approach to prompting differ from existing libraries like LangChain and LlamaIndex, and what are the key advantages?\nA: Whereas LangChain and LlamaIndex focus on providing pre-packaged components and chains that rely on manual prompt engineering, DSPy introduces a more fundamental shift by translating prompting techniques into parameterized, modular components that can be automatically optimized. This reduces the brittleness and lack of scalability inherent in hand-crafted prompts.\n\nQ: How does the DSPy compiler's ability to bootstrap demonstrations from limited training data enable label-efficient pipeline development?\nA: DSPy's teleprompters can generate demonstrations for intermediate steps of a pipeline using only final output labels, without requiring annotations for each stage. This allows new pipelines to be rapidly constructed and optimized without the need for extensive data collection and labeling.\n</text>\n</reference>",
  "output": "<summary>The paper explores the field of machine learning, specifically focusing on the development and optimization of language model (LM) pipelines. It introduces a systematic approach to creating and refining these pipelines by abstracting them into text transformation graphs and using parameterized modules. The approach emphasizes the automation of prompt generation and optimization, moving away from manual prompt engineering. This method allows for the rapid and efficient construction of complex LM-powered systems, improving their performance and scalability.</summary>"
}