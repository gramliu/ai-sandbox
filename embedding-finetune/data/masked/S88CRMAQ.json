{
  "input": "<reference id=\"S88CRMAQ\">\n<metadata>\n{\n  \"title\": \"Linearizing Large Language Models\",\n  \"abstract\": \"  Linear transformers have emerged as a subquadratic-time alternative to\\nsoftmax attention and have garnered significant interest due to their\\nfixed-size recurrent state that lowers inference cost. However, their original\\nformulation suffers from poor scaling and underperforms compute-matched\\ntransformers. Recent linear models such as RWKV and Mamba have attempted to\\naddress these shortcomings by proposing novel time-mixing and gating\\narchitectures, but pre-training large language models requires significant data\\nand compute investments. Thus, the search for subquadratic architectures is\\nlimited by the availability of compute and quality pre-training datasets. As a\\ncost-effective alternative to pre-training linear transformers, we propose\\nScalable UPtraining for Recurrent Attention (SUPRA). We present a method to\\nuptrain existing large pre-trained transformers into Recurrent Neural Networks\\n(RNNs) with a modest compute budget. This allows us to leverage the strong\\npre-training data and performance of existing transformer LLMs, while requiring\\n5% of the training cost. We find that our linearization technique leads to\\ncompetitive performance on standard benchmarks, but we identify persistent\\nin-context learning and long-context modeling shortfalls for even the largest\\nlinear models. Our code and models can be found at\\nhttps://github.com/TRI-ML/linear_open_lm.\\n\",\n  \"published\": \"2024-05-10T17:59:08Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces Scalable UPtraining for Recurrent Attention (SUPRA), a method for converting large pre-trained transformer language models into efficient recurrent neural networks (RNNs). The key contributions are:\n\n1. SUPRA allows leveraging the strong pre-training of existing large language models (e.g. Mistral, Llama2) to create competitive RNN models, requiring only 5% of the original pre-training compute.\n\n2. The SUPRA models achieve performance competitive with state-of-the-art pre-trained linear transformer models like RWKV and Mamba on standard NLP benchmarks, while benefiting from the efficiency of RNN inference.\n\n3. However, the paper identifies persistent limitations of the linearized RNN models compared to the original transformers, particularly on tasks requiring in-context learning and long-context understanding. Transformers can be made to scale to long contexts through techniques like rotary positional embeddings.\n\n4. The authors show their SUPRA uptraining approach does not simply approximate the softmax attention of the original transformers, but replaces it with a novel linear attention mechanism.\n\nKeywords: Linearized language models, recurrent neural networks, efficient inference, transformer uptraining\n\nExample Questions:\nQ: How does the performance of SUPRA-converted RNN models compare to state-of-the-art linear transformer models like RWKV and Mamba on standard NLP benchmarks?\nA: The SUPRA models are competitive with the best linear transformer models on standard benchmarks like HellaSwag, PIQA, and ARC, despite requiring only 5% of the original pre-training compute.\n\nQ: What are some of the key limitations of the SUPRA RNN models compared to the original transformer models they are derived from?\nA: The SUPRA RNN models struggle on tasks requiring in-context learning (e.g. MMLU) and long-context understanding (e.g. Qasper, NarrativeQA), showing a persistent gap compared to the original transformers. Techniques like rotary positional embeddings allow transformers to better scale to long contexts.\n\nQ: How does the SUPRA uptraining approach differ from simply approximating the softmax attention of the original transformer models?\nA: The authors show that the SUPRA models do not actually approximate the softmax attention of the original transformers. Instead, SUPRA replaces the softmax attention with a novel linear attention mechanism, indicating the uptraining process fundamentally changes the model's attention computation.\n</text>\n</reference>",
  "output": "<summary>\nThe paper explores the field of optimizing large language models by transforming them into more efficient recurrent neural networks (RNNs). It focuses on methods to leverage the pre-training of existing transformer models to create competitive RNNs with significantly reduced computational costs. The research addresses the challenges of maintaining performance while improving efficiency, particularly in tasks requiring long-context understanding and in-context learning. The proposed approach involves replacing traditional attention mechanisms with more efficient linear attention methods.\n</summary>"
}