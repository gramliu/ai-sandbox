{
  "input": "<reference id=\"DQD7MARR\">\n<metadata>\n{\n  \"title\": \"Training Compute-Optimal Large Language Models\",\n  \"abstract\": \"  We investigate the optimal model size and number of tokens for training a\\ntransformer language model under a given compute budget. We find that current\\nlarge language models are significantly undertrained, a consequence of the\\nrecent focus on scaling language models whilst keeping the amount of training\\ndata constant. By training over 400 language models ranging from 70 million to\\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\\ncompute-optimal training, the model size and the number of training tokens\\nshould be scaled equally: for every doubling of model size the number of\\ntraining tokens should also be doubled. We test this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\\nas Gopher but with 70B parameters and 4$\\\\times$ more more data. Chinchilla\\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\\nevaluation tasks. This also means that Chinchilla uses substantially less\\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\\non the MMLU benchmark, greater than a 7% improvement over Gopher.\\n\",\n  \"published\": \"2022-03-29T13:38:03Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper investigates the optimal trade-off between model size and the number of training tokens for training large language models under a fixed compute budget. The key findings are:\n\n- Current large language models are significantly undertrained, as the focus has been on scaling model size while keeping the amount of training data constant.\n\n- The authors find that for compute-optimal training, the model size and the number of training tokens should be scaled equally - for every doubling of model size, the number of training tokens should also be doubled.\n\n- The authors train a 70B parameter model called \"Chinchilla\" that uses the same compute budget as the 280B parameter Gopher model, but with 4x more training data. Chinchilla significantly outperforms Gopher and other large models across a wide range of benchmarks.\n\n- The authors provide detailed scaling laws that predict the optimal model size and number of training tokens for a given compute budget. This suggests that current large models are overparameterized compared to what would be optimal.\n\n- The results highlight the importance of dataset scaling in addition to model scaling, and raise concerns about the potential biases and privacy issues that can arise from training on ever-larger web-scraped datasets.\n\nKeywords: large language models, model scaling, dataset scaling, compute-optimal training, Chinchilla\n\nExample Questions:\n1. How does the performance of the 70B parameter Chinchilla model compare to the 280B parameter Gopher model across different benchmarks?\n2. What are the key insights from the authors' analysis on the optimal trade-off between model size and training data size for a fixed compute budget?\n3. What are some of the potential risks and challenges associated with training large language models on ever-expanding web-scraped datasets?\n4. How could the ability to run highly capable language models like Chinchilla directly on consumer devices change the development and deployment of AI assistants in the future?\n5. What are some potential beneficial applications of compact yet capable language models like Chinchilla that can run locally on user devices?\n</text>\n</reference>",
  "output": "<summary>\nThe paper explores the field of optimizing the training of large language models by balancing model size and the amount of training data within a fixed computational budget. It highlights that current models are often undertrained due to a disproportionate focus on increasing model size without a corresponding increase in training data. The research suggests that for optimal performance, both model size and training data should be scaled equally. This approach not only improves model performance but also reduces computational requirements for fine-tuning and inference, making the models more efficient and practical for downstream applications.\n</summary>"
}