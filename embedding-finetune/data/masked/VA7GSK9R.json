{
  "input": "<reference id=\"VA7GSK9R\">\n<metadata>\n{\n  \"title\": \"Chameleon: Mixed-Modal Early-Fusion Foundation Models\",\n  \"abstract\": \"  We present Chameleon, a family of early-fusion token-based mixed-modal models\\ncapable of understanding and generating images and text in any arbitrary\\nsequence. We outline a stable training approach from inception, an alignment\\nrecipe, and an architectural parameterization tailored for the early-fusion,\\ntoken-based, mixed-modal setting. The models are evaluated on a comprehensive\\nrange of tasks, including visual question answering, image captioning, text\\ngeneration, image generation, and long-form mixed modal generation. Chameleon\\ndemonstrates broad and general capabilities, including state-of-the-art\\nperformance in image captioning tasks, outperforms Llama-2 in text-only tasks\\nwhile being competitive with models such as Mixtral 8x7B and Gemini-Pro, and\\nperforms non-trivial image generation, all in a single model. It also matches\\nor exceeds the performance of much larger models, including Gemini Pro and\\nGPT-4V, according to human judgments on a new long-form mixed-modal generation\\nevaluation, where either the prompt or outputs contain mixed sequences of both\\nimages and text. Chameleon marks a significant step forward in a unified\\nmodeling of full multimodal documents.\\n\",\n  \"published\": \"2024-05-16T05:23:41Z\"\n}\n</metadata>\n<text>\nSummary:\n\nChameleon is a family of early-fusion, token-based mixed-modal models that can understand and generate interleaved sequences of images and text. The key innovation is the use of a unified token-based representation for both modalities, allowing seamless reasoning and generation across images and text.\n\nThe Chameleon models demonstrate broad and general capabilities, achieving state-of-the-art performance on image captioning and visual question answering tasks, while also maintaining competitive performance on text-only benchmarks like commonsense reasoning and math problem solving. Importantly, Chameleon also unlocks new capabilities in mixed-modal reasoning and generation, outperforming strong baselines like Gemini Pro and GPT-4V in human evaluations on open-ended prompts containing both images and text.\n\nThe technical innovations that enable Chameleon's performance include architectural modifications to the transformer, such as query-key normalization and revised layer norm placement, as well as specialized training techniques to ensure stable and scalable training of early-fusion token-based models.\n\nKeywords: mixed-modal models, early-fusion, token-based representations, multimodal reasoning, image captioning, visual question answering\n\nExample Questions:\nQ: How does the performance of Chameleon compare to state-of-the-art large language models on standard text-only NLP benchmarks?\nA: Chameleon outperforms its Llama-2 counterparts across a range of text-only tasks, including commonsense reasoning, reading comprehension, math problem solving, and world knowledge. It approaches the performance of larger models like Mistral 7B and Mixtral 8x7B on many of these benchmarks.\n\nQ: What key architectural and training innovations were required to enable Chameleon's stable and scalable training as a mixed-modal early-fusion model?\nA: Chameleon introduced several critical innovations, including query-key normalization to control norm growth in the attention mechanism, revised layer norm placement, and specialized regularization techniques like z-loss to address logit drift. These were crucial for achieving stable training, especially when scaling Chameleon to larger model sizes.\n\nQ: How did Chameleon perform in human evaluations on open-ended mixed-modal prompts compared to strong baselines like Gemini Pro and GPT-4V?\nA: In carefully designed human evaluation experiments, Chameleon substantially outperformed Gemini Pro and GPT-4V on mixed-modal prompts, achieving a 60.4% preference rate over Gemini Pro and a 51.6% preference rate over GPT-4V. This demonstrates Chameleon's unique capabilities in mixed-modal reasoning and generation.\n</text>\n</reference>",
  "output": "<summary> \nThe paper explores the field of mixed-modal models that integrate and process both images and text within a unified framework. It focuses on early-fusion techniques, where a single token-based representation is used for both modalities, enabling seamless reasoning and generation across them. The research emphasizes architectural modifications and specialized training methods to ensure stable and scalable performance. The models are evaluated on a variety of tasks, demonstrating state-of-the-art capabilities in both image and text processing, as well as in mixed-modal reasoning and generation.\n</summary>"
}