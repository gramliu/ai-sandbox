{
  "input": "<reference id=\"Q8QXPA9K\">\n<metadata>\n{\n  \"title\": \"Vision Transformers Need Registers\",\n  \"abstract\": \"  Transformers have recently emerged as a powerful tool for learning visual\\nrepresentations. In this paper, we identify and characterize artifacts in\\nfeature maps of both supervised and self-supervised ViT networks. The artifacts\\ncorrespond to high-norm tokens appearing during inference primarily in\\nlow-informative background areas of images, that are repurposed for internal\\ncomputations. We propose a simple yet effective solution based on providing\\nadditional tokens to the input sequence of the Vision Transformer to fill that\\nrole. We show that this solution fixes that problem entirely for both\\nsupervised and self-supervised models, sets a new state of the art for\\nself-supervised visual models on dense visual prediction tasks, enables object\\ndiscovery methods with larger models, and most importantly leads to smoother\\nfeature maps and attention maps for downstream visual processing.\\n\",\n  \"published\": \"2023-09-28T16:45:46Z\"\n}\n</metadata>\n<text>\nHere is a summary of the key points from the paper:\n\n**Key Findings:**\n- Vision Transformers (ViTs) exhibit artifacts in their feature maps and attention maps, in the form of high-norm \"outlier\" tokens that appear primarily in low-informative background areas.\n- These outlier tokens are repurposed by the model to aggregate global image information, while discarding local spatial information.\n- The outlier tokens appear in the middle layers of sufficiently large and trained ViT models, and are not present in smaller or earlier-stage models.\n- The outlier tokens contain less information about their original patch position and pixel values, but more global information about the image.\n\n**Proposed Solution:**\n- The authors propose adding additional \"register\" tokens to the input sequence of the ViT, which the model can use to store and retrieve global information, without corrupting the local patch features.\n- This simple architectural change completely removes the outlier tokens, leading to smoother feature maps and attention maps.\n- The models with registers also achieve improved performance on dense prediction tasks and enable better object discovery methods.\n\n**Keywords:**\n- Vision Transformers\n- Feature map artifacts\n- Attention map artifacts\n- Register tokens\n- Global vs local information\n- Object discovery\n\n**Example Questions:**\nQ: How do the authors detect and characterize the artifacts present in Vision Transformer feature maps?\nA: The authors observe that the artifacts correspond to tokens with much higher norm values at the output of the ViT model. They analyze the properties of these high-norm \"outlier\" tokens and find that they contain less local information about their patch position and pixels, but more global information about the image.\n\nQ: What is the key insight behind the authors' proposed solution of adding \"register\" tokens to the ViT input?\nA: The authors hypothesize that the ViT model naturally learns to repurpose some tokens to store and retrieve global image information, at the expense of discarding local spatial information in those tokens. By explicitly providing additional \"register\" tokens, the model can isolate this global information aggregation behavior, leading to smoother feature maps without artifacts.\n\nQ: How does the addition of register tokens impact the performance of ViT models on dense prediction tasks and unsupervised object discovery?\nA: The authors show that adding register tokens improves the performance of ViT models on dense prediction tasks like semantic segmentation and monocular depth estimation. It also enables much better results on unsupervised object discovery methods like LOST, which rely on the smoothness of the ViT feature maps.\n</text>\n</reference>",
  "output": "<summary>\nThe paper explores the field of Vision Transformers (ViTs) and their application in visual representation learning. It identifies issues with artifacts in feature and attention maps, which affect the model's ability to process local and global information effectively. The authors propose a method to mitigate these artifacts by modifying the input sequence, leading to improved performance in dense visual prediction tasks and object discovery. This research contributes to enhancing the efficiency and accuracy of ViTs in visual processing tasks.\n</summary>"
}