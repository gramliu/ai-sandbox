{
  "input": "<reference id=\"QQNU6BSI\">\n<metadata>\n{\n  \"title\": \"Efficient Streaming Language Models with Attention Sinks\",\n  \"abstract\": \"  Deploying Large Language Models (LLMs) in streaming applications such as\\nmulti-round dialogue, where long interactions are expected, is urgently needed\\nbut poses two major challenges. Firstly, during the decoding stage, caching\\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\\npopular LLMs cannot generalize to longer texts than the training sequence\\nlength. Window attention, where only the most recent KVs are cached, is a\\nnatural approach -- but we show that it fails when the text length surpasses\\nthe cache size. We observe an interesting phenomenon, namely attention sink,\\nthat keeping the KV of initial tokens will largely recover the performance of\\nwindow attention. In this paper, we first demonstrate that the emergence of\\nattention sink is due to the strong attention scores towards initial tokens as\\na \\\"sink\\\" even if they are not semantically important. Based on the above\\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\\ntrained with a finite length attention window to generalize to infinite\\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\\nmodeling with up to 4 million tokens and more. In addition, we discover that\\nadding a placeholder token as a dedicated attention sink during pre-training\\ncan further improve streaming deployment. In streaming settings, StreamingLLM\\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\\n\",\n  \"published\": \"2023-09-29T17:59:56Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThe paper introduces StreamingLLM, an efficient framework that enables large language models (LLMs) trained with a finite attention window to perform stable and efficient language modeling on text of infinite length without fine-tuning. The key insights are:\n\n1. LLMs exhibit an \"attention sink\" phenomenon, where they disproportionately focus on the initial tokens in a sequence, even if those tokens are not semantically important. This causes the model's performance to collapse when the initial tokens are evicted from the attention cache.\n\n2. StreamingLLM addresses this by keeping a small number of initial tokens (e.g. 4) as \"attention sinks\" in the attention cache, along with the most recent tokens. This allows the model to maintain stable performance even on extremely long texts.\n\n3. The paper also shows that pre-training LLMs with a dedicated learnable \"sink token\" at the start of each sequence can further improve the model's streaming performance, eliminating the need for multiple initial tokens as attention sinks.\n\nKey results:\n- StreamingLLM enables Llama-2, MPT, Falcon, and Pythia models to reliably model up to 4 million tokens, outperforming baselines.\n- StreamingLLM achieves up to 22.2x speedup over the sliding window recomputation baseline.\n- Pre-training with a dedicated sink token improves streaming performance compared to vanilla models.\n\nThe paper highlights the importance of addressing the attention sink phenomenon to enable efficient deployment of LLMs in streaming applications like multi-round dialogue systems. The techniques proposed can be broadly applied to autoregressive Transformer-based models.\n\nExample Questions:\nQ: How does the attention sink phenomenon in LLMs lead to their performance collapse on long texts?\nA: LLMs disproportionately focus attention on the initial tokens in a sequence, even if those tokens are not semantically important. When the initial tokens are evicted from the attention cache as the text length exceeds the training window, a large portion of the denominator in the softmax attention computation is removed, causing a significant shift in the attention distribution and leading to performance collapse.\n\nQ: How does StreamingLLM address the attention sink issue to enable stable performance on long texts?\nA: StreamingLLM keeps a small number of initial tokens (e.g. 4) as \"attention sinks\" in the attention cache, along with the most recent tokens. This anchors the attention computation and prevents the performance collapse that occurs when the initial tokens are evicted in standard window attention approaches.\n\nQ: How can pre-training LLMs with a dedicated learnable \"sink token\" further improve their streaming performance?\nA: By including a learnable sink token at the start of each training sequence, the model learns to direct the unnecessary attention scores to this dedicated token, rather than inappropriately using the actual content tokens as attention sinks. This eliminates the need for multiple initial tokens as attention sinks during inference, further enhancing the model's streaming capabilities.\n</text>\n</reference>",
  "output": "<note>\nToday was an exciting day as I delved deeper into the research on efficient streaming language models with attention sinks. This is a critical problem to solve, especially as we look to deploy large language models (LLMs) in real-world applications like multi-round dialogue systems.\n\nI started the day by reviewing the key insights from the paper. The attention sink phenomenon, where LLMs disproportionately focus on the initial tokens in a sequence, is a fascinating discovery. It explains why these models struggle to maintain performance as the text length exceeds the training window size. The StreamingLLM framework's approach of keeping a small number of initial tokens as \"attention sinks\" is a clever solution to this problem.\n\nDuring our team meeting, we discussed how we could integrate the StreamingLLM techniques into our current dialogue agent. One of the main challenges we've faced is the agent's inability to maintain context and coherence over long conversations. By enabling our LLM to reliably handle text of infinite length, StreamingLLM could be a game-changer.\n\nIn the afternoon, I spent some time setting up the StreamingLLM codebase and running experiments on our in-house LLM models, including Llama-2 and Pythia. The results were quite impressive â€“ we saw up to a 22.2x speedup over the sliding window recomputation baseline, and the models were able to maintain stable performance on text up to 4 million tokens long.\n\nI was particularly intrigued by the idea of pre-training the LLMs with a dedicated learnable \"sink token.\" This seems like a clever way to further enhance the streaming capabilities without the need for multiple initial tokens as attention sinks. I'm excited to try this approach and see how it impacts the performance of our dialogue agent.\n\nAs I wrapped up the day, I couldn't help but think about the broader implications of this research. Efficient streaming LLMs could unlock a whole new world of possibilities for AI assistants and other real-time language applications. The ability to maintain context and coherence over long interactions is crucial for building truly intelligent and helpful systems.\n\nI'm looking forward to continuing this work and exploring ways to push the boundaries of what's possible with large language models. It's an exciting time to be working in this field, and I can't wait to see what the future holds.\n\n</note>"
}