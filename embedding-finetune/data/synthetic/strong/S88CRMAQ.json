{
  "input": "<reference id=\"S88CRMAQ\">\n<metadata>\n{\n  \"title\": \"Linearizing Large Language Models\",\n  \"abstract\": \"  Linear transformers have emerged as a subquadratic-time alternative to\\nsoftmax attention and have garnered significant interest due to their\\nfixed-size recurrent state that lowers inference cost. However, their original\\nformulation suffers from poor scaling and underperforms compute-matched\\ntransformers. Recent linear models such as RWKV and Mamba have attempted to\\naddress these shortcomings by proposing novel time-mixing and gating\\narchitectures, but pre-training large language models requires significant data\\nand compute investments. Thus, the search for subquadratic architectures is\\nlimited by the availability of compute and quality pre-training datasets. As a\\ncost-effective alternative to pre-training linear transformers, we propose\\nScalable UPtraining for Recurrent Attention (SUPRA). We present a method to\\nuptrain existing large pre-trained transformers into Recurrent Neural Networks\\n(RNNs) with a modest compute budget. This allows us to leverage the strong\\npre-training data and performance of existing transformer LLMs, while requiring\\n5% of the training cost. We find that our linearization technique leads to\\ncompetitive performance on standard benchmarks, but we identify persistent\\nin-context learning and long-context modeling shortfalls for even the largest\\nlinear models. Our code and models can be found at\\nhttps://github.com/TRI-ML/linear_open_lm.\\n\",\n  \"published\": \"2024-05-10T17:59:08Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces Scalable UPtraining for Recurrent Attention (SUPRA), a method for converting large pre-trained transformer language models into efficient recurrent neural networks (RNNs). The key contributions are:\n\n1. SUPRA allows leveraging the strong pre-training of existing large language models (e.g. Mistral, Llama2) to create competitive RNN models, requiring only 5% of the original pre-training compute.\n\n2. The SUPRA models achieve performance competitive with state-of-the-art pre-trained linear transformer models like RWKV and Mamba on standard NLP benchmarks, while benefiting from the efficiency of RNN inference.\n\n3. However, the paper identifies persistent limitations of the linearized RNN models compared to the original transformers, particularly on tasks requiring in-context learning and long-context understanding. Transformers can be made to scale to long contexts through techniques like rotary positional embeddings.\n\n4. The authors show their SUPRA uptraining approach does not simply approximate the softmax attention of the original transformers, but replaces it with a novel linear attention mechanism.\n\nKeywords: Linearized language models, recurrent neural networks, efficient inference, transformer uptraining\n\nExample Questions:\nQ: How does the performance of SUPRA-converted RNN models compare to state-of-the-art linear transformer models like RWKV and Mamba on standard NLP benchmarks?\nA: The SUPRA models are competitive with the best linear transformer models on standard benchmarks like HellaSwag, PIQA, and ARC, despite requiring only 5% of the original pre-training compute.\n\nQ: What are some of the key limitations of the SUPRA RNN models compared to the original transformer models they are derived from?\nA: The SUPRA RNN models struggle on tasks requiring in-context learning (e.g. MMLU) and long-context understanding (e.g. Qasper, NarrativeQA), showing a persistent gap compared to the original transformers. Techniques like rotary positional embeddings allow transformers to better scale to long contexts.\n\nQ: How does the SUPRA uptraining approach differ from simply approximating the softmax attention of the original transformer models?\nA: The authors show that the SUPRA models do not actually approximate the softmax attention of the original transformers. Instead, SUPRA replaces the softmax attention with a novel linear attention mechanism, indicating the uptraining process fundamentally changes the model's attention computation.\n</text>\n</reference>",
  "output": "<note>\nToday was an exciting day as I delved into the latest research on linearizing large language models. The paper on Scalable UPtraining for Recurrent Attention (SUPRA) caught my eye, as it presents a novel approach to converting pre-trained transformer models into efficient recurrent neural networks (RNNs).\n\nI spent the morning reviewing the key ideas behind SUPRA. The ability to leverage the strong pre-training of existing large language models, like Mistral and Llama2, while only requiring 5% of the original compute, is a game-changer. This cost-effective approach to creating competitive linear models is exactly the kind of innovation we need to democratize powerful language AI capabilities.\n\nAfter the team meeting, I started experimenting with the SUPRA uptraining process on some of our in-house transformer models. The results were quite promising - the SUPRA-converted RNN models were able to match the performance of state-of-the-art linear transformer models like RWKV and Mamba on standard NLP benchmarks. This is a significant achievement, considering the efficiency gains from the RNN architecture.\n\nHowever, as the paper pointed out, the SUPRA models do have some persistent limitations compared to the original transformers. Tasks requiring in-context learning and long-context understanding seem to be particularly challenging for the linearized RNN models. I made a note to investigate techniques like rotary positional embeddings, which could help the SUPRA models better scale to long-range dependencies.\n\nOne interesting finding from the paper was that the SUPRA uptraining process doesn't simply approximate the softmax attention of the original transformers. Instead, it replaces the attention mechanism with a novel linear approach. I'm curious to dive deeper into the details of this new attention formulation and understand how it differs from the original transformer attention.\n\nIn the afternoon, I started planning a series of experiments to further evaluate the SUPRA models on our internal web-based task environment, WebArena. The ability to run these efficient RNN models on resource-constrained devices could be a game-changer for our privacy-preserving mobile AI assistant project. I'm eager to see how the SUPRA models perform on the diverse, long-horizon tasks in WebArena.\n\nAs I wrapped up the day, I couldn't help but feel excited about the potential of this research. Linearizing large language models while preserving their performance is a crucial step towards making powerful AI capabilities more accessible and deployable. I'm looking forward to continuing my exploration of SUPRA and seeing how we can integrate these advancements into our own projects.\n\n</note>"
}