{
  "input": "<reference id=\"X36D5P8H\">\n<metadata>\n{\n  \"title\": \"Emu: Enhancing Image Generation Models Using Photogenic Needles in a\\n  Haystack\",\n  \"abstract\": \"  Training text-to-image models with web scale image-text pairs enables the\\ngeneration of a wide range of visual concepts from text. However, these\\npre-trained models often face challenges when it comes to generating highly\\naesthetic images. This creates the need for aesthetic alignment post\\npre-training. In this paper, we propose quality-tuning to effectively guide a\\npre-trained model to exclusively generate highly visually appealing images,\\nwhile maintaining generality across visual concepts. Our key insight is that\\nsupervised fine-tuning with a set of surprisingly small but extremely visually\\nappealing images can significantly improve the generation quality. We pre-train\\na latent diffusion model on $1.1$ billion image-text pairs and fine-tune it\\nwith only a few thousand carefully selected high-quality images. The resulting\\nmodel, Emu, achieves a win rate of $82.9\\\\%$ compared with its pre-trained only\\ncounterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred\\n$68.4\\\\%$ and $71.3\\\\%$ of the time on visual appeal on the standard PartiPrompts\\nand our Open User Input benchmark based on the real-world usage of\\ntext-to-image models. In addition, we show that quality-tuning is a generic\\napproach that is also effective for other architectures, including pixel\\ndiffusion and masked generative transformer models.\\n\",\n  \"published\": \"2023-09-27T17:30:19Z\"\n}\n</metadata>\n<text>\nThis paper introduces Emu, a quality-tuned latent diffusion model for high-quality text-to-image generation. The key insights are:\n\n1. Quality-tuning, which involves fine-tuning a pre-trained model on a small set of carefully curated high-quality images, can significantly improve the visual appeal of generated images without compromising the generality of visual concepts.\n\n2. The quality of the fine-tuning dataset is much more important than the quantity. The authors show that as little as 100-2000 carefully selected high-quality images can lead to substantial improvements in visual appeal compared to the pre-trained model.\n\n3. Quality-tuning is a generic approach that can be applied to improve the performance of various text-to-image model architectures, including latent diffusion, pixel diffusion, and masked generative transformer models.\n\n4. Compared to the state-of-the-art SDXLv1.0 model, the quality-tuned Emu model is preferred 68.4% and 71.3% of the time on visual appeal on the PartiPrompts and Open User Input benchmark, respectively.\n\nKey contributions:\n- Introduction of the Emu model, a quality-tuned latent diffusion model that outperforms state-of-the-art on visual appeal.\n- Insights on the importance of high-quality fine-tuning data for aesthetic alignment of text-to-image models.\n- Demonstration that quality-tuning is a generic approach applicable to various model architectures.\n\nExample questions:\nQ: How does the visual appeal of images generated by Emu compare to the pre-trained model and the state-of-the-art SDXLv1.0 model?\nA: Emu significantly outperforms both the pre-trained model and SDXLv1.0 on visual appeal, being preferred 82.9% and 68.4% of the time respectively on the evaluation prompts.\n\nQ: What is the key insight behind the quality-tuning approach used to train Emu?\nA: The key insight is that a surprisingly small number (100-2000) of carefully curated high-quality images can have a significant impact on improving the visual appeal of generated images, without compromising the generality of visual concepts the model can depict.\n\nQ: How does the quality-tuning approach generalize beyond latent diffusion models?\nA: The authors show that the quality-tuning approach is generic and can also improve the performance of pixel diffusion and masked generative transformer models on both visual appeal and text faithfulness.\n</text>\n</reference>",
  "output": "<note>\nIt's been an interesting day at work today. I spent a good chunk of the morning experimenting with some new techniques for improving the visual quality of our text-to-image generation models. We've been using a pre-trained latent diffusion model as the backbone, but the results haven't been as aesthetically pleasing as we'd like.\n\nThat's where this new \"quality-tuning\" approach comes in. The idea is to fine-tune the pre-trained model on a carefully curated set of high-quality images, rather than just relying on the original training data. It's amazing how just a few thousand carefully selected images can make such a big difference in the visual appeal of the generated outputs. \n\nI've been playing around with this on our internal benchmarks, and the results are really promising. Compared to the pre-trained model, our quality-tuned version is preferred over 80% of the time on visual appeal. And when we stack it up against the current state-of-the-art, it's still coming out on top by a significant margin.\n\nWhat's even more exciting is that this quality-tuning approach seems to be pretty generic. We've tried applying it to other model architectures like pixel diffusion and masked generative transformers, and it works just as well. It's like finding a few \"photogenic needles in a haystack\" that can really elevate the overall performance.\n\nOf course, there's still a lot of work to be done. We need to figure out the optimal size and composition of the fine-tuning dataset, and explore ways to make the process more efficient and scalable. But I'm really encouraged by the progress we're seeing. It feels like we're inching closer to that elusive goal of generating truly stunning, high-quality images from text.\n\nIn the meantime, I'm going to keep tinkering and experimenting. Who knows what other little tricks and techniques we might uncover along the way? It's all part of the fun of working in this rapidly evolving field of AI. Can't wait to see what tomorrow brings!\n</note>"
}