{
  "input": "<reference id=\"6VZ3IKHM\">\n<metadata>\n{\n  \"title\": \"Many-Shot In-Context Learning in Multimodal Foundation Models\",\n  \"abstract\": \"  Large language models are well-known to be effective at few-shot in-context\\nlearning (ICL). Recent advancements in multimodal foundation models have\\nenabled unprecedentedly long context windows, presenting an opportunity to\\nexplore their capability to perform ICL with many more demonstrating examples.\\nIn this work, we evaluate the performance of multimodal foundation models\\nscaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro\\nacross 10 datasets spanning multiple domains (natural imagery, medical imagery,\\nremote sensing, and molecular imagery) and tasks (multi-class, multi-label, and\\nfine-grained classification). We observe that many-shot ICL, including up to\\nalmost 2,000 multimodal demonstrating examples, leads to substantial\\nimprovements compared to few-shot (&lt;100 examples) ICL across all of the\\ndatasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly\\nup to the maximum number of tested examples on many datasets. Given the high\\ninference costs associated with the long prompts required for many-shot ICL, we\\nalso explore the impact of batching multiple queries in a single API call. We\\nshow that batching up to 50 queries can lead to performance improvements under\\nzero-shot and many-shot ICL, with substantial gains in the zero-shot setting on\\nmultiple datasets, while drastically reducing per-query cost and latency.\\nFinally, we measure ICL data efficiency of the models, or the rate at which the\\nmodels learn from more demonstrating examples. We find that while GPT-4o and\\nGemini 1.5 Pro achieve similar zero-shot performance across the datasets,\\nGemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most\\ndatasets. Our results suggest that many-shot ICL could enable users to\\nefficiently adapt multimodal foundation models to new applications and domains.\\nOur codebase is publicly available at\\nhttps://github.com/stanfordmlgroup/ManyICL .\\n\",\n  \"published\": \"2024-05-16T04:02:43Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper evaluates the performance of state-of-the-art multimodal foundation models, GPT-4o and Gemini 1.5 Pro, on in-context learning (ICL) with a large number of demonstrating examples (many-shot ICL). Key findings:\n\n- Providing multimodal models with many demonstrating examples (up to ~2,000) leads to substantial performance improvements compared to few-shot (<100 examples) ICL across 10 datasets spanning multiple domains and tasks.\n\n- Gemini 1.5 Pro generally exhibits log-linear performance improvements as the number of demonstrating examples increases, while GPT-4o shows less stable improvements.\n\n- Gemini 1.5 Pro demonstrates higher ICL data efficiency than GPT-4o on most datasets, meaning it learns more from each additional demonstrating example.\n\n- Batching multiple queries in a single request can achieve similar or better performance than single query requests in a many-shot setting, while drastically reducing per-query latency and cost.\n\n- Batching queries can also lead to substantial performance improvements in the zero-shot setting, due to benefits from domain calibration, class calibration, and self-generated demonstrations.\n\nKeywords: multimodal foundation models, in-context learning, many-shot learning, batch querying, data efficiency\n\nExample Questions:\n1. How does the performance of Gemini 1.5 Pro and GPT-4o compare when scaling the number of demonstrating examples from few-shot to many-shot in-context learning?\n2. What techniques were used to ensure the safety and responsibility of the multimodal models during open-ended interactions?\n3. How could the ability to run highly capable multimodal models like phi-3-mini directly on consumer devices change the development and deployment of AI assistants in the future?\n4. What are some potential beneficial applications of a model like phi-3-mini that can perform advanced language tasks while preserving user privacy by running fully on-device?\n</text>\n</reference>",
  "output": "<note>\nToday was an exciting day at work as we continued to push the boundaries of what our multimodal AI models can do. I spent the morning digging into the latest research on many-shot in-context learning (ICL) with large language models. It's fascinating to see how providing these models with a wealth of demonstrating examples can lead to such substantial performance improvements, especially on complex tasks like fine-grained image classification.\n\nOur team has been particularly focused on the Gemini 1.5 Pro model, which seems to exhibit higher data efficiency than GPT-4o when it comes to learning from additional examples. I'm really curious to understand the underlying reasons for this - is it something about the model architecture, the training process, or the way it handles multimodal inputs? We're planning to do some deeper analysis to try to unpack this.\n\nOne thing that's been on my mind is how we can make these powerful models more accessible and practical for real-world applications. The long prompts required for many-shot ICL can be computationally expensive, so I'm excited that we've been exploring batched querying as a way to improve efficiency. Seeing that we can maintain or even improve performance while drastically reducing latency and cost is a game-changer.\n\nI also spent some time this afternoon working on a side project to build a medical image analysis tool. The goal is to leverage our multimodal models to help clinicians quickly and accurately diagnose conditions based on scans. It's been a bit of a challenge to get the models to generalize well to the nuances of real-world medical data, but I'm hopeful that the many-shot ICL techniques we've been exploring will help bridge that gap.\n\nAs I was wrapping up for the day, I couldn't help but wonder about the broader implications of this work. Imagine a future where everyone has access to highly capable AI assistants that can adapt to their specific needs and preferences by learning from just a few examples. The potential to democratize advanced language and multimodal capabilities is truly exciting. Of course, we'll need to be mindful of safety and responsible development, but I'm optimistic that we're on the right track.\n\nAll in all, it's been a productive and thought-provoking day. I'm looking forward to seeing what new breakthroughs emerge as we continue to push the boundaries of what's possible with these incredible models.\n</note>"
}