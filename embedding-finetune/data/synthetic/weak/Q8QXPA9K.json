{
  "input": "<reference id=\"Q8QXPA9K\">\n<metadata>\n{\n  \"title\": \"Vision Transformers Need Registers\",\n  \"abstract\": \"  Transformers have recently emerged as a powerful tool for learning visual\\nrepresentations. In this paper, we identify and characterize artifacts in\\nfeature maps of both supervised and self-supervised ViT networks. The artifacts\\ncorrespond to high-norm tokens appearing during inference primarily in\\nlow-informative background areas of images, that are repurposed for internal\\ncomputations. We propose a simple yet effective solution based on providing\\nadditional tokens to the input sequence of the Vision Transformer to fill that\\nrole. We show that this solution fixes that problem entirely for both\\nsupervised and self-supervised models, sets a new state of the art for\\nself-supervised visual models on dense visual prediction tasks, enables object\\ndiscovery methods with larger models, and most importantly leads to smoother\\nfeature maps and attention maps for downstream visual processing.\\n\",\n  \"published\": \"2023-09-28T16:45:46Z\"\n}\n</metadata>\n<text>\nHere is a summary of the key points from the paper:\n\n**Key Findings:**\n- Vision Transformers (ViTs) exhibit artifacts in their feature maps and attention maps, in the form of high-norm \"outlier\" tokens that appear primarily in low-informative background areas.\n- These outlier tokens are repurposed by the model to aggregate global image information, while discarding local spatial information.\n- The outlier tokens appear in the middle layers of sufficiently large and trained ViT models, and are not present in smaller or earlier-stage models.\n- The outlier tokens contain less information about their original patch position and pixel values, but more global information about the image.\n\n**Proposed Solution:**\n- The authors propose adding additional \"register\" tokens to the input sequence of the ViT, which the model can use to store and retrieve global information, without corrupting the local patch features.\n- This simple architectural change completely removes the outlier tokens, leading to smoother feature maps and attention maps.\n- The models with registers also achieve improved performance on dense prediction tasks and enable better object discovery methods.\n\n**Keywords:**\n- Vision Transformers\n- Feature map artifacts\n- Attention map artifacts\n- Register tokens\n- Global vs local information\n- Object discovery\n\n**Example Questions:**\nQ: How do the authors detect and characterize the artifacts present in Vision Transformer feature maps?\nA: The authors observe that the artifacts correspond to tokens with much higher norm values at the output of the ViT model. They analyze the properties of these high-norm \"outlier\" tokens and find that they contain less local information about their patch position and pixels, but more global information about the image.\n\nQ: What is the key insight behind the authors' proposed solution of adding \"register\" tokens to the ViT input?\nA: The authors hypothesize that the ViT model naturally learns to repurpose some tokens to store and retrieve global image information, at the expense of discarding local spatial information in those tokens. By explicitly providing additional \"register\" tokens, the model can isolate this global information aggregation behavior, leading to smoother feature maps without artifacts.\n\nQ: How does the addition of register tokens impact the performance of ViT models on dense prediction tasks and unsupervised object discovery?\nA: The authors show that adding register tokens improves the performance of ViT models on dense prediction tasks like semantic segmentation and monocular depth estimation. It also enables much better results on unsupervised object discovery methods like LOST, which rely on the smoothness of the ViT feature maps.\n</text>\n</reference>",
  "output": "<note>\nIt's been an interesting day at work, filled with a mix of technical challenges and exciting new developments. This morning, I spent some time investigating an issue with our computer vision models. It seems that the Vision Transformers we've been using are exhibiting some strange artifacts in their feature maps and attention maps. These high-norm \"outlier\" tokens are appearing primarily in the background areas of images, and it looks like the model is repurposing them to aggregate global information about the image, rather than focusing on the local spatial details.\n\nAfter doing some research, I came across a paper that proposes a clever solution to this problem. The authors suggest adding additional \"register\" tokens to the input of the Vision Transformer, which can be used to store and retrieve global information without corrupting the local patch features. I'm really excited to try this out and see if it helps improve the performance and robustness of our models, especially on dense prediction tasks like semantic segmentation.\n\nIn the afternoon, I had a meeting with the team to discuss our progress on a new object discovery project. We've been experimenting with some unsupervised methods that rely heavily on the smoothness of the Vision Transformer feature maps. The paper I read mentioned that the register token approach can actually enable much better results on these types of tasks, so I'm eager to give it a shot.\n\nIt's been really fascinating to see how the field of computer vision is evolving, with Transformers playing an increasingly important role. I'm constantly amazed by the rapid advancements in this area, and the creative solutions that researchers are coming up with to address the challenges. It's a great time to be working in this field, and I'm grateful for the opportunity to be a part of it.\n\nAs I wrap up my day and head home, I can't help but feel a sense of excitement about the future. Who knows what other breakthroughs are just around the corner? I'm looking forward to continuing to push the boundaries of what's possible with these powerful AI models.\n</note>"
}