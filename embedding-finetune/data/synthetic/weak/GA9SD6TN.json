{
  "input": "<reference id=\"GA9SD6TN\">\n<metadata>\n{\n  \"title\": \"Chain-of-Verification Reduces Hallucination in Large Language Models\",\n  \"abstract\": \"  Generation of plausible yet incorrect factual information, termed\\nhallucination, is an unsolved issue in large language models. We study the\\nability of language models to deliberate on the responses they give in order to\\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\\nwhereby the model first (i) drafts an initial response; then (ii) plans\\nverification questions to fact-check its draft; (iii) answers those questions\\nindependently so the answers are not biased by other responses; and (iv)\\ngenerates its final verified response. In experiments, we show CoVe decreases\\nhallucinations across a variety of tasks, from list-based questions from\\nWikidata, closed book MultiSpanQA and longform text generation.\\n\",\n  \"published\": \"2023-09-20T17:50:55Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces the Chain-of-Verification (CoVe) method to reduce hallucinations in large language models. The key steps are:\n\n1. Generate a baseline response to the query using the language model.\n2. Plan a set of verification questions to check the factual claims in the baseline response. \n3. Execute the verification by independently answering the planned questions.\n4. Generate a final verified response that incorporates the results of the verification.\n\nThe authors explore different variants of the verification execution step, including joint, 2-step, and factored approaches. The factored approach, where the verification questions are answered independently without conditioning on the original response, is found to perform the best.\n\nThe paper evaluates CoVe on a variety of tasks - list-based questions from Wikidata, closed-book MultiSpanQA, and longform biography generation. Across these tasks, CoVe is shown to significantly reduce hallucinations compared to the baseline language model, while maintaining or improving overall performance.\n\nKey findings:\n\n- CoVe improves precision on list-based tasks by reducing the number of hallucinated answers.\n- CoVe improves performance on closed-book QA, increasing F1 by 23% over the baseline.\n- On longform biography generation, CoVe improves the FACTSCORE metric by 28% compared to the baseline.\n- The factored and 2-step variants of CoVe outperform the joint approach, as they avoid repeating hallucinations from the original response.\n- Explicit reasoning steps in the \"factor+revise\" variant further improve performance by detecting and removing inconsistencies.\n- CoVe-based Llama 65B outperforms other large models like InstructGPT, ChatGPT and PerplexityAI on the biography generation task.\n\nThe paper also finds that language models can often answer short verification questions more accurately than they can generate the full longform response, suggesting an opportunity for targeted deliberation.\n\nExample Questions:\n\nQ: How does the performance of phi-3-mini compare to state-of-the-art large language models when evaluated on standard academic NLP benchmarks?\nA: Phi-3-mini achieves performance rivaling much larger models like Mixtral 8x7B and GPT-3.5 on academic benchmarks, achieving 69% on MMLU and 8.38 on MT-bench despite having only 3.8B parameters.\n\nQ: What techniques were used to ensure phi-3-mini behaves in a safe, robust, and responsible manner during open-ended interactions with humans?\nA: The phi-3-mini model underwent an iterative process of safety alignment via finetuning on helpfulness and harmlessness preference datasets, red-teaming to identify potential issues, and extensive automated testing across dozens of responsibility and safety harm categories.\n\nQ: How might the ability to run highly capable language models like phi-3-mini directly on consumer devices change the way AI assistants are developed and deployed in the future?\nA: Some potential applications include privacy-preserving mobile AI assistants, embedded NLP systems for domains like healthcare where privacy is critical, and democratization of advanced language AI capabilities to developers/researchers without access to massive computing resources.\n</text>\n</reference>",
  "output": "<note>\nToday was an interesting day at work as we continued exploring ways to improve the reliability and safety of our AI language models. One of the key challenges we've been grappling with is the issue of hallucination - the generation of plausible but factually incorrect information. It's a tricky problem, but I'm excited about a new approach we've been experimenting with called Chain-of-Verification (CoVe).\n\nThe basic idea behind CoVe is to have the model go through a deliberative process to fact-check its own responses before finalizing them. First, the model generates an initial draft response. Then, it plans a set of verification questions to check the key claims in that draft. The model answers those questions independently, without looking back at the original response, to avoid simply repeating any mistakes. Finally, it generates the final verified response incorporating the results of the self-checking.\n\nWe've been testing this approach on a variety of tasks, from short-form question answering to longform text generation, and the results have been really promising. Compared to our baseline models, CoVe is able to significantly reduce the amount of hallucinated information while maintaining or even improving overall performance. It's exciting to see how this deliberative process can help our models become more reliable and trustworthy.\n\nOf course, there's still a lot of work to be done. We're exploring different variants of the verification step, like having the model answer the questions jointly versus independently. And we're looking at ways to further improve the robustness and consistency of the final responses. But I'm optimistic that techniques like CoVe will be key to unlocking the full potential of large language models in real-world applications.\n\nIn the meantime, I'm also working on a side project to build a more interactive and intuitive AI assistant for our e-commerce platform. The goal is to leverage natural language processing to create a seamless user experience, where customers can simply chat with the assistant to complete their tasks. It's been a fun challenge, trying to balance the capabilities of the AI with the need for it to behave in a safe and responsible manner.\n\nOverall, it's been a productive and thought-provoking day. I'm excited to see how our work on hallucination reduction and interactive AI assistants continues to evolve. There's so much potential in this field, and I feel lucky to be a part of it.\n</note>"
}