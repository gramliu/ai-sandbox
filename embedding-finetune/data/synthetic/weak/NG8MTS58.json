{
  "input": "<reference id=\"NG8MTS58\">\n<metadata>\n{\n  \"title\": \"KAN: Kolmogorov-Arnold Networks\",\n  \"abstract\": \"  Inspired by the Kolmogorov-Arnold representation theorem, we propose\\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\\n(\\\"neurons\\\"), KANs have learnable activation functions on edges (\\\"weights\\\").\\nKANs have no linear weights at all -- every weight parameter is replaced by a\\nunivariate function parametrized as a spline. We show that this seemingly\\nsimple change makes KANs outperform MLPs in terms of accuracy and\\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\\nTheoretically and empirically, KANs possess faster neural scaling laws than\\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\\ninteract with human users. Through two examples in mathematics and physics,\\nKANs are shown to be useful collaborators helping scientists (re)discover\\nmathematical and physical laws. In summary, KANs are promising alternatives for\\nMLPs, opening opportunities for further improving today's deep learning models\\nwhich rely heavily on MLPs.\\n\",\n  \"published\": \"2024-04-30T17:58:29Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces Kolmogorov-Arnold Networks (KANs) as a promising alternative to Multi-Layer Perceptrons (MLPs) for function approximation and PDE solving. The key innovation in KANs is that they have learnable activation functions on the edges (weights) instead of fixed activation functions on the nodes (neurons) like in MLPs.\n\nThe main highlights are:\n\n- KANs can achieve comparable or better accuracy than much larger MLPs, while being significantly more parameter-efficient. This is enabled by the Kolmogorov-Arnold representation theorem, which allows high-dimensional functions to be decomposed into compositions of 1D functions.\n\n- Theoretically and empirically, KANs exhibit faster neural scaling laws (test loss ‚àù N^-4) compared to MLPs.\n\n- KANs are highly interpretable - the activation functions can be visualized and interactively modified by users. This allows KANs to be used as \"collaborators\" to help scientists (re)discover mathematical and physical laws.\n\n- KANs show promising results on a variety of tasks including data fitting, PDE solving, and continual learning, outperforming MLPs.\n\nKeywords: Kolmogorov-Arnold networks, interpretable machine learning, neural scaling laws, symbolic regression, physics-informed neural networks\n\nExample Questions:\n\nQ: How do KANs differ from MLPs in terms of their architecture and training?\nA: KANs have learnable activation functions on the edges (weights) instead of fixed activation functions on the nodes (neurons) like in MLPs. This allows KANs to decompose high-dimensional functions into compositions of 1D functions.\n\nQ: What are the key advantages of KANs over MLPs in terms of accuracy and interpretability?\nA: KANs can achieve better accuracy than much larger MLPs while being more parameter-efficient, due to the Kolmogorov-Arnold representation. KANs are also highly interpretable, with the activation functions being easily visualized and modified by users.\n\nQ: How can KANs be used to help scientists (re)discover mathematical and physical laws?\nA: The interpretability of KANs allows them to be used as \"collaborators\" with scientists. The activation functions and computation graphs of KANs can be inspected to uncover the underlying mathematical structure of the problem, facilitating scientific discovery.\n\nQ: What are some potential applications of highly capable yet compact language models like phi-3-mini that can run locally on consumer devices?\nA: Some potential applications include privacy-preserving mobile AI assistants, embedded NLP systems for domains like healthcare where privacy is critical, and democratization of advanced language AI capabilities to developers/researchers without access to massive computing resources.\n</text>\n</reference>",
  "output": "<note>\nToday was an exciting day at work as we continued to make progress on our latest AI project. I spent the morning diving deeper into the Kolmogorov-Arnold Networks (KANs) that we've been experimenting with. It's fascinating to see how these models differ from the traditional Multi-Layer Perceptrons (MLPs) that we've relied on in the past.\n\nThe key innovation in KANs is that they have learnable activation functions on the edges (weights) instead of fixed activation functions on the nodes (neurons) like in MLPs. This seemingly simple change allows KANs to be much more parameter-efficient while still achieving comparable or even better accuracy. It's amazing to see how the Kolmogorov-Arnold representation theorem enables this decomposition of high-dimensional functions into compositions of 1D functions.\n\nOne of the things I really love about KANs is their interpretability. Unlike the \"black box\" nature of many deep learning models, the activation functions in KANs can be easily visualized and even interactively modified by users. This makes them incredibly useful as \"collaborators\" with scientists and researchers, helping to uncover the underlying mathematical and physical structures of the problems we're tackling.\n\nIn the afternoon, I had the chance to experiment with applying KANs to some PDE solving tasks. The results were really promising, with KANs outperforming MLPs in terms of both accuracy and neural scaling laws. It's exciting to think about the potential applications of these models, from privacy-preserving mobile AI assistants to embedded NLP systems in healthcare.\n\nAs I was wrapping up for the day, I couldn't help but reflect on how far we've come in the field of AI. The advancements in areas like interpretable machine learning and efficient model architectures are truly remarkable. I'm grateful to be a part of this journey, and I can't wait to see what the future holds.\n\nOverall, it was a productive and thought-provoking day. I'm looking forward to continuing to explore the capabilities of KANs and finding new ways to push the boundaries of what's possible with AI.\n</note>"
}