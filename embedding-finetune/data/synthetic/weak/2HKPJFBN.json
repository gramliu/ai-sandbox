{
  "input": "<reference id=\"2HKPJFBN\">\n<metadata>\n{\n  \"title\": \"Fine-tuning Language Models for Factuality\",\n  \"abstract\": \"  The fluency and creativity of large pre-trained language models (LLMs) have\\nled to their widespread use, sometimes even as a replacement for traditional\\nsearch engines. Yet language models are prone to making convincing but\\nfactually inaccurate claims, often referred to as 'hallucinations.' These\\nerrors can inadvertently spread misinformation or harmfully perpetuate\\nmisconceptions. Further, manual fact-checking of model responses is a\\ntime-consuming process, making human factuality labels expensive to acquire. In\\nthis work, we fine-tune language models to be more factual, without human\\nlabeling and targeting more open-ended generation settings than past work. We\\nleverage two key recent innovations in NLP to do so. First, several recent\\nworks have proposed methods for judging the factuality of open-ended text by\\nmeasuring consistency with an external knowledge base or simply a large model's\\nconfidence scores. Second, the direct preference optimization algorithm enables\\nstraightforward fine-tuning of language models on objectives other than\\nsupervised imitation, using a preference ranking over possible model responses.\\nWe show that learning from automatically generated factuality preference\\nrankings, generated either through existing retrieval systems or our novel\\nretrieval-free approach, significantly improves the factuality (percent of\\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\\ngenerating biographies and answering medical questions, respectively.\\n\",\n  \"published\": \"2023-11-14T18:59:15Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces a novel approach to fine-tuning language models to be more factual, without relying on expensive human labeling. The key innovations are:\n\n1. Leveraging recent advances in automated factuality evaluation, including reference-based methods that measure consistency with external knowledge bases, as well as a novel reference-free approach that uses the model's own confidence as a proxy for truthfulness.\n\n2. Using these automated factuality scores to construct preference datasets, where responses with higher truthfulness scores are labeled as preferred. This preference data is then used to fine-tune the language model using the Direct Preference Optimization (DPO) algorithm.\n\nThe authors show that fine-tuning Llama-2 models with this factuality-aware preference learning significantly reduces the number of factual errors (hallucinations) in both biography generation and medical question-answering tasks, outperforming RLHF and decoding-based factuality improvement methods. Importantly, the reference-free confidence-based approach also provides strong factuality improvements, without requiring access to external knowledge sources.\n\nKeywords: factuality, language model fine-tuning, preference learning, automated factuality evaluation\n\nExample Questions:\nQ: How does the factuality tuning approach introduced in this paper differ from previous work on improving language model factuality?\nA: The key innovations are the use of automated factuality scoring, both reference-based and reference-free, to construct preference datasets for fine-tuning, rather than relying on expensive human labeling.\n\nQ: What are the main advantages of the reference-free, confidence-based factuality scoring approach compared to reference-based methods?\nA: The reference-free approach avoids the need for retrieving and aligning to external knowledge sources, making it more scalable and applicable in domains where high-quality reference texts are not available.\n\nQ: How well do the factuality improvements from this approach transfer to language models fine-tuned for open-ended dialogue, like Llama-2-Chat?\nA: The paper shows that factuality tuning can be composed with RLHF to further improve the factual accuracy of chat models, suggesting the techniques are complementary.\n\nQ: What are some potential future research directions building on this work to further improve language model factuality? (no_answer)\n</text>\n</reference>",
  "output": "<note>\nIt's been an interesting day at work as we continue to explore ways to make our language models more factual and reliable. This morning, I spent some time reviewing the latest research on automated factuality evaluation techniques. The idea of using a model's own confidence as a proxy for truthfulness is really intriguing - it could make the process of fine-tuning for factuality a lot more scalable and accessible.\n\nI had a meeting with the team to discuss how we might incorporate these ideas into our current projects. We're particularly interested in applying the factuality-aware preference learning approach to our open-ended dialogue models. While the language models we've built are quite fluent and creative, we've definitely encountered issues with them making convincing but inaccurate claims. Reducing these \"hallucinations\" is a top priority.\n\nOne of the challenges we're grappling with is how to balance factuality with other desirable traits, like empathy and personality. We don't want our models to become overly cautious or robotic in their responses. It's a delicate balance, but I'm hopeful that with the right fine-tuning techniques, we can find a sweet spot.\n\nIn the afternoon, I worked on integrating some of the reference-free factuality scoring methods into our existing model evaluation pipelines. It's been really interesting to see how the model's own confidence correlates with truthfulness, and how we can leverage that signal to improve performance. I'm excited to start running some experiments and see how it all comes together.\n\nAs I was wrapping up for the day, I couldn't help but reflect on how far the field of language AI has come. It's amazing to see the rapid progress, but also humbling to realize how much more work there is to be done. Ensuring the factuality and reliability of these models is crucial, especially as they become more integrated into our daily lives. I'm glad to be a part of this journey, and I'm looking forward to seeing what the future holds.\n</note>"
}