{
  "input": "<metadata>\n{\n  \"title\": \"LoRA: Low-Rank Adaptation of Large Language Models\",\n  \"abstract\": \"  An important paradigm of natural language processing consists of large-scale\\npre-training on general domain data and adaptation to particular tasks or\\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\\ndeploying independent instances of fine-tuned models, each with 175B\\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\\nLoRA, which freezes the pre-trained model weights and injects trainable rank\\ndecomposition matrices into each layer of the Transformer architecture, greatly\\nreducing the number of trainable parameters for downstream tasks. Compared to\\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\\ntraining throughput, and, unlike adapters, no additional inference latency. We\\nalso provide an empirical investigation into rank-deficiency in language model\\nadaptation, which sheds light on the efficacy of LoRA. We release a package\\nthat facilitates the integration of LoRA with PyTorch models and provide our\\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\\nhttps://github.com/microsoft/LoRA.\\n\",\n  \"published\": \"2021-06-17T17:37:18Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces Low-Rank Adaptation (LoRA), a parameter-efficient technique for adapting large pre-trained language models to downstream tasks. The key idea is to freeze the pre-trained model weights and inject trainable low-rank decomposition matrices into the model, greatly reducing the number of trainable parameters compared to full fine-tuning.\n\nKey Findings:\n- LoRA can reduce the number of trainable parameters by up to 10,000x compared to full fine-tuning on GPT-3 175B, while matching or exceeding the performance of full fine-tuning.\n- LoRA does not introduce any additional inference latency, unlike adapter-based methods.\n- Empirical analysis shows the update matrices learned by LoRA have a very low intrinsic rank, suggesting the changes needed for downstream adaptation can be well-captured by a compact low-rank representation.\n- LoRA can be combined with other efficient adaptation methods like prefix tuning for further improvements.\n\nKeywords:\n- Parameter-efficient adaptation\n- Low-rank matrix factorization\n- Transformer language models\n- GPT-3\n\nExample Questions:\nQ: How does LoRA compare to full fine-tuning in terms of the number of trainable parameters and computational efficiency?\nA: LoRA can reduce the number of trainable parameters by up to 10,000x compared to full fine-tuning on GPT-3 175B, while also providing a 25% speedup during training.\n\nQ: What are the key advantages of LoRA over adapter-based methods for efficient model adaptation?\nA: Unlike adapter layers, LoRA does not introduce any additional inference latency, as the trainable matrices can be merged with the frozen pre-trained weights during deployment.\n\nQ: How does the rank of the update matrices learned by LoRA relate to the intrinsic dimensionality of the changes needed for downstream adaptation?\nA: The empirical analysis shows the update matrices have a very low intrinsic rank, suggesting the changes needed for downstream tasks can be well-captured by a compact low-rank representation.\n\nQ: How can LoRA be combined with other efficient adaptation methods like prefix tuning?\nA: The paper shows that combining LoRA with prefix-embedding tuning can provide further performance improvements on some tasks, demonstrating the orthogonality of these approaches.</text>",
  "output": "Today I worked on implementing the Low-Rank Adaptation (LoRA) technique for fine-tuning large language models like GPT-3. The key idea is to freeze the pre-trained model weights and inject trainable low-rank matrices into each Transformer layer during fine-tuning. This drastically reduces the number of trainable parameters compared to full fine-tuning.\n\nIn the morning, I set up the codebase by forking the official LoRA repository and installing the required dependencies. I spent some time going through the code to understand how the low-rank matrices are injected into the model layers.\n\nAfter lunch, I ran some experiments to fine-tune GPT-2 on the GLUE benchmark tasks using LoRA. I was really impressed by how much faster the fine-tuning was compared to the baseline, while still matching the performance! The low GPU memory usage of LoRA also meant I could run bigger batch sizes.\n\nIn the evening, I explored combining LoRA with prefix tuning, which is another efficient way to adapt language models. Interestingly, using both methods together gave me another boost in performance on certain tasks. I'm really excited about these parameter-efficient techniques that could make it feasible to personalize huge models like GPT-3 for individual users and applications.\n\nBefore wrapping up, I spent some time analyzing the ranks of the trained LoRA matrices. As the paper mentioned, I found the ranks to be surprisingly low, suggesting that task-specific adaptation can indeed be captured very compactly. I'm really looking forward to trying out LoRA on some of my own projects!"
}