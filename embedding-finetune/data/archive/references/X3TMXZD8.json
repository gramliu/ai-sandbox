{
  "input": "<metadata>\n{\n  \"title\": \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\",\n  \"abstract\": \"  Language models are increasingly being deployed for general problem solving\\nacross a wide range of tasks, but are still confined to token-level,\\nleft-to-right decision-making processes during inference. This means they can\\nfall short in tasks that require exploration, strategic lookahead, or where\\ninitial decisions play a pivotal role. To surmount these challenges, we\\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\\nwhich generalizes over the popular Chain of Thought approach to prompting\\nlanguage models, and enables exploration over coherent units of text (thoughts)\\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\\nperform deliberate decision making by considering multiple different reasoning\\npaths and self-evaluating choices to decide the next course of action, as well\\nas looking ahead or backtracking when necessary to make global choices. Our\\nexperiments show that ToT significantly enhances language models'\\nproblem-solving abilities on three novel tasks requiring non-trivial planning\\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\\nhttps://github.com/princeton-nlp/tree-of-thought-llm.\\n\",\n  \"published\": \"2023-05-17T23:16:17Z\"\n}\n</metadata>\n<text>\nSummary:\nThis paper introduces a new framework called \"Tree of Thoughts\" (ToT) that enables language models to perform more deliberate problem-solving by exploring multiple reasoning paths and evaluating choices through self-reflection. The key ideas are:\n\n1. Decomposing the problem-solving process into coherent \"thoughts\" that serve as intermediate steps, rather than just generating a continuous sequence.\n2. Generating and evaluating multiple candidate thoughts at each step using prompts that leverage the language model's own reasoning capabilities. \n3. Combining this thought generation and evaluation with search algorithms like breadth-first search and depth-first search to systematically explore the space of possible solutions.\n\nThe authors evaluate ToT on three novel tasks - Game of 24, Creative Writing, and Mini Crosswords - that challenge the token-level, left-to-right decision making of standard language models. They show ToT significantly outperforms standard prompting approaches on these tasks, achieving success rates up to 74% compared to 4-49% for baselines.\n\nKeywords: language models, problem solving, planning, search, deliberate reasoning\n\nExample Questions:\nQ: How does the Tree of Thoughts framework differ from standard language model prompting approaches like chain-of-thought?\nA: ToT decomposes the problem-solving process into coherent \"thoughts\" that are generated and evaluated independently, rather than just generating a continuous sequence. It also integrates this thought generation/evaluation with systematic search algorithms to explore multiple possible solutions.\n\nQ: What are the key components of the ToT framework, and how can they be customized for different problem domains?\nA: The key components are: 1) Thought decomposition, 2) Thought generation, 3) State evaluation, and 4) Search algorithm. These can be varied independently to adapt ToT to different problems, e.g. using different prompts for thought generation, different heuristics for state evaluation, and different search algorithms.\n\nQ: How could the ability to run highly capable language models like the one used in this paper directly on consumer devices impact the development and deployment of AI assistants in the future?\nA: (No definitive answer provided, as the paper does not speculate on this specific application.)\n\nQ: What are some potential beneficial applications of a model like the one described that can perform advanced language tasks while preserving user privacy by running fully on-device?\nA: Potential applications include: 1) Privacy-preserving mobile AI assistants that can engage in open-ended dialogue and help with tasks without sending user data to the cloud, 2) Embedded NLP systems for domains like healthcare where privacy is critical, and 3) Democratizing advanced language AI capabilities to developers/researchers without access to massive computing resources.</text>",
  "output": "Today I worked on implementing the Tree of Thoughts (ToT) framework for deliberate problem solving with large language models. Here are the key steps I took:\n\n1. I started by studying the ToT paper and understanding the core ideas behind decomposing problems into \"thoughts\", generating and evaluating candidate thoughts, and using search algorithms to explore the solution space.\n\n2. I chose the Game of 24 as my first test problem and defined prompts for thought generation and state evaluation based on the examples in the paper. For thought generation, I prompted the model to provide a series of arithmetic operations that could potentially lead to the target number 24 using the given input numbers.\n\n3. I implemented a simple depth-first search algorithm that would generate candidate thoughts, evaluate their resulting state using the model's own scoring, and recursively explore the most promising paths first.\n\n4. After some trial and error with the prompts and search parameters, I was able to get decent performance on solving random Game of 24 instances using the ToT approach with GPT-4. The model could now methodically explore different lines of reasoning and backtrack when necessary.\n\n5. Encouraged by these initial results, I spent some time studying the other example applications like creative writing and crossword puzzles. I'm really excited to try adapting the ToT framework to these more open-ended domains next.\n\n6. Throughout the day, I kept reviewing the paper and brainstorming ideas for potential extensions or improvements to the core ToT algorithm. Some thoughts:\n    - Incorporating techniques from classical planning or reinforcement learning\n    - Exploring different search strategies beyond depth-first (e.g. beam search)\n    - Developing more advanced thought pruning heuristics using the model itself\n    - Applying ToT to other domains like math word problems, code generation, etc."
}