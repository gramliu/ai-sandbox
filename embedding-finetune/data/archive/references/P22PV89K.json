{
  "input": "<metadata>\n{\n  \"title\": \"In-Context Learning with Long-Context Models: An In-Depth Exploration\",\n  \"abstract\": \"  As model context lengths continue to increase, the number of demonstrations\\nthat can be provided in-context approaches the size of entire training\\ndatasets. We study the behavior of in-context learning (ICL) at this extreme\\nscale on multiple datasets and models. We show that, for many datasets with\\nlarge label spaces, performance continues to increase with hundreds or\\nthousands of demonstrations. We contrast this with example retrieval and\\nfinetuning: example retrieval shows excellent performance at low context\\nlengths but has diminished gains with more demonstrations; finetuning is more\\ndata hungry than ICL but can sometimes exceed long-context ICL performance with\\nadditional data. We use this ICL setting as a testbed to study several\\nproperties of both in-context learning and long-context models. We show that\\nlong-context ICL is less sensitive to random input shuffling than short-context\\nICL, that grouping of same-label examples can negatively impact performance,\\nand that the performance boosts we see do not arise from cumulative gain from\\nencoding many examples together. We conclude that although long-context ICL can\\nbe surprisingly effective, most of this gain comes from attending back to\\nsimilar examples rather than task learning.\\n\",\n  \"published\": \"2024-04-30T21:06:52Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper explores the behavior of in-context learning (ICL) with large language models that can handle extremely long context lengths, up to thousands of demonstrations. The key findings are:\n\n- Performance on many datasets continues to improve significantly as the number of demonstrations in-context is scaled up to hundreds or thousands, approaching or even exceeding the performance of finetuning on the same data.\n\n- As the context length increases, ICL becomes less sensitive to the order of examples and the benefits of carefully selecting relevant examples through retrieval diminish. This allows for more computationally efficient use of a single set of cached demonstrations.\n\n- The performance gains from long-context ICL are primarily due to the model's ability to retrieve and attend to more relevant examples, rather than learning a better task-specific decision boundary.\n\n- Compared to finetuning, ICL is more data-efficient in the low-data regime, but finetuning can sometimes exceed long-context ICL performance when given a very large number of examples.\n\nKeywords: in-context learning, long-context models, few-shot learning, parameter-efficient finetuning\n\nExample Questions:\n\nQ: How does the performance of in-context learning compare to finetuning as the number of available training examples increases?\nA: The paper shows that in-context learning outperforms finetuning when the number of examples is relatively low, but finetuning can sometimes exceed long-context ICL performance when given a very large number of examples (thousands).\n\nQ: What are some key differences in the behavior of in-context learning with short vs. long context lengths?\nA: As the context length increases, ICL becomes less sensitive to the order of examples and the benefits of carefully selecting relevant examples through retrieval diminish. This suggests the performance gains from long-context ICL are primarily due to retrieving and attending to more relevant examples, rather than learning a better task-specific decision boundary.\n\nQ: How could the findings in this paper inform the development of practical AI assistants that leverage large language models?\nA: The ability to run highly capable language models locally on devices like smartphones, as demonstrated with phi-3-mini, could enable the development of privacy-preserving mobile AI assistants that can engage in open-ended dialogue. Additionally, the efficiency gains of long-context ICL compared to finetuning could make it a compelling approach for deploying advanced language AI capabilities to developers and researchers without access to massive computing resources.</text>",
  "output": "Today I spent some time exploring the exciting new field of in-context learning with large language models. After reading a recent paper on the topic, I decided to try out some experiments myself:\n\nIn the morning, I fine-tuned a large language model on a few-shot text classification dataset using the traditional approach of updating the model's parameters. I was impressed by the model's ability to learn the task from just a handful of examples, but the fine-tuning process was quite computationally expensive.\n\nIn the afternoon, I switched gears and tried in-context learning instead. I simply provided the model with hundreds of examples from the dataset as part of its input context, without any parameter updates. To my surprise, the model was able to achieve strong performance on the classification task just by attending to the relevant examples in its context!\n\nI spent some time varying the number of in-context examples and found that performance kept improving as I added more, even up to over a thousand examples. This is quite different from traditional fine-tuning where providing too much data can lead to overfitting.\n\nOne interesting thing I noticed is that the order of examples didn't seem to matter much for in-context learning with a long context window. The model was able to effectively retrieve and attend to the relevant examples regardless of their position.\n\nOverall, I'm really excited about the potential of in-context learning, especially for efficient few-shot adaptation of large language models. Being able to achieve strong performance without any parameter updates could enable all kinds of applications on resource-constrained devices. I'll definitely be exploring this area more in the coming weeks!"
}