{
  "input": "<metadata>\n{\n  \"title\": \"Fine-tuning Language Models for Factuality\",\n  \"abstract\": \"  The fluency and creativity of large pre-trained language models (LLMs) have\\nled to their widespread use, sometimes even as a replacement for traditional\\nsearch engines. Yet language models are prone to making convincing but\\nfactually inaccurate claims, often referred to as 'hallucinations.' These\\nerrors can inadvertently spread misinformation or harmfully perpetuate\\nmisconceptions. Further, manual fact-checking of model responses is a\\ntime-consuming process, making human factuality labels expensive to acquire. In\\nthis work, we fine-tune language models to be more factual, without human\\nlabeling and targeting more open-ended generation settings than past work. We\\nleverage two key recent innovations in NLP to do so. First, several recent\\nworks have proposed methods for judging the factuality of open-ended text by\\nmeasuring consistency with an external knowledge base or simply a large model's\\nconfidence scores. Second, the direct preference optimization algorithm enables\\nstraightforward fine-tuning of language models on objectives other than\\nsupervised imitation, using a preference ranking over possible model responses.\\nWe show that learning from automatically generated factuality preference\\nrankings, generated either through existing retrieval systems or our novel\\nretrieval-free approach, significantly improves the factuality (percent of\\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\\ngenerating biographies and answering medical questions, respectively.\\n\",\n  \"published\": \"2023-11-14T18:59:15Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces a novel approach to fine-tuning language models to be more factual, without relying on expensive human labeling. The key innovations are:\n\n1. Leveraging recent advances in automated factuality evaluation, including reference-based methods that measure consistency with external knowledge bases, as well as a novel reference-free approach that uses the model's own confidence as a proxy for truthfulness.\n\n2. Using these automated factuality scores to construct preference datasets, where responses with higher truthfulness scores are labeled as preferred. This preference data is then used to fine-tune the language model using the Direct Preference Optimization (DPO) algorithm.\n\nThe authors show that fine-tuning Llama-2 models with this factuality-aware preference learning significantly reduces the number of factual errors (hallucinations) in both biography generation and medical question-answering tasks, outperforming RLHF and decoding-based factuality improvement methods. Importantly, the reference-free confidence-based approach also provides strong factuality improvements, without requiring access to external knowledge sources.\n\nKeywords: factuality, language model fine-tuning, preference learning, automated factuality evaluation\n\nExample Questions:\nQ: How does the factuality tuning approach introduced in this paper differ from previous work on improving language model factuality?\nA: The key innovations are the use of automated factuality scoring, both reference-based and reference-free, to construct preference datasets for fine-tuning, rather than relying on expensive human labeling.\n\nQ: What are the main advantages of the reference-free, confidence-based factuality scoring approach compared to reference-based methods?\nA: The reference-free approach avoids the need for retrieving and aligning to external knowledge sources, making it more scalable and applicable in domains where high-quality reference texts are not available.\n\nQ: How well do the factuality improvements from this approach transfer to language models fine-tuned for open-ended dialogue, like Llama-2-Chat?\nA: The paper shows that factuality tuning can be composed with RLHF to further improve the factual accuracy of chat models, suggesting the techniques are complementary.\n\nQ: What are some potential future research directions building on this work to further improve language model factuality? (no_answer)</text>",
  "output": "I started my day exploring the new factuality fine-tuning techniques for large language models described in the paper. First, I set up the automated factuality scoring systems - both the reference-based approach using Wikipedia retrieval and the novel reference-free approach that uses the model's own confidence scores. I generated some sample text on various topics using the base Llama-2 model and ran the factuality scorers to get preference rankings between more and less factual outputs.\n\nWith the preference data in hand, I fine-tuned a copy of the Llama-2 model using the Direct Preference Optimization algorithm, creating a new \"factuality-tuned\" version of the model. To test it out, I had the original and fine-tuned models generate biographies for historical figures and answer medical questions. I was impressed to see the factuality-tuned model make far fewer factual errors and hallucinations compared to the base model!\n\nExcited by these results, I spent the afternoon exploring ways to combine the factuality fine-tuning with other techniques like RLHF for open-ended dialogue. I created a factuality-tuned version of the Llama-2-Chat model and had some promising initial conversations with far fewer factual inconsistencies. I'm really intrigued by the potential of these automated factuality methods to improve the reliability of large language models without costly human labeling. I made notes on some ideas to extend the approach further, like exploring better reference-free scoring functions and looking into few-shot factuality adaptation for rapidly updating models on new domains. Lots of interesting avenues to pursue!"
}