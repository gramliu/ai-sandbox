{
  "input": "<metadata>\n{\n  \"title\": \"RAGGED: Towards Informed Design of Retrieval Augmented Generation\\n  Systems\",\n  \"abstract\": \"  Retrieval-augmented generation (RAG) greatly benefits language models (LMs)\\nby providing additional context for tasks such as document-based question\\nanswering (DBQA). Despite its potential, the power of RAG is highly dependent\\non its configuration, raising the question: What is the optimal RAG\\nconfiguration? To answer this, we introduce the RAGGED framework to analyze and\\noptimize RAG systems. On a set of representative DBQA tasks, we study two\\nclassic sparse and dense retrievers, and four top-performing LMs in\\nencoder-decoder and decoder-only architectures. Through RAGGED, we uncover that\\ndifferent models suit substantially varied RAG setups. While encoder-decoder\\nmodels monotonically improve with more documents, we find decoder-only models\\ncan only effectively use &lt; 5 documents, despite often having a longer context\\nwindow. RAGGED offers further insights into LMs' context utilization habits,\\nwhere we find that encoder-decoder models rely more on contexts and are thus\\nmore sensitive to retrieval quality, while decoder-only models tend to rely on\\nknowledge memorized during training.\\n\",\n  \"published\": \"2024-03-14T02:26:31Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces the RAGGED framework to analyze and optimize retrieval-augmented generation (RAG) systems. The key findings are:\n\n1. Different language models (LMs) benefit from varied RAG setups. Encoder-decoder models like FLAN can effectively utilize up to 30 retrieved passages, while decoder-only models like LLAMA can only effectively use < 5 passages despite having longer context windows.\n\n2. The differences in context utilization are due to the models' reliance on provided contexts vs. memorized knowledge. Encoder-decoder models rely more on contexts and are more sensitive to retrieval quality, while decoder-only models tend to rely more on their pre-trained knowledge.\n\n3. The quality of the retriever has a larger impact on encoder-decoder models, especially for single-hop questions. Neural retrievers like ColBERT provide significant benefits over sparse retrievers like BM25 for open-domain questions, but the benefits are less pronounced for decoder-only models and multi-hop questions.\n\nKeywords: retrieval-augmented generation, context utilization, retriever quality, language model architecture\n\nExample Questions:\n1. How do the context utilization behaviors of encoder-decoder and decoder-only language models differ, and what are the implications for RAG system design?\n2. Under what conditions do neural retrievers like ColBERT provide the most significant benefits over sparse retrievers like BM25 for RAG systems?\n3. How can the RAGGED framework be used to analyze the performance of new RAG components as they evolve?</text>",
  "output": "Today I worked on exploring the concepts from the research paper on Retrieval Augmented Generation (RAG) systems using the RAGGED framework. Here are the key activities:\n\nIn the morning, I set up my development environment and installed the necessary libraries for working with language models and retrievers. I spent some time going through the code examples provided in the RAGGED framework to understand how to configure different RAG setups.\n\nAfter lunch, I experimented with fine-tuning an encoder-decoder model like FLAN on a document-based question answering task. I tried different settings for the number of retrieved passages and observed how the model's performance varied. As suggested by the paper, FLAN could effectively utilize up to 30 retrieved passages, and its accuracy improved with more context.\n\nIn the afternoon, I switched to working with a decoder-only model like LLAMA on the same task. Surprisingly, despite LLAMA having a longer context window, it could only effectively utilize less than 5 retrieved passages. This aligned with the paper's finding that decoder-only models tend to rely more on their pre-trained knowledge than the provided contexts.\n\nTo better understand the impact of retriever quality, I experimented with using different retrievers like BM25 and ColBERT. As expected, the neural retriever ColBERT provided significant benefits over BM25 for open-domain questions, especially when used with the encoder-decoder model FLAN. However, the benefits were less pronounced for LLAMA and multi-hop questions.\n\nThroughout the day, I kept referring back to the RAGGED framework's analysis and insights to guide my experiments and interpret the results. I made notes on potential improvements or extensions to the framework based on my observations."
}