{
  "input": "<metadata>\n{\n  \"title\": \"StructLM: Towards Building Generalist Models for Structured Knowledge\\n  Grounding\",\n  \"abstract\": \"  Structured data sources, such as tables, graphs, and databases, are\\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\\nutilizing structured data remains limited. Our investigation reveals a notable\\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\\nUtilizing this dataset, we train a series of models, referred to as StructLM,\\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\\nFurthermore, StructLM demonstrates strong generalization across 6 novel\\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\\\% and Flan-UL2\\n20B by an average of 10\\\\%. Contrary to expectations, we observe that scaling\\nmodel size offers marginal benefits, with StructLM-34B showing only slight\\nimprovements over StructLM-7B. This suggests that structured knowledge\\ngrounding is still a challenging task and requires more innovative design to\\npush to a new level.\\n\",\n  \"published\": \"2024-02-26T15:47:01Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThe paper introduces StructLM, a series of large language models (7B to 34B parameters) trained to excel at Structured Knowledge Grounding (SKG) tasks. Key points:\n\n- Motivation: LLMs struggle with SKG tasks compared to specialized models, despite their strong performance on plain text. StructLM aims to build a generalist model for diverse SKG tasks.\n\n- Dataset: The authors curated a 1.1M example dataset covering 18 SKG tasks across structured data types like tables, databases, and knowledge graphs. This dataset was used for instruction-based finetuning.\n\n- Results: StructLM outperforms specialized SKG models on 16 out of 18 held-in tasks, and establishes new state-of-the-art on 8 tasks. It also shows strong zero-shot generalization to 6 novel held-out SKG tasks, outperforming other generalist models like Flan-UL2 and TableLlama.\n\n- Ablations: The authors find that code-pretraining is most beneficial for SKG performance, and that including general instruction-following data helps preserve generalization. They also observe diminishing returns from scaling model size.\n\nKeywords: Structured Knowledge Grounding, Large Language Models, Instruction Tuning, Generalization\n\nExample Questions:\nQ: How does the performance of StructLM compare to specialized SKG models and other generalist LLMs like Flan-UL2 and TableLlama?\nA: StructLM outperforms specialized SKG models on 16 out of 18 held-in tasks, and establishes new state-of-the-art on 8 tasks. It also significantly outperforms Flan-UL2 and TableLlama on the 6 held-out SKG tasks, by 10% and 35% on average respectively.\n\nQ: What factors were found to be most important for StructLM's strong SKG performance?\nA: The authors found that code-pretraining was the most beneficial pretraining regime for SKG tasks, outperforming math-pretraining and the base Llama2 model. They also observed that including general instruction-following data helped preserve StructLM's generalization ability on the held-out tasks.</text>",
  "output": "Today I worked on building a generalist model for structured knowledge grounding called StructLM. The key steps were:\n\n1. I started by curating a large dataset of 1.1 million examples covering 18 different structured knowledge grounding tasks like table question answering, database querying, and knowledge graph reasoning. This dataset spanned various types of structured data formats.\n\n2. Using this dataset, I fine-tuned a series of large language models ranging from 7B to 34B parameters based on the Mistral and CodeLlama architectures. I experimented with different pretraining regimes and found that code pretraining worked best for these structured tasks.\n\n3. I evaluated the StructLM models on the 18 held-in tasks as well as 6 novel held-out structured knowledge grounding tasks. StructLM outperformed specialized models on 16/18 held-in tasks and established new state-of-the-art results on 8 tasks.\n\n4. For the held-out generalization tasks, StructLM significantly outperformed other generalist models like TableLlama and Flan-UL2 by 35% and 10% on average respectively. Interestingly, I found that including general instruction data during finetuning helped preserve strong generalization.\n\n5. In my ablation experiments, I observed diminishing returns from scaling model size beyond 7B parameters for these structured tasks. This suggests there is still room for more innovative modeling approaches to push structured knowledge grounding capabilities further.\n\nOverall, it was an exciting day making progress towards building a powerful generalist model that can flexibly handle diverse structured data formats and tasks! I'm looking forward to exploring StructLM's capabilities further."
}