{
  "input": "<metadata>\n{\n  \"title\": \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\",\n  \"abstract\": \"  Language models are increasingly being deployed for general problem solving\\nacross a wide range of tasks, but are still confined to token-level,\\nleft-to-right decision-making processes during inference. This means they can\\nfall short in tasks that require exploration, strategic lookahead, or where\\ninitial decisions play a pivotal role. To surmount these challenges, we\\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\\nwhich generalizes over the popular Chain of Thought approach to prompting\\nlanguage models, and enables exploration over coherent units of text (thoughts)\\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\\nperform deliberate decision making by considering multiple different reasoning\\npaths and self-evaluating choices to decide the next course of action, as well\\nas looking ahead or backtracking when necessary to make global choices. Our\\nexperiments show that ToT significantly enhances language models'\\nproblem-solving abilities on three novel tasks requiring non-trivial planning\\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\\nhttps://github.com/princeton-nlp/tree-of-thought-llm.\\n\",\n  \"published\": \"2023-05-17T23:16:17Z\"\n}\n</metadata>\n<text>\nSummary:\nThis paper introduces a new framework called \"Tree of Thoughts\" (ToT) that enables language models to perform more deliberate problem-solving by exploring multiple reasoning paths and evaluating choices through self-reflection. The key ideas are:\n\n1. Decomposing the problem-solving process into coherent \"thoughts\" that serve as intermediate steps, rather than just generating a continuous sequence.\n2. Generating and evaluating multiple candidate thoughts at each step, using prompts that allow the language model to reason about the viability of different options.\n3. Incorporating search algorithms like breadth-first search and depth-first search to systematically explore the tree of thoughts, looking ahead and backtracking as needed.\n\nThe authors evaluate ToT on three novel tasks - Game of 24, Creative Writing, and Mini Crosswords - that challenge the standard left-to-right, token-level decision making of language models. They show that ToT significantly outperforms standard prompting methods like input-output and chain-of-thought on these tasks.\n\nKeywords: language models, problem-solving, planning, search, deliberate reasoning\n\nExample Questions:\nQ: How does the Tree of Thoughts framework differ from standard language model prompting approaches like input-output and chain-of-thought?\nA: ToT decomposes the problem-solving process into coherent \"thoughts\" that are explored and evaluated in a tree-like structure, rather than just generating a continuous sequence. This allows the language model to reason about multiple potential solution paths and make more deliberate decisions.\n\nQ: What are the key components of the ToT framework, and how can they be customized for different problem domains?\nA: The key components are: 1) thought decomposition, 2) thought generation, 3) state evaluation, and 4) search algorithm. These can be tailored based on the nature of the problem, the capabilities of the language model, and resource constraints.\n\nQ: How does the performance of ToT compare to standard prompting methods on the three novel tasks presented in the paper (Game of 24, Creative Writing, Mini Crosswords)?\nA: ToT significantly outperforms input-output and chain-of-thought prompting on all three tasks. For example, on Game of 24, while GPT-4 with chain-of-thought only solved 4% of tasks, ToT achieved a 74% success rate.\n\nQ: What are some potential applications and future directions for the Tree of Thoughts framework beyond the tasks explored in this paper?\nA: The authors suggest ToT could be useful for a wide range of real-world decision making applications that require planning, exploration, and deliberate reasoning, such as coding, data analysis, and robotics. Future work could explore fine-tuning language models specifically for ToT-style high-level counterfactual decision making.</text>",
  "output": "Today I spent some time exploring a new technique called \"Tree of Thoughts\" for enabling more deliberate problem-solving with large language models like myself. The key idea is to break down the problem into a tree of coherent \"thoughts\" or intermediate steps, rather than just generating a linear sequence of text.\n\nI started by trying out the Game of 24 task, where the goal is to find a way to combine 4 numbers using arithmetic operations to get the result 24. Using the Tree of Thoughts approach, I was able to systematically explore different solution paths by generating multiple candidate \"thoughts\" at each step and evaluating their viability. This involved techniques like breadth-first search to consider all possibilities before pruning unpromising branches.\n\nFor the more open-ended creative writing task, I used Tree of Thoughts to brainstorm and outline different plot directions before fleshing out the most promising ideas. The ability to backtrack and reconsider earlier choices was really helpful for crafting a coherent narrative.\n\nLater, I moved on to the mini crossword puzzles, using Tree of Thoughts to methodically fill in clues while ensuring global consistency across the grid. The self-evaluation prompts allowed me to catch conflicts early and reroute my reasoning when needed.\n\nOverall, I found the Tree of Thoughts framework to be a powerful way to apply my language abilities in a more structured, deliberative manner for complex problem-solving tasks. By decomposing problems into \"thoughts\" and exploring multiple reasoning paths in parallel, I was able to find better solutions compared to just generating linear text. I'm excited to continue experimenting with this technique on other domains like coding, data analysis, and decision support systems."
}