{
  "input": "<metadata>\n{\n  \"title\": \"Efficient Streaming Language Models with Attention Sinks\",\n  \"abstract\": \"  Deploying Large Language Models (LLMs) in streaming applications such as\\nmulti-round dialogue, where long interactions are expected, is urgently needed\\nbut poses two major challenges. Firstly, during the decoding stage, caching\\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\\npopular LLMs cannot generalize to longer texts than the training sequence\\nlength. Window attention, where only the most recent KVs are cached, is a\\nnatural approach -- but we show that it fails when the text length surpasses\\nthe cache size. We observe an interesting phenomenon, namely attention sink,\\nthat keeping the KV of initial tokens will largely recover the performance of\\nwindow attention. In this paper, we first demonstrate that the emergence of\\nattention sink is due to the strong attention scores towards initial tokens as\\na \\\"sink\\\" even if they are not semantically important. Based on the above\\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\\ntrained with a finite length attention window to generalize to infinite\\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\\nmodeling with up to 4 million tokens and more. In addition, we discover that\\nadding a placeholder token as a dedicated attention sink during pre-training\\ncan further improve streaming deployment. In streaming settings, StreamingLLM\\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\\n\",\n  \"published\": \"2023-09-29T17:59:56Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThe paper introduces StreamingLLM, an efficient framework that enables large language models (LLMs) trained with a finite attention window to perform stable and efficient language modeling on text of infinite length without fine-tuning. The key insights are:\n\n1. LLMs exhibit an \"attention sink\" phenomenon, where they disproportionately focus on the initial tokens in a sequence, even if those tokens are not semantically important. This causes the model's performance to collapse when the initial tokens are evicted from the attention cache.\n\n2. StreamingLLM addresses this by keeping a small number of initial tokens (e.g. 4) as \"attention sinks\" in the attention cache, along with the most recent tokens. This allows the model to maintain stable performance even on extremely long texts.\n\n3. The paper also shows that pre-training LLMs with a dedicated learnable \"sink token\" at the start of each sequence can further improve the model's streaming performance, eliminating the need for multiple initial tokens as attention sinks.\n\nKey results:\n- StreamingLLM enables Llama-2, MPT, Falcon, and Pythia models to reliably model up to 4 million tokens, outperforming baselines.\n- StreamingLLM achieves up to 22.2x speedup over the sliding window recomputation baseline.\n- Pre-training with a dedicated sink token improves streaming performance compared to vanilla models.\n\nThe paper highlights the importance of addressing the attention sink phenomenon to enable efficient deployment of LLMs in streaming applications like multi-round dialogue systems. The techniques proposed can be broadly applied to autoregressive Transformer-based models.\n\nExample Questions:\nQ: How does the attention sink phenomenon in LLMs lead to their performance collapse on long texts?\nA: LLMs disproportionately focus attention on the initial tokens in a sequence, even if those tokens are not semantically important. When the initial tokens are evicted from the attention cache as the text length exceeds the training window, a large portion of the denominator in the softmax attention computation is removed, causing a significant shift in the attention distribution and leading to performance collapse.\n\nQ: How does StreamingLLM address the attention sink issue to enable stable performance on long texts?\nA: StreamingLLM keeps a small number of initial tokens (e.g. 4) as \"attention sinks\" in the attention cache, along with the most recent tokens. This anchors the attention computation and prevents the performance collapse that occurs when the initial tokens are evicted in standard window attention approaches.\n\nQ: How can pre-training LLMs with a dedicated learnable \"sink token\" further improve their streaming performance?\nA: By including a learnable sink token at the start of each training sequence, the model learns to direct the unnecessary attention scores to this dedicated token, rather than inappropriately using the actual content tokens as attention sinks. This eliminates the need for multiple initial tokens as attention sinks during inference, further enhancing the model's streaming capabilities.</text>",
  "output": "I started my day exploring the new StreamingLLM framework that enables efficient deployment of large language models like Llama-2 and MPT for streaming applications like multi-round dialogue systems. The key innovation is addressing the \"attention sink\" phenomenon, where the models disproportionately focus on the initial tokens even if they are not semantically important.\n\nFirst, I set up the StreamingLLM codebase and tried out the example notebooks for streaming inference on long texts. I was amazed to see the Llama-2 model reliably generate coherent text up to 4 million tokens in length! The attention cache management techniques like keeping a few initial tokens as \"sinks\" really made a huge difference.\n\nInspired by the paper's findings on using a dedicated \"sink token\" during pre-training, I spent the afternoon modifying the language modeling pipeline to incorporate this idea. I added a special [SINK] token to the start of each training sequence and updated the loss masking to encourage the model to direct unnecessary attention to this token. After a few epochs of pre-training on a smaller dataset, I could already see promising results - the model seemed more stable on long prompts without needing the initial token caching hacks.\n\nIn the evening, I joined the project's Discord server and discussed potential applications of StreamingLLM with other developers. Some really exciting ideas came up, like using it to enable long-form question-answering over entire books or documents. I'm looking forward to exploring this space further and maybe even publishing an extension of the StreamingLLM work! The possibilities of deploying powerful language models in streaming, interactive settings seem endless."
}