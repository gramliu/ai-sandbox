{
  "input": "<metadata>\n{\n  \"title\": \"Corrective Retrieval Augmented Generation\",\n  \"abstract\": \"  Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.\\n\",\n  \"published\": \"2024-01-29T04:36:39Z\"\n}\n</metadata>\n<text>\nHere is a summary of the key points from the paper:\n\nHighlights:\n- The paper introduces Corrective Retrieval Augmented Generation (CRAG), a method to improve the robustness of retrieval-augmented generation (RAG) approaches.\n- CRAG uses a lightweight retrieval evaluator to assess the quality of retrieved documents and trigger different knowledge retrieval actions (Correct, Incorrect, Ambiguous).\n- For Incorrect retrievals, CRAG leverages large-scale web searches to complement the limited corpus.\n- CRAG also uses a decompose-then-recompose algorithm to selectively focus on key information in retrieved documents.\n- Experiments show CRAG can significantly improve the performance of standard RAG and state-of-the-art Self-RAG across short-form and long-form generation tasks.\n\nKeywords:\n- Retrieval-augmented generation (RAG)\n- Hallucination\n- Retrieval evaluator\n- Knowledge refinement\n- Web search\n\nExample Questions:\nQ: How does the performance of phi-3-mini compare to state-of-the-art large language models when evaluated on standard academic NLP benchmarks?\nA: Phi-3-mini achieves performance rivaling much larger models like Mixtral 8x7B and GPT-3.5 on academic benchmarks, demonstrating the power of carefully curated training data to enable high quality in compact models.\n\nQ: What techniques were used to ensure phi-3-mini behaves in a safe, robust, and responsible manner during open-ended interactions with humans?\nA: The phi-3-mini model underwent an iterative process of safety alignment, red-teaming, and extensive automated testing to mitigate issues around factual inaccuracies, bias, inappropriate content, and safety, though challenges still remain in fully solving these problems.</text>",
  "output": "ately, the provided summary does not seem to be related to the paper titled \"Corrective Retrieval Augmented Generation\". The summary appears to be discussing a different language model called \"phi-3-mini\" and its performance on academic benchmarks, safety, and robustness. It does not cover the key points about the Corrective Retrieval Augmented Generation (CRAG) method described in the abstract. I cannot provide a relevant coding activity summary based on this mismatch between the abstract and summary. Please provide the correct summary for the \"Corrective Retrieval Augmented Generation\" paper"
}