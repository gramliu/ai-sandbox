{
  "input": "<metadata>\n{\n  \"title\": \"Textbooks Are All You Need II: phi-1.5 technical report\",\n  \"abstract\": \"  We continue the investigation into the power of smaller Transformer-based\\nlanguage models as initiated by \\\\textbf{TinyStories} -- a 10 million parameter\\nmodel that can produce coherent English -- and the follow-up work on\\n\\\\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance\\nclose to the state-of-the-art. The latter work proposed to use existing Large\\nLanguage Models (LLMs) to generate ``textbook quality\\\" data as a way to enhance\\nthe learning process compared to traditional web data. We follow the\\n``Textbooks Are All You Need\\\" approach, focusing this time on common sense\\nreasoning in natural language, and create a new 1.3 billion parameter model\\nnamed \\\\textbf{phi-1.5}, with performance on natural language tasks comparable\\nto models 5x larger, and surpassing most non-frontier LLMs on more complex\\nreasoning tasks such as grade-school mathematics and basic coding. More\\ngenerally, \\\\textbf{phi-1.5} exhibits many of the traits of much larger LLMs,\\nboth good -- such as the ability to ``think step by step\\\" or perform some\\nrudimentary in-context learning -- and bad, including hallucinations and the\\npotential for toxic and biased generations -- encouragingly though, we are\\nseeing improvement on that front thanks to the absence of web data. We\\nopen-source \\\\textbf{phi-1.5} to promote further research on these urgent\\ntopics.\\n\",\n  \"published\": \"2023-09-11T14:01:45Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces phi-1.5, a 1.3 billion parameter language model that achieves performance comparable to much larger models on common sense reasoning, language understanding, and multi-step reasoning tasks. The key innovation is the use of a carefully curated dataset of synthetic, \"textbook-quality\" data, combined with a small amount of filtered web data, to train the model. \n\nThe results show that phi-1.5 performs on par with 5-10x larger models on benchmarks like MMLU, HellaSwag, and GSM8K, demonstrating the power of high-quality training data over pure model scale. The authors also discuss the potential benefits of the synthetic data in mitigating issues like toxic and biased content generation, which are still challenges for the model.\n\nThe authors open-source phi-1.5 to enable further research on important topics like in-context learning, interpretability, and safety in large language models. They suggest that achieving ChatGPT-level capabilities at the 1 billion parameter scale may be feasible with the right data and techniques.\n\nKeywords:\n- Compact language models\n- Synthetic training data\n- Common sense reasoning\n- Multi-step reasoning\n- Responsible AI\n\nExample Questions:\nQ: How does the performance of phi-1.5 compare to state-of-the-art large language models on common sense reasoning benchmarks?\nA: phi-1.5 achieves comparable or better performance than 5-10x larger models like Llama 7B and Vicuna 13B on common sense reasoning tasks like WinoGrande, ARC-Easy, and ARC-Challenge.\n\nQ: What are the potential benefits of using synthetic, \"textbook-quality\" data to train language models like phi-1.5?\nA: The synthetic data appears to help mitigate issues like toxic and biased content generation, which are still challenges for models trained on web data. The authors suggest the textbook-like data leads to more controllable and governable language models.\n\nQ: How might the open-sourcing of phi-1.5 contribute to research on important topics in large language models?\nA: The authors propose that phi-1.5's unique properties, particularly its reliance on synthetic data, make it a useful platform for exploring issues like in-context learning, interpretability, and safety - areas that are critical for the development of more robust and responsible AI systems.</text>",
  "output": "Today I worked on exploring the new phi-1.5 language model released by OpenAI. This compact 1.3 billion parameter model achieves impressive performance on common sense reasoning and multi-step problem-solving tasks, comparable to much larger models.\n\nIn the morning, I read through the technical report and tried out some examples on the model's hosted demo. I was amazed at its ability to break down complex problems step-by-step and provide coherent explanations. For example, when given a grade school math word problem, phi-1.5 could methodically walk through understanding the problem, identifying the key information, and applying the appropriate operations to derive the solution.\n\nAfter lunch, I spent some time fine-tuning phi-1.5 on a dataset of coding exercises to see how it could assist with programming tasks. The results were quite promising - the model could not only generate code snippets, but also explain the logic behind the implementations. I could see this being an invaluable tool for learning to code and debugging.\n\nIn the evening, I explored some of the safety and interpretability aspects of phi-1.5. Since it was trained primarily on curated data rather than web crawl, it exhibited less toxic behavior and seemed more controllable than some larger language models. However, I did still encounter some instances of hallucinations and biased outputs, highlighting the need for further work in this area. I'm excited to dig deeper into the open-source model and contribute to improving the safety and trustworthiness of these systems."
}