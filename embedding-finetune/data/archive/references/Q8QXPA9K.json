{
  "input": "<metadata>\n{\n  \"title\": \"Vision Transformers Need Registers\",\n  \"abstract\": \"  Transformers have recently emerged as a powerful tool for learning visual\\nrepresentations. In this paper, we identify and characterize artifacts in\\nfeature maps of both supervised and self-supervised ViT networks. The artifacts\\ncorrespond to high-norm tokens appearing during inference primarily in\\nlow-informative background areas of images, that are repurposed for internal\\ncomputations. We propose a simple yet effective solution based on providing\\nadditional tokens to the input sequence of the Vision Transformer to fill that\\nrole. We show that this solution fixes that problem entirely for both\\nsupervised and self-supervised models, sets a new state of the art for\\nself-supervised visual models on dense visual prediction tasks, enables object\\ndiscovery methods with larger models, and most importantly leads to smoother\\nfeature maps and attention maps for downstream visual processing.\\n\",\n  \"published\": \"2023-09-28T16:45:46Z\"\n}\n</metadata>\n<text>\nHere is a summary of the key points from the paper:\n\n**Key Findings:**\n- Vision Transformers (ViTs) exhibit artifacts in their feature maps and attention maps, in the form of high-norm \"outlier\" tokens that appear primarily in low-informative background areas.\n- These outlier tokens are repurposed by the model to aggregate global image information, while discarding local spatial information.\n- The outlier tokens appear in the middle layers of sufficiently large and trained ViT models, and are not present in smaller or earlier-stage models.\n- The outlier tokens contain less information about their original patch position and pixel values, but more global information about the image.\n\n**Proposed Solution:**\n- The authors propose adding additional \"register\" tokens to the input sequence of the ViT, which the model can use to store and retrieve global information, without corrupting the local patch features.\n- This simple architectural change completely removes the outlier tokens, leading to smoother feature maps and attention maps.\n- The models with registers also achieve improved performance on dense prediction tasks and enable better object discovery methods.\n\n**Keywords:**\n- Vision Transformers\n- Feature map artifacts\n- Attention map artifacts\n- Register tokens\n- Global vs local information\n- Object discovery\n\n**Example Questions:**\nQ: How do the authors detect and characterize the artifacts present in Vision Transformer feature maps?\nA: The authors observe that the artifacts correspond to tokens with much higher norm values at the output of the ViT model. They analyze the properties of these high-norm \"outlier\" tokens and find that they contain less local information about their patch position and pixels, but more global information about the image.\n\nQ: What is the key insight behind the authors' proposed solution of adding \"register\" tokens to the ViT input?\nA: The authors hypothesize that the ViT model naturally learns to repurpose some tokens to store and retrieve global image information, at the expense of discarding local spatial information in those tokens. By explicitly providing additional \"register\" tokens, the model can isolate this global information aggregation behavior, leading to smoother feature maps without artifacts.\n\nQ: How does the addition of register tokens impact the performance of ViT models on dense prediction tasks and unsupervised object discovery?\nA: The authors show that adding register tokens improves the performance of ViT models on dense prediction tasks like semantic segmentation and monocular depth estimation. It also enables much better results on unsupervised object discovery methods like LOST, which rely on the smoothness of the ViT feature maps.</text>",
  "output": "Today I worked on implementing the \"register tokens\" concept from the recent \"Vision Transformers Need Registers\" paper. The key idea is to provide additional tokens to the input sequence of a Vision Transformer model, which the model can use to store and retrieve global information about the image.\n\nIn the morning, I read through the paper and the official code release to understand the proposed architecture changes. I then spent some time modifying the input pipeline of my existing Vision Transformer codebase to add the register tokens to the input sequence.\n\nAfter lunch, I trained the modified Vision Transformer model on the ImageNet dataset, using the same hyperparameters as my previous baseline model. I monitored the training process and noticed that the model with register tokens converged slightly faster than the baseline.\n\nIn the late afternoon, I evaluated the trained model on the ImageNet validation set and compared the results to my baseline model. As reported in the paper, I observed a small but consistent improvement in top-1 accuracy with the register token model. More importantly, when visualizing the attention maps, I could clearly see the absence of the high-norm \"outlier\" tokens that plagued the baseline model.\n\nTo wrap up the day, I spent some time experimenting with the pre-trained register token model on a few dense prediction tasks like semantic segmentation. I was able to replicate the improved performance reported in the paper. Overall, I'm quite impressed with how such a simple architectural change can lead to smoother and more interpretable representations from Vision Transformers. I plan to incorporate this technique into my future Vision Transformer projects."
}