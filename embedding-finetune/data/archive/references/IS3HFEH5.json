{
  "input": "<metadata>\n{\n  \"title\": \"Internet Explorer: Targeted Representation Learning on the Open Web\",\n  \"abstract\": \"  Modern vision models typically rely on fine-tuning general-purpose models\\npre-trained on large, static datasets. These general-purpose models only\\ncapture the knowledge within their pre-training datasets, which are tiny,\\nout-of-date snapshots of the Internet -- where billions of images are uploaded\\neach day. We suggest an alternate approach: rather than hoping our static\\ndatasets transfer to our desired tasks after large-scale pre-training, we\\npropose dynamically utilizing the Internet to quickly train a small-scale model\\nthat does extremely well on the task at hand. Our approach, called Internet\\nExplorer, explores the web in a self-supervised manner to progressively find\\nrelevant examples that improve performance on a desired target dataset. It\\ncycles between searching for images on the Internet with text queries,\\nself-supervised training on downloaded images, determining which images were\\nuseful, and prioritizing what to search for next. We evaluate Internet Explorer\\nacross several datasets and show that it outperforms or matches CLIP oracle\\nperformance by using just a single GPU desktop to actively query the Internet\\nfor 30--40 hours. Results, visualizations, and videos at\\nhttps://internet-explorer-ssl.github.io/\\n\",\n  \"published\": \"2023-02-27T18:59:55Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces \"Internet Explorer\", a method for efficiently improving representations for a target dataset by actively searching the open web for relevant training data. The key innovations are:\n\n1. Treating the Internet as a dynamic, open-ended dataset that can be queried as needed, rather than relying on static, curated datasets.\n2. Using a self-supervised agent that cycles between searching for relevant images on the web, training on the downloaded data, and updating its search strategy to focus on the most useful concepts.\n3. Leveraging text-based search queries combined with GPT-generated descriptors to efficiently explore a large vocabulary of visual concepts.\n4. Using a Gaussian process to estimate the relevance of unseen concepts, enabling rapid identification of useful search terms.\n\nExperiments show that Internet Explorer can outperform or match the performance of much larger, pre-trained models like CLIP on a variety of datasets, while using only a single GPU for 30-40 hours and downloading around 1 million relevant images. The method is also shown to be effective when searching other data sources like Flickr and a custom LAION-5B search engine.\n\nKeywords: self-supervised learning, open-ended data collection, targeted representation learning, Gaussian processes\n\nExample Questions:\n1. How does Internet Explorer's approach of dynamically searching the web for relevant training data differ from the standard practice of relying on large, static datasets for pre-training?\n2. What are the key components of the Internet Explorer method that enable it to efficiently identify and download useful images from the web, without access to labeled data?\n3. How does Internet Explorer's use of Gaussian processes to estimate the relevance of unseen concepts help accelerate the discovery of useful search terms?\n4. In what ways does Internet Explorer's performance compare to that of large, pre-trained models like CLIP across the evaluated datasets? What factors contribute to its strong performance?\n5. How might the ability to run a highly capable language model like Internet Explorer's on-device change the development and deployment of AI assistants in the future? (no_answer)</text>",
  "output": "Today I worked on exploring a new approach to representation learning called \"Internet Explorer\". The key idea is to dynamically search the open web for relevant images to train a model for a specific target dataset, rather than relying on static, pre-curated datasets.\n\nIn the morning, I set up the Internet Explorer codebase and read through the paper to understand the main components:\n\n1. A search agent that cycles between querying the web for images, training a self-supervised model on the downloaded data, and updating its search strategy based on what images were useful.\n\n2. Using text-based search queries combined with GPT-generated descriptors to efficiently explore a wide range of visual concepts on the web.\n\n3. A Gaussian process model to estimate the relevance of unseen concepts and prioritize promising search terms.\n\nAfter lunch, I ran some initial experiments on the CIFAR-10 dataset. I defined a set of seed search terms related to the classes, and let Internet Explorer run for a few hours, actively searching, downloading, and training on relevant web images.\n\nIt was really cool to see the model's performance improve over time as it discovered more useful visual concepts and examples from the open web. Even with just a few hours of searching and a single GPU, it was able to match the accuracy of the CLIP baseline by the end.\n\nIn the evening, I set up a longer run on the ImageNet dataset to let it search overnight. I'm excited to analyze the results tomorrow and see what kind of performance gains are possible with more compute. The idea of tapping into the open web as a virtually unlimited source of training data is really powerful."
}