{
  "input": "<metadata>\n{\n  \"title\": \"Large Language Models as Optimizers\",\n  \"abstract\": \"  Optimization is ubiquitous. While derivative-based algorithms have been\\npowerful tools for various problems, the absence of gradient imposes challenges\\non many real-world applications. In this work, we propose Optimization by\\nPROmpting (OPRO), a simple and effective approach to leverage large language\\nmodels (LLMs) as optimizers, where the optimization task is described in\\nnatural language. In each optimization step, the LLM generates new solutions\\nfrom the prompt that contains previously generated solutions with their values,\\nthen the new solutions are evaluated and added to the prompt for the next\\noptimization step. We first showcase OPRO on linear regression and traveling\\nsalesman problems, then move on to our main application in prompt optimization,\\nwhere the goal is to find instructions that maximize the task accuracy. With a\\nvariety of LLMs, we demonstrate that the best prompts optimized by OPRO\\noutperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on\\nBig-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.\\n\",\n  \"published\": \"2023-09-07T00:07:15Z\"\n}\n</metadata>\n<text>\nSummary:\n\nThis paper introduces OPRO (Optimization by PROmpting), a novel approach to leverage large language models (LLMs) as optimizers. The key idea is to describe the optimization task in natural language, and then have the LLM iteratively generate new solutions based on the prompt containing the previous solutions and their scores.\n\nThe paper first showcases OPRO on linear regression and traveling salesman problems, demonstrating that LLMs can optimize these tasks simply through prompting, sometimes matching or surpassing specialized algorithms. \n\nThe main application of OPRO is prompt optimization, where the goal is to find prompts that maximize the task accuracy on natural language benchmarks. The authors show that OPRO can significantly outperform human-designed prompts, improving accuracy by up to 8% on GSM8K and up to 50% on Big-Bench Hard tasks. This is achieved by having the LLM leverage the optimization trajectory, where past prompts and their scores are used to guide the generation of new, higher-performing prompts.\n\nThe paper also provides detailed ablation studies on the key components of the meta-prompt design, and analyzes the potential overfitting issue in prompt optimization. Comparisons to concurrent work on prompt optimization, such as EvoPrompt, are also presented.\n\nOverall, this work demonstrates the potential of using LLMs as general-purpose optimizers, beyond their traditional applications in language tasks. The ability to optimize prompts through natural language interaction opens up new possibilities for making advanced AI capabilities more accessible.\n\nKeywords: large language models, optimization, prompt engineering, prompt optimization\n\nExample Questions:\nQ: How does OPRO leverage the optimization trajectory to guide the LLM in generating better prompts over time?\nA: OPRO includes the past prompts and their scores in the meta-prompt, allowing the LLM to identify patterns in high-performing prompts and build upon them to generate new, improved prompts. This is in contrast to approaches that only consider a single prompt at a time.\n\nQ: What are some of the key design choices in the meta-prompt that were found to be important for the effectiveness of OPRO in prompt optimization?\nA: The ablation studies show that factors like the order of previous prompts, the inclusion of accuracy scores, and the number of exemplars in the meta-prompt can significantly impact the optimization performance. For example, presenting the prompts in ascending order of scores, and including the accuracy scores, were found to be important for the LLM to effectively exploit the optimization trajectory.\n\nQ: How does OPRO's approach to prompt optimization differ from concurrent work like EvoPrompt, and what are the advantages of OPRO's approach?\nA: Unlike EvoPrompt, which relies on explicit instructions for mutation and crossover of prompts, OPRO allows the LLM to directly generate new prompts based on the optimization trajectory. This gives the LLM more flexibility to discover patterns and generate prompts that improve upon the past ones, without being constrained by predefined operations. OPRO also provides richer information in the meta-prompt, such as exemplars and accuracy scores, which was shown to be beneficial for the LLM's optimization performance.</text>",
  "output": "Today I spent some time exploring the idea of using large language models as general optimizers through natural language prompting. After reading the OPRO paper, I was intrigued by the potential of this approach and decided to try it out on a few coding challenges.\n\nFirst, I tackled the classic traveling salesman problem. I described the problem and some example city distances in natural language, and prompted the language model to generate an initial solution tour. I then evaluated the tour length and added that information back into the prompt, asking the model to improve upon the previous solution. After a few iterations of this process, the model was able to find a near-optimal tour that was competitive with specialized solvers.\n\nNext, I moved on to a more open-ended task of optimizing the hyperparameters for a machine learning model I was training. I prompted the language model with the model architecture, dataset details, and an initial set of hyperparameter values and validation performance. The model then suggested new hyperparameter configurations, which I evaluated and incorporated back into the prompt. This iterative process allowed me to quickly explore the hyperparameter space and find a well-performing configuration.\n\nFinally, I tried using OPRO for prompt optimization itself. I tasked the language model with generating prompts to solve a few natural language processing tasks from the Big-Bench benchmark. By including the previous prompts and their scores in the meta-prompt, the model was able to learn from its own trajectory and generate increasingly better prompts over time. The best prompts found through this process significantly outperformed the hand-crafted prompts I had initially tried.\n\nOverall, I was impressed by the versatility and effectiveness of the OPRO approach. Being able to leverage the language model's capabilities for optimization tasks through natural language interaction opens up many exciting possibilities. I'm looking forward to exploring this technique further and finding more creative applications for it in my coding projects."
}