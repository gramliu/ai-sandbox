{
  "note": "I started my day exploring the new factuality fine-tuning techniques for large language models described in the paper. First, I set up the automated factuality scoring systems - both the reference-based approach using Wikipedia retrieval and the novel reference-free approach that uses the model's own confidence scores. I generated some sample text on various topics using the base Llama-2 model and ran the factuality scorers to get preference rankings between more and less factual outputs.\n\nWith the preference data in hand, I fine-tuned a copy of the Llama-2 model using the Direct Preference Optimization algorithm, creating a new \"factuality-tuned\" version of the model. To test it out, I had the original and fine-tuned models generate biographies for historical figures and answer medical questions. I was impressed to see the factuality-tuned model make far fewer factual errors and hallucinations compared to the base model!\n\nExcited by these results, I spent the afternoon exploring ways to combine the factuality fine-tuning with other techniques like RLHF for open-ended dialogue. I created a factuality-tuned version of the Llama-2-Chat model and had some promising initial conversations with far fewer factual inconsistencies. I'm really intrigued by the potential of these automated factuality methods to improve the reliability of large language models without costly human labeling. I made notes on some ideas to extend the approach further, like exploring better reference-free scoring functions and looking into few-shot factuality adaptation for rapidly updating models on new domains. Lots of interesting avenues to pursue!",
  "references": [
    "2HKPJFBN"
  ]
}