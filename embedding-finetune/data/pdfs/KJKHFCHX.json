{
  "key": "KJKHFCHX",
  "url": "https://arxiv.org/pdf/2401.04088v1",
  "metadata": {
    "title": "Mixtral of Experts",
    "abstract": "  We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.\n",
    "published": "2024-01-08T18:47:34Z"
  },
  "text": [
    "Mixtral of Experts\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,\nBlanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour,\nGuillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,\nPierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao,\nThéophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed\nAbstract\nWe  introduce  Mixtral  8x7B,  a  Sparse  Mixture  of  Experts  (SMoE)  language\nmodel.   Mixtral  has  the  same  architecture  as  Mistral  7B,  with  the  difference\nthat each layer is composed of 8 feedforward blocks (i.e.  experts).  For every\ntoken, at each layer, a router network selects two experts to process the current\nstate and combine their outputs.  Even though each token only sees two experts,\nthe selected experts can be different at each timestep.  As a result, each token\nhas  access  to  47B  parameters,  but  only  uses  13B  active  parameters  during\ninference. Mixtral was trained with a context size of 32k tokens and it outperforms\nor  matches  Llama  2  70B  and  GPT-3.5  across  all  evaluated  benchmarks.   In\nparticular,  Mixtral  vastly  outperforms  Llama  2  70B  on  mathematics,  code\ngeneration,  and  multilingual  benchmarks.We  also  provide  a  model  fine-\ntuned to follow instructions,  Mixtral 8x7B – Instruct,  that surpasses GPT-3.5\nTurbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human bench-\nmarks. Both the base and instruct models are released under the Apache 2.0 license.\nCode:https://github.com/mistralai/mistral-src\nWebpage:https://mistral.ai/news/mixtral-of-experts/\n1    Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights,\nlicensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As\nit only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low\nbatch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward\nblock picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router\nnetwork chooses two of these groups (the “experts”) to process the token and combine their output\nadditively. This technique increases the number of parameters of a model while controlling cost and\nlatency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens.  It either matches\nor exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\narXiv:2401.04088v1  [cs.LG]  8 Jan 2024",
    "Figure 1:  Mixture of Experts Layer.Each input vector is assigned to 2 of the 8 experts by a router.  The\nlayer’s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard\nfeedforward block as in a vanilla transformer architecture.\nMixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require\nmultilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments\nshow that Mixtral is able to successfully retrieve information from its context window of 32k tokens,\nregardless of the sequence length and the location of the information in the sequence.\nWe  also  present  Mixtral  8x7B  –  Instruct,  a  chat  model  fine-tuned  to  follow  instructions  using\nsupervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses\nthat of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human evaluation\nbenchmarks. Mixtral – Instruct also demonstrates reduced biases, and a more balanced sentiment\nprofile in benchmarks such as BBQ, and BOLD.\nWe release both Mixtral 8x7B and Mixtral 8x7B – Instruct under the Apache 2.0 license\n1\n, free for\nacademic and commercial usage, ensuring broad accessibility and potential for diverse applications.\nTo enable the community to run Mixtral with a fully open-source stack, we submitted changes to\nthe vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also\nallows the deployment of vLLM endpoints on any instance in the cloud.\n2    Architectural details\nParameterValue\ndim4096\nn_layers32\nhead_dim128\nhidden_dim14336\nn_heads32\nn_kv_heads8\ncontext_len32768\nvocab_size32000\nnum_experts8\ntop_k_experts2\nTable 1: Model architecture.\nMixtral is based on a transformer architecture [31] and uses the same\nmodifications as described in [18], with the notable exceptions that Mix-\ntral supports a fully dense context length of 32k tokens, and the feed-\nforward blocks are replaced by Mixture-of-Expert layers (Section 2.1).\nThe model architecture parameters are summarized in Table 1.\n2.1    Sparse Mixture of Experts\nWe present a brief overview of the Mixture of Experts layer (Figure 1).\nFor a more in-depth overview, see [12]. The output of the MoE module\nfor a given inputxis determined by the weighted sum of the outputs\nof the expert networks,  where the weights are given by the gating\nnetwork’s output. i.e. givennexpert networks{E\n0\n, E\ni\n, ..., E\nn−1\n}, the\noutput of the expert layer is given by:\nn−1\nX\ni=0\nG(x)\ni\n·E\ni\n(x).\nHere,G(x)\ni\ndenotes then-dimensional output of the gating network for thei-th expert, andE\ni\n(x)\nis the output of thei-th expert network.  If the gating vector is sparse, we can avoid computing\nthe outputs of experts whose gates are zero. There are multiple alternative ways of implementing\nG(x)[6,15,35], but a simple and performant one is implemented by taking the softmax over the\nTop-K logits of a linear layer [28]. We use\nG(x) :=Softmax(TopK(x·W\ng\n)),\nwhere(TopK(ℓ))\ni\n:=ℓ\ni\nifℓ\ni\nis among the top-K coordinates of logitsℓ∈R\nn\nand(TopK(ℓ))\ni\n:=−∞\notherwise. The value of K – the number of experts used per token – is a hyper-parameter that modu-\nlates the amount of compute used to process each token. If one increasesnwhile keepingKfixed, one\n1\nhttps://mistral.ai/news/mixtral-of-experts/\n2",
    "can increase the model’s parameter count while keeping its computational cost effectively constant.\nThis motivates a distinction between the model’s total parameter count (commonly referenced as the\nsparseparameter count), which grows withn, and the number of parameters used for processing an\nindividual token (called theactiveparameter count), which grows withKup ton.\nMoE layers can be run efficiently on single GPUs with high performance specialized kernels. For\nexample, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large\nsparse matrix multiplications, significantly enhancing the execution speed and naturally handling\ncases where different experts get a variable number of tokens assigned to them.  Moreover, the\nMoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and\nthrough a particular kind of partitioning strategy called Expert Parallelism (EP) [28]. During the MoE\nlayer’s execution, tokens meant to be processed by a specific expert are routed to the corresponding\nGPU for processing, and the expert’s output is returned to the original token location. Note that EP\nintroduces challenges in load balancing, as it is essential to distribute the workload evenly across the\nGPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\nIn  a  Transformer  model,  the  MoE  layer  is  applied  independently  per  token  and  replaces  the\nfeed-forward (FFN) sub-block of the transformer block.  For Mixtral we use the same SwiGLU\narchitecture as the expert functionE\ni\n(x)and setK= 2.  This means each token is routed to two\nSwiGLU sub-blocks with different sets of weights. Taking this all together, the outputyfor an input\ntokenxis computed as:\ny=\nn−1\nX\ni=0\nSoftmax(Top2(x·W\ng\n))\ni\n·SwiGLU\ni\n(x).\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all\nFFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a\nmore elaborate gating strategy for the second expert assigned to each token.\n3    Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair\ncomparison. We measure performance on a wide variety of tasks categorized as follow:\n•Commonsense Reasoning (0-shot):Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27],\nOpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\n•World Knowledge (5-shot):NaturalQuestions [20], TriviaQA [19]\n•Reading Comprehension (0-shot):BoolQ [7], QuAC [5]\n•Math:GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4\n•Code:Humaneval [4] (0-shot) and MBPP [1] (3-shot)\n•Popular  aggregated  results:MMLU  [16]  (5-shot),  BBH  [29]  (3-shot),  and  AGI  Eval  [34]\n(3-5-shot, English multiple-choice questions only)\nFigure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models\nwere re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or\nmatches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\n3",
    "Model\nActive\nParams\nMMLU   HellaS    WinoGPIQAArc-eArc-cNQTriQAHumanEMBPPMath    GSM8K\nLLaMA 2 7B7B44.4%77.1%69.5%77.9%68.7%43.2%17.5%56.6%11.6%26.1%3.9%16.0%\nLLaMA 2 13B13B55.6%80.7%72.9%80.8%75.2%48.8%16.7%64.0%18.9%35.4%6.0%34.3%\nLLaMA 1 33B33B56.8%83.7%76.2%82.2%79.6%54.4%24.1%68.5%25.0%40.9%8.4%44.1%\nLLaMA 2 70B70B69.9%85.4%80.4%82.6%79.9%56.5%25.4%73.0%29.3%49.8%13.8%69.6%\nMistral 7B7B62.5%81.0%74.2%82.2%80.5%54.9%23.2%62.5%26.2%50.2%12.7%50.0%\nMixtral 8x7B13B70.6%84.4%77.2%83.6%83.1%59.7%30.6%71.5%40.2%60.7%28.4%74.4%\nTable 2: Comparison of Mixtral with Llama.Mixtral outperforms or matches Llama 2 70B performance on\nalmost all popular benchmarks while using 5x fewer active parameters during inference.\nFigure 3:  Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,\nmath and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B\non all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It\nis also vastly superior to Llama 2 70B on code and math.\nDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B\n2\nare reported\nin Table 2.   Figure 2 compares the performance of Mixtral with the Llama models in different\ncategories.  Mixtral surpasses Llama 2 70B across most metrics.  In particular, Mixtral displays a\nsuperior performance in code and mathematics benchmarks.\nSize and Efficiency.We compare our performance to the Llama 2 family, aiming to understand\nMixtral models’ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture-\nof-Experts model, Mixtral only uses 13B active parameters for each token.  With 5x lower active\nparameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly\nproportional to the inference compute cost, but does not consider the memory costs and hardware\nutilization.  The memory costs for serving Mixtral are proportional to itssparseparameter count,\n47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer\nintroduces additional overhead due to the routing mechanism and due to the increased memory loads\nwhen running more than one expert per device. They are more suitable for batched workloads where\none can reach a good degree of arithmetic intensity.\nComparison with Llama 2 70B and GPT-3.5.In Table 3, we report the performance of Mixtral 8x7B\ncompared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the\ntwo other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller\ncapacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest\nGPT-3.5-Turbo model available,gpt-3.5-turbo-1106.\n2\nSince Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
    "LLaMA 2 70BGPT-3.5Mixtral 8x7B\nMMLU\n(MCQ in 57 subjects)\n69.9%70.0%70.6%\nHellaSwag\n(10-shot)\n87.1%85.5%86.7%\nARC Challenge\n(25-shot)\n85.1%85.2%85.8%\nWinoGrande\n(5-shot)\n83.2%81.6%81.2%\nMBPP\n(pass@1)\n49.8%52.2%60.7%\nGSM-8K\n(5-shot)\n53.6%57.1%58.4%\nMT Bench\n(for Instruct Models)\n6.868.328.30\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5.Mixtral outperforms or matches Llama 2\n70B and GPT-3.5 performance on most metrics.\nEvaluation Differences.On some benchmarks, there are some differences between our evaluation\nprotocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\non TriviaQA, we do not provide Wikipedia contexts.\n3.1    Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during\npretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while\nmaintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B\nin French, German, Spanish, and Italian, as shown in Table 4.\nActive\nParams\nFrenchGermanSpanishItalian\nModelArc-c    HellaS   MMLUArc-cHellaS    MMLUArc-cHellaS    MMLUArc-c   HellaS   MMLU\nLLaMA 1 33B33B39.3%    68.1%49.9%41.1%63.3%48.7%45.7%69.8%52.3%42.9%   65.4%    49.0%\nLLaMA 2 70B70B49.9%    72.5%64.3%47.3%68.7%64.2%50.5%74.5%66.0%49.4%   70.9%    65.1%\nMixtral 8x7B13B58.2%   77.4%    70.9%54.3%    73.0%71.5%55.4%    77.6%72.5%52.8%  75.1%   70.9%\nTable 4: Comparison of Mixtral with Llama on Multilingual Benchmarks.On ARC Challenge, Hellaswag,\nand MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n3.2    Long range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval\ntask introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a\npasskey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a\n100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.\nFigure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases\nmonotonically as the size of the context increases.\nFigure 4: Long range performance of Mixtral.(Left) Mixtral has 100% retrieval accuracy of the Passkey task\nregardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on\nthe proof-pile dataset decreases monotonically as the context length increases.\n5",
    "3.3    Bias Benchmarks\nLlama 2 70BMixtral 8x7B\nBBQ accuracy51.5%56.0%\nBOLD sentiment score (avg±std)\ngender0.293±0.0730.323±0.045\nprofession0.218±0.0730.243±0.087\nreligious_ideology0.188±0.1330.144±0.089\npolitical_ideology0.149±0.1400.186±0.146\nrace0.232±0.0490.232±0.052\nFigure  5:  Bias  Benchmarks.Compared  Llama  2  70B,\nMixtral presents less bias (higher accuracy on BBQ, lower\nstd on BOLD) and displays more positive sentiment (higher\navg on BOLD).\nTo identify possible flaws to be corrected\nby fine-tuning / preference modeling,  we\nmeasure  the  base  model  performance  on\nBias Benchmark for QA (BBQ) [24] and\nBias in Open-Ended Language Generation\nDataset  (BOLD)  [10].   BBQ  is  a  dataset\nof  hand-written  question  sets  that  target\nattested  social  biases  against  nine  differ-\nent  socially-relevant  categories:  age,  dis-\nability status,  gender identity,  nationality,\nphysical appearance, race/ethnicity, religion,\nsocio-economic status,  sexual orientation.\nBOLD is a large-scale dataset that consists\nof 23,679 English text generation prompts\nfor bias benchmarking across five domains.\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report\nthe results in Table 5.  Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark\n(56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive\nsentiments and a lower standard deviation indicates less bias within the group.  Overall, Mixtral\ndisplays more positive sentiments than Llama 2, with similar variances within each group.\n4    Instruction Fine-tuning\nWe train Mixtral – Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by\nDirect Preference Optimization (DPO) [25] on a paired feedback dataset. Mixtral – Instruct reaches a\nscore of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December\n2023. Independent human evaluation conducted by LMSys is reported in Figure 6\n3\nand shows that\nMixtral – Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.\nFigure 6: LMSys Leaderboard.(Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena\nElo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro\n(1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.\n3\nhttps://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n6",
    "5    Routing analysis\nIn this section, we perform a small analysis on the expert selection by the router.  In particular,\nwe are interested to see if during training some experts specialized to some specific domains (e.g.\nmathematics, biology, philosophy, etc.).\nTo investigate this, we measure the distribution of selected experts on different subsets of The Pile\nvalidation dataset [14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31\nrespectively being the first and the last layers of the model). Surprisingly, we do not observe obvious\npatterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of\nexpert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts),\nand for Philosophy (PhilPapers) documents.\nOnly for DM Mathematics we note a marginally different distribution of experts. This divergence is\nlikely a consequence of the dataset’s synthetic nature and its limited coverage of the natural language\nspectrum, and is particularly noticeable at the first and last layers, where the hidden states are very\ncorrelated to the input and output embeddings respectively.\nThis  suggests  that  the  router  does  exhibit  some  structured  syntactic  behavior.   Figure  8  shows\nexamples of text from different domains (Python code, mathematics, and English), where each token\nis highlighted with a background color corresponding to its selected expert. The figure shows that\nwords such as ‘self’ in Python and ‘Question’ in English often get routed through the same expert\neven though they involve multiple tokens.  Similarly, in code, the indentation tokens are always\nassigned to the same experts, particularly at the first and last layers where the hidden states are more\ncorrelated to the input and output of the model.\nWe also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we\nobserve some degree of positional locality in The Pile datasets. Table 5 shows the proportion of con-\nsecutive tokens that get the same expert assignments per domain and layer. The proportion of repeated\n0\n0.05\n0.10\n0.15\n0.20\nlayer: 0\n0\n0.05\n0.10\n0.15\n0.20\nlayer: 15\n01234567\n0\n0.05\n0.10\n0.15\n0.20\nlayer: 31\nExpert ID\nSelection proportion\nArXiv\nDM Mathematics\nGithub\nGutenberg\nPhilPapers\nPubMed Abstracts\nStackExchange\nWikipedia (en)\nFigure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for\nlayers 0, 15, and 31.The gray dashed vertical line marks1/8, i.e.  the proportion expected with uniform\nsampling.   Here,  we consider experts that are either selected as a first or second choice by the router.   A\nbreakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.\n7",
    "First choiceFirst or second choice\nLayer 0Layer 15Layer 31Layer 0Layer 15Layer 31\nArXiv14.0%27.9%22.7%46.5%62.3%52.9%\nDM Mathematics14.1%28.4%19.7%44.9%67.0%44.5%\nGithub14.9%28.1%19.7%49.9%66.9%49.2%\nGutenberg13.9%26.1%26.3%49.5%63.1%52.2%\nPhilPapers\n13.6%25.3%22.1%46.9%61.9%51.3%\nPubMed Abstracts14.2%24.6%22.0%48.6%61.6%51.8%\nStackExchange13.6%27.2%23.6%48.2%64.6%53.6%\nWikipedia (en)14.4%23.6%25.3%49.8%62.1%51.8%\nTable 5: Percentage of expert assignment repetitions.We evaluate the proportion of times the same expert is\nassigned to a tokeniand its following tokeni+1. We report whether the first chosen expert is the same, or whether\nthe same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion\nof repetitions in the case of random assignments is\n1\n8\n= 12.5%for “First choice” and1−\n6\n8\n5\n7\n≈46%for “First\nand second choice”. Repetitions at the first layer are close to random, but are significantly higher at layers 15\nand 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.\nconsecutive assignments is significantly higher than random for higher layers. This has implications\nin how one might optimize the model for fast training and inference. For example, cases with high\nlocality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.\nConversely, this locality can be leveraged for caching, as is done in [11]. A more complete view of\nthese same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.\n6    Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the-\nart performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem-\nini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each\ntime step, Mixtral only uses 13B active parameters per token while outperforming the previous best\nmodel using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod-\nels publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de-\nvelopment of new techniques and applications that can benefit a wide range of industries and domains.\nFigure 8: Text samples where each token is colored with the first expert choice.The selection of experts\nappears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.\n8",
    "Acknowledgements\nWe thank the CoreWeave and Scaleway teams for technical support as we trained our models. We\nare grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working\nalongside us to make a sparse mixture of experts compatible with TensorRT-LLM.\nReferences\n[1]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.  Program synthesis with large\nlanguage models.arXiv preprint arXiv:2108.07732, 2021.\n[2]Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.  Llemma:  An open language\nmodel for mathematics.arXiv preprint arXiv:2310.10631, 2023.\n[3]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.  Piqa: Reasoning about phys-\nical commonsense in natural language.  InProceedings of the AAAI conference on artificial\nintelligence, pages 7432–7439, 2020.\n[4]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code.arXiv preprint arXiv:2107.03374, 2021.\n[5]Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and\nLuke Zettlemoyer.  Quac: Question answering in context.arXiv preprint arXiv:1808.07036,\n2018.\n[6]Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\nHoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified\nscaling laws for routed language models. InInternational Conference on Machine Learning,\npages 4057–4086. PMLR, 2022.\n[7]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova.  Boolq:  Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044, 2019.\n[8]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord.  Think you have solved question answering?  try arc, the ai2 reasoning\nchallenge.arXiv preprint arXiv:1803.05457, 2018.\n[9]\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.  Training verifiers to\nsolve math word problems.arXiv preprint arXiv:2110.14168, 2021.\n[10]Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei\nChang, and Rahul Gupta.   Bold:  Dataset and metrics for measuring biases in open-ended\nlanguage generation. InProceedings of the 2021 ACM conference on fairness, accountability,\nand transparency, pages 862–872, 2021.\n[11]Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with\noffloading.arXiv preprint arXiv:2312.17238, 2023.\n[12]William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning.\narXiv preprint arXiv:2209.01667, 2022.\n[13]Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse\ntraining with mixture-of-experts.arXiv preprint arXiv:2211.15841, 2022.\n[14]Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse\ntext for language modeling.arXiv preprint arXiv:2101.00027, 2020.\n[15]Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\nRahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture\nof experts with applications to multi-task learning.Advances in Neural Information Processing\nSystems, 34:29335–29347, 2021.\n9",
    "[16]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob  Steinhardt.   Measuring  massive  multitask  language  understanding.arXiv  preprint\narXiv:2009.03300, 2020.\n[17]Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXiv preprint arXiv:2103.03874, 2021.\n[18]\nAlbert Q Jiang,  Alexandre Sablayrolles,  Arthur Mensch,  Chris Bamford,  Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023.\n[19]\nMandar  Joshi,  Eunsol  Choi,  Daniel  S  Weld,  and  Luke  Zettlemoyer.    Triviaqa:   A  large\nscale  distantly  supervised  challenge  dataset  for  reading  comprehension.arXiv  preprint\narXiv:1705.03551, 2017.\n[20]Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research.Transactions of the Association for Computational\nLinguistics, pages 453–466, 2019.\n[21]Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\ntional computation and automatic sharding.arXiv preprint arXiv:2006.16668, 2020.\n[22]Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering.arXiv preprint arXiv:1809.02789,\n2018.\n[23]Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers.arXiv preprint arXiv:2305.16300, 2023.\n[24]Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-\nson, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question\nanswering.arXiv preprint arXiv:2110.08193, 2021.\n[25]Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\narXiv preprint arXiv:2305.18290, 2023.\n[26]Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale.Communications of the ACM, pages 99–106,\n2021.\n[27]Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-\nmonsense reasoning about social interactions.arXiv preprint arXiv:1904.09728, 2019.\n[28]Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean.  Outrageously large neural networks:  The sparsely-gated mixture-of-experts\nlayer.arXiv preprint arXiv:1701.06538, 2017.\n[29]\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung,  Aakanksha  Chowdhery,  Quoc  V  Le,  Ed  H  Chi,  Denny  Zhou,  ,  and  Jason  Wei.\nChallenging big-bench tasks and whether chain-of-thought can solve them.arXiv preprint\narXiv:2210.09261, 2022.\n[30]Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-\ntion answering challenge targeting commonsense knowledge.arXiv preprint arXiv:1811.00937,\n2018.\n[31]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information\nprocessing systems, 30, 2017.\n[32]Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\nmachine really finish your sentence?arXiv preprint arXiv:1905.07830, 2019.\n[33]Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena.arXiv preprint arXiv:2306.05685, 2023.\n10",
    "[34]  Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels.arXiv preprint arXiv:2304.06364, 2023.\n[35]Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,\nQuoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing.Advances in\nNeural Information Processing Systems, 35:7103–7114, 2022.\n11",
    "0\n0.1\n0.2\n0.3\nLayer 0 -- Either choice\n0\n0.1\n0.2\n0.3\nLayer 0 -- First choice\n0\n0.1\n0.2\n0.3\nLayer 0 -- Second choice\n0\n0.1\n0.2\n0.3\nLayer 15 -- Either choice\n0\n0.1\n0.2\n0.3\nLayer 15 -- First choice\n0\n0.1\n0.2\n0.3\nLayer 15 -- Second choice\n0\n0.1\n0.2\n0.3\nLayer 31 -- Either choice\n0\n0.1\n0.2\n0.3\nLayer 31 -- First choice\n01234567\n0\n0.1\n0.2\n0.3\nLayer 31 -- Second choice\nExpert ID\nSelection proportion\nArXiv\nDM Mathematics\nGithub\nGutenberg\nPhilPapers\nPubMed Abstracts\nStackExchange\nWikipedia (en)\nFigure 9: Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated\nby whether the expert was selected as first or second choice, or either.The “Either choice” case is equivalent\nto Figure 7. The gray dashed vertical line marks\n1\n8\n, i.e. the proportion expected with uniform sampling.\n12",
    "0.15\n0.20\n0.25\n0.30\n0.35\nFirst choice\n0102030\n0.5\n0.6\n0.7\nFirst or second choice\nLayer\nProportion of repeated assignments\nsource\nArXiv\nDM Mathematics\nGithub\nGutenberg\nPhilPapers\nPubMed Abstracts\nStackExchange\nWikipedia (en)\nFigure 10:  Repeated consecutive assignments per MoE layer.Repeated assignments occur a lot more\noften than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across\ndatasets with less repetitions for DM Mathematics.\n13"
  ]
}