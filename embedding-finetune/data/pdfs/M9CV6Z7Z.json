{
  "key": "M9CV6Z7Z",
  "url": "http://arxiv.org/pdf/2402.16671",
  "metadata": {
    "title": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding",
    "abstract": "  Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.\n",
    "published": "2024-02-26T15:47:01Z"
  },
  "text": [
    "Manuscript\nStructLM: Towards Building Generalist Models for Structured\nKnowledge Grounding\nAlex Zhuang\n1∗\n,Ge Zhang\n1,2,7∗\n,\nTianyu Zheng\n2\n,Xinrun Du\n2\n,Junjie Wang\n3,4\n,Weiming Ren\n1,7\n,\nStephen W. Huang\n6\n,Jie Fu\n2,4 †\n,Xiang Yue\n2,5\n,Wenhu Chen\n1,2,7 †\n1\nUniversity of Waterloo,\n2\nMultimodal Art Projection Research Community,\n3\nWaseda University,\n4\nHKUST,\n5\nOhio State University,\n6\nharmony.ai,\n7\nVector Institute\nhttps://tiger-ai-lab.github.io/StructLM/\nAbstract\nStructured data sources, such as tables, graphs, and databases, are ubiq-\nuitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting\nand utilizing structured data remains limited. Our investigation reveals a\nnotable deficiency in LLMs’ ability to process structured data, e.g., Chat-\nGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To\naugment the Structured Knowledge Grounding (SKG) capabilities in LLMs,\nwe have developed a comprehensive instruction tuning dataset comprising\n1.1 million examples.  Utilizing this dataset, we train a series of models,\nreferred to asStructLM, based on Mistral and the CodeLlama model family,\nranging from 7B to 34B parameters.  OurStructLMseries surpasses task-\nspecific models (Xie et al., 2022) on 16 out of 18 evaluated datasets and\nestablishes new SoTA performance on 8 SKG tasks. Furthermore,StructLM\ndemonstrates strong generalization across 6 novel held-out SKG tasks, out-\nperforming TableLlama by an average of 35% and Flan-UL2 20B by an\naverage of 10%. Contrary to expectations, we observe that scaling model\nsize offers marginal benefits, withStructLM-34B showing only slight im-\nprovements overStructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design\nto push to a new level. We release the model weights and training dataset\nto the community, along with relevant code on Github.\n1    Introduction\nTraditionally, users need to write programs to interface with structured data like tables,\ndatabases, knowledge graphs, etc. This often requires that they master a the domain-specific\nlanguage like SQL, SPARQL, etc. or develop domain specific skills for that structured type.\nRecently, researchers have explored the possibility of automating the interface with natural\nlanguage to enable potential use cases in question-answering (Pasupat & Liang, 2015; Zhong\net al., 2017; Nan et al., 2022), summarization (Parikh et al., 2020; Nan et al., 2021; Bao et al.,\n2018), and fact verification (Aly et al., 2021; Chen et al., 2019; Gupta et al., 2020b), among\nothers, all grounded to a structured knowledge source. This effort can lower the barrier for\nend users to access a massive amount of structured data.\nPrevious work (Yu et al., 2020; Liu et al., 2021; Xie et al., 2022; Zhang et al., 2023) has been\nmostly focused on building task-specific models for different tasks with rather limited\ngeneralization ability. Building a generalist structure knowledge grounding (SKG) system\nacross a wide range of tasks proves to be challenging. This is mainly due to the heterogeneity\nof data format and use cases. We evaluated GPT-3.5-Turbo (Jiang et al., 2023) on 18 SKG\ntasks and observed that its performance is on average 35% lower than the SoTA specialized\nmodels. It shows that the LLM’s ability on SKG is heavily overlooked during pre-training.\n1\narXiv:2402.16671v6  [cs.CL]  24 Apr 2024",
    "Manuscript\nIn this paper, we explore the possibility of building a generalist model based on LLMs that\ncan ground on diverse types of structure and unstructured knowledge to interface with\nhumans. Specifically, we construct a large data set of over a million instruction-following\nexamples,  a majority of which is SKG data,  along with additional general instruction-\nfollowing data, which we find improves generalizability.  We fine-tune models at three\nscales: 7B, 13B, and 34B, based on Mistral and the CodeLlama family of code foundation\nmodels. When compared to USKG, we find that our models surpass these single-task models\non 16 of 18 tasks.StructLMachieves SoTA on 8 of 18 evaluated tasks, beating ChatGPT by a\nhuge margin. Morever, we show that compared with TableLlama, a recent open language\nmodel finetuned for tabular inference tasks, our held-out performance is vastly superior.\nCompared with Flan-UL2-20B, our models also generalize better overall on the held out\ntasks by as much as 10% on average.\nWe  study  the  performance  ofStructLM,  namely  whether  the  model  experiences  cross-\ntask generalization benefits from the dataset mixture, and find that our multi-task model\nperforms significantly better overall than single-task models of the exact same parameter\nscale. We also study the effect of different pretraining data on our finetuned performance to\ndetermine whether special pretraining regimes, such as code or math, contribute to effective\nSKG reasoning ability.  We find that code pretraining is the most effective.  We perform\nadditional ablations to confirm our results and support our claims. Our contributions are:\n•\nWe construct a large SKG instruction-tuning dataset with 1.1 million samples. We\ntrain and release our models that outperform the previous 3B USKG fine-tuned on\nindividual tasks on a total of 16 of 18 tasks.StructLMalso achieves SoTA results on\n8 of them.\n•We show thatStructLMis able to show strong zero-shot generalization capability\non unseen structure knowledge grounding tasks, which was not shown by previous\nmodels.StructLMoutperforms Flan-UL2 20B on 3 of 5 tasks and TableLlama on all\n6 tasks. The average absolute improvement over them are 10% and 35%.\n•We find that mixing in general-domain instruction-tuning data during finetuning\npreserves generalization ability, and that code-pretrained base models can improve\nmodel performance on the SKG tasks.\n2    Related Works\n2.1    Solving SKG tasks\nStructured knowledge, such as web tables, knowledge graphs, and databases, have long\nbeen the subject of study in knowledge grounding.  However, SKG tasks have heteroge-\nneous data formats which have inspired methods that leverage specific training setups to\nFigure 1:StructLMcan ground on structured and unstructured knowledge to respond to\nhuman queries. The previous SoTA was attained by many different task-specific models like\nTAPEX (Liu et al., 2021), USKG (Xie et al., 2022), TableLlama (Zhang et al., 2023), BINDER-\nCodex (Cheng et al., 2022), etc.StructLMbeats the SoTAs on seven SKG tasks.\n2",
    "Manuscript\nHeld-in Evaluation\n   Input Types:\nUnseen Held-out Tasks\nknowledge graphstablesdb schema...\nsummary\nOutput Types:\nsingle answerboolean\ncode / formal grammartext (code translation)\n...\nAnswer the following question with the help of the \ngiven data table.\ndata table: <linearized table> \nquestion: who won the 1982 illinois gubernatorial \nelection, and how many votes was the margin?\nList out the answers to the latest \nquestion in the dialog based on the \ninformation in the given table.\ndata table: <linearized table>\ndialog: what were the loss totals\nof all these teams? || what were all\nthe teams?\nthompson prevailed in the 1982 illinois\ngubernatorial election by a 5,074 vote margin.\n6, 6, 9, 10, 10, 12, 12, 11, 13, 14,\n15, 14, 13, 14, 15, 22\nFigure 2: Overview of StructLM. This figure illustrates the prompting structure of StructLM,\nhighlighting its capability to process various forms of structured data beyond linearized\ndata tables, including linearized database schemas and knowledge graphs.\nlearn those representations.  For example, PTab (Liu et al., 2022) and MultiHiertt (Zhao\net al., 2022) learn the contextual representation of tabular data by incorporating semantic\ninformation through specific training methods or reasoning approaches. RASAT (Qi et al.,\n2022a) integrates relation-aware self-attention with the Transformer seq2seq architecture\nand utilizes various relational structures to address SQL problems.   TAPEX (Liu et al.,\n2021) conducts pretraining over tabular/database data with the help of an SQL executor to\nprovide supervision.\nMore recently, methods have begun to move away from these auxiliary task-specific struc-\ntures.  USKG (Xie et al., 2022) were the first to unify many SKG tasks into a sequence-to-\nsequence format, allowing them to be aggregated into the same data mixture.  However,\nthey were not able to show strong performance improvements to constructing a multi-task\nmix of SKG data over task-specific tuning methods. StructGPT (Jiang et al., 2023) represents\na line of work that uses prompting frameworks on powerful LLMs to solve tasks with\nmore robustness and accuracy. In contrast, our work examines open models and tries to\nassess their fundamental capabilities. Contemporary to our work, TableLlama (Zhang et al.,\n2023) has argued that tabular data deserves special attention.  Focusing on this domain,\ntheir method fine-tunes on several new tabular tasks to improve table understanding, and\noperates on a longer 8k context length. These improvements can be additive to our work.\n2.2    LLMs with Instruction Tuning\nInstruction-tuning (IT) has been popularized as a method to address the gap between train-\ning objectives and user goals in LLMs. This technique involves additional training of LLMs\nusing pairs of instructions and outputs. IT enhances both the controllability and the pre-\ndictability of the models, aligning them more closely with user expectations. Furthermore,\nrecent studies such as FLAN (Wei et al., 2022), UL2 (Tay et al., 2023a), and Llama2 (Touvron\net al., 2023) have shown that IT can improve the performance of downstream tasks through\nmulti-task learning across diverse data types.  While FLAN-UL2 trains on a subset of 11\nSKG tasks, it also trains on many more unrelated language tasks. In our work, by focusing\non SKG data, we hope to provide a focused study that can act as a reference for future work\nto improve performance on this task type.\n2.3    Reasoning Capability in LLMs\nReasoning stands as a pivotal skill for LLMs in the development of real-world AI ap-\nplications which would enable the autonomous completion of many thought-intensive\ntasks viewed traditionally to require human thinking, like programming or mathematical\nproblem-solving (Li et al., 2022). Recent studies (Li et al., 2022; 2023c; Rozi\n`\nere et al., 2023;\n3",
    "Manuscript\nFigure 3: Breakdown of Structured Knowledge Types and Tasks in the Training Data. On\nthe left side, we see a coarse breakdown of the different categories of structured inputs in\nour dataset. On the right side, we see an overview of the task groups that are represented\nfor those structured knowledge types.\nAzerbayev et al., 2023a) indicate that LLMs trained on code and mathematical datasets\nexhibit profound reasoning skills, and can even achieve performance on par with human\nlevels. For example, CodeLlama (Rozi\n`\nere et al., 2023), a foundation model trained on more\nprogramming data, has significantly improved reasoning capabilities across a variety of pro-\ngramming and mathematical benchmarks. Furthermore, Llemma (Azerbayev et al., 2023a)\ncontinues to pretrain the CodeLlama model on a mix of scientific papers, math-related\nweb data, and mathematical code. Its results show excellent reasoning capabilities on the\nMATH benchmark (Hendrycks et al., 2021) and the ability to prove theorems without further\nfine-tuning. On the fine-tuning side, WizardMath (Luo et al., 2023a), and WizardCoder (Luo\net al., 2023c) have shown the effectiveness of instruction tuning on reasoning capabilities,\ngiven high quality data.\nIn this work, we view structured data as a third testbed for a different kind of reasoning\nwithin LLMs. We posit that in addition to mathematical or logical reasoning, the ability to\nrecognize and make use of patterns within a structured input indicates that a model has\nrobust representations of relationships in data. These representations may serve as a strong\nprior for further reasoning downstream.\n3    Method\n3.1    Dataset Curation\nMotivated by the goal of training a language model generally capable of a wide range\nof structured data tasks, we select a total of 25 SKG tasks to study.  We report results on\n18 held-in and 6 held-out tasks, where each held-out task meant to roughly evaluate the\ngeneralization capability of a held-in task group.  In total, our held-in training dataset\ncontains approximately 700k SKG examples. We describe the held-in dataset groups below.\nData to Text Generation. This group of datasets deals with the summarization or interpreta-\ntion of structured data from tables to knowledge triples to formal languages. Their inclusion\nis motivated by the idea that useful LMs should be able to make sense of a wide variety\nof structured information and map it to meaning in natural language. The corresponding\nheld-out dataset for this task group is intended to be WikiTableText.\nTable based Question Answering. This group of datasets deals specifically with tabular\ndata, optionally combined with text passages.  LMs which are able to accurately answer\nquestions and retrieve information from tables can be widely useful as assistants.  The\ncorresponding held-out dataset for this task group is SQA.\n4",
    "Manuscript\nKnowledge-grounded Conversations. This group of tasks evaluates knowledge grounding\nin-conversation.  Humans naturally interface with LMs through chat, and enabling this\ncapability can lower the barrier to accessing the information in stored structured data. These\ntasks track user intention through provided dialogue and ask the model to provide an\nanswer to the latest question. The held-out dataset for this task group is CoSQL.\nFact verification.  One common use case for tables is to reference facts.  In addition to\nquestion answering, the ability to reliably determine if data in a table supports a statement\nsignals the existence of a robust representation of the table’s data. The held-out dataset for\nthis task group is InfoTabs.\nSQL or domain-specific languagesSQL is the language most commonly used to interface\nwith structured data today. Understanding how to write SQL also requires understanding of\nabstractions of tables and how they are linked together. In other domain-specific languages,\nthe MTOP task measures a model’s ability to parse a specification and generate an API call,\nwhich sees potential in LLM tool use (e.g., (Qin et al., 2023)). The corresponding held-out\ndataset for this task group is intended to be BIRD (Li et al., 2023b), which further tests SQL\ngeneration abilities.\nMathematical reasoning. An analysis of tabular data may also require performing quick\nmathematical computations over their contents.  Performance on these datasets tells us\nhow well models can combine both structured knowledge and mathematical reasoning.\nAs there are currently a limited number of datasets that combine mathematical reasoning\nwith SKG, this category includes just TabMWP in the held-in corpus. We set FinQA as a\nchallenging held-out dataset analogue. Not only does it require financial domain knowledge,\nbut it combines tabular information with long text passages, and requires the generation of\nmathematical code.\nGeneral  instruction  data.   In  addition  to  the  SKG  datasets  within  the  held-in  dataset\nmixture, we also included general instruction tuning data without any structured knowledge\ncomponent, to maintain the instruction-following ability of our model. We use SlimOrca\n(Lian et al., 2023),  which is constructed from cleaned GPT-4 responses to a number of\nprompts from existing general large-scale instruction-following datasets. We detect no signs\nof data contamination for our held-out datasets based on our ablation results. We give a\ndetailed overview of all dataset statistics in  Table 1.\n3.2    Instruction Finetuning Approach\nTo instruction tune our model, each example in our dataset consists of a system prompt,\ninstruction, input, and output. For all SKG data examples, we use the same system prompt.\nFor each dataset,  we write 10 instruction variations, which are randomized when con-\nstructing the training samples. For SKG data, the input is composed of a combination of a\nstructured knowledge input and accompanying text that could be a question, statement, or\nanything that would be required to specify the task. The prompt is provided in Figure 6.\n3.3    Training and Evaluation Details\nThe base models forStructLMare the CodeLlama-Instruct family of models (Rozi\n`\nere et al.,\n2023). We finetune all models with a batch size of 512 for 3 epochs on A800 gpus. We train\nour 7-34B models on 16-64 GPUs using DeepSpeed ZeRO-3 (Rasley et al., 2020) such that\nthe total training time for each model is between 3-5 days. This training setup is largely in\nline with community conventions, such as the settings used for the WizardLM (Xu et al.,\n2023), WizardMath (Luo et al., 2023a), and WizardCoder (Luo et al., 2023c) models.\nWe follow the structured data linearization conventions in USKG (Xie et al., 2022). However,\nwe use a different truncation scheme as described below. During training, we maintain a\nmaximum sequence length of 2048. To preserve as much input context as possible, when\ntruncating we consider the combined token length of the prompt input and output label. We\ntruncate only the structured knowledge portion of the input so that the example becomes at\nmost 2048 tokens long. As shown in the dataset statistics in Table 1, setting the max token\nlength of the examples in our dataset to 2048 allows nearly all examples to fit within the\n5",
    "Manuscript\nOverall LengthTrainTest\nDataset\nInput\n(avg)\nOutput\n(avg)\nCount\nInput\n(max)\nOutput\n(max)\n# Trunc.Count\nInput\n(max)\nOutput\n(max)\n# Trunc.\nTabMWP207.84.5230597093307686703310\nToTTo251.831.012076120401554677700204811931\nGrailQA281.044.144337884134064635461230\nSQL2Text122.318.156003376101034245380\nMMQA656.27.7156882047146234150120489411\nSpider\n266.636.070001369226010344531460\nKVRet573.417.16288121716108071147820\nHybridQA700.46.86268220479120034662048796\nSParC276.332.6120591417226016254671460\nCompWebQ\n1350.311.9276392047321321281620482568\nTabFact660.14.69228320455212779168740\nWikiTQ831.85.811321202827304344204814810\nWikiSQL689.27.1563552047518161587820482441\nFeTaQA\n653.238.8732618531580200315481140\nFEVEROUS799.33.440669204752052428520484195\nMultiWOZ777.2154.55666816561960736813441850\nDART133.730.362659406258050972611090\nLogic2Text166.126.985663586701092347600\nMTOP961.034.4156671002215043869901130\nSlimOrca278.9152.4512069204718080----\nBIRD439.863.39428199234799153412143860\nCoSQL287.434.995021640226013005351900\nSQA\n656.934.912275181210122301117257690\nInfotabs276.93.7165381009505400110540\nWikiTableText149.627.4100003139702000226890\nFinqa1230.321.06251204072186114720486125\nTable 1: Token sequence length statistics for each dataset in our train and test sets. Input\nand output statistics are in tokens.  We report the number of examples which have been\ntruncated in each dataset.\ncontext window with rare truncations. We discard examples for which even this structured\ninput truncation is insufficient (e.g. the output is too long). During inference, we set the\ninput token length to 2048, to allow even more structured information to be placed within\nthe input context. We set the maximum generation length to 1024, which is sufficient for\nall correct responses in all datasets.  For each model, including our single-task finetuned\nmodels, we choose the best performing checkpoint of the 3-epoch checkpoints.\n4    Experiments\nBaselinesFirstly, to illustrate the current performance of language models on SKG tasks,\nwe evaluate ChatGPT (GPT-3.5-turbo) and the base model CodeLlama-7B-Instruct under\na 1-shot setting. Our prompting scheme, using the same linearized knowledge structures\nas in our held-in training, sees them struggle across the board with many tasks due to the\nunseen structure knowledge format. Although ChatGPT is superior on text-based tasks, its\nperformance is lackluster on SKG tasks. Its gap with SoTA models is as significant as 35%.\nHeld-in ResultsTo evaluate the benefits of our instruction-tuning dataset mix, we also run\nsingle-task baseline (each a 7B model) on each task and report their individual performance.\nWe again use CodeLlama-7B-Instruct as the base model for each, and match each single\ntask model on the same number of epochs (3) that was used to train the multitask models,\nensuring that each model has seen the same data the same number of times. We observe\nthat our multi-task models outperform these single-task models on nearly every task, with\nsome by a considerable margin of up to 7%.  This demonstrates the effectiveness of our\ninstruction tuning dataset and supports the presence of cross-task generalization.\nWhen compared to the 18 task-specific USKG models,StructLM-7B can surpass USKG by a\naverage of 2%. From a parameter-count perspective, each of the USKG models is a T5-3B\nmodel, which means over the entire held-in set, these results require 54B parameters. Our\n7B-MStructLMin comparison can be viewed as being nearly 8x as parameter efficient while\n6",
    "Manuscript\nDatasetMetricSoTAChatGPTBase-MBaseSTFLAN-UL2TableLlamaUSKGStructLM(Ours)∆\nSize---7B7B7B×1820B7B3B×187B-M7B13B34B-\nHeld In\nToTToBLEU49.920.717.917.548.8--49.049.849.449.350.2+0.3\nGrailQAEM77.19.31.51.077.0--70.181.280.479.282.2+5.1\nSQL2TextBlec94.888.690.782.995.2--94.895.293.888.592.6+0.4\nMMQAF185.359.641.530.781.5--85.385.585.286.088.1+2.8\nSpiderEM\n80.543.831.05.267.3--71.872.472.474.174.6-5.9\nKVRetAll Micro67.952.934.439.570.9--67.972.272.669.569.3+4.7\nHybridQAAcc68.423.712.92.358.461.0-59.462.659.259.161.1-5.8\nSParCEM68.232.223.73.262.3--61.563.361.964.963.4-3.3\nCompWebQAcc76.848.930.93.175.675.9-73.379.978.380.481.9+5.1\nTabFactAcc93.062.425.70.079.687.182.583.784.680.884.786.6-6.4\nWikiTQAll Ex65.924.86.70.245.754.6-49.356.850.153.455.7-9.1\nWikiSQLAll Ex93.031.521.50.486.587.3-86.087.088.787.287.6-4.3\nFeTaQABLEU39.07.413.75.633.835.839.033.437.536.035.637.5-1.5\nFEVEROUSAcc85.657.873.258.478.185.6-82.485.984.485.085.7+0.3\nMultiWOZJoint Acc60.68.90.30.053.0--55.455.454.553.053.8-5.2\nDARTBLEU52.059.047.454.660.350.4-46.763.262.261.461.8+11.2\nLogic2TextBlec95.378.581.559.189.5--91.489.588.990.189.1-5.2\nMTOPEM87.51.40.80.077.487.5-86.875.881.281.682.1-5.4\nAverage74.939.530.820.268.2--69.372.171.171.372.6-1.2\nHeld Out\nBIRDAcc36.6*21.811.50.024.4*1.00022.822.322.824.7+2.9\nCoSQLEM58.3*33.726.50.252.4*5.10052.849.852.255.0+21.3\nSQAAcc70.5*18.77.42.360.4*70.1*0042.649.736.144.2+31\nInfotabsAcc75.6*46.949.140.268.7*70.316.2047.255.358.161.8-8.5\nWikiTableTextBLEU33.7*3.83.95.739.8*19.43.4017.18.39.38.8-2.3\nFinqaAcc71.1*31.40.71.779.7*5.92.6029.527.325.636.2+4.8\nAverage57.6*26.116.58.454.2*28.6*3.7035.335.534.038.4+8.2\nTable 2:  The overall evaluation results of our model against other baselines.  7B-M was\ntrained with Mistral-7B as the base model. Cells with ”-” in the held-in part mean that the\nmodel did not train on this dataset, and results are not comparable. USKG models are overfit\nto the held-in dataset labels, and thus cannot generalize comparably. Cells in the held-out\nsection with ”*” are held-in results.  SoTA results are copied from the original papers for\nreference. ST refers to the single-task fine-tuning result of CodeLlama-Instruct-7B on each\ndataset. BASE and BASE-M refer to the 1-shot performance of CodeLlama-Instruct-7B, and\nMistral-7B-Instruct-v0.2 respectively.∆refers to the difference betweenStructLMand the\nbest known result.scoredenotes the state-of-the-art score on specific tasks. AllStructLM\nheld-out results are 0-shot. Specifications as to how SoTA results are selected are given in\nTable 4.\nstill surpassing USKG models on 16 of 18 datasets.  It is worth noting that although the\nsingle-task (ST) models are more than double the size in parameters compared to USKG,\nthey do not perform much better on average. This fact indicates that there may be significant\nunused model capacity that can be better utilized via more effective training regimes, such\nas our instruction tuning.  Regarding FLAN-UL2-20B (Tay et al., 2023b), which was also\nextensively trained on structure knowledge grounding tasks,StructLMoutperforms it on 7\nof the 9 mutually held-in datasets. Our results on held-in datasets (Tabfact and FeTaQA)\nare on par with TableLlama (Zhang et al., 2023), which is an LLM pre-trained on 2.6M table\nunderstanding tasks. We are vastly outperforming TableLlama on the held-out datasets.\nHeld-out ResultsOn held out tasks,StructLMshows strong generalization performance,\noutperforming ChatGPT on 5 of 6 tasks.  These novel tasks contain remarkably different\ndata format than the training dataset.  For example, FinQA (Chen et al., 2021) requires\nthe model to generate a mathematical expression to answer a question in the financial\ndomain, while BIRD (Li et al., 2023b) requires the model to interface with multiple databases.\nThese skills are not exactly covered in the training set, thus they put high demand for the\nmodels’ generalization capabilities.  Such generalization capabilities are non-existent in\nUSKG models as they are task-specific. Another table foundation model TableLlama (Zhang\net al., 2023) would also fail on most of the held-out tasks with performance close to zero.\nIn contrast,StructLMis much more capable of generalization on these novel tasks.  The\naverage improvement over TableLlama is 35%.  Compared to another strong foundation\n7",
    "Manuscript\nFigure 4: Effect of different pretraining curricula on SKG finetuning performance in relevant\ntask groupings. We can observe the advantages of CodeLlma over the others.\nPurposeTrainEvalFTResult\nSchema task transferSpider, SParC, Logic2TextLogic2Text89.4789.93\nKT task transferCompWebQ, WebQSP, GrailQa, DARTDART60.2860.34\nTable task transfer\nFetaQA, HybridQA, WikiTQ,\nTabMWP, ToTTo, MMQA,\nWikiSQL, KVRet, Tab Fact,\nFeverous, Infotabs\nTabFact,\nFeverous\nInfotabs\n75.4680.81\nSumm. data type transferToTTo, DARTDART60.2861.42\nQA data type transferCompWebQ, WikiSQLWikiSQL85.4986.36\nTable 3:  Cross task and cross datatype transfer results.  FT is an average of single-task\nperformance over the datasets in the Eval column.\nFigure 5:  Effect of general instruction-following data on averaged held-out SKG dataset\nperformance. Performance is measured as the average over evaluation metrics across all\ntasks within held-in or held-out groups.\nstructure-knowledge-grounding model Flan-UL2 20B (Tay et al., 2023b), we also outperform\nit on held-out tasks by an average of 10%.\nAdditionally, our results in Table 2 support that the Mistral base model has stronger gener-\nalization and ability to handle SKG tasks than CodeLlama, with about a 10% advantage on\nmetrics over all tasks. We can see that this strength transfers to the fine-tuned versions of\nthe models.\n5    Ablation Studies\nEffect of base model pretraining data. We ablate our choice of base model, CodeLlama-\n7b-Instruct, by finetuning the unspecialized Llama2-7b base model and Llemma, which is\nfurther pretrained on mathematical texts (Azerbayev et al., 2023b). Intuitively, one might\nguess that coding ability has the most transferability to performance on the types of SKG\ntasks we are studying due to the symbolic nature of programming languages and code-\n8",
    "Manuscript\nwriting scenarios. However, it is possible that other types of pretraining to boost reasoning\nability, such as math, have even greater transferability.\nOur ablation results in Table 6 can be broken down into groupings of tasks, as in Figure 4.\nModels  pretrained  on  code  indeed  perform  slightly  better,  and  these  gains  are  not\nnecessarily limited to tasks which explicitly involve a grammatically regular input,  or\nrequire the generation of code. Math pretraining does seem to improve the performance of\nthe Llama2 base model, but not by as much as code pretraining. Overall, it seems that code\npretraining is a useful step in training a performant model in this SKG setting, which may\nbe due to conceptual similarity on certain tasks.\nEffect of general instruction data in mixtureAs we see in Figure 5, the held-in performance\nis relatively unaffected by the added general examples, but held-out performance improves\nsignificantly with more general data.  Empirically, we also observe that when training a\nlarge volume of task-specific input and output formats, the model becomes less capable of\nfollowing instructions on new tasks in a zero-shot setting. We hypothesize that training on\nthis general mixture helps zero-shot performance because it can reduce overfitting to the\ntask formats in the training set.\nCross-task and cross-format transferabilityWe ablate the transferability of performance\nbetween input structure knowledge types and between output task types.  To test this,\nwe  train  a  number  of  tasks  together  and  compare  them  to  their  single-task  finetuned\nmodels.  Our results are indicative that there is cross-task transferability of performance\noccurring. On tables, we see this effect clearly. This may be explained by the volume and\nvariety of table tasks included in the training mix. In schema and knowledge triples, the\nperformance improvement is not pronounced, perhaps due to the limited size of those\ndatasets. Nevertheless, evidence supports that diversifying tasks on the same structured\nknowledge type benefits performance.\nOn the other hand, we see that finetuning on different datatypes with the same task (i.e.\nsummarization) also yields benefits to performance. On the summarization and question-\nanswering (QA) experiments, we train on both tabular and knowledge graph data.  We\nevaluate summarization with DART and QA with WikiSQL. We see that in both cases, the\nextra dataset yielded about 1% improvement. Considering that the added datasets in each\ncase organize information in a completely different way, this result suggests diversifying\ndata types for similar tasks do indeed benefit each other as well.\n6    Discussion\nWe argue that SKG is an important capability for future language models. We have seen\nthrough our experiments on ChatGPT and the Llama2 family that there is significant room\nfor improvement. We found that we could produce a strong model by focused instruction-\ntuning on SKG tasks, however, we also observe that the performance difference between 7B\nto 34BStructLMmodels was not dramatic. This raises a concern about the state of SKG data:\ncould we be approaching a performance ceiling? Combined with the fact that we were able\nto outperform UL2-20b, a much larger model, with our 7B model on 3 tasks, it seems that\nLLMs at various scales are struggling with SKG capabilities.\nIndeed, grounding to structured knowledge directly in a model’s input represents a chal-\nlenge in reasoning and input sensitivity. However, it has a wide range of potential benefits.\nTo meaningfully improve SKG capability, we propose that future work may explore con-\ntinued pretraining of open foundation models on more structured data formats. Similar to\ncurrent attempts at code or math pretraining, it is possible that pretraining models on text\ninterleaved with tables or other types of regular data formatting will help us move towards\nestablishing SKG as a foundational model capability.\n9",
    "Manuscript\n7    Conclusion\nIn this paper, we explore the current capabilities of open language models on structured\nknowledge grounding tasks.  We show that LLMs are currently weak at SKG tasks.  To\naddress this gap, we construct an instruction-tuning dataset mixture of 1.1M examples and\nrelease models that and achieve SOTA on 7 of 18 held-in tasks, and that outperform strong\nexisting models such as TableLlama and Flan UL2 on held-out tasks.  We also study the\neffects of various factors that influence the performance of our model on these task types.\nWe hope that our work provides an updated understanding of what is achievable in the\nSKG domain, and can serve as a strong baseline for future improvements.\nReferences\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, Oana Cocarascu, and Arpit Mittal. FEVEROUS: Fact extraction and\nVERification over unstructured and structured information. 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics.CoRR, abs/2310.10631, 2023a.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023b.\nJunwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, and Tiejun Zhao.\nTable-to-text:  Describing table region with natural language.Proceedings of the AAAI\nConference on Artificial Intelligence, 32(1), April 2018. ISSN 2159-5399. doi: 10.1609/aaai.\nv32i1.11944. URLhttp://dx.doi.org/10.1609/aaai.v32i1.11944.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\n ̃\nnigo Casanueva, Stefan Ultes,\nOsman Ramadan, and Milica Ga\nˇ\nsi\n ́\nc.  MultiWOZ - a large-scale multi-domain Wizard-\nof-Oz dataset for task-oriented dialogue modelling. In Ellen Riloff, David Chiang, Julia\nHockenmaier, and Jun’ichi Tsujii (eds.),Proceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 5016–5026, Brussels, Belgium, October-November\n2018. Association for Computational Linguistics.   doi:  10.18653/v1/D18-1547.   URL\nhttps://aclanthology.org/D18-1547.\nWenhu Chen,  Hongmin Wang,  Jianshu Chen,  Yunkai Zhang,  Hong Wang,  Shiyang Li,\nXiyou Zhou, and William Yang Wang. Tabfact: A large-scale dataset for table-based fact\nverification. InInternational Conference on Learning Representations, 2019.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang.\nHybridqa:  A dataset of multi-hop question answering over tabular and textual data.\nFindings of EMNLP 2020, 2020a.\nZhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan,\nand William Yang Wang.  Logic2Text: High-fidelity natural language generation from\nlogical forms. In Trevor Cohn, Yulan He, and Yang Liu (eds.),Findings of the Association\nfor Computational Linguistics:  EMNLP 2020,  pp. 2096–2111,  Online,  November 2020b.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.190.\nURLhttps://aclanthology.org/2020.findings-emnlp.190.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon,\nReema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang.\nFinqa: A dataset of numerical reasoning over financial data.Proceedings of EMNLP 2021,\n2021.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming\nXiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al.  Binding language\nmodels in symbolic languages. InThe Eleventh International Conference on Learning Repre-\nsentations, 2022.\n10",
    "Manuscript\nJordan Clive, Kris Cao, and Marek Rei. Control prefixes for parameter-efficient text genera-\ntion.arXiv preprint arXiv:2110.08329, 2021.\nYinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, and Xiaodan Zhu. Preview,\nattend and review:  Schema-aware curriculum learning for multi-domain dialog state\ntracking, 2021.\nMihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value\nretrieval networks for task-oriented dialogue.  InProceedings of the 18th Annual SIGdial\nMeeting on Discourse and Dialogue. Association for Computational Linguistics, 2017. doi:\n10.18653/v1/w17-5506. URLhttp://dx.doi.org/10.18653/v1/W17-5506.\nYu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su.  Be-\nyond iid: three levels of generalization for question answering on knowledge bases. In\nProceedings of the Web Conference 2021, pp. 3477–3488. ACM.\nDeepak Gupta,  Surabhi Kumari,  Asif Ekbal,  and Pushpak Bhattacharyya.   MMQA: A\nmulti-domain multi-lingual question-answering framework for English and Hindi.  In\nNicoletta Calzolari, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi,\nKoiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H\n ́\nel\n`\nene Mazo, Asun-\ncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga (eds.),Proceedings\nof the Eleventh International Conference on Language Resources and Evaluation (LREC 2018),\nMiyazaki, Japan, May 2018. European Language Resources Association (ELRA).  URL\nhttps://aclanthology.org/L18-1440.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. INFOTABS: Inference on\ntables as semi-structured data. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 2309–2324, Online, July 2020a. Association for Computa-\ntional Linguistics. URLhttps://www.aclweb.org/anthology/2020.acl-main.210.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. Infotabs: Inference on\ntables as semi-structured data.arXiv preprint arXiv:2005.06117, 2020b.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt.  Measuring mathematical problem solving with the\nmath dataset, 2021.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for\nsequential question answering. InProceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 1821–1831, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1167. URL\nhttps://aclanthology.org/P17-1167.\nJinhao Jiang,  Kun Zhou,  Zican Dong,  Keming Ye,  Wayne Xin Zhao,  and Ji-Rong Wen.\nStructGPT: A General Framework for Large Language Model to Reason over Structured\nData, October 2023. URLhttp://arxiv.org/abs/2305.09645. arXiv:2305.09645 [cs].\nSung-Min Lee, Eunhwan Park, Daeryong Seo, Donghyeon Jeon, Inho Kang, and Seung-\nHoon Na. MAFiD: Moving average equipped fusion-in-decoder for question answering\nover tabular and textual data. In Andreas Vlachos and Isabelle Augenstein (eds.),Findings\nof the Association for Computational Linguistics:  EACL 2023, pp. 2337–2344, Dubrovnik,\nCroatia, May 2023. Association for Computational Linguistics.  doi: 10.18653/v1/2023.\nfindings-eacl.177. URLhttps://aclanthology.org/2023.findings-eacl.177.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad.\nMTOP: A comprehensive multilingual task-oriented semantic parsing benchmark.  In\nPaola Merlo, Jorg Tiedemann, and Reut Tsarfaty (eds.),Proceedings of the 16th Conference\nof the European Chapter of the Association for Computational Linguistics:  Main Volume, pp.\n2950–2962, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.eacl-main.257. URLhttps://aclanthology.org/2021.eacl-main.257.\n11",
    "Manuscript\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking\nand skeleton parsing for text-to-sql.  InProceedings of the AAAI Conference on Artificial\nIntelligence, volume 37, pp. 13067–13075, 2023a.\nJinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin,\nRuiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei\nHuang, Reynold Cheng, and Yongbin Li. Can llm already serve as a database interface? a\nbig bench for large-scale database grounded text-to-sqls, 2023b.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao  Mou,  Marc  Marone,  Christopher  Akiki,  Jia  Li,  Jenny  Chim,  Qian  Liu,  Evgenii\nZheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier,  Jo\n ̃\nao Monteiro,  Oleh Shliazhko,  Nicolas Gontier,  Nicholas Meade,\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-\ntasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel,\nDmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Ur-\nvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim\nKunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer\nRobinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\nDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\n ̃\nnoz Ferrandis, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the\nsource be with you!CoRR, abs/2305.06161, 2023c.\nShujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu Yuan, Wanwei He, Shao\nYuan, Can Ma, Fei Huang, and Yongbin Li. Unifying structured data as graph for data-to-\ntext pre-training.ArXiv, abs/2401.01183, 2024. URLhttps://api.semanticscholar.org/\nCorpusID:266725545.\nYujia Li,  David H. Choi,  Junyoung Chung,  Nate Kushman,  Julian Schrittwieser,  R\n ́\nemi\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert,\nPeter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang,\nJohannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. Competition-level code generation with alphacode.CoRR, abs/2203.07814,\n2022.\nWing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,\nand ”Teknium”.  Slimorca: An open dataset of gpt-4 augmented flan reasoning traces,\nwith verification, 2023. URLhttps://https://huggingface.co/Open-Orca/SlimOrca.\nGuang Liu, Jie Yang, and Ledell Wu.   Ptab:  Using the pre-trained language model for\nmodeling tabular data.CoRR, abs/2209.08060, 2022.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou.\nTapex: Table pre-training via learning a neural sql executor, 2021.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit,\nPeter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-\nstructured mathematical reasoning. InInternational Conference on Learning Representations\n(ICLR), 2023.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo\nGeng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.  Wizardmath:  Empowering\nmathematical reasoning for large language models via reinforced evol-instruct.arXiv\npreprint arXiv:2308.09583, 2023a.\nHaoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma,\nGuanting Dong, Meina Song, and Wei Lin. Chatkbqa: A generate-then-retrieve framework\nfor knowledge base question answering with fine-tuned large language models, 2023b.\n12",
    "Manuscript\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao,\nJing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language\nmodels with evol-instruct.arXiv preprint arXiv:2306.08568, 2023c.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun\nHsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia\nIrwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar,\nAnkit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and\nNazneen Fatema Rajani. DART: Open-domain structured data record to text generation.\nInProceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 432–447, Online, June 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.37. URL\nhttps://aclanthology.org/2021.naacl-main.37.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang,\nWojciech Kry\n ́\nsci\n ́\nnski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma,\nBen Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong,\nDragomir Radev, and Dragomir Radev.   Fetaqa:  Free-form table question answering.\nTransactions of the Association for Computational Linguistics, 10:35–49, 2022. ISSN 2307-387X.\ndoi: 10.1162/tacla00446. URLhttp://dx.doi.org/10.1162/tacla00446.\nAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra,\nDiyi Yang, and Dipanjan Das.  ToTTo: A controlled table-to-text generation dataset.  In\nProceedings of EMNLP, 2020.\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured\ntables. In Chengqing Zong and Michael Strube (eds.),Proceedings of the 53rd Annual Meeting\nof the Association for Computational Linguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1:  Long Papers), pp. 1470–1480, Beijing, China,\nJuly 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1142. URL\nhttps://aclanthology.org/P15-1142.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing\nWang, Quanshi Zhang, and Zhouhan Lin. RASAT: integrating relational structures into\npretrained seq2seq model for text-to-sql.   InEMNLP, pp. 3215–3229. Association for\nComputational Linguistics, 2022a.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing\nWang, Quanshi Zhang, and Zhouhan Lin.  Rasat: Integrating relational structures into\npretrained seq2seq model for text-to-sql.arXiv preprint arXiv:2205.06983, 2022b.\nCheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji.  Creator:  Tool\ncreation for disentangling abstract and concrete reasoning of large language models, 2023.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\nXiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou,\nMark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.  Toolllm:  Facilitating large\nlanguage models to master 16000+ real-world apis, 2023.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System\noptimizations enable training deep learning models with over 100 billion parameters.\nInProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining,  KDD ’20,  pp. 3505–3506,  New York,  NY, USA, 2020. Association for\nComputing  Machinery.   ISBN  9781450379984.   doi:  10.1145/3394486.3406703.   URL\nhttps://doi.org/10.1145/3394486.3406703.\nBaptiste Rozi\n`\nere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Tal Remez, J\n ́\ner\n ́\nemy Rapin, Artyom Kozhevnikov, Ivan Evtimov,\nJoanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong,\nAlexandre D\n ́\nefossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas\nUsunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models\nfor code.CoRR, abs/2308.12950, 2023.\n13",
    "Manuscript\nChang  Shu,  Yusen  Zhang,  Xiangyu  Dong,  Peng  Shi,  Tao  Yu,  and  Rui  Zhang.   Logic-\nconsistency text generation from semantic parses.  In Chengqing Zong, Fei Xia, Wenjie\nLi, and Roberto Navigli (eds.),Findings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, pp. 4414–4426, Online, August 2021. Association for Computational\nLinguistics.  doi: 10.18653/v1/2021.findings-acl.388.  URLhttps://aclanthology.org/\n2021.findings-acl.388.\nYiheng Shu and Zhiwei Yu. Data distribution bottlenecks in grounding language models to\nknowledge bases, 2023.\nJiashuo Sun,  Hang Zhang,  Chen Lin,  Yeyun Gong,  Jian Guo,  and Nan Duan.   Apollo:\nAn  optimized  training  approach  for  long-form  numerical  reasoning.arXiv  preprint\narXiv:2212.07249, 2022.\nAlon Talmor and Jonathan Berant.   The web as a knowledge-base for answering com-\nplex questions.  InProceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics:  Human Language Technologies, Volume 1 (Long\nPapers). Association for Computational Linguistics, 2018.  doi:  10.18653/v1/n18-1059.\nURLhttp://dx.doi.org/10.18653/v1/N18-1059.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and\nDonald Metzler. UL2: unifying language learning paradigms. InICLR. OpenReview.net,\n2023a.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou,\nNeil Houlsby, and Donald Metzler. Ul2: Unifying language learning paradigms, 2023b.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher,  Cristian  Canton-Ferrer,  Moya  Chen,  Guillem  Cucurull,  David  Esiobu,  Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal,  Anthony Hartshorn,  Saghar Hosseini,  Rui Hou,  Hakan Inan,  Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur\n ́\nelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom.  Llama 2: Open foundation and fine-tuned chat\nmodels.CoRR, abs/2307.09288, 2023.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners.\nInICLR. OpenReview.net, 2022.\nDongling Xiao, Linzheng Chai, Qian-Wen Zhang, Zhao Yan, Zhoujun Li, and Yunbo Cao.\nCqr-sql: Conversational question reformulation enhanced context-dependent text-to-sql\nparsers, 2022.\nTianbao  Xie,  Chen  Henry  Wu,  Peng  Shi,  Ruiqi  Zhong,  Torsten  Scholak,  Michihiro  Ya-\nsunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong,\nBailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caim-\ning Xiong,  Lingpeng Kong,  Rui Zhang,  Noah A. Smith,  Luke Zettlemoyer,  and Tao\nYu.   Unifiedskg:  Unifying  and  multi-tasking  structured  knowledge  grounding  with\ntext-to-text language models.Proceedings of the 2022 Conference on Empirical Methods\nin  Natural  Language  Processing,  2022.    doi:   10.18653/v1/2022.emnlp-main.39.    URL\nhttp://dx.doi.org/10.18653/v1/2022.emnlp-main.39.\n14",
    "Manuscript\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\nand Daxin Jiang.   Wizardlm:  Empowering large language models to follow complex\ninstructions.arXiv preprint arXiv:2304.12244, 2023.\nKuan Xu, Yongbo Wang, Yongliang Wang, Zujie Wen, and Yang Dong. Sead: End-to-end\ntext-to-sql generation with schema-aware denoising.arXiv preprint arXiv:2105.07911, 2021.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Large language\nmodels are versatile decomposers: Decomposing evidence and questions for table-based\nreasoning. InProceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pp. 174–184, 2023.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene\nLi, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-\nscale human-labeled dataset for complex and cross-domain semantic parsing and text-\nto-sql task. InProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-1425.\nURLhttp://dx.doi.org/10.18653/v1/D18-1425.\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang\nEr, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim,\nJonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev.\nSparc: Cross-domain semantic parsing in context. InProceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics. Association for Computational Linguistics,\n2019. doi: 10.18653/v1/p19-1443. URLhttp://dx.doi.org/10.18653/v1/P19-1443.\nTao Yu,  Chien-Sheng Wu,  Xi Victoria Lin,  Yi Chern Tan,  Xinyi Yang,  Dragomir Radev,\nCaiming Xiong, et al.   Grappa:  Grammar-augmented pre-training for table semantic\nparsing. InInternational Conference on Learning Representations, 2020.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.  TableLlama:  Towards Open Large\nGeneralist Models for Tables, November 2023. URLhttp://arxiv.org/abs/2311.09206.\narXiv:2311.09206 [cs].\nYilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. Multihiertt: Numerical reasoning\nover multi hierarchical tabular and textual data. InACL (1), pp. 6588–6600. Association\nfor Computational Linguistics, 2022.\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning.CoRR, abs/1709.00103, 2017.\n15",
    "Manuscript\nA    SoTA Results\nDatasetMetricSplitSOTA ScoreBest Performing Model\nTabMWPAcctest94.7CREATOR (ChatGPT)(Qian et al., 2023)\nToTToBLEUtest49.9UniK2G (Li et al., 2024)\nGrailQAEMval77.1TIARA + GAIN (T5-3B) (Shu & Yu, 2023)\nSQL2TextBlectest94.78UnifiedSKG\nMMQAF1test85.28UnifiedSKG\nSpiderEMdev80.5RESDSQL (Li et al., 2023a)\nKVRetAll Microtest67.88UnifiedSKG\nHybridQAAccdev68.4S3HQA(Lee et al., 2023)\nSParCEMtest68.2CQR-SQL(Qi et al., 2022b)\nCompWebQAcctest76.8ChatKBQALuo et al. (2023b)\nTabFactAcctest\nsmall93.0Dater(Ye et al., 2023)\nWikiTQAll Extest65.9Dater(Ye et al., 2023)\nWikiSQLAll Extest93.0SeaD+Execution-Guided Decoding(Xu et al., 2021)\nFeTaQABLEUtest39.0TableLlama (Zhang et al., 2023)\nFeverousAccdev85.6FLAN UL2 20b(Tay et al., 2023b)\nMultiWOZJoint Acctest60.6TripPy + SaCLog(Dai et al., 2021)\nDartBLEUtest52.0Control Prefixes (T5-large)(Clive et al., 2021)\nLogic2TextBlectest95.3UnifiedSKG\nMTOPEMtest87.5FLAN UL2 20b(Tay et al., 2023b)\nBIRDAccdev36.6ChatGPT + COT(Li et al., 2023b)\nCoSQLEMtest58.3CQR-SQL(Xiao et al., 2022)\nSQAAcctest70.5FLAN UL2 20b(Tay et al., 2023b)\nInfotabsAccdev75.6Infotabs paper(Gupta et al., 2020b)\nWikiTableTextBLEUtest33.7UnifiedK2G(Li et al., 2024)\nFinqaAccprivate\ntest71.1APOLLO(Sun et al., 2022)\nTable 4: Specification of SoTA scores and their sources on the most prevalent metrics used\nfor assessment.\nAcross the datasets that we evaluate, the SoTA scores are chosen based on methods that do\nnot use agent based methods on models as large as GPT-4 for a fairer comparison. Across\nthese datasets, we can see that many of the best performing methods are purpose-built for\nthe type of data structure used in the task. For example, RESDSQL focuses exclusively on\ncontrolled SQL generation, DATER uses SQL-based reasoning for tabular tasks, and S3HQA\nfocuses on table and text multi-hop QA. Compared to these methods, the performance of\nStructLMcan be seen a strong baseline to determine if these domain-specific designs yield\nreal benefits.\n16",
    "Manuscript\nB    SlimOrca Dataset Mixture Details\nMetric0%10%20%50%57%\nHeld-In Datasets\nTabMWPAcc71.1470.3570.5269.0169.36\nToTToBLEU49.7849.5149.4749.3149.38\nGrailQAEM81.0980.4680.2980.8980.38\nSQL2TextBlec95.0794.3994.4994.9793.81\nMMQAF184.2684.3184.1183.4085.15\nSpiderEM\n72.9271.5773.4072.7372.44\nKVRetAll Micro71.6073.9070.3472.2572.61\nHybridQAAcc59.2359.0959.0959.0359.17\nSParCEM63.0962.3463.2664.5961.93\nCompWebQAcc\n80.6179.1578.7678.7378.34\nTabFactAcc83.4181.0981.4280.9280.77\nWikiTQAll Ex50.0248.5049.2448.3050.09\nWikiSQLAll Ex87.3386.4586.7386.6888.67\nFeTaQABLEU\n36.5837.2636.5536.7236.03\nFeverousAcc\n85.0284.1384.1183.7384.41\nMultiWOZJoint Acc\n54.6654.1053.7353.9254.49\nDartBLEU\n61.3861.8961.0862.2462.24\nLogic2TextBlec88.8389.4789.1990.5788.92\nMTOPEM82.4481.7181.1980.9281.21\nHeld-Out Datasets\nBIRDAcc21.3022.3022.3023.0022.30\nCoSQLEM51.2449.9550.8450.7449.75\nSQAAcc49.0246.0343.1148.3949.72\nInfotabsAcc\n38.0056.2657.8762.3562.46\nWikiTableTextBLEU\n14.7813.516.667.278.27\nFinqaAcc\n19.7024.3227.5525.3727.29\nTable 5: Ablation results for the mixtures of general data in the training set.\nIn total, we train 5 models, where the percentage represents the percent of the training data\nthat is general. In the held out data, we see noticeable gains in generalization performance\nfor FinQA and InfoTabs datasets.  Notably, FinQA requires the generation of a python-\nexecutable math expression and InfoTabs requires an exact match to 3 previously unseen\n(boolean) options. WikiTableText performance seems to suffer, but is evaluated based on\nthe BLEU score with only one target sentence. As a result, we place more emphasis on the\nmodel’s 0-shot adaptation ability to new output specifications unseen in the training data.\n17",
    "Manuscript\nC    Per-dataset Pretraining Data Comparison\nTasksMetricCode-LMLLaMAMath-LM\nHeld-In Datasets\nTabMWPAcc71.1462.9666.5\nToTToBLEU49.7848.2647.4\nGrailQAEM\n81.0975.7277.66\nSQL2TextBlec95.0794.4994.58\nMMQAF184.2683.9682.13\nSpiderEM72.9265.9671.95\nKVRetAll Micro\n71.670.3670.03\nHybridQAAcc59.2359.2657.04\nSParCEM63.0956.9460.35\nCompWebQAcc80.6177.3176.6\nTabFactAcc83.4180.4679.47\nWikiTQAll Ex\n50.0245.646.89\nWikiSQLAll Ex87.3383.9385.49\nFeTaQABLEU\n36.5834.3734.1\nFeverousAcc85.0283.282.52\nMultiWOZJoint Acc54.6655.4353.79\nDartBLEU61.3861.5261.24\nLogic2TextBlec\n88.8388.090.38\nMTOPEM82.4477.1875.56\nHeld-Out Datasets\nBIRDAcc21.315.918.8\nCoSQLEM\n51.2442.848.76\nSQAAcc\n49.0237.0349.05\nInfotabsAcc\n38.04.4432.54\nWikiTableTextBLEU14.7813.014.82\nFinqaAcc19.76.6321.53\nTable 6: Fine-grained evaluation results comparing finetuning done on different base models.\nCode refers to CodeLlama-Instruct-7B. Math refers to Llemma-7b. LLaMA refers to Llama2-\n7b.\nIn these fine-grained results we can compare the performance of Llemma, Llama, and\nCodeLlama in more detail. Llemma does seem to hold an advantage on tasks that involve\nmath (TabMWP, FinQA). Math pretraining does not seem to benefit tabular tasks overall\n(WikiSQL, WikiTQ, HybridQA, MMQA, etc.), but does show advantages on SQL coding\ntasks (Spider, SparC). CodeLlama, however, seems to show a performance improvement\nover the base Llama model on not just coding tasks, but also math and tabular tasks as well.\nThese findings may underscore the need to understand what constitutes ”reasoning” in a\nlanguage model.\n18",
    "Manuscript\nD    Prompt Format\n[INST] <<SYS>>\nYou are an AI assistant that specializes in analyzing and reasoning\nover structured information. You will be given a task, optionally\nwith some structured knowledge input. Your answer must strictly\nadhere to the output format, if specified.\n<</SYS>>\n{instruction} {input} [/INST]\nFigure 6: Prompt format for all SKG examples. This formatting convention follows LLama2\nTouvron et al. (2023). The input contains the linearized structured data, together with any\nother context, question or statement. The instruction specifies the task.\n19",
    "Manuscript\nE    Held-Out Generation Examples\nFor illustration purposes, we provide examples of successful and unsuccessful responses of\nStructLM-13B on the FinQA held-out dataset.\nE.1    SuccessfulStructLM-13B Response on FinQA\nUse  the  financial  data  in  the  following  passage  and  table  to  calculate  the  answer  the\nquestion. table:\nperformance graph the following graph compares the yearly change in the cumulative\ntotal stockholder return for our last five full fiscal years , based upon the market price of\nour common stock , with the cumulative total return on a nasdaq composite index ( u.s .\ncompanies ) and a peer group , the nasdaq medical equipment-sic code 3840-3849 index ,\nwhich is comprised of medical equipment companies , for that period . the performance\ngraph assumes the investment of$ 100 on march 31 , 2007 in our common stock , the nasdaq\ncomposite index ( u.s . companies ) and the peer group index , and the reinvestment of any\nand all dividends. .\ncol :|3/31/2007|3/31/2008|3/31/2009|3/31/2010|3/31/2011|3/31/2012 row 1 : abiomed\ninc|100|96.19|35.87|75.55|106.37|162.45 row 2 : nasdaq composite index|100|94.11|63.12\n|99.02|114.84|127.66 row 3 :  nasdaq medical equipment sic code 3840-|100|82.91|41.56\n|77.93|94.54|74.40\nthis graph is not 201csoliciting material 201d under regulation 14a or 14c of the rules\npromulgated  under  the  securities  exchange  act  of  1934  ,  is  not  deemed  filed  with  the\nsecurities and exchange commission and is not to be incorporated by reference in any of our\nfilings under the securities act of 1933 , as amended , or the exchange act whether made\nbefore or after the date hereof and irrespective of any general incorporation language in any\nsuch filing . transfer agent american stock transfer & trust company , 59 maiden lane , new\nyork , ny 10038 , is our stock transfer agent. .\nquestion:\nwhat is the roi of an investment in nasdaq composite index from march 2007 to march 2010?\nwrite down a short python expression.\n(99.02 - 100) / 100\nIn the above example,StructLM-13B identified the relevant quantity within the given lin-\nearized table, then determined how to calculate the required ROI quantity in a python-\nexecutable expression.\n20",
    "Manuscript\nE.2    UnsuccessfulStructLM-13B Response on FinQA\nUse  the  financial  data  in  the  following  passage  and  table  to  calculate  the  answer  the\nquestion. table:\nperformance graph the following graph and table compares the cumulative five-year total\nreturn provided to shareholders on our class a common stock relative to the cumulative\ntotal returns of the s&p 500 index and our customized peer group . the peer group includes\ncboe holdings , inc. , intercontinentalexchange group , inc . and nasdaq , inc . an investment\nof$ 100 ( with reinvestment of all dividends ) is assumed to have been made in our class\na common stock , in the peer group and the s&p 500 index on december 31 , 2012 , and\nits relative performance is tracked through december 31 , 2017 .  comparison of 5 year\ncumulative total return* among cme group inc. , the s&p 500 index , and a peer group 12/12\n12/13 12/14 12/15 12/16 cme group inc . s&p 500 peer group *$ 100 invested on 12/31/12\nin stock or index , including reinvestment of dividends . fiscal year ending december 31 .\ncopyright a9 2018 standard & poor 2019s , a division of s&p global . all rights reserved . the\nstock price performance included in this graph is not necessarily indicative of future stock\nprice performance. .\ncol :|2013|2014|2015|2016|2017 row 1 :  cme group inc .|$ 164.01|$ 194.06|$ 208.95|$\n279.85|$ 370.32 row 2 : s&p 500|132.39|150.51|152.59|170.84|208.14 row 3 : peer group\n|176.61|187.48|219.99|249.31|323.23\nunregistered sales of equity securities during the past three years there have not been any\nunregistered sales by the company of equity securities. .\nquestion:\nin 2017 what was the ratio of the the cme group inc . stock perfomamce to the s&p write\ndown a short python expression.\n(cmegroupincstockperformance / s&p500stockperformance)\nIn the above example,StructLM-13B understood how to calculate the required quantity, but\ncould not select the relevant numbers from the given table.\n21",
    "Manuscript\nLimitations\nThe collection process used to construct the training data forStructLMtries to include a\nwide a variety of data types. As we have seen, there is evidence that this diversity is capable\nof affording transferable benefits to each dataset in the mixture. However, the tasks that\nwe train and evaluate on are still academic datasets which have each been curated and\ndesigned for a specific purpose. While these are indeed diverse, the SKG domain relies on\nspecific formatting and prompting conventions, which may result in our models having\nunnecessary specificity towards the conventions within our train set. To develop a clearer\npicture of how SKG performs as its own domain, we may require larger scale datasets with\nmore heterogeneous formatting conventions. Further opportunities for training more robust\nSKG models may lie in increasing the diversity of structured data types in this way.\nAdditionally, while we have tried to evaluate our models to the best of our ability, many of\nthe tasks of our held-out datasets measure accuracy through a heuristic matching step of a\nmodel’s output. In zero or few-shot settings, it is quite challenging to exactly control the\ngenerations of an autoregressive transformer to be adherent to a certain rule or grammar,\nand this has been a subject of study in may of the other works cited in Table 4. We note that\nbecause of this reality, poor results in zero or few-shot context may betray the existence of\nuseful representations that the model has already learned. Without further prompting or\nfinetuning efforts, it may be difficult to bring these capabilities to light. As such, another\nopportunity for improvement upon our methods may involve more flexible constrained\nmethods of language model evaluation.\n22"
  ]
}