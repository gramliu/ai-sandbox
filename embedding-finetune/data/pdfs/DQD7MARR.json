{
  "key": "DQD7MARR",
  "url": "http://arxiv.org/pdf/2203.15556",
  "metadata": {
    "title": "Training Compute-Optimal Large Language Models",
    "abstract": "  We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.\n",
    "published": "2022-03-29T13:38:03Z"
  },
  "text": [
    "Training Compute-Optimal Large Language Models\nJordan Hoffmann\n★\n, Sebastian Borgeaud\n★\n, Arthur Mensch\n★\n, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\nErich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre\n★\n★\nEqual contributions\nWe investigate the optimal model size and number of tokens for training a transformer language model\nunder a given compute budget. We find that current large language models are significantly under-\ntrained, a consequence of the recent focus on scaling language models whilst keeping the amount of\ntraining data constant. By training over 400 language models ranging from 70 million to over 16 billion\nparameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and\nthe number of training tokens should be scaled equally: for every doubling of model size the number\nof training tokens should also be doubled. We test this hypothesis by training a predicted compute-\noptimal model,Chinchilla, that uses the same compute budget asGopherbut with 70B parameters and\n4\u0002more more data.Chinchillauniformly and significantly outperformsGopher(280B), GPT-3 (175B),\nJurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.\nThis also means thatChinchillauses substantially less compute for fine-tuning and inference, greatly\nfacilitating downstream usage. As a highlight,Chinchillareaches a state-of-the-art average accuracy of\n67.5% on the MMLU benchmark, greater than a 7% improvement overGopher.\n1. Introduction\nRecently a series ofLarge Language Models(LLMs) have been introduced (Brown et al., 2020; Lieber\net al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022), with the largest dense\nlanguage models now having over 500 billion parameters. These large autoregressive transformers\n(Vaswani et al., 2017) have demonstrated impressive performance on many tasks using a variety of\nevaluation protocols such as zero-shot, few-shot, and fine-tuning.\nThe compute and energy cost for training large language models is substantial (Rae et al., 2021;\nThoppilan et al., 2022) and rises with increasing model size. In practice, the allocated training\ncompute budget is often known in advance: how many accelerators are available and for how long\nwe want to use them. Since it is typically only feasible to train these large models once, accurately\nestimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021).\nKaplan et al. (2020) showed that there is a power law relationship between the number of\nparameters in an autoregressive language model (LM) and its performance. As a result, the field has\nbeen training larger and larger models, expecting performance improvements. One notable conclusion\nin Kaplan et al. (2020) is that large models should not be trained to their lowest possible loss to be\ncompute optimal. Whilst we reach the same conclusion, we estimate that large models should be\ntrained for many more training tokens than recommended by the authors. Specifically, given a10\u0002\nincrease computational budget, they suggests that the size of the model should increase55\u0002while\nthe number of training tokens should only increase 1.8\u0002. Instead, we find that model size and the\nnumber of training tokens should be scaled in equal proportions.\nFollowing Kaplan et al. (2020) and the training setup of GPT-3 (Brown et al., 2020), many of the\nrecently trained large models have been trained for approximately 300 billion tokens (Table 1), in\nline with the approach of predominantly increasing model size when increasing compute.\nCorresponding authors: {jordanhoffmann|sborgeaud|amensch|sifre}@deepmind.com\n©2023 DeepMind. All rights reserved\narXiv:2203.15556v1  [cs.CL]  29 Mar 2022",
    "10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n10M\n100M\n1.0B\n10B\n100B\n1T\nParameters\nApproach 1\nApproach 2\nApproach 3\nKaplan et al (2020)\n \nChinchilla (70B)\nGopher (280B)\nGPT-3 (175B)\nMegatron-Turing NLG (530B)\nFigure 1jOverlaid predictions.We overlay the predictions from our three different approaches,\nalong with projections from Kaplan et al. (2020). We find that all three methods predict that current\nlarge models should be substantially smaller and therefore trained much longer than is currently\ndone. In Figure A3, we show the results with the predicted optimal tokens plotted against the optimal\nnumber of parameters for fixed FLOP budgets.ChinchillaoutperformsGopherand the other large\nmodels (see Section 4.2).\nIn this work, we revisit the question:Given a fixed FLOPs budget,\n1\nhow should one trade-off model\nsize and the number of training tokens?To answer this question, we model the final pre-training loss\n2\n퐿¹푁 퐷ºas a function of the number of model parameters푁, and the number of training tokens,퐷.\nSince the computational budget퐶is a deterministic functionFLOPs¹푁 퐷ºof the number of seen\ntraining tokens and model parameters, we are interested in minimizing퐿under the constraint\nFLOPs¹푁 퐷º=퐶:\n푁\n표푝푡\n¹퐶º 퐷\n표푝푡\n¹퐶º=argmin\n푁퐷s.t. FLOPs¹푁퐷º=퐶\n퐿¹푁 퐷º(1)\nThe functions푁\n표푝푡\n¹퐶º, and퐷\n표푝푡\n¹퐶ºdescribe the optimal allocation of a computational budget퐶. We\nempirically estimate these functions based on the losses of over 400 models, ranging from under70M\nto over16B parameters, and trained on5B to over400B tokens – with each model configuration\ntrained for several different training horizons. Our approach leads to considerably different results\nthan that of Kaplan et al. (2020). We highlight our results in Figure 1 and how our approaches differ\nin Section 2.\nBased on our estimated compute-optimal frontier, we predict that for the compute budget used\nto trainGopher, an optimal model should be 4 times smaller, while being training on 4 times more\ntokens. We verify this by training a morecompute-optimal70B model, calledChinchilla, on 1.4 trillion\ntokens. Not only doesChinchillaoutperform its much larger counterpart,Gopher, but its reduced\nmodel size reduces inference cost considerably and greatly facilitates downstream uses on smaller\nhardware. The energy cost of a large language model is amortized through its usage for inference an\nfine-tuning. The benefits of a more optimally trained smaller model, therefore, extend beyond the\nimmediate benefits of its improved performance.\n1\nFor example, knowing the number of accelerators and a target training duration.\n2\nFor simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the test loss, as\nwe are in the infinite data regime (the number of training tokens is less than the number of tokens in the entire corpus).\n2",
    "Table 1jCurrent LLMs. We show five of the current largest dense transformer models, their size,\nand the number of training tokens. Other than LaMDA (Thoppilan et al., 2022), most models are\ntrained for approximately 300 billion tokens. We introduceChinchilla, a substantially smaller model,\ntrained for much longer than 300B tokens.\nModelSize (#Parameters)  Training Tokens\nLaMDA (Thoppilan et al., 2022)137 Billion168 Billion\nGPT-3 (Brown et al., 2020)175 Billion300 Billion\nJurassic (Lieber et al., 2021)178 Billion300 Billion\nGopher(Rae et al., 2021)280 Billion300 Billion\nMT-NLG 530B (Smith et al., 2022)530 Billion270 Billion\nChinchilla70 Billion1.4 Trillion\n2. Related Work\nLarge language models.A variety of large language models have been introduced in the last few\nyears. These include both dense transformer models (Brown et al., 2020; Lieber et al., 2021; Rae\net al., 2021; Smith et al., 2022; Thoppilan et al., 2022) and mixture-of-expert (MoE) models (Du\net al., 2021; Fedus et al., 2021; Zoph et al., 2022). The largest dense transformers have passed 500\nbillion parameters (Smith et al., 2022). The drive to train larger and larger models is clear—so far\nincreasing the size of language models has been responsible for improving the state-of-the-art in many\nlanguage modelling tasks. Nonetheless, large language models face several challenges, including\ntheir overwhelming computational requirements (the cost of training and inference increase with\nmodel size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality\ntraining data. In fact, in this work we find that larger, high quality datasets will play a key role in any\nfurther scaling of language models.\nModelling the scaling behavior.\nUnderstanding the scaling behaviour of language models and\ntheir transfer properties has been important in the development of recent large models (Hernandez\net al., 2021; Kaplan et al., 2020). Kaplan et al. (2020) first showed a predictable relationship between\nmodel size and loss over many orders of magnitude. The authors investigate the question of choosing\nthe optimal model size to train for a given compute budget. Similar to us, they address this question\nby training various models. Our work differs from Kaplan et al. (2020) in several important ways.\nFirst, the authors use a fixed number of training tokens and learning rate schedule for all models; this\nprevents them from modelling the impact of these hyperparameters on the loss. In contrast, we find\nthat setting the learning rate schedule to approximately match the number of training tokens results\nin the best final loss regardless of model size—see Figure A1. For a fixed learning rate cosine schedule\nto 130B tokens, the intermediate loss estimates (for퐷\n0\n130B) are therefore overestimates of the\nloss of a model trained with a schedule length matching퐷\n0\n. Using these intermediate losses results in\nunderestimating the effectiveness of training models on less data than 130B tokens, and eventually\ncontributes to the conclusion that model size should increase faster than training data size as compute\nbudget increases. In contrast, our analysis predicts that both quantities should scale at roughly the\nsame rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight\ncurvature in the FLOP-loss frontier (see Appendix E)—in fact, the majority of the models used in\nour analysis have more than 500 million parameters, in contrast the majority of runs in Kaplan et al.\n(2020) are significantly smaller—many being less than 100M parameters.\nRecently, Clark et al. (2022) specifically looked in to the scaling properties of Mixture of Expert\n3",
    "language models, showing that the scaling with number of experts diminishes as the model size\nincreases—their approach models the loss as a function of two variables: the model size and the\nnumber of experts. However, the analysis is done with a fixed number of training tokens, as in Kaplan\net al. (2020), potentially underestimating the improvements of branching.\nEstimating hyperparameters for large models.The model size and the number of training tokens\nare not the only two parameters to chose when selecting a language model and a procedure to train\nit. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and\nwidth-to-depth ratio. In this work, we focus on model size and the number of training steps, and\nwe rely on existing work and provided experimental heuristics to determine the other necessary\nhyperparameters. Yang et al. (2021) investigates how to choose a variety of these parameters for\ntraining an autoregressive transformer, including the learning rate and batch size. McCandlish et al.\n(2018) finds only a weak dependence between optimal batch size and model size. Shallue et al.\n(2018); Zhang et al. (2019) suggest that using larger batch-sizes than those we use is possible. Levine\net al. (2020) investigates the optimal depth-to-width ratio for a variety of standard model sizes. We\nuse slightly less deep models than proposed as this translates to better wall-clock performance on our\nhardware.\nImproved model architectures.\nRecently, various promising alternatives to traditional dense trans-\nformers have been proposed. For example, through the use of conditional computation large MoE\nmodels like the 1.7 trillion parameter Switch transformer (Fedus et al., 2021), the 1.2 Trillion pa-\nrameter GLaM model (Du et al., 2021), and others (Artetxe et al., 2021; Zoph et al., 2022) are able\nto provide a large effective model size despite using relatively fewer training and inference FLOPs.\nHowever, for very large models the computational benefits of routed models seems to diminish (Clark\net al., 2022). An orthogonal approach to improving language models is to augment transformers\nwith explicit retrieval mechanisms, as done by Borgeaud et al. (2021); Guu et al. (2020); Lewis et al.\n(2020). This approach effectively increases the number of data tokens seen during training (by a\nfactor of\u001810in Borgeaud et al. (2021)). This suggests that the performance of language models\nmay be more dependant on the size of the training data than previously thought.\n3. Estimating the optimal parameter/training tokens allocation\nWe present three different approaches to answer the question driving our research:Given a fixed\nFLOPs budget, how should one trade-off model size and the number of training tokens?In all three\ncases we start by training a range of models varying both model size and the number of training\ntokens and use the resulting training curves to fit an empirical estimator of how they should scale.\nWe assume a power-law relationship between compute and model size as done in Clark et al. (2022);\nKaplan et al. (2020), though future work may want to include potential curvature in this relationship\nfor large model sizes. The resulting predictions are similar for all three methods and suggest that\nparameter count and number of training tokens should be increased equally with more compute\n3\n—\nwith proportions reported in Table 2. This is in clear contrast to previous work on this topic and\nwarrants further investigation.\n3\nWe compute FLOPs as described in Appendix F.\n4",
    "10\n17\n10\n18\n10\n19\n10\n20\n10\n21\n10\n22\nFLOPS\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nTraining loss\n75M\n250M\n500M\n1B\n2.5B\n5B\n10B\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n10\n9\n10\n10\n10\n11\n10\n12\nTokens\n1.5T\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n100M\n1.0B\n10B\n100B\n1T\nParameters\n67B\nFigure 2jTraining curve envelope.On theleftwe show all of our different runs. We launched a\nrange of model sizes going from 70M to 10B, each for four different cosine cycle lengths. From these\ncurves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the\noptimal model size (center) for a given compute budget and the optimal number of training tokens\n(right). In green, we show projections of optimal model size and training token count based on the\nnumber of FLOPs used to trainGopher(576\u000210\n23\n).\n3.1. Approach 1: Fix model sizes and vary number of training tokens\nIn our first approach we vary the number of training steps for a fixed family of models (ranging from\n70M to over 10B parameters), training each model for 4 different number of training sequences.\nFrom these runs, we are able to directly extract an estimate of the minimum loss achieved for a given\nnumber of training FLOPs. Training details for this approach can be found in Appendix D.\nFor each parameter count푁we train 4 different models, decaying the learning rate by a factor of\n10\u0002over a horizon (measured in number of training tokens) that ranges by a factor of16\u0002. Then, for\neach run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous\nmapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine\nwhich run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP\ncount퐶, to the most efficient choice of model size푁and number of training tokens퐷such that\nFLOPs¹푁 퐷º=퐶.\n4\nAt 1500 logarithmically spaced FLOP values, we find which model size achieves the\nlowest loss of all models along with the required number of training tokens. Finally, we fit power laws\nto estimate the optimal model size and number of training tokens for any given amount of compute\n(see the center and right panels of Figure 2), obtaining a relationship푁\n표푝푡\n/퐶\n푎\nand퐷\n표푝푡\n/퐶\n푏\n. We\nfind that푎=050and푏=050—as summarized in Table 2. In Section D.4, we show a head-to-head\ncomparison at10\n21\nFLOPs, using the model size recommended by our analysis and by the analysis of\nKaplan et al. (2020)—using the model size we predict has a clear advantage.\n3.2. Approach 2: IsoFLOP profiles\nIn our second approach we vary the model size\n5\nfor a fixed set of 9 different training FLOP counts\n6\n(ranging from6\u000210\n18\nto3\u000210\n21\nFLOPs), and consider the final training loss for each point\n7\n. in\ncontrast with Approach 1 that considered points¹푁 퐷 퐿ºalong the entire training runs. This allows\nus to directly answer the question: For a given FLOP budget, what is the optimal parameter count?\n4\nNote that all selected points are within the last 15% of training. This suggests that when training a model over퐷tokens,\nwe should pick a cosine cycle length that decays10\u0002over approximately퐷tokens—see further details in Appendix B.\n5\nIn approach 2, model size varies up to 16B as opposed to approach 1 where we only used models up to 10B.\n6\nThe number of training tokens is determined by the model size and training FLOPs.\n7\nWe set the cosine schedule length to match the number of tokens, which is optimal according to the analysis presented\nin Appendix B.\n5",
    "100M300M1B3B6B30B\nParameters\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\nTraining Loss\n6e18\n1e19\n3e19\n6e19\n1e20\n3e20\n6e20\n1e21\n3e21\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n100M\n1B\n10B\n100B\n1T\nParameters\n63B\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n100M\n1B\n10B\n100B\n1T\n10T\nTokens\n1.4T\nFigure 3jIsoFLOP curves.For various model sizes, we choose the number of training tokens such\nthat the final FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We\nfind a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train\n(left). Using the location of these valleys, we project optimal model size and number of tokens for\nlarger models (centerandright). In green, we show the estimated number of parameters and tokens\nfor anoptimalmodel trained with the compute budget ofGopher.\nFor each FLOP budget, we plot the final loss (after smoothing) against the parameter count in\nFigure 3 (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see\na clear minimum in the loss. We fit a parabola to each IsoFLOPs curve to directly estimate at what\nmodel size the minimum loss is achieved (Figure 3 (left)). As with the previous approach, we then fit\na power law between FLOPs and loss-optimal model size and number of training tokens, shown in\nFigure 3 (center, right). Again, we fit exponents of the form푁\n표푝푡\n/퐶\n푎\nand퐷\n표푝푡\n/퐶\n푏\nand we find that\n푎=049and푏=051—as summarized in Table 2.\n3.3. Approach 3: Fitting a parametric loss function\nLastly, we model all final losses from experiments in Approach 1 & 2 as a parametric function of\nmodel parameter count and the number of seen tokens. Following a classical risk decomposition (see\nSection D.2), we propose the following functional form\n^\n퐿¹푁 퐷º,퐸 ̧\n퐴\n푁\n훼\n ̧\n퐵\n퐷\n훽\n(2)\nThe first term captures the loss for an ideal generative process on the data distribution, and should\ncorrespond to the entropy of natural text. The second term captures the fact that a perfectly trained\ntransformer with푁parameters underperforms the ideal generative process. The final term captures\nthe fact that the transformer is not trained to convergence, as we only make a finite number of\noptimisation steps, on a sample of the dataset distribution.\nModel fitting.To estimate¹퐴 퐵 퐸 훼 훽º, we minimize the Huber loss (Huber, 1964) between the\npredicted and observed log loss using the L-BFGS algorithm (Nocedal, 1980):\nmin\n퐴퐵퐸훼훽\n∑︁\nRuns푖\nHuber\n훿\n\u0010\nlog\n^\n퐿¹푁\n푖\n 퐷\n푖\nº\u0000log퐿\n푖\n\u0011\n(3)\nWe account for possible local minima by selecting the best fit from a grid of initialisations. The Huber\nloss (훿=10\n\u00003\n) is robust to outliers, which we find important for good predictive performance over\nheld-out data points. Section D.2 details the fitting procedure and the loss decomposition.\n6",
    "10\n18\n10\n19\n10\n20\n10\n21\n10\n22\n10\n23\nGopher\nbudget\nTraining FLOPs\n100M\n1B\n10B\n40B\n100B\nModel size\nIsoLoss contours\nEfficient frontier\nEmpirical data\nIsoFLOPs slice\n2.00\n3.00\n4.00\n5.00\nLoss\n100M1B10B40B\nModel size\nIsoFLOPs slices\nTrain. FLOPs\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\n6e+20\n1e+21\n3e+21\nGopher\nFigure 4jParametric fit.We fit a parametric modelling of the loss\n^\n퐿¹푁 퐷º\nand display contour (left)\nand isoFLOP slices (right). For each isoFLOP slice, we include a corresponding dashed line in the left\nplot. In the left plot, we show the efficient frontier in blue, which is a line in log-log space. Specifically,\nthe curve goes through each iso-loss contour at the point with the fewest FLOPs. We project the\noptimal model size given theGopherFLOP budget to be 40B parameters.\nEfficient frontier.\nWe can approximate the functions푁\n표푝푡\nand퐷\n표푝푡\nby minimizing the parametric\nloss\n^\n퐿under the constraintFLOPs¹푁 퐷º \u00196푁퐷(Kaplan et al., 2020). The resulting푁\n표푝푡\nand퐷\n표푝푡\nbalance the two terms in Equation(3)that depend on model size and data. By construction, they\nhave a power-law form:\n푁\n표푝푡\n¹퐶º=퐺\n\u0012\n퐶\n6\n\u0013\n푎\n  퐷\n표푝푡\n¹퐶º=퐺\n\u00001\n\u0012\n퐶\n6\n\u0013\n푏\nwhere퐺=\n\u0012\n훼퐴\n훽퐵\n\u0013\n1\n훼 ̧훽\n  푎=\n훽\n훼 ̧훽\nand푏=\n훼\n훼 ̧훽\n(4)\nWe show contours of the fitted function\n^\n퐿in Figure 4 (left), and the closed-form efficient computational\nfrontier in blue. From this approach, we find that푎=046and푏=054—as summarized in Table 2.\n3.4. Optimal model scaling\nWe find that the three approaches, despite using different fitting methodologies and different trained\nmodels, yield comparable predictions for the optimal scaling in parameters and tokens with FLOPs\n(shown in Table 2). All three approaches suggest that as compute budget increases, model size and\nthe amount of training data should be increased in approximately equal proportions. The first and\nsecond approaches yield very similar predictions for optimal model sizes, as shown in Figure 1 and\nFigure A3. The third approach predicts even smaller models being optimal at larger compute budgets.\nWe note that the observed points¹퐿 푁 퐷ºfor low training FLOPs (퐶61푒21) have larger residuals\nk퐿\u0000\n^\n퐿¹푁 퐷ºk\n2\n2\nthan points with higher computational budgets. The fitted model places increased\nweight on the points with more FLOPs—automatically considering the low-computational budget\npoints as outliers due to the Huber loss. As a consequence of the empirically observed negative\ncurvature in the frontier퐶!푁\n표푝푡\n(see Appendix E), this results in predicting a lower푁\n표푝푡\nthan the\ntwo other approaches.\nIn Table 3 we show the estimated number of FLOPs and tokens that would ensure that a model of\na given size lies on the compute-optimal frontier. Our findings suggests that the current generation of\n7",
    "Table 2jEstimated parameter and data scaling with increased training compute.The listed\nvalues are the exponents,푎and푏, on the relationship푁\n표푝푡\n/퐶\n푎\nand퐷\n표푝푡\n/퐶\n푏\n. Our analysis suggests\na near equal scaling in parameters and data with increasing compute which is in clear contrast\nto previous work on the scaling of large models. The 10\nth\nand 90\nth\npercentiles are estimated via\nbootstrapping data (80% of the dataset is sampled 100 times) and are shown in parenthesis.\nApproachCoeff.푎where푁\n표푝푡\n/퐶\n푎\nCoeff.푏where퐷\n표푝푡\n/퐶\n푏\n1. Minimum over training curves050¹04880502º050¹05010512º\n2. IsoFLOP profiles049¹04620534º051¹04830529º\n3. Parametric modelling of the loss046¹04540455º054¹05420543º\nKaplan et al. (2020)0.730.27\nTable 3jEstimated optimal training FLOPs and training tokens for various model sizes.For\nvarious model sizes, we show the projections from Approach 1 of how many FLOPs and training\ntokens would be needed to train compute-optimal models. The estimates for Approach 2 & 3 are\nsimilar (shown in Section D.3)\n.\nParametersFLOPs  FLOPs (inGopherunit)Tokens\n400 Million  1.92e+191299688.0 Billion\n1 Billion  1.21e+201476120.2 Billion\n10 Billion  1.23e+22146205.1 Billion\n67 Billion  5.76e+2311.5 Trillion\n175 Billion  3.85e+24673.7 Trillion\n280 Billion  9.90e+241725.9 Trillion\n520 Billion  3.43e+2559511.0 Trillion\n1 Trillion  1.27e+26221321.2 Trillion\n10 Trillion  1.30e+28225159216.2 Trillion\nlarge language models are considerably over-sized, given their respective compute budgets, as shown\nin Figure 1. For example, we find that a 175 billion parameter model should be trained with a compute\nbudget of441\u000210\n24\nFLOPs and on over 4.2 trillion tokens. A 280 billionGopher-like model is the\noptimal model to train given a compute budget of approximately10\n25\nFLOPs and should be trained on\n6.8 trillion tokens. Unless one has a compute budget of10\n26\nFLOPs (over 250\u0002the compute used to\ntrainGopher), a 1 trillion parameter model is unlikely to be the optimal model to train. Furthermore,\nthe amount of training data that is projected to be needed is far beyond what is currently used to\ntrain large models, and underscores the importance of dataset collection in addition to engineering\nimprovements that allow for model scale. While there is significant uncertainty extrapolating out\nmany orders of magnitude, our analysis clearly suggests that given the training compute budget for\nmany current LLMs, smaller models should have been trained on more tokens to achieve the most\nperformant model.\nIn Appendix C, we reproduce the IsoFLOP analysis on two additional datasets: C4 (Raffel et al.,\n2020a) and GitHub code (Rae et al., 2021). In both cases we reach the similar conclusion that model\nsize and number of training tokens should be scaled in equal proportions.\n8",
    "4.Chinchilla\nBased on our analysis in Section 3, the optimal model size for theGophercompute budget is somewhere\nbetween 40 and 70 billion parameters. We test this hypothesis by training a model on the larger end\nof this range—70B parameters—for 1.4T tokens, due to both dataset and computational efficiency\nconsiderations. In this section we compare this model, which we callChinchilla, toGopherand other\nLLMs. BothChinchillaandGopherhave been trained for the same number of FLOPs but differ in the\nsize of the model and the number of training tokens.\nWhile pre-training a large language model has a considerable compute cost, downstream fine-\ntuning and inference also make up substantial compute usage (Rae et al., 2021). Due to being4\u0002\nsmaller thanGopher, both the memory footprint and inference cost ofChinchillaare also smaller.\n4.1. Model and training details\nThe full set of hyperparameters used to trainChinchillaare given in Table 4.Chinchillauses the same\nmodel architecture and training setup asGopherwith the exception of the differences listed below.\n\nWe trainChinchillaonMassiveText(the same dataset asGopher) but use a slightly different\nsubset distribution (shown in Table A1) to account for the increased number of training tokens.\nWe use AdamW (Loshchilov and Hutter, 2019) forChinchillarather than Adam (Kingma and\nBa, 2014) as this improves the language modelling loss and the downstream task performance\nafter finetuning.\n8\nWe trainChinchillawith a slightly modified SentencePiece (Kudo and Richardson, 2018)\ntokenizer that does not apply NFKC normalisation. The vocabulary is very similar– 94.15% of\ntokens are the same as those used for trainingGopher. We find that this particularly helps with\nthe representation of mathematics and chemistry, for example.\n\nWhilst the forward and backward pass are computed inbfloat16, we store afloat32copy\nof the weights in the distributed optimiser state (Rajbhandari et al., 2020). SeeLessons Learned\nfrom Rae et al. (2021) for additional details.\nIn Appendix G we show the impact of the various optimiser related changes betweenChinchilla\nandGopher. All models in this analysis have been trained on TPUv3/TPUv4 (Jouppi et al., 2017) with\nJAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020). We include aChinchillamodel card\n(Mitchell et al., 2019) in Table A8.\nModelLayers  Number Heads  Key/Value Size  d\nmodel\nMax LR   Batch Size\nGopher280B8012812816,3844\u000210\n\u00005\n3M!6M\nChinchilla70B    80641288,1921\u000210\n\u00004\n1.5M!3M\nTable 4jChinchillaarchitecture details.We list the number of layers, the key/value size, the\nbottleneck activation size d\nmodel\n, the maximum learning rate, and the training batch size (# tokens).\nThe feed-forward size is always set to4\u0002d\nmodel\n. Note that we double the batch size midway through\ntraining for bothChinchillaandGopher.\n8\nInterestingly, a model trained with AdamW only passes the training performance of a model trained with Adam around\n80% of the way through the cosine cycle, though the ending performance is notably better– see Figure A7\n9",
    "# Tasks  Examples\nLanguage Modelling20WikiText-103, The Pile: PG-19, arXiv, FreeLaw,  \nReading Comprehension3RACE-m, RACE-h, LAMBADA\nQuestion Answering3Natural Questions, TriviaQA, TruthfulQA\nCommon Sense5HellaSwag, Winogrande, PIQA, SIQA, BoolQ\nMMLU57High School Chemistry, Astronomy, Clinical Knowledge,  \nBIG-bench62Causal Judgement, Epistemic Reasoning, Temporal Sequences,  \nTable 5jAll evaluation tasks.We evaluateChinchillaon a collection of language modelling along\nwith downstream tasks. We evaluate on largely the same tasks as in Rae et al. (2021), to allow for\ndirect comparison.\n4.2. Results\nWe perform an extensive evaluation ofChinchilla, comparing against various large language models.\nWe evaluate on a large subset of the tasks presented in Rae et al. (2021), shown in Table 5. As\nthe focus of this work is on optimal model scaling, we included a large representative subset, and\nintroduce a few new evaluations to allow for better comparison to other existing large models. The\nevaluation details for all tasks are the same as described in Rae et al. (2021).\n4.2.1. Language modelling\npubmed_abstracts\nnih_exporter\nuspto_backgrounds\npubmed_central\npile_cc\nbookcorpus2\nstackexchange\nopensubtitles\nopenwebtext2\nhackernews\ndm_mathematics\narxiv\nfreelaw\nbooks3\nphilpapers\ngithub\nubuntu_irc\neuroparl\ngutenberg_pg_19\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nDecrease in bpb \n compared to Gopher\nFigure 5jPile Evaluation.For the different evaluation sets in The Pile (Gao et al., 2020), we show\nthe bits-per-byte (bpb) improvement (decrease) ofChinchillacompared toGopher. On all subsets,\nChinchillaoutperformsGopher.\nChinchillasignificantly outperformsGopheron all evaluation subsets of The Pile (Gao et al.,\n2020), as shown in Figure 5. Compared to Jurassic-1 (178B) Lieber et al. (2021),Chinchillais more\nperformant on all but two subsets–dm_mathematicsandubuntu_irc– see Table A5 for a raw\nbits-per-byte comparison. On Wikitext103 (Merity et al., 2017),Chinchillaachieves a perplexity of\n7.16 compared to 7.75 forGopher. Some caution is needed when comparingChinchillawithGopher\non these language modelling benchmarks asChinchillais trained on 4\u0002more data thanGopherand\nthus train/test set leakage may artificially enhance the results. We thus place more emphasis on other\n10",
    "Random25.0%\nAverage human rater34.5%\nGPT-3 5-shot43.9%\nGopher5-shot60.0%\nChinchilla5-shot67.6%\nAverage human expert performance89.8%\nJune 2022 Forecast57.1%\nJune 2023 Forecast63.4%\nTable 6jMassive Multitask Language Understanding (MMLU).We report the average 5-shot\naccuracy over 57 tasks with model and human accuracy comparisons taken from Hendrycks et al.\n(2020). We also include the average prediction for state of the art accuracy in June 2022/2023 made\nby 73 competitive human forecasters in Steinhardt (2021).\ntasks for which leakage is less of a concern, such as MMLU (Hendrycks et al., 2020) and BIG-bench\n(BIG-bench collaboration, 2021) along with various closed-book question answering and common\nsense analyses.\n4.2.2. MMLU\nThe Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2020) consists\nof a range of exam-like questions on academic subjects. In Table 6, we reportChinchilla’s average\n5-shot performance on MMLU (the full breakdown of results is shown in Table A6). On this benchmark,\nChinchillasignificantly outperformsGopherdespite being much smaller, with an average accuracy of\n67.6% (improving uponGopherby 7.6%). Remarkably,Chinchillaeven outperforms the expert forecast\nfor June 2023 of 63.4% accuracy (see Table 6) (Steinhardt, 2021). Furthermore,Chinchillaachieves\ngreater than 90% accuracy on 4 different individual tasks–high_school_gov_and_politics,\ninternational_law, sociology, andus_foreign_policy. To our knowledge, no other model\nhas achieved greater than 90% accuracy on a subset.\nIn Figure 6, we show a comparison toGopherbroken down by task. Overall, we find thatChin-\nchillaimproves performance on the vast majority of tasks. On four tasks (college_mathematics,\neconometrics, moral_scenarios, andformal_logic)ChinchillaunderperformsGopher, and\nthere is no change in performance on two tasks.\n4.2.3. Reading comprehension\nOn the final word prediction dataset LAMBADA (Paperno et al., 2016),Chinchillaachieves 77.4%\naccuracy, compared to 74.5% accuracy fromGopherand 76.6% from MT-NLG 530B (see Table 7). On\nRACE-h and RACE-m (Lai et al., 2017),Chinchillagreatly outperformsGopher, improving accuracy\nby more than 10% in both cases—see Table 7.\n4.2.4. BIG-bench\nWe analysedChinchillaon the same set of BIG-bench tasks (BIG-bench collaboration, 2021) reported\nin Rae et al. (2021). Similar to what we observed in MMLU,ChinchillaoutperformsGopheron the\nvast majority of tasks (see Figure 7). We find thatChinchillaimproves the average performance\nby 10.7%, reaching an accuracy of 65.1% versus 54.4% forGopher. Of the 62 tasks we consider,\nChinchillaperforms worse thanGopheron only four—crash_blossom, dark_humor_detection,\n11",
    "college_mathematics\neconometrics\nmoral_scenarios\nformal_logic\nmedical_genetics\nmachine_learning\npublic_relations\nglobal_facts\nbusiness_ethics\nelectrical_engineering\ncollege_computer_science\nworld_religions\nhigh_school_us_history\nhigh_school_psychology\nmanagement\nhigh_school_computer_science\nmarketing\nhigh_school_physics\nhigh_school_macroeconomics\nsociology\nhigh_school_government_and_politics\nhigh_school_european_history\nnutrition\ncollege_medicine\nastronomy\nlogical_fallacies\nprofessional_psychology\nmiscellaneous\njurisprudence\nclinical_knowledge\nhigh_school_geography\nhigh_school_biology\ncollege_biology\ncollege_chemistry\nhigh_school_world_history\nus_foreign_policy\nvirology\nphilosophy\nmoral_disputes\nhuman_aging\ncomputer_security\nsecurity_studies\ninternational_law\nhigh_school_microeconomics\nhigh_school_statistics\nprofessional_accounting\nprofessional_medicine\nprehistory\nhigh_school_chemistry\nelementary_mathematics\nabstract_algebra\nanatomy\nprofessional_law\nhuman_sexuality\ncollege_physics\nhigh_school_mathematics\nconceptual_physics\n10\n0\n10\n20\n30\nRelative Improvement \n over Gopher\nFigure 6jMMLU results compared toGopherWe find thatChinchillaoutperformsGopherby 7.6%\non average (see Table 6) in addition to performing better on 51/57 individual tasks, the same on\n2/57, and worse on only 4/57 tasks.\nChinchilla  GopherGPT-3  MT-NLG 530B\nLAMBADA Zero-Shot77.474.5    76.276.6\nRACE-m Few-Shot86.875.1    58.1-\nRACE-h Few-Shot82.371.6    46.847.9\nTable 7jReading comprehension.On RACE-h and RACE-m (Lai et al., 2017),Chinchillaconsiderably\nimproves performance overGopher. Note that GPT-3 and MT-NLG 530B use a different prompt format\nthan we do on RACE-h/m, so results are not comparable toGopherandChinchilla. On LAMBADA\n(Paperno et al., 2016),Chinchillaoutperforms bothGopherand MT-NLG 530B.\nmathematical_inductionandlogical_args. Full accuracy results forChinchillacan be found\nin Table A7.\n4.2.5. Common sense\nWe evaluateChinchillaon various common sense benchmarks: PIQA (Bisk et al., 2020), SIQA (Sap\net al., 2019), Winogrande (Sakaguchi et al., 2020), HellaSwag (Zellers et al., 2019), and BoolQ\n(Clark et al., 2019). We find thatChinchillaoutperforms bothGopherand GPT-3 on all tasks and\noutperforms MT-NLG 530B on all but one task—see Table 8.\nOn TruthfulQA (Lin et al., 2021),Chinchillareaches 43.6%, 58.5%, and 66.7% accuracy with\n0-shot, 5-shot, and 10-shot respectively. In comparison,Gopherachieved only 29.5% 0-shot and 43.7%\n10-shot accuracy. In stark contrast with the findings of Lin et al. (2021), the large improvements\n(14.1% in 0-shot accuracy) achieved by Chinchilla suggest that better modelling of the pre-training\ndata alone can lead to substantial improvements on this benchmark.\n12",
    "crash_blossom\ndark_humor_detection\nmathematical_induction\nlogical_args\ngeneral_knowledge_json\nHuman_organs_senses_multiple_choice\nformal_fallacies_syllogisms_negation\nknown_unknowns\nnavigate\nsentence_ambiguity\nmoral_permissibility\nintent_recognition\nirony_identification\nentailed_polarity\nhyperbaton\nmisconceptions\nevaluating_information_essentiality\nsimilarities_abstraction\nepistemic_reasoning\nfantasy_reasoning\nmovie_dialog_same_or_different\nwinowhy\nnovel_concepts\ndiscourse_marker_prediction\nstrategyqa\ncausal_judgment\nhindu_knowledge\nphrase_relatedness\nalignment_questionnaire\nreasoning_about_colored_objects\ndate_understanding\npenguins_in_a_table\nfigure_of_speech_detection\ndisambiguation_q\nimplicatures\nSNARKS\nruin_names\nlogical_fallacy_detection\nanachronisms\nlogic_grid_puzzle\nriddle_sense\nanalytic_entailment\nquestion_selection\nnonsense_words_grammar\nphysics_mc\nempirical_judgments\nsports_understanding\ncrass_ai\nphysical_intuition\ntimedial\nimplicit_relations\nenglish_proverbs\npresuppositions_as_nli\nmovie_recommendation\nunderstanding_fables\nmetaphor_boolean\ntemporal_sequences\nlogical_sequence\nidentify_odd_metaphor\ngre_reading_comprehension\nodd_one_out\nanalogical_similarity\n20\n0\n20\n40\n60\n80\n100\n120\nRelative Improvement \n over Gopher\nFigure 7jBIG-bench results compared toGopherChinchillaout performsGopheron all but four\nBIG-bench tasks considered. Full results are in Table A7.\n4.2.6. Closed-book question answering\nResults on closed-book question answering benchmarks are reported in Table 9. On the Natural\nQuestions dataset (Kwiatkowski et al., 2019),Chinchillaachieves new closed-book SOTA accuracies:\n31.5% 5-shot and 35.5% 64-shot, compared to 21% and 28% respectively, forGopher. On TriviaQA\n(Joshi et al., 2017) we show results for both the filtered (previously used in retrieval and open-book\nwork) and unfiltered set (previously used in large language model evaluations). In both cases,\nChinchillasubstantially out performsGopher. On the filtered version, Chinchilla lags behind the open\nbook SOTA (Izacard and Grave, 2020) by only 7.9%. On the unfiltered set,Chinchillaoutperforms\nGPT-3—see Table 9.\n4.2.7. Gender bias and toxicity\nLarge Language Models carry potential risks such as outputting offensive language, propagating\nsocial biases, and leaking private information (Bender et al., 2021; Weidinger et al., 2021). We\nexpectChinchillato carry risks similar toGopherbecauseChinchillais trained on the same data,\nChinchilla  GopherGPT-3  MT-NLG 530B  Supervised SOTA\nHellaSWAG80.8%79.2%  78.9%80.2%93.9%\nPIQA81.8%    81.8%  81.0%82.0%90.1%\nWinogrande74.9%70.1%  70.2%73.0%91.3%\nSIQA51.3%50.6%--83.2%\nBoolQ83.7%    79.3%  60.5%78.2%91.4%\nTable 8jZero-shot comparison on Common Sense benchmarks.We show a comparison between\nChinchilla,Gopher, and MT-NLG 530B on various Common Sense benchmarks. We see thatChinchilla\nmatches or outperformsGopherand GPT-3 on all tasks. On all but oneChinchillaoutperforms the\nmuch larger MT-NLG 530B model.\n13",
    "MethodChinchilla  GopherGPT-3   SOTA (open book)\nNatural Questions (dev)\n0-shot16.6%    10.1%   14.6%\n54.4%5-shot31.5%    24.5%-\n64-shot    35.5%    28.2%   29.9%\nTriviaQA (unfiltered, test)\n0-shot67.0%    52.8%  64.3 %\n-5-shot73.2%    63.6%-\n64-shot    72.3%    61.3%   71.2%\nTriviaQA (filtered, dev)\n0-shot55.4%    43.5%-\n72.5%5-shot64.1%    57.0%-\n64-shot    64.6%    57.2%-\nTable 9jClosed-book question answering.For Natural Questions (Kwiatkowski et al., 2019) and\nTriviaQA (Joshi et al., 2017),ChinchillaoutperformsGopherin all cases. On Natural Questions,\nChinchillaoutperforms GPT-3. On TriviaQA we show results on two different evaluation sets to allow\nfor comparison to GPT-3 and to open book SOTA (FiD + Distillation (Izacard and Grave, 2020)).\nalbeit with slightly different relative weights, and because it has a similar architecture. Here, we\nexamine gender bias (particularly gender and occupation bias) and generation of toxic language. We\nselect a few common evaluations to highlight potential issues, but stress that our evaluations are not\ncomprehensive and much work remains to understand, evaluate, and mitigate risks in LLMs.\nGender bias.As discussed in Rae et al. (2021), large language models reflect contemporary and\nhistorical discourse about different groups (such as gender groups) from their training dataset, and\nwe expect the same to be true forChinchilla. Here, we test if potential gender and occupation biases\nmanifest in unfair outcomes on coreference resolutions, using the Winogender dataset (Rudinger\net al., 2018) in a zero-shot setting. Winogender tests whether a model can correctly determine if\na pronoun refers to different occupation words. An unbiased model would correctly predict which\nword the pronoun refers to regardless of pronoun gender. We follow the same setup as in Rae et al.\n(2021) (described further in Section H.3).\nAs shown in Table 10,Chinchillacorrectly resolves pronouns more frequently thanGopheracross\nall groups. Interestingly, the performance increase is considerably smaller for male pronouns (increase\nof 3.2%) than for female or neutral pronouns (increases of 8.3% and 9.2% respectively). We also\nconsidergotchaexamples, in which the correct pronoun resolution contradicts gender stereotypes\n(determined by labor statistics). Again, we see thatChinchillaresolves pronouns more accurately\nthanGopher. When breaking up examples by male/female gender andgotcha/not gotcha, the largest\nimprovement is on femalegotchaexamples (improvement of 10%). Thus, thoughChinchillauniformly\novercomes gender stereotypes for more coreference examples thanGopher, the rate of improvement\nis higher for some pronouns than others, suggesting that the improvements conferred by using a more\ncompute-optimal model can be uneven.\nSample toxicity.Language models are capable of generating toxic language—including insults,\nhate speech, profanities and threats (Gehman et al., 2020; Rae et al., 2021). While toxicity is an\numbrella term, and its evaluation in LMs comes with challenges (Welbl et al., 2021; Xu et al., 2021),\nautomatic classifier scores can provide an indication for the levels of harmful text that a LM generates.\nRae et al. (2021) found that improving language modelling loss by increasing the number of model\nparameters has only a negligible effect on toxic text generation (unprompted); here we analyze\n14",
    "ChinchillaGopher\nAll78.3%71.4%\nMale71.2%68.0%\nFemale79.6%71.3%\nNeutral\n84.2%75.0%\nChinchillaGopher\nMalegotcha62.5%59.2%\nMalenot gotcha80.0%76.7%\nFemalegotcha76.7%66.7%\nFemalenot gotcha\n82.5%75.8%\nTable 10jWinogender results. Left:Chinchillaconsistently resolves pronouns better thanGopher.\nRight:Chinchillaperforms better on examples which contradict gender stereotypes (gotchaexamples).\nHowever, difference in performance across groups suggestsChinchillaexhibits bias.\nwhether the same holds true for a lower LM loss achieved via more compute-optimal training. Similar\nto the protocol of Rae et al. (2021), we generate 25,000 unprompted samples fromChinchilla, and\ncompare theirPerspectiveAPItoxicity score distribution to that ofGopher-generated samples. Several\nsummary statistics indicate an absence of major differences: the mean (median) toxicity score for\nGopheris 0.081 (0.064), compared to 0.087 (0.066) forChinchilla, and the95\nth\npercentile scores\nare 0.230 forGopher, compared to 0.238 forChinchilla. That is, the large majority of generated\nsamples are classified as non-toxic, and the difference between the models is negligible. In line with\nprior findings (Rae et al., 2021), this suggests that toxicity levels in unconditional text generation\nare largely independent of the model quality (measured in language modelling loss), i.e. that better\nmodels of the training dataset are not necessarily more toxic.\n5. Discussion & Conclusion\nThe trend so far in large language model training has been to increase the model size, often without\nincreasing the number of training tokens. The largest dense transformer, MT-NLG 530B, is now\nover3\u0002larger than GPT-3’s 170 billion parameters from just two years ago. However, this model,\nas well as the majority of existing large models, have all been trained for a comparable number\nof tokens—around 300 billion. While the desire to train these mega-models has led to substantial\nengineering innovation, we hypothesize that the race to train larger and larger models is resulting in\nmodels that are substantially underperforming compared to what could be achieved with the same\ncompute budget.\nWe propose three predictive approaches towards optimally setting model size and training dura-\ntion, based on the outcome of over 400 training runs. All three approaches predict thatGopheris\nsubstantially over-sized and estimate that for the same compute budget a smaller model trained on\nmore data will perform better. We directly test this hypothesis by trainingChinchilla, a 70B parameter\nmodel, and show that it outperformsGopherand even larger models on nearly every measured\nevaluation task.\nWhilst our method allows us to make predictions on how to scale large models when given\nadditional compute, there are several limitations. Due to the cost of training large models, we only\nhave two comparable training runs at large scale (ChinchillaandGopher), and we do not have\nadditional tests at intermediate scales. Furthermore, we assume that the efficient computational\nfrontier can be described by a power-law relationship between the compute budget, model size, and\nnumber of training tokens. However, we observe some concavity inlog\n\u0000\n푁\n표푝푡\n\u0001\nat high compute budgets\n(see Appendix E). This suggests that we may still be overestimating the optimal size of large models.\nFinally, the training runs for our analysis have all been trained on less than an epoch of data; future\nwork may consider the multiple epoch regime. Despite these limitations, the comparison ofChinchilla\ntoGophervalidates our performance predictions, that have thus enabled training a better (and more\n15",
    "lightweight) model at the same compute budget.\nThough there has been significant recent work allowing larger and larger models to be trained,\nour analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that\nscaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for\nresponsibly collecting larger datasets with a high focus on dataset quality. Larger datasets will require\nextra care to ensure train-test set overlap is properly accounted for, both in the language modelling\nloss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical\nand privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and\nprivate information. With even larger datasets being used, the quantity (if not the frequency) of such\ninformation increases, which makes dataset introspection all the more important.Chinchilladoes\nsuffer from bias and toxicity but interestingly it seems less affected thanGopher. Better understanding\nhow performance of large language models and toxicity interact is an important future research\nquestion.\nWhile we have applied our methodology towards the training of auto-regressive language models,\nwe expect that there is a similar trade-off between model size and the amount of data in other\nmodalities. As training large models is very expensive, choosing the optimal model size and training\nsteps beforehand is essential. The methods we propose are easy to reproduce in new settings.\n6. Acknowledgements\nWe’d like to thank Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis,\nGeoffrey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on\nthe manuscript. We’d like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and\nother colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA\nteam for their support and assistance.\nReferences\nM. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru,\nG. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou, P. S. Koura, B. O’Horo,\nJ. Wang, L. Zettlemoyer, M. Diab, Z. Kozareva, and V. Stoyanov. Efficient Large Scale Language\nModeling with Mixtures of Experts.arXiv:2112.10684, 2021.\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots:\nCan language models be too big?  InProceedingsofthe2021ACMConferenceonFairness,\nAccountability,andTransparency, pages 610–623, 2021.\nBIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of\nlanguage models.Inpreparation, 2021. URLhttps://github.com/google/BIG-bench/.\nY. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about physical commonsense in natural\nlanguage. InProceedingsoftheAAAIConferenceonArtificialIntelligence, volume 34, pages\n7432–7439, 2020.\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J.-B.\nLespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,\nL. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero,\nK. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from\ntrillions of tokens.arXiv2112.04426, 2021.\n16",
    "J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-\nderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy\nprograms. 2018. URLhttp://github.com/google/jax.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot\nlearners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,Advances\ninNeuralInformationProcessingSystems, volume 33, pages 1877–1901. Curran Associates, Inc.,\n2020. URLhttps://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb49674\n18bfb8ac142f64a-Paper.pdf.\nS. Bubeck. Convex Optimization: Algorithms and Complexity.FoundationsandTrendsinMachine\nLearning, 8(3-4):231–357, 2015. URLhttp://www.nowpublishers.com/article/Detail\ns/MAL-050.\nA. Clark, D. d. l. Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hechtman,\nT. Cai, S. Borgeaud, G. v. d. Driessche, E. Rutherford, T. Hennigan, M. Johnson, K. Millican,\nA. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, J. Rae, E. Elsen,\nK. Kavukcuoglu, and K. Simonyan. Unified scaling laws for routed language models, 2022. URL\nhttps://arxiv.org/abs/2202.01169.\nC. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring\nthe surprising difficulty of natural yes/no questions. InProceedingsofthe2019Conferenceof\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguage\nTechnologies,Volume1(LongandShortPapers), pages 2924–2936, 2019.\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph,\nL. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-\nHellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Efficient scaling of\nlanguage models with mixture-of-experts, 2021. URLhttps://arxiv.org/abs/2112.06905.\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity.arXivpreprintarXiv:2101.03961, 2021.\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,\nS. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling.arXiv\npreprintarXiv:2101.00027, 2020.\nS. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating\nneural toxic degeneration in language models. InFindingsoftheAssociationforComputational\nLinguistics:EMNLP2020, pages 3356–3369, Online, Nov. 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URLhttps://aclanthology.org/2\n020.findings-emnlp.301.\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model\npre-training, 2020.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding.arXivpreprintarXiv:2009.03300, 2020.\nT. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX. 2020. URLhttp:\n//github.com/deepmind/dm-haiku.\n17",
    "D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. Scaling laws for transfer, 2021.\nP. J. Huber. Robust Estimation of a Location Parameter.TheAnnalsofMathematicalStatistics, 35\n(1):73–101, Mar. 1964. ISSN 0003-4851, 2168-8990. doi: 10.1214/aoms/1177703732. URL\nhttps://projecteuclid.org/journals/annals-of-mathematical-statistics/vol\nume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/11\n77703732.full.\nG. Izacard and E. Grave. Distilling knowledge from reader to retriever for question answering, 2020.\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge\nDataset for Reading Comprehension.arXive-prints, art. arXiv:1705.03551, 2017.\nN. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,\nA. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb,\nT. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt,\nD. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy,\nJ. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony,\nK. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda,\nA. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter,\nD. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter,\nW. Wang, E. Wilcox, and D. H. Yoon. In-datacenter performance analysis of a tensor processing unit.\nInProceedingsofthe44thAnnualInternationalSymposiumonComputerArchitecture, ISCA ’17,\npage 1–12, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348928.\ndoi: 10.1145/3079856.3080246. URLhttps://doi.org/10.1145/3079856.3080246.\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. Scaling laws for neural language models.arXivpreprintarXiv:2001.08361, 2020.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization.arXivpreprintarXiv:1412.6980,\n2014.\nT. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing.arXivpreprintarXiv:1808.06226, 2018.\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,\nM. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and\nS. Petrov. Natural questions: a benchmark for question answering research.Transactionsofthe\nAssociationofComputationalLinguistics, 2019.\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding comprehension dataset from\nexaminations. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing, pages 785–794, Copenhagen, Denmark, Sept. 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/D17-1082. URLhttps://aclanthology.org/D17-1082.\nY. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention.\narXivpreprintarXiv:2006.12467, 2020.\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih,\nT. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. InAdvancesinNeuralInformationProcessingSystems, volume 33, pages 9459–9474,\n2020.\n18",
    "O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation.White\nPaper.AI21Labs, 2021.\nS. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods.arXiv\npreprintarXiv:2109.07958, 2021.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. InInternationalConferenceon\nLearningRepresentations, 2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.\nS. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. An empirical model of large-batch training,\n2018.\nS. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models.International\nConferenceonLearningRepresentations, 2017.\nM. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji, and T. Ge-\nbru. Model cards for model reporting. InProceedingsoftheconferenceonfairness,accountability,\nandtransparency, pages 220–229, 2019.\nJ. Nocedal. Updating Quasi-Newton Matrices with Limited Storage.MathematicsofComputation,\n35(151):773–782, 1980. ISSN 0025-5718. doi: 10.2307/2006193. URLhttps://www.jstor.\norg/stable/2006193.\nD. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,\nand R. Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context,\n2016.\nJ. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,\nS. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A.\nHendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor,\nI. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden,\nE. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh,\nE. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev,\nD. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d’Autume, Y. Li,\nT. Terzi, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, J. Bradbury, M. Johnson, L. Weidinger,\nI. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway,\nL. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis\n& insights from training Gopher.arXiv2112.11446, 2021.\nJ. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, K. Choromanski, V. Likhosherstov, D. Dohan,\nX. Song, A. Gane, T. Sarlos, et al. Compressive transformers for long-range sequence modelling.\nAdvancesinNeuralInformationProcessingSystems, 33:6154–6158, 2020.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a unified text-to-text transformer.JournalofMachineLearning\nResearch, 21(140):1–67, 2020a. URLhttp://jmlr.org/papers/v21/20-074.html.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a unified text-to-text transformer.JournalofMachineLearning\nResearch, 21(140):1–67, 2020b.\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training\ntrillion parameter models. InSC20:InternationalConferenceforHighPerformanceComputing,\nNetworking,StorageandAnalysis, pages 1–16. IEEE, 2020.\n19",
    "H. Robbins and S. Monro. A Stochastic Approximation Method.TheAnnalsofMathematicalStatistics,\n22(3):400–407, Sept. 1951.\nR. Rudinger, J. Naradowsky, B. Leonard, and B. Van Durme. Gender bias in coreference resolu-\ntion. InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationfor\nComputationalLinguistics:HumanLanguageTechnologies, New Orleans, Louisiana, June 2018.\nAssociation for Computational Linguistics.\nK. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema\nchallenge at scale. InProceedingsoftheAAAIConferenceonArtificialIntelligence, volume 34,\npages 8732–8740, 2020.\nM. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi. SocialIQA: Commonsense reasoning about\nsocial interactions.Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing, 2019.\nC. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring the effects\nof data parallelism on neural network training.arXivpreprintarXiv:1811.03600, 2018.\nJ. W. Siegel and J. Xu. Approximation rates for neural networks with general activation functions.\nNeuralNetworks, 128:313–321, Aug. 2020. URLhttps://www.sciencedirect.com/scienc\ne/article/pii/S0893608020301891.\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,\nG. Zerveas, V. Korthikanti, E. Zhang, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi,\nY. He, M. Houston, S. Tiwary, and B. Catanzaro. Using Deepspeed and Megatron to Train Megatron-\nturing NLG 530b, A Large-Scale Generative Language Model.arXivpreprintarXiv:2201.11990,\n2022.\nJ. Steinhardt. Updates and lessons from AI forecasting, 2021. URLhttps://bounded-regret.g\nhost.io/ai-forecasting/.\nY. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama, A. Vaswani,\nand D. Metzler. Scale efficiently: Insights from pre-training and fine-tuning transformers, 2021.\nR. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,\nY. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin,\nJ. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch,\nM. Pickett, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zeven-\nbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee,\nL. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein,\nR. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. LaMDA: Language models for\ndialog applications, 2022.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. InAdvancesinneuralinformationprocessingsystems, pages 5998–6008,\n2017.\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle,\nA. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell,\nL. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical and social risks of harm\nfrom language models.arXivsubmission, 2021.\n20",
    "J. Welbl, A. Glaese, J. Uesato, S. Dathathri, J. Mellor, L. A. Hendricks, K. Anderson, P. Kohli, B. Coppin,\nand P.-S. Huang. Challenges in detoxifying language models. InFindingsoftheAssociationfor\nComputationalLinguistics:EMNLP2021, pages 2447–2469, Punta Cana, Dominican Republic,\nNov. 2021. Association for Computational Linguistics. URLhttps://aclanthology.org/2021.\nfindings-emnlp.210.\nA. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein. Detoxifying language models\nrisks marginalizing minority voices. InProceedingsofthe2021ConferenceoftheNorthAmerican\nChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies, pages\n2390–2397, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021\n.naacl-main.190. URLhttps://aclanthology.org/2021.naacl-main.190.\nG. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao.\nTuning large neural networks via zero-shot hyperparameter transfer. In A. Beygelzimer, Y. Dauphin,\nP. Liang, and J. W. Vaughan, editors,AdvancesinNeuralInformationProcessingSystems, 2021.\nURLhttps://openreview.net/forum?id=Bx6qKuBM2AD.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish\nyour sentence? InProceedingsofthe57thAnnualMeetingoftheAssociationforComputational\nLinguistics, 2019.\nG. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which\nalgorithmic choices matter at which batch sizes? insights from a noisy quadratic model.  In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances\ninNeuralInformationProcessingSystems, volume 32. Curran Associates, Inc., 2019. URLhttps:\n//proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b621\n4-Paper.pdf.\nB. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. Designing effective\nsparse expert models, 2022.\n21",
    "Appendix\nA. Training dataset\nIn Table A1 we show the training dataset makeup used forChinchillaand all scaling runs. Note that\nboth theMassiveWeband Wikipedia subsets are both used for more than one epoch.\nDisk Size  Documents  Sampling proportion  Epochs in 1.4T tokens\nMassiveWeb1.9 TB604M45% (48%)1.24\nBooks2.1 TB4M30% (27%)0.75\nC40.75 TB361M10% (10%)0.77\nNews2.7 TB1.1B10% (10%)0.21\nGitHub3.1 TB142M4% (3%)0.13\nWikipedia   0.001 TB6M1% (2%)3.40\nTable A1jMassiveTextdata makeup.For each subset ofMassiveText, we list its total disk size, the\nnumber of documents and the sampling proportion used during training—we use a slightly different\ndistribution than in Rae et al. (2021) (shown in parenthesis). In the rightmost column show the\nnumber of epochs that are used in 1.4 trillion tokens.\nB. Optimal cosine cycle length\nOne key assumption is made on the cosine cycle length and the corresponding learning rate drop\n(we use a 10\u0002learning rate decay in line with Rae et al. (2021)).\n9\nWe find that setting the cosine\ncycle length too much longer than the target number of training steps results in sub-optimally trained\nmodels, as shown in Figure A1. As a result, we assume that an optimally trained model will have the\ncosine cycle length correctly calibrated to the maximum number of steps, given the FLOP budget; we\nfollow this rule in our main analysis.\nC. Consistency of scaling results across datasets\nWe show scaling results from an IsoFLOP (Approach 2) analysis after training on two different datasets:\nC4 (Raffel et al., 2020b) and GitHub code (we show results with data from Rae et al. (2021)), results\nare shown in Table A2. For both set of experiments using subsets ofMassiveText, we use the same\ntokenizer as theMassiveTextexperiments.\nWe find that the scaling behaviour on these datasets is very similar to what we found onMassiveText,\nas shown in Figure A2 and Table A2. This suggests that our results are independent of the dataset as\nlong as one does not train for more than one epoch.\n9\nWe find the difference between decaying by10\u0002and decaying to 0.0 (over the same number of steps) to be small,\nthough decaying by a factor of10\u0002to be slightly more performant. Decaying by less (5\u0002) is clearly worse.\n22",
    "02468\nMillion Sequences\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLearning Rate/Max LR\n02468\nMillion Sequences\n2.70\n2.75\n2.80\n2.85\n2.90\n2.95\n3.00\nTraining Loss\n0246\nMillion Sequences\n2.80\n2.85\n2.90\n2.95\n3.00\n3.05\n3.10\n3.15\n3.20\nC4 Loss\nCosine Cycle Length\n1.0× num. steps\n1.1× num. steps\n1.25× num. steps\n1.5× num. steps\n2.0× num. steps\n5.0× num. steps\n0.02.55.07.510.012.5\nMillion Sequences\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLearning Rate/Max LR\n0.02.55.07.510.012.5\nMillion Sequences\n2.70\n2.75\n2.80\n2.85\n2.90\n2.95\n3.00\nTraining Loss\n0.02.55.07.510.012.5\nMillion Sequences\n2.80\n2.85\n2.90\n2.95\n3.00\n3.05\n3.10\n3.15\n3.20\nC4 Loss\nFigure A1jGrid over cosine cycle length.We show 6 curves with the cosine cycle length set to 1,\n1.1, 1.25, 1.5, 2, and 5\u0002longer than the target number of training steps. When the cosine cycle length\nis too long, and the learning rate does not drop appropriately, then performance is impaired. We find\nthat overestimating the number of training steps beyond 25% leads to clear drops in performance.\nWe show results where we have set the number of training steps to two different values (top and\nbottom).\n100M300M1B3B6B30B\nParameters\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\nC4 Training Loss\n1e19\n1e20\n6e20\n1e21\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n100M\n1B\n10B\n100B\n1T\nParameters\n73B\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n100M\n1B\n10B\n100B\n1T\n10T\nTokens\n1.3T\n100M300M1B3B6B30B\nParameters\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nGitHub Training Loss\n1e19\n1e20\n6e20\n1e21\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n100M\n1B\n10B\n100B\n1T\nParameters\n59B\n10\n17\n10\n19\n10\n21\n10\n23\n10\n25\nFLOPs\n100M\n1B\n10B\n100B\n1T\n10T\nTokens\n1.6T\nFigure A2jC4 and GitHub IsoFLOP curves.Using the C4 dataset (Raffel et al., 2020b) and a GitHub\ndataset (Rae et al., 2021), we generate 4 IsoFLOP profiles and show the parameter and token count\nscaling, as in Figure 3. Scaling coefficients are shown in Table A2.\n23",
    "ApproachCoef.푎where푁\n표푝푡\n/퐶\n푎\nCoef.푏where퐷\n표푝푡\n/퐶\n푏\nC40.500.50\nGitHub0.530.47\nKaplan et al. (2020)0.730.27\nTable A2jEstimated parameter and data scaling with increased training compute on two al-\nternate datasets.The listed values are the exponents,푎and푏, on the relationship푁\n표푝푡\n/퐶\n푎\nand\n퐷\n표푝푡\n/퐶\n푏\n. Using IsoFLOP profiles, we estimate the scaling on two different datasets.\nD. Details on the scaling analyses\nD.1. Approach 1: Fixing model sizes and varying training sequences\nWe use a maximum learning rate of2\u000210\n\u00004\nfor the smallest models and125\u000210\n\u00004\nfor the largest\nmodels. In all cases, the learning rate drops by a factor of10\u0002during training, using a cosine schedule.\nWe make the assumption that the cosine cycle length should be approximately matched to the number\nof training steps. We find that when the cosine cycle overshoots the number of training steps by more\nthan 25%, performance is noticeably degraded—see Figure A1.\n10\nWe use Gaussian smoothing with a\nwindow length of 10 steps to smooth the training curve.\nD.2. Approach 3: Parametric fitting of the loss\nIn this section, we first show how Equation(2)can be derived. We repeat the equation below for\nclarity,\n^\n퐿¹푁 퐷º,퐸 ̧\n퐴\n푁\n훼\n ̧\n퐵\n퐷\n훽\n(5)\nbased on a decomposition of the expected risk between a function approximation term and an\noptimisation suboptimality term. We then give details on the optimisation procedure for fitting the\nparameters.\nLoss decomposition.Formally, we consider the task of predicting the next token푦2Ybased on\nthe previous tokens in a sequence푥2 Y\n푠\n, with푠varying from0to푠\nmax\n—the maximum sequence\nlength. We consider a distribution푃2 D¹X\u0002Yºof tokens inYand their past inX. A predictor\n푓:X !D¹Yºcomputes the probability of each token given the past sequence. The Bayes classifier,\n푓\n★\n, minimizes the cross-entropy of푓¹푥ºwith the observed tokens푦, with expectation taken on the\nwhole data distribution. We let퐿be the expected risk\n퐿¹푓º,피»log푓¹푥º\n푦\n¼and set푓\n★\n,argmin\n푓2F¹XD¹Yºº\n퐿¹푓º(6)\nThe set of all transformers of size푁, that we denoteH\n푁\n, forms a subset of all functions that map\nsequences to distributions of tokensX !D¹Yº. Fitting a transformer of size푁on the expected risk\n퐿¹푓ºamounts to minimizing such risk on a restricted functional space\n푓\n푁\n,argmin\n푓2H\n푁\n퐿¹푓º(7)\nWhen we observe a dataset¹푥\n푖\n 푦\n푖\nº\n푖\n푖2»1퐷¼\nof size퐷, we do not have access to피\n푃\n, but instead to the\nempirical expectation\n^\n피\n퐷\nover the empirical distribution\n^\n푃\n퐷\n. What happens when we are given퐷\n10\nThis further emphasises the point of not only determining model size, but also training length before training begins.\n24",
    "datapoints that we can only see once, and when we constrain the size of the hypothesis space to be\n푁-dimensional ? We are making steps toward minimizing the empirical risk within a finite-dimensional\nfunctional spaceH\n푁\n:\n^\n퐿\n퐷\n¹푓º,\n^\n피\n퐷\n»log푓¹푥º\n푦\n¼setting\n^\n푓\n푁퐷\n,argmin\n푓2H\n푁\n^\n퐿\n퐷\n¹푓º(8)\nWe are never able to obtain\n^\n푓\n푁퐷\nas we typically perform a single epoch over the dataset of size퐷.\nInstead, be obtain\n\u0016\n푓\n푁퐷\n, which is the result of applying a certain number of gradient steps based on\nthe퐷datapoints—the number of steps to perform depends on the gradient batch size, for which we\nuse well-tested heuristics.\nUsing the Bayes-classifier푓\n★\n, the expected-risk minimizer푓\n푁\nand the “single-epoch empirical-risk\nminimizer”\n\u0016\n푓\n푁퐷\n, we can finally decompose the loss퐿¹푁 퐷ºinto\n퐿¹푁 퐷º,퐿¹\n\u0016\n푓\n푁퐷\nº=퐿¹푓\n★\nº ̧\n\u0000\n퐿¹푓\n푁\nº\u0000퐿¹푓\n★\nº\n\u0001\n ̧\n\u0000\n퐿¹\n\u0016\n푓\n푁퐷\nº\u0000퐿¹푓\n푁\nº\n\u0001\n(9)\nThe loss comprises three terms: the Bayes risk, i.e. the minimal loss achievable for next-token\nprediction on the full distribution푃, a.k.a the “entropy of natural text.”; a functional approximation\nterm that depends on the size of the hypothesis space; finally, a stochastic approximation term that\ncaptures the suboptimality of minimizing\n^\n퐿\n퐷\ninstead of퐿, and of making a single epoch on the provided\ndataset.\nExpected forms of the loss terms.In the decomposition(9), the second term depends entirely on\nthe number of parameters푁that defines the size of the functional approximation space.On the set\nof two-layer neural networks, it is expected to be proportional to\n1\n푁\n12\n(Siegel and Xu, 2020). Finally,\ngiven that it corresponds to early stopping in stochastic first order methods, the third term should\nscale as the convergence rate of these methods, which is lower-bounded by\n1\n퐷\n12\n(Robbins and Monro,\n1951) (and may attain the bound). This convergence rate is expected to be dimension free (see e.g.\nBubeck, 2015, for a review) and depends only on the loss smoothness; hence we assume that the\nsecond term only depends on퐷in (2). Empirically, we find after fitting (2) that\n퐿¹푁 퐷º=퐸 ̧\n퐴\n푁\n034\n ̧\n퐵\n퐷\n028\n(10)\nwith퐸=169,퐴=4064,퐵=4107. We note that the parameter/data coefficients are both lower\nthan\n1\n2\n; this is expected for the data-efficiency coefficient (but far from the known lower-bound).\nFuture models and training approaches should endeavor to increase these coefficients.\nFitting the decomposition to data.We effectively minimize the following problem\nmin\n푎푏푒훼훽\n∑︁\nRun푖\nHuber\n훿\n\u0010\nLSE\n\u0000\n푎\u0000훼log푁\n푖\n 푏\u0000훽log퐷\n푖\n 푒\n\u0001\n\u0000log퐿\n푖\n\u0011\n(11)\nwhere퐿푆퐸is the log-sum-exp operator. We then set퐴 퐵 퐸=exp¹푎ºexp¹푏ºexp¹푒º.\nWe use the LBFGS algorithm to find local minima of the objective above, started on a grid\nof initialisation given by:훼2 f005    2g,훽2 f005    2g,푒2 f\u00001\u00005    1g,푎2\nf05    25g, and푏2 f05    25g. We find that the optimal initialisation is not on the boundary of\nour initialisation sweep.\nWe use훿=10\n\u00003\nfor the Huber loss. We find that using larger values of훿pushes the model to\noverfit the small compute regime and poorly predict held-out data from larger runs. We find that\nusing a훿smaller than10\n\u00003\ndoes not impact the resulting predictions.\n25",
    "D.3. Predicted compute optimal frontier for all three methods\nFor Approaches 2 and 3, we show the estimated model size and number of training tokens for a\nvariety of compute budgets in Table A3. We plot the predicted number of tokens and parameters for a\nvariety of FLOP budgets for the three methods in Figure A3.\nApproach 2Approach 3\nParametersFLOPsTokensFLOPsTokens\n400 Million1.84e+197.7 Billion2.21e+199.2 Billion\n1 Billion1.20e+20    20.0 Billion1.62e+2027.1 Billion\n10 Billion\n1.32e+22   219.5 Billion2.46e+22    410.1 Billion\n67 Billion6.88e+231.7 Trillion1.71e+244.1 Trillion\n175 Billion4.54e+244.3 Trillion1.26e+2412.0 Trillion\n280 Billion1.18e+257.1 Trillion3.52e+2520.1 Trillion\n520 Billion\n4.19e+25   13.4 Trillion1.36e+2643.5 Trillion\n1 Trillion1.59e+26   26.5 Trillion5.65e+2694.1 Trillion\n10 Trillion1.75e+28  292.0 Trillion8.55e+28  1425.5 Trillion\nTable A3jEstimated optimal training FLOPs and training tokens for various model sizes.Analo-\ngous to Table 3, we show the model size/token count projections from Approaches 2 and 3 for various\ncompute budgets.\n.\n10\n10\n10\n11\n10\n12\n10\n13\nTokens\n10\n8\n10\n9\n10\n10\n10\n11\n10\n12\nParameters\n1e+18\n1e+19\n1e+20\n1e+21\n1e+22\n1e+23\n1e+24\n1e+25\n1e+26\nApproach 1\nApproach 2\nApproach 3\nChinchilla\nGopher\nGPT-3\nMegatron-Turing NLG\nFigure A3jOptimal number of tokens and parameters for a training FLOP budget.For a fixed\nFLOP budget, we show the optimal number of tokens and parameters as predicted by Approaches 1,\n2, and 3. For an alternate representation, see Figure 1.\nD.4. Small-scale comparison to Kaplanet al.(2020)\nFor10\n21\nFLOPs, we perform a head-to-head comparison of a model predicted by Approach 1 and\nthat predicted by Kaplan et al. (2020). For both models, we use a batch size of 0.5M tokens and a\n26",
    "maximum learning rate of15\u000210\n\u00004\nthat decays by10\u0002. From Kaplan et al. (2020), we find that\nthe optimal model size should be 4.68 billion parameters. From our approach 1, we estimate a 2.86\nbillion parameter model should be optimal. We train a 4.74 billion parameter and a 2.80 billion\nparameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many\nconfounding factors as possible. We find that our predicted model outperforms the model predicted\nby Kaplan et al. (2020) as shown in Figure A4.\n012\nSequences\n1e7\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\nTraining Loss\n0.00.20.40.60.81.0\nFLOPs ×10\n21\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\nTraining Loss\nKaplan et al (2020)\nApproach 1\nFigure A4jComparison to Kaplan et al. (2020) at10\n21\nFLOPs.We train 2.80 and 4.74 billion\nparameter transformers predicted as optimal for10\n21\nFLOPs by Approach 1 and by Kaplan et al.\n(2020). We find that our prediction results in a more performant model at the end of training.\nE. Curvature of the FLOP-loss frontier\nWe observe that as models increase there is a curvature in the FLOP-minimal loss frontier. This means\nthat projections from very small models lead to different predictions than those from larger models.\nIn Figure A5 we show linear fits using the first, middle, and final third of frontier-points. In this work,\nwe do not take this in to account and we leave this as interesting future work as it suggests that even\nsmaller models may be optimal for large FLOP budgets.\nF. FLOPs computation\nWe include all training FLOPs, including those contributed to by the embedding matrices, in our\nanalysis. Note that we also count embeddings matrices in the total parameter count. For large models\nthe FLOP and parameter contribution of embedding matrices is small. We use a factor of 2 to describe\nthe multiply accumulate cost. For the forward pass, we consider contributions from:\nEmbeddings\n–2\u0002seq_len\u0002vocab_size\u0002d_model\nAttention (Single Layer)\n– Key, query and value projections\n:2\u00023\u0002seq_len\u0002d_model\u0002¹key_size\u0002num_headsº\n27",
    "10\n17\n10\n18\n10\n19\n10\n20\n10\n21\n10\n22\nFLOPS\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nTraining loss\n75\n250\n500\n1000\n2500\n5000\n10000\nMillion Parameters\nFigure A5jTraining curve envelopes.We fit to the first third (orange), the middle third (green),\nand the last third (blue) of all points along the loss frontier. We plot only a subset of the points.\n– Key @ Query logits:2\u0002seq_len\u0002seq_len\u0002¹key_size\u0002num_headsº\n– Softmax:3\u0002num_heads\u0002seq_len\u0002seq_len\n– Softmax @ query reductions:2\u0002seq_len\u0002seq_len\u0002¹key_size\u0002num_headsº\n– Final Linear:2\u0002seq_len\u0002¹key_size\u0002num_headsº\u0002d_model\nDense Block (Single Layer)\n–2\u0002seq_len\u0002¹d_model\u0002ffw_size ̧d_model\u0002ffw_sizeº\nFinal Logits\n–2\u0002seq_len\u0002d_model\u0002vocab_size\nTotal forward pass FLOPs:embeddings ̧num_layers\u0002¹total_attention ̧dense_blockº\n+logits\nAs in Kaplan et al. (2020) we assume that the backward pass has twice the FLOPs of the forward pass.\nWe show a comparison between our calculation and that using the common approximation퐶=6퐷푁\n(Kaplan et al., 2020) where퐶is FLOPs,퐷is the number of training tokens, and푁is the number of\nparameters in Table A4. We find the differences in FLOP calculation to be very small and they do not\nimpact our analysis. Compared to the results presented in Rae et al. (2021), we use a slightly more\nParameters  num_layers  d_model  ffw_size  num_heads  k/q sizeFLOP Ratio (Ours/6푁퐷)\n73M10640256010641.03\n305M20102440961664\n1.10\n552M2412805120101281.08\n1.1B2617927168141281.04\n1.6B2820488192161281.03\n6.8B4035841433628128\n0.99\nTable A4jFLOP comparison.For a variety of different model sizes, we show the ratio of the FLOPs\nthat we compute per sequence to that using the6푁퐷approximation.\naccurate calculation giving a slightly different value (63\u000210\n23\ncompared to576\u000210\n23\n).\n28",
    "G. Other differences betweenChinchillaandGopher\nBeyond differences in model size and number of training tokens, there are some additional minor\ndifferences betweenChinchillaandGopher. Specifically,Gopherwas trained with Adam (Kingma and\nBa, 2014) whereasChinchillawas trained with AdamW (Loshchilov and Hutter, 2019). Furthermore,\nas discussed inLessons Learnedin Rae et al. (2021),Chinchillastored a higher-precision copy of the\nweights in the sharded optimiser state.\nWe show comparisons of models trained with Adam and AdamW in Figure A6 and Figure A7.\nWe find that, independent of the learning rate schedule, AdamW trained models outperform models\ntrained with Adam. In Figure A6 we show a comparison of an 680 million parameter model trained\n051015202530\nMillion Sequences\n2.45\n2.50\n2.55\n2.60\n2.65\n2.70\nTraining Loss\n051015202530\nMillion Sequences\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nWikitext103 Perplexity\n051015202530\nMillion Sequences\n2.60\n2.65\n2.70\n2.75\n2.80\n2.85\n2.90\n2.95\n3.00\nC4 Loss\nTraining Setup\nAdam w/ High Precision\nAdamW w/ High Precision\nAdam No High Precision\nAdamW No High Precision\nFigure A6jComparison of other differences.Using an 680 million parameter model, we show a\ncomparison between the setup used to trainGopherandChinchilla— the change in optimiser and\nusing a higher precision copy of the weights in the optimiser state. The setup used forChinchilla\n(orange) clearly outperforms the setup used to trainGopher(green).\n0255075100125150\nMillion Sequences\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\nC4 Loss\n0255075100125150\nMillion Sequences\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nWikitext103 Perplexity\n0255075100125150\nMillion Sequences\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLAMBADA Accuracy\n417M, Adam\n417M, AdamW\n1.4B, Adam\n1.4B, AdamW\nFigure A7jAdam vs AdamW.For a 417M (blue) and 1.4B model (green), we find that training with\nAdamW improves performance over training with Adam.\nwith and without the higher precision copy of the weights and with Adam/AdamW for comparison.\nH. Results\nH.1. The Pile\nIn Table A5 we show the bits-per-byte (bpb) on The Pile (Gao et al., 2020) ofChinchilla,Gopher,\nand Jurassic-1.ChinchillaoutperformsGopheron all subsets. Jurassic-1 outperformsChinchillaon 2\nsubsets—dm_mathematicsandubuntu_irc.\n29",
    "SubsetChinchilla(70B)Gopher(280B)  Jurassic-1 (170B)\npile_cc0.6670.6910.669\npubmed_abstracts0.5590.5780.587\nstackexchange0.6140.6410.655\ngithub0.3370.3770.358\nopenwebtext20.6470.677-\narxiv0.6270.6620.680\nuspto_backgrounds0.5260.5460.537\nfreelaw0.4760.5130.514\npubmed_central0.5040.5250.579\ndm_mathematics1.1111.1421.037\nhackernews0.8590.8900.869\nnih_exporter0.5720.5900.590\nopensubtitles0.8710.9000.879\neuroparl0.8330.938-\nbooks30.6750.7120.835\nphilpapers0.6560.6950.742\ngutenberg_pg_190.5480.6560.890\nbookcorpus20.7140.741-\nubuntu_irc1.0261.0900.857\nTable A5jBits-per-Byte on The Pile.We show the bpb on The Pile forChinchillacompared toGopher\nand Jurassic-1.\nH.2. MMLU\nIn Table A6 we show the performance ofChinchillaandGopheron each subset of MMLU.\nH.3. Winogender Setup\nWe follow the same setup as in Rae et al. (2021). To test coreference resolution inChinchilla, we\ninput a sentence which includes a pronoun reference (e.g., “The librarian helped the child pick out a\nbook because {pronoun} liked to encourage reading.”), then measure the probability of the model\ncompleting the sentence “‘{Pronoun}’ refers to the” with different sentence roles (“librarian” and\n“child” in this example). Each example is annotated with the correct pronoun resolution (the pronoun\ncorresponds to the librarian in this example). Each sentence is tested with a female, male, and\ngender-neutral pronoun. An unbiased model would correctly predict which word the pronoun refers\nto regardless of pronoun gender.\nH.4. BIG-bench\nIn Table A7 we showChinchillaandGopherperformance on each subset of BIG-bench that we consider.\nI. Model Card\nWe present theChinchillamodel card in Table A8, following the framework presented by Mitchell\net al. (2019).\n30",
    "TaskChinchilla  GopherTaskChinchilla  Gopher\nabstract_algebra31.025.0anatomy70.456.3\nastronomy73.065.8business_ethics72.070.0\nclinical_knowledge75.167.2college_biology79.970.8\ncollege_chemistry51.045.0\ncollege_computer_science51.049.0\ncollege_mathematics32.037.0college_medicine66.560.1\ncollege_physics46.134.3computer_security76.065.0\nconceptual_physics67.249.4\neconometrics38.643.0\nelectrical_engineering62.160.0elementary_mathematics41.533.6\nformal_logic33.335.7global_facts39.038.0\nhigh_school_biology80.371.3\nhigh_school_chemistry58.147.8\nhigh_school_computer_science  58.054.0high_school_european_history  78.872.1\nhigh_school_geography86.476.8high_school_gov_and_politics   91.283.9\nhigh_school_macroeconomics   70.565.1\nhigh_school_mathematics31.923.7\nhigh_school_microeconomics    77.766.4high_school_physics36.433.8\nhigh_school_psychology86.681.8high_school_statistics58.850.0\nhigh_school_us_history83.378.9\nhigh_school_world_history85.275.1\nhuman_aging77.666.4human_sexuality86.367.2\ninternational_law90.977.7jurisprudence79.671.3\nlogical_fallacies80.472.4machine_learning41.141.1\nmanagement82.577.7marketing89.783.3\nmedical_genetics69.069.0miscellaneous84.575.7\nmoral_disputes77.566.8moral_scenarios36.540.2\nnutrition77.169.9philosophy79.468.8\nprehistory81.267.6professional_accounting52.144.3\nprofessional_law56.544.5\nprofessional_medicine75.464.0\nprofessional_psychology75.768.1public_relations73.671.8\nsecurity_studies75.964.9sociology91.084.1\nus_foreign_policy92.081.0virology53.647.0\nworld_religions87.784.2\nTable A6jChinchillaMMLU results.For each subset of MMLU (Hendrycks et al., 2020), we show\nChinchilla’s accuracy compared toGopher.\nModel Details\nOrganization Developing the ModelDeepMind\nModel DateMarch 2022\nModel TypeAutoregressive Transformer Language Model (Section 4.1 for\ndetails)\nFeedback on the Model{jordanhoffmann, sborgeaud,\namensch,sifre}@deepmind.com\nIntended Uses\nPrimary Intended UsesThe primary use is research on language models, including:\nresearch on the scaling behaviour of language models along\nwith those listed in Rae et al. (2021).\n31",
    "Primary Intended UsersDeepMind researchers. We will not make this model available\npublicly.\nOut-of-Scope UsesUses of the language model for language generation in harm-\nful or deceitful settings. More generally, the model should not\nbe used for downstream applications without further safety\nand fairness mitigations.\nFactors\nCard Prompts – Relevant FactorRelevant factors include which language is used. Our model is\ntrained on English data. Furthermore, in the analysis of mod-\nels trained on the same corpus in Rae et al. (2021), we found\nit has unequal performance when modelling some dialects\n(e.g., African American English). Our model is designed for\nresearch. The model should not be used for downstream ap-\nplications without further analysis on factors in the proposed\ndownstream application.\nCard Prompts – Evaluation FactorsSee the results in Rae et al. (2021) which analyzes models\ntrained on the same text corpus.\nMetrics\nModel Performance Measures\nPerplexity and bits per byte on language modelling\ndatasets\nAccuracy on completion tasks, reading comprehension,\nMMLU, BIG-bench and fact checking.\nExact match accuracy for question answering.\n\nGeneration toxicity from Real Toxicity Prompts (RTP)\nalongside toxicity classification accuracy.\n\nGender and occupation bias. Test include comparing\nthe probability of generating different gender terms\nand the Winogender coreference resolution task.\nWe principally focus onChinchilla’s performance compared\ntoGopheron text likelihood prediction.\nDecision thresholdsN/A\nApproaches to Uncertainty and Vari-\nability\nDue to the costs of training large language models, we did\nnot trainChinchillamultiple times. However, the breadth\nof our evaluation on a range of different task types gives a\nreasonable estimate of the overall performance of the model.\nFurthermore, the existence of another large model trained\non the same dataset (Gopher) provides a clear point of com-\nparison.\nEvaluation Data\n32",
    "Datasets\nLanguage modelling on LAMBADA, Wikitext103 (Mer-\nity et al., 2017), C4 (Raffel et al., 2020a), PG-19 (Rae\net al., 2020) and the Pile (Gao et al., 2020).\n\nLanguage understanding,  real world knowledge,\nmathematical and logical reasoning on the Massive\nMultitask Language Understanding (MMLU) bench-\nmark (Hendrycks et al., 2020) and on the “Beyond the\nImitation Game Benchmark” (BIG-bench) (BIG-bench\ncollaboration, 2021).\nQuestion answering (closed book) on Natural Ques-\ntions (Kwiatkowski et al., 2019) and TriviaQA (Joshi\net al., 2017).\nReading comprehension on RACE (Lai et al., 2017)\n\nCommon sense understanding on HellaSwag (Zellers\net al., 2019),  PIQA (Bisk et al., 2020),  Wino-\ngrande (Sakaguchi et al., 2020), SIQA (Sap et al., 2019),\nBoolQ (Clark et al., 2019), and TruthfulQA (Lin et al.,\n2021).\nMotivationWe chose evaluations from Rae et al. (2021) to allow us to\nmost directly compare toGopher.\nPreprocessingInput text is tokenized using a SentencePiece tokenizer with\na vocabulary of size 32,000. Unlike the tokenizer used for\nGopher, the tokenizer used forChinchilladoes not perform\nNFKC normalization.\nTraining Data\nThe same dataset is used as in Rae et al. (2021). Differences in sampling are shown in Table A1.\nQuantitative Analyses\nUnitary ResultsSection 4.2 gives a detailed description of our analysis. Main\ntake-aways include:\nOur model is capable of outputting toxic language as\nmeasured by the PerspectiveAPI. This is particularly\ntrue when the model is prompted with toxic prompts.\n\nGender: Our model emulates stereotypes found in our\ndataset, with occupations such as “dietician” and “re-\nceptionist” being more associated with women and “car-\npenter” and “sheriff” being more associated with men.\nRace/religion/country sentiment:  Prompting our\nmodel to discuss some groups leads to sentences with\nlower or higher sentiment, likely reflecting text in our\ndataset.\n33",
    "Intersectional ResultsWe did not investigate intersectional biases.\nEthical Considerations\nDataThe data is the same as described in Rae et al. (2021).\nHuman LifeThe model is not intended to inform decisions about matters\ncentral to human life or flourishing.\nMitigationsWe considered filtering the dataset to remove toxic content\nbut decided against it due to the observation that this can\nintroduce new biases as studied by Welbl et al. (2021). More\nwork is needed on mitigation approaches to toxic content and\nother types of risks associated with language models, such\nas those discussed in Weidinger et al. (2021).\nRisks and HarmsThe data is collected from the internet, and thus undoubtedly\nthere is toxic/biased content in our training dataset. Fur-\nthermore, it is likely that personal information is also in the\ndataset that has been used to train our models. We defer to\nthe more detailed discussion in Weidinger et al. (2021).\nUse CasesEspecially fraught use cases include the generation of fac-\ntually incorrect information with the intent of distributing\nit or using the model to generate racist, sexist or otherwise\ntoxic text with harmful intent. Many more use cases that\ncould cause harm exist. Such applications to malicious use\nare discussed in detail in Weidinger et al. (2021).\nTable A8jChinchillamodel card.We follow the framework presented in Mitchell et al. (2019).\nJ. List of trained models\nIn Table A9 we list the model size and configuration of all models used in this study. Many models\nhave been trained multiple times, for a different number of training steps.\n34",
    "TaskChinchilla  GopherTaskChinchilla  Gopher\nhyperbaton54.251.7movie_dialog_same_or_diff   54.550.7\ncausal_judgment57.450.8winowhy62.556.7\nformal_fallacies_syllogisms_neg   52.150.7\nmovie_recommendation75.650.5\ncrash_blossom47.663.6moral_permissibility57.355.1\ndiscourse_marker_prediction13.111.7strategyqa68.361.0\ngeneral_knowledge_json94.393.9\nnonsense_words_grammar    78.061.4\nsports_understanding71.054.9metaphor_boolean93.159.3\nimplicit_relations49.436.4navigate52.651.1\npenguins_in_a_table48.740.6\npresuppositions_as_nli49.934.0\nintent_recognition92.888.7temporal_sequences32.019.0\nreasoning_about_colored_objects  59.749.2question_selection52.641.4\nlogic_grid_puzzle44.035.1\nlogical_fallacy_detection72.158.9\ntimedial68.850.9physical_intuition79.059.7\nepistemic_reasoning60.656.4physics_mc65.550.9\nruin_names47.138.6\nidentify_odd_metaphor68.838.6\nhindu_knowledge91.480.0\nunderstanding_fables60.339.6\nmisconceptions65.361.7\nlogical_sequence64.136.4\nimplicatures75.062.0mathematical_induction47.357.6\ndisambiguation_q54.745.5fantasy_reasoning69.064.1\nknown_unknowns65.263.6SNARKS58.648.3\ndark_humor_detection66.283.1\ncrass_ai75.056.8\nanalogical_similarity38.117.2\nentailed_polarity94.089.5\nsentence_ambiguity71.769.1\nirony_identification73.069.7\nriddle_sense85.768.2evaluating_info_essentiality   17.616.7\ndate_understanding52.344.1phrase_relatedness94.081.8\nanalytic_entailment67.153.0novel_concepts65.659.1\nodd_one_out70.932.5\nempirical_judgments67.752.5\nlogical_args56.259.1\nfigure_of_speech_detection   63.352.7\nalignment_questionnaire91.379.2\nenglish_proverbs82.457.6\nsimilarities_abstraction87.081.8Human_organs_senses_mcc   85.784.8\nanachronisms69.156.4gre_reading_comprehension  53.127.3\nTable A7jChinchillaBIG-bench results.For each subset of BIG-bench (BIG-bench collaboration,\n2021), we showChinchillaandGopher’s accuracy.\n35",
    "Parameters (million)d_model   ffw_size   kv_size   n_heads   n_layers\n4451220486488\n5757623046499\n74\n6402560641010\n906402560641013\n106\n6402560641016\n1177683072641212\n140\n7683072641215\n1637683072641218\n1758963584641414\n196\n8963584641416\n2178963584641418\n251\n10244096641616\n27810244096641618\n306\n10244096641620\n425128051201281018\n489\n128051201281021\n509140856321281118\n552\n128051201281024\n587140856321281121\n632153661441281219\n664140856321281124\n724153661441281222\n816153661441281225\n893179271681281420\n1,018\n179271681281423\n1,143179271681281426\n1,266204881921281622\n1,424217687041281722\n1,429204881921281625\n1,593204881921281628\n1,609217687041281725\n1,731230492161281824\n1,794217687041281728\n2,007230492161281828\n2,283\n230492161281832\n2,2982560102401282026\n2,6392560102401282030\n2,9802560102401282034\n3,5302688107521282236\n3,8022816112641282236\n4,0842944117761282236\n4,5163072122881282436\n6,7963584143361282840\n9,2934096163841283242\n11,452\n4352174081283247\n12,2954608184321283644\n12,5694608184321283247\n13,7354864194561283247\n14,9404992199681283249\n16,1835120204801284047\nTable A9jAll models.We list the hyperparameters and size of all models trained as part of this work.\nMany shown models have been trained with multiple learning rate schedules/number of training\ntokens.\n36"
  ]
}