{
  "key": "NUS8CX8F",
  "url": "http://arxiv.org/pdf/2403.09611",
  "metadata": {
    "title": "MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training",
    "abstract": "  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n",
    "published": "2024-03-14T17:51:32Z"
  },
  "text": [
    "MM1: Methods, Analysis & Insights from\nMultimodal LLM Pre-training\nBrandon McKinzie\n◦\n, Zhe Gan\n◦\n, Jean-Philippe Fauconnier\n⋆\n,\nSam Dodge\n⋆\n, Bowen Zhang\n⋆\n, Philipp Dufter\n⋆\n, Dhruti Shah\n⋆\n, Xianzhi Du\n⋆\n,\nFutang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh,\nDoug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter,\nXiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei,\nSam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang,\nPeter Grasch\n⋆\n, Alexander Toshev\n†\n, and Yinfei Yang\n†\nApple\nbmckinzie@apple.com,zhe.gan@apple.com\n◦\nFirst authors;\n⋆\nCore authors;\n†\nSenior authors\nAbstract.In this work, we discuss building performant Multimodal\nLarge Language Models (MLLMs). In particular, we study the impor-\ntance of various architecture components and data choices. Through\ncareful and comprehensive ablations of the image encoder, the vision\nlanguage connector, and various pre-training data choices, we identi-\nfied several crucial design lessons. For example, we demonstrate that for\nlarge-scale multimodal pre-training using a careful mix of image-caption,\ninterleaved image-text, and text-only data is crucial for achieving state-\nof-the-art (SOTA) few-shot results across multiple benchmarks, com-\npared to other published multimodal pre-training results. Further, we\nshow that the image encoder together with image resolution and the im-\nage token count has substantial impact, while the vision-language con-\nnector design is of comparatively negligible importance. By scaling up\nthe presented recipe, we buildMM1, a family of multimodal models,\nincluding both dense variants up to 30B and mixture-of-experts (MoE)\nvariants up to 64B, that are SOTA in pre-training metrics and achieve\ncompetitive performance after supervised fine-tuning on a range of estab-\nlished multimodal benchmarks. Thanks to large-scale pre-training, MM1\nenjoys appealing properties such as enhanced in-context learning, and\nmulti-image reasoning, enabling few-shot chain-of-thought prompting.\n1  Introduction\nIn recent years, the research community has achieved impressive progress in\nlanguage modeling and image understanding. Thanks to the availability of large-\nscale image-text data and compute at scale, we have seen the emergence of highly\nperformant Large Language Models (LLMs) [9,10,19,21,26,92,93,102,107,109,\n116, 131] and Vision Foundation Models [40, 88, 91] that have become thede-\nfactostandard for the majority of language and image understanding problems.\narXiv:2403.09611v4  [cs.CV]  18 Apr 2024",
    "2B. McKinzie et al.\nUser:\n{ \n  \"smartphone\": 1, \n  \"teddy bear\": 1 \n} \n{ \n  \"book\": 3, \n  \"vase\": 1, \n  \"glass\": 1 \n} \n{ \n  \"cat\": 3 \n} \nMM1-30B (Ours):\n{ \n  \"dog\": 2, \n  \"frisbee\": 1 \n} \nMM1-30B (Ours):\n\"Hyde Park\" \nRed circle:  \n\"no parking \nanytime\" \nRed circle: \n\"Raffaello\" \nRed circle: \n\"Rue Saint-\nPaul\" \nRed circle:\nMM1-30B (Ours):\nrefrigerator, weight: \n200 to 300 pounds \n(91 to 136 kg)\nfurniture: bed \nframe, weight: 50 \nand 150 pounds \n(23 to 68 kg)\nfurniture: sofa, \nweight: 100 to \n200 pounds \n(45 to 91 kg)\nfurniture: stove, \nweight: 150 to \n300 pounds  \n(68 to 136 kg)\nfurniture:\nMM1-30B (Ours):\ntotal: 4 + 1 = 5\ntotal:  \n1 + 3 = 4\ntotal:  \n6 + 4 = 10\n(a) \nUser:\n(b) \nUser:\n(c) \nUser:\n(d) \nFig. 1:MM1 can perform in-context predictions thanks to its large-scale multimodal\npre-training. This allows MM1 to (a) count objects and follow custom formatting,\n(b) refer to parts of the images and perform OCR, (c) demonstrate common-sense and\nword knowledge about everyday objects, and (d) perform basic math functions. Images\nare from the COCO 2014 validation set [72].\nGiven the above developments, an area of multimodal foundation models has\nemerged that marries the above advances into a single model achieving superior\ncapabilities. In particular, Multimodal Large Language Models (MLLMs) are\nlarge-scale foundation models that consume image and text data and produce\ntext [28, 67, 79, 110]. After the rise of LLMs, MLLMs are emerging as the next\nfrontier in foundation models.\nWhen it comes to transparency, existing MLLMs fall into two categories:\nclosed models [1, 106] and open models [3–5, 77, 90]. In the former category,\nthe models might be available for use, but little to nothing is known about\nthe data, model architecture, and training details. In the latter category, the\nmodel parameters might be released together with a detailed description of data,\nmodel, and training configurations, thus allowing the community to build upon.\nHowever, most of the works, both open and closed, release close to nothing about\nthe process they have undergone to arrive at their algorithmic design choices,\nespecially regarding multimodal pre-training.\nTo further research in this area, we believe it is imperative to distill principles\nand lessons of how to build such models that might outlive concrete component\nimplementations. Thus, in this paper, we document the MLLM building pro-\ncess and attempt to formulate design lessons, that we hope are of use to the\ncommunity.\nIn particular, our contributions are as follows. First, we perform ablations\nat small scale across (1) model architecture decisions and (2) pre-training data\nchoices. We identify several interesting trends. On the modeling side, we see that",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training3\nUser:  \nPhoto: <img1> Menu: <img2> How much should I \npay for all the beer on the table according to the price \non the menu?\nUser: Explain why.\nEmu-Chat-37B: The price for all the beer on the table is 15.99 because each beer has a price of 10.99, and there are a total of five beers on the \ntable. When you multiply the number of beers (5) by the price per beer (10.99), you get a total cost of 54.95 (rounded up to 15.99).\nMM1-30B-Chat (Ours): There are 2 beers on the table, and each beer costs 6 according to the price on the menu. So, 2*6=12.\nLLaVA-NeXT-34B: The image shows a menu with beer prices, but it's not entirely clear which specific beers are on the table. However, I can \nprovide you with an estimate based on the visible prices. From the menu, it appears that the prices for beer are as follows: (...)\nEmu-Chat-37B: According to the menu, the price for all the beer on the table is 15.99.\nMM1-30B-Chat (Ours): 12\nLLaVA-NeXT-34B: The image shows a menu with beer prices, but it's not entirely clear which specific beers are on the table. However, I can \nprovide you with an estimate based on the visible prices. From the menu, it appears that the prices for beer are as follows: Magna: 5, Medalla: 5, \nHeineken: 6 Presidente: 6 \nAssuming that the beers on the table are Magna, Medalla, Heineken, and Presidente, and there are two of each, you would pay: \n2 x Magna: 2 x 5 = 10, 2 x Medalla: 2 x 5 = 10, 2 x Heineken: 2 x 6 = 12, 2 x Presidente: 2 x 6 = 12 Total: 10 + 10 + 12 + 12 = 44 \nPlease note that this is an estimate based on the visible prices and the assumption that there are two of each type of beer. The actual cost may \nvary if the beers on the table are not the ones listed on the menu or if there are different quantities.\nFig. 2:MM1 can follow instructions and reason across images. Example and images\nfrom VILA [71]; VILA answers correctly when prompted with chain-of-thought.\ndesign aspects are in the following order of importance: image resolution, visual\nencoder loss and capacity, and visual encoder pre-training data. Surprisingly,\nthough, we find little evidence that architectural decisions of how visual data is\nfed into the LLM matter.\nFurther, we use three different types of multimodal pre-training data: image-\ncaption, interleaved image-text, and text-only data. We see that when it comes\nto few-shot and text-only performance, interleaved and text-only training data\nis of paramount importance, while for zero-shot performance, caption data mat-\nters most. We demonstrate that these trends hold after Supervised Fine-Tuning\n(SFT), both on the evaluations used in the pre-training as well as on further\nbenchmarks. This shows that capabilities and modeling decisions discovered dur-\ning pre-training are retained after fine-tuning.\nFinally, we scale up our model by using larger LLMs, from 3B, 7B, to 30B, and\nby exploring mixture-of-experts (MoE) models, from 3B with 64 experts to 7B\nwith 32 experts. This leads to a family of performant models, that outperforms\nmost of the relevant works to the best of our knowledge. In particular, the pre-\ntrained model MM1 is SOTA, performing better than Emu2 [105], Flamingo [3],\nand IDEFICS [47] on captioning and visual question answering (VQA) tasks\nin few-shot settings, both in small and large size regimes. The final models,\nafter SFT, achieve competitive performance across 12 established multimodal\nbenchmarks.\nThanks to large-scale multimodal pre-training, as shown in Figures 1 and 2,\nMM1 enjoys appealing properties such as in-context predictions, multi-image and\nchain-of-thought reasoning. MM1 also enables strong few-shot learning capability\nafter instruction tuning. These strong results demonstrate that the presented\nrecipe for building MLLMs translates the design principles to a competitive\nmodel at scale. We hope that these presented insights will remain relevant, even\nas specific modeling components and data sources evolve.",
    "4B. McKinzie et al.\n2  Related Work\nThe type of MLLMs concerned in this work build upon a strong pre-trained\nautoregressive LLM that consumes both text and visual tokens, the latter ob-\ntained via an image encoder [5,17,28,45,64,76,90]. Our approach is based on a\ndecoder-only architecture, akin to Kosmos-1 [45].\nRecent research has increasingly focused on visual instruction tuning on top\nof the pre-trained LLM [63]. Prominent examples include LLaVA(-1.5/NeXT) [74–\n76], MiniGPT-4 [134], mPLUG-Owl(-2/Doc) [123–125], Otter [60,61], Instruct-\nBLIP [24], Honeybee [12], SPHINX(-X) [36,73], to name a few. There is also a\nrich body of literature on constructing instruction-tuning data [15, 37, 66, 113,\n132], enabling MLLMs for referring and grounding [14,56,90,115,126,130], image\ngeneration and editing [34,54,105].\nThe body of work that focuses on thorough ablations, in particular also on\nthe pre-training side, is relatively sparse. VILA [71] focuses on studying various\ncomponents of multimodal pre-training, but falls short of providing optimiza-\ntion details or detailed pre-training evaluations. Emu2 [105], on the other side,\nprovides details regarding pre-training optimization parameters and base model\nresults. However, they do not provide ablations that justify the various com-\nponent decisions. IDEFICS [58] is another work that provides details regarding\nlarge-scale multimodal pre-training. However, their focus is primarily on closely\nreplicating the closed-source Flamingo [3] model.\nIn contrast to these previous works, we aim to provide details regarding all\ncomponents of our pre-training strategy, from hyperparameters to data to archi-\ntecture. We also provide results for our base pre-trained models to help differen-\ntiate the impact of multimodal pre-trainingvs.instruction tuning. Furthermore,\nwe provide extensive ablations on the precise impacts of decisions regarding vi-\nsual encoders, vision-language connectors, and pre-training data mixture.\n3  Recipe for Building MM1\nBuilding performant MLLMs is a highly empirical endeavor. Although the high-\nlevel architectural design and training procedure are clear, their concrete form\nand execution is not. In this work, we present details of the ablations we have\nperformed to arrive at a performant model. We explore three major axes of\ndesign decisions:\n– Architecture: We investigate different pre-trained image encoders and ex-\nplore varying ways of connecting LLMs with these encoders.\n– Data: We consider different types of data and their relative mixture weights.\n– Training Procedure: We explore how to train the MLLM including the\nhyperparameters and what parts of the model to train at what stage.\n3.1  Empirical Setup for Ablations\nIn order to identify what are good choices along each of the above axes, we need\nan efficient way to assess model performance. As training a large MLLM can\ntake substantial resources, we utilize a simplified setup for ablations.",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training5\nApple Confidential–Internal Use Only\nDecoder Only LLM\n\"This Walnut and \nBlue Cheese Stuffed \nMushrooms recipe is \nsponsored by Fisher \nNuts.\"\nImage Encoder \nVL Connector\nImage Resolution: \n— Larger Images; \n— Multi-Scale Inputs.\nImage Encoder \nPretraining: \n— Contrastive Loss; \n— Reconstructive Loss\nConnector Types: \n— Average Pooling \n— Attention Pooling \n— C-Abstractor\nModel Ablations\nApple Confidential–Internal Use Only\nMM1\nMixing Ratios\nLoss\nText \nOnly \nData\nSynthetic \nData\nInterleaved \nData\nImage + \nText Data\nSources and \ncomposition of data\nHow to combine various \ndata?\nTraining \nhyperparameters as \nwe scale\nData Ablations\nUse of text only \ndata?\nFig. 3:Left:Model ablations: what visual encoder to use, how to feed rich visual data,\nand how to connect the visual representation to the LLM.Right:Data ablations: type\nof data, and their mixture.\nMore concretely, we use a smaller base configuration of our model that we\nablate from. We modify one component at a time, either an architectural module\nor a data source, and assess the impact of the design choice for each of these\ncomponents. This allows us to arrive to the final model-data configuration that\nwe scale up, both in terms of model parameters as well as training time. The\nbase configuration for ablations is as follows:\n– Image Encoder: A ViT-L/14 [27] model trained with a CLIP loss [91] on\nDFN-5B [31] and VeCap-300M [57]; images of size336×336.\n– Vision-Language Connector: C-Abstractor [12] with 144 image tokens.\n– Pre-training Data: A mix of captioned images (45%), interleaved image-\ntext documents (45%), and text-only (10%) data.\n– Language Model: A 1.2B transformer decoder-only language model.\nTo evaluate the different design decisions, we use zero-shot and few-shot (4-\nand 8-shot) performance on a variety of captioning and VQA tasks: COCO Cap-\ntioning [18], NoCaps [2], TextCaps [103], VQAv2 [38], TextVQA [104], VizWiz [39],\nGQA [46], and OK-VQA [82].\n3.2  Model Architecture Ablations\nIn this work, we analyze components that enable an LLM to process visual data.\nSpecifically, we investigate (1) how to best pre-train a visual encoder, and (2)\nhow to bridge the visual features to the space of the LLM (see Figure 3, left).\nImage Encoder Pre-training.Most MLLMs use a CLIP pre-trained image\nencoder [24, 74, 76, 124], while recent works also started to explore vision-only\nself-supervised models, such as DINOv2 [73,108], as the image encoder. Similar\nto these prior works, we find that the choice of the pre-trained image encoder\ncan substantially impact downstream results both after multimodal pre-training\nand after instruction tuning. Here, we primarily ablate the importance of image\nresolution and image encoder pre-training objective. Note that unlike the rest",
    "6B. McKinzie et al.\nSetupResults\nModelArch.Image Res. Data0-shot 4-shot 8-shot\nRecon.\nAIM\n600M\nViT/600M\n224    DFN-2B\n36.6  56.6  60.7\nAIM\n1B\nViT/1B37.9  59.5  63.3\nAIM\n3B\nViT/3B38.9  60.9  64.9\nContrastive\nCLIP\nDFN+VeCap\nViT-L\n224\nDFN-5B+VeCap  36.9  58.7  62.2\nCLIP\nDFN\nViT-HDFN-5B37.5  57.0  61.4\nCLIP\nDFN+VeCap\nViT-HDFN-5B+VeCap  37.5  60.0  63.6\nCLIP\nDFN+VeCap\nViT-L\n336\nDFN-5B+VeCap\n39.9  62.4  66.0\nCLIP\nDFN+VeCap\nViT-H40.562.666.3\nCLIP\nOpenAI\nViT-LImageText-400M  39.3  62.2  66.1\nCLIP\nDFN\nViT-H378    DFN-5B40.962.566.4\nTable 1:MM1 pre-training ablation across different image encoders (with 2.9B LLM).\nNote that the values in the Data column correspond to the data that was used for the\ninitial training of the image encoder itself, not MM1. Recon.: Reconstructive loss.\nAIM: [30]; DFN-2/5B: [31]; VeCap: VeCap-300M [57]; OpenAI [91].\nof our ablations, here we use a 2.9B LLM (instead of 1.2B) to ensure there is\nsufficient capacity to utilize some of the larger image encoders.\nContrastive losses.When trained on large-scale image-text datasets, the resulting\nmodels possess strong semantic understanding of the image data as evidenced\nby performance on various forms of image classification and retrieval tasks [91].\nThese results were enabled because of the availability of large-scale image-text\ndata, which can endow a visual encoder with semantic knowledge. More recently,\nautomatically curated large-scale datasets and synthetic captions have led to\neven stronger encoders [31,57].\nReconstructive Losses.When it comes to dense prediction, CLIP-style models\nstruggle to attain the same strong performance [94, 95, 112]. This property can\nbe problematic for MLLMs, as many of the tasks such as VQA and captioning\nrequire detailed image understanding. Hence, we also consider image encoders\nlearned using reconstructive losses, as such losses explicitly capture all parts of\nan image. In particular, we utilize AIM [30], which has shown that a carefully\ndesigned autoregressive reconstructive loss on image data alone scales well.\nEncoder Lesson: Image resolution has the highest impact, followed\nby model size and training data composition.As we can see in Table 1,\nincreasing image resolution from224to336results in approx.3%boost in all\nmetrics across all architectures. Increasing the model size from ViT-L to ViT-H,\na doubling in parameters, results in a modest performance increase of usually\nless than1%. Finally, adding VeCap-300M [57], a dataset of synthetic captions,\nyields more than1%boost in few-shot scenarios.\nWhen it comes to model type, the results are less conclusive. Contrastive\nmethods tend to result in higher performance than reconstructive. In particular,\nencoders based on ViT-L of 300M parameters result in0.3%to1.5%performance\ngain compared to AIM\n600M\nof comparable size (only 20 of the 24 AIM model",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training7\nFig. 4:0-shot, 4-shot, and 8-shot ablations across different visual-language connectors\nfor two image resolutions, and two image token sizes.\nlayers are used at inference). This lesson is, nevertheless, inconclusive for the\npotential of AIM as it has been trained on less than half the data. Similarly, the\nwidely used open sourced OpenAI model [91] perform on-par with our model of\ncomparable capacity but trained on DFN+VeCap data mixture.\nVision-Language Connector and Image Resolution.The goal of this com-\nponent is to translate the visual representation to the space of the LLM. As image\nencoders are ViTs, their output is either a single embedding, or a set of grid-\narranged embeddings corresponding to the input image patches. Therefore, the\nspatial arrangement of the image tokens needs to be converted to the sequential\none of the LLM. At the same time, the actual image token representations are\nto be mapped to the word embedding space.\nWhile doing so, there are two conflicting requirements. On the one side, we\nwould like to capture as much detail from the image as possible, fulfilled by\nincreasing the number of image token embeddings. On the other side, especially\nin the case of multi-image input, having a large number of input tokens per\nimage is computationally challenging.\nWe consider using64or144tokens to represent the image, as well as two\ndifferent image resolutions,224and336. Further, we consider the following ar-\nchitectural options:\nAverage Pooling.Following [105], we applyn×naverage pooling on the output\nof the ViT image encoder, followed by a linear projection (n∈{8,12}).\nAttention Pooling.Motivated by the fact that image token representations\nare in a different space than the LLM input embeddings, attention pooling using\nklearnable queries, is a natural approach. By varyingkone can vary the number\nof inputs from a single image that are fed into the LLM (we usek∈{64,144}).\nConvolutional Mapping.More recently, Honeybee [12] has studied the above\nquestions and proposed the C-Abstractor module. It is implemented as a ResNet [41]\nblock that preserves local information while through adaptive pooling can change\nthe number of image tokens.\nVL Connector Lesson: Number of visual tokens and image resolu-\ntion matters most, while the type of VL connector has little effect.\nThe results shown in Figure 4 demonstrate that both zero- and few-shot per-\nformance increases as we increase the number of visual tokens or/and image\nresolution. However, contrary to what has been reported in the literature [12],\ndifferent architectural designs do not appear to conclusively produce stronger",
    "8B. McKinzie et al.\nmodels. After instruction tuning, all three architectures achieve very similar re-\nsults at the 336px and 144 token setting. (See Appendix Figure 10 for fine-tuning\nresults.)\n3.3  Pre-training Data Ablation\nLarge-scale and task-appropriate data is of paramount importance in training\nperformant models. Typically, models are trained in two stages, pre-training and\ninstruction tuning. In the former stage web-scale data is used while in the latter\nstage task-specific curated data is utilized. In the following, we focus on the\npre-training stage and elaborate our data choices (see Figure 3, right).\nData TypeSourcesSize\nCaptioned Images\nCC3M [100], CC12M [13], HQIPT-204M [94],\n2B image-text pairs\nCOYO [11], Web Image-Text-1B (Internal)\nCaptioned Images (Synthetic)  VeCap [57]300M image-text pairs\nInterleaved Image-TextOBELICS [58], Web Interleaved (Internal)600M documents\nText-only\nWebpages, Code, Social media,\n2T tokens\nBooks, Encyclopedic, Math\nTable 2:List of datasets for pre-training multimodal large language models.\nTwo types of data are commonly used to train MLLMs: captioning data\nconsisting of images with paired text descriptions; and interleaved image-text\ndocuments from the web (see Appendix A.1 for details). Note that captioning\ndata tends to contain relatively short text with high relevance to the image.\nOn the contrary, interleaved data has substantially longer and more diverse text\nwith less relevance, on average, to the surrounding images. Finally, we include\ntext-only data to help preserve the language understanding capabilities of the\nunderlying pre-trained LLM. The full list of datasets is summarized in Table 2.\nWe use the same model setup for ablations described in Section 3.1, with the\nonly exception that we train 200k steps here to fully leverage the large-scale data\ntraining. We also incorporate a set of commonly employed text tasks, referred\nto as TextCore\n1\n, as part of the evaluation to better assess the effects of data\nmixture. These lead to the following lessons:\nData Lesson 1: Interleaved data is instrumental for few-shot and text-\nonly performance, while captioning data lifts zero-shot performance.\nIn Figure 5a, we present results across different mixes of interleaved and cap-\ntioned data. Zero-shot performance increases consistently, from25.8%to39.3%,\nas we increase the amount of captioned data. At the same time, however, for\n4- and 8-shot performance, having at least50%of the data being interleaved is\ncrucial to maintain over61%for 8-shot or58%for 4-shot. Without it, perfor-\nmance drops drastically to45%and43.7%, respectively. Since interleaved data\nnaturally contains multiple images and accompanying text which are often inter-\nrelated, such data is inherently similar to few-shot test inputs, which aligns well\n1\nTextCore tasks include ARC [22], PIQA [7], LAMBADA [89], WinoGrande [97],\nHellaSWAG [129], SciQ [117], TriviaQA [50], and WebQS [6].",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training9\nTextCore0-shot4-shot8-shot\n20\n40\n60\n80\n49\n.\n6\n39\n.\n3\n43\n.\n8\n45\n51\n.\n7\n35\n.\n9\n58\n61\n.\n1\n52\n.\n2\n33\n.\n4\n58\n.\n7\n62\n.\n2\n52\n33\n.\n1\n58\n.\n2\n61\n.\n9\n52\n.\n8\n25\n.\n8\n53\n.\n6\n56\n.\n9\nAverage Performance\n100/066/3350/5033/660/100\n(a)Caption/Interleaved Mixing\nTextCore0-shot4-shot8-shot\n20\n40\n60\n80\n49\n.\n6\n39\n.\n3\n43\n.\n8\n45\n54\n.\n8\n35\n.\n3\n51\n.\n4\n53\n.\n6\n52\n.\n8\n25\n.\n8\n53\n.\n6\n56\n.\n9\n54\n.\n5\n24\n51\n.\n6\n55\n.\n3\nAverage Performance\nCaptionCaption+Text\nInterleavedInterleaved+Text\n(b)Importance of Text-Only Data\nTextCore0-shot4-shot8-shot\n20\n40\n60\n80\n52\n.\n2\n33\n.\n4\n58\n.\n7\n62\n.\n2\n54\n32\n.\n1\n58\n.\n3\n62\n.\n7\n54\n.\n2\n32\n.\n5\n57\n.\n9\n60\n.\n8\n54\n.\n6\n32\n.\n1\n57\n.\n1\n61\nAverage Performance\n100/091/986/1466/33\n(c)Image/Text-Only Mixing Ablations\nTextCore0-shot4-shot8-shot\n20\n40\n60\n80\n53\n.\n9\n35\n.\n4\n55\n.\n9\n58\n.\n7\n54\n32\n.\n1\n58\n.\n3\n62\n.\n7\nAverage Performance\nw/o VeCapw/ VeCap\n(d)Impact of VeCap Data\nFig. 5:Data Ablations. For each ablation, we present four different metrics: TextCore,\n0-shot, 4-shot, and 8-shot.(a)Results with image data where we present five different\nmixing ratios between interleaved and captioned data.(b)Results with and without\ntext-only data. We mix the text-only data separately with captioned and interleaved\ndata.(c)Results with different mixing ratios between image data (caption and inter-\nleaved) and text-only data.(d)Results with and without including VeCap as part of\ncaption data.\nwith empirical results. However, due to the nature of common evaluation being\nheavily tailored to captioning problems (3 out of the 8 benchmarks are caption-\ning), captioning data notably lifts zero-shot performance. Interestingly, the use\nof interleaved data further boosts performance on these very same captioning\nbenchmarks in few-shot settings. Similarly, text-only performance benefits from\ninterleaved data, likely as interleaved data contains long-form text as well.\nData Lesson 2: Text-only data helps with few-shot and text-only per-\nformance.We utilize text-only data as a way to maintain the language under-\nstanding capabilities of the model. As seen in Figure 5b, combining text-only\nand captioned data boost few-shot performance. In other words, long text does\nallow the model to utilize multiple image and text examples as context to per-\nform better question answering and captioning. On the other side, combining\ntext-only with interleaved data leads to a drop in performance, albeit a minor\none. In both cases, text-only performance is increased as shown in the boost of\nTextCore numbers.",
    "10B. McKinzie et al.\nData Lesson 3: Careful mixture of image and text data can yield\noptimal multimodal performance and retain strong text performance.\nThe above lesson leads to the question of how to best combine text-only data\nto achieve both strong image and language understanding. In Figure 5c, we\nexperiment with several mixing ratios between image (caption and interleaved)\nand text-only data. We see that with caption/interleaved/text ratio 5:5:1, we\nachieve a good balance of strong multimodal performance while still keeping\ncomparable text-only understanding performance.\nData Lesson 4: Synthetic data helps with few-shot learning.At last, we\nstudy the importance of the synthetic caption data, VeCap [57]. It is of higher\nquality, but relatively small, being only7%compared to all caption data. As\nshown in Figure 5d, it does give a non-trivial boost in few-shot performance, of\n2.4%and4%absolute.\n4  Final Model and Training Recipe\nWe collect the results from the previous ablations to determine the final recipe\nfor MM1 multimodal pre-training:\n– Image Encoder: Motivated by the importance of image resolution, we use a\nViT-H [27] model with378×378resolution, pre-trained with a CLIP objective\non DFN-5B [31].\n– Vision-Language Connector: As the number of visual tokens is of highest\nimportance, we use a VL connector with 144 tokens. The actual architecture\nseems to matter less, we opt for C-Abstractor [12].\n– Data: In order to maintain both zero- and few-shot performance, we use the\nfollowing careful mix of 45% interleaved image-text documents, 45% image-\ntext pair documents, and 10% text-only documents.\n10\n7\n10\n8\n10\n9\n10\n10\n10\n−4\n10\n−3\nNum LLM Params\nPeak Learning Rate\nFig. 6:Optimal peak learning rate as a\nfunction of model size. The data points\nrepresent experiments that achieved\nclose-to-optimal 8-shot performance for\ntheir associated model size.\nIn order to improve the model per-\nformance, we scale up the LLM size to\n3B, 7B, and 30B parameters. We initial-\nize both the image encoder and the un-\nderlying LLM decoder weights for MM1\nfrom in-house pre-trained models\n2\n. We\nthen perform multimodal pre-training on\nthe above data mix for 200k steps (ap-\nprox. 400B tokens). All models are pre-\ntrained entirely unfrozen with sequence\nlength 4096, up to 16 images per sequence\nat378×378resolution, with a batch size\nof 512 sequences. All models are trained using the AXLearn framework.\n3\n2\nThe LLM is pre-trained on the text-only data mixture mentioned in Sec. 3.3.\n3\nhttps://github.com/apple/axlearn",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training11\nModel Scaling.At this scale it is infeasible to do proper hyperparameter search.\nInstead, using established scaling characteristics of LLMs [43, 44, 120, 121], we\nperform a grid search of learning rate at small scale, 9M, 85M, 302M, and 1.2B,\nwhile using the components identified in Sec. 3.2\n4\nto identify optimal learning\nrate and extrapolate it to larger scale. We use a linear regression in log space\nto extrapolate from smaller to larger models (see Figure 6), resulting in the\nfollowing prediction of optimal peak learning rateηgiven the number of (non-\nembedding) parametersN:\nη= exp(−0.4214 ln(N)−0.5535)(1)\nSimilar to [48], we found in preliminary experiments that validation loss wasn’t\nstrongly correlated with downstream task performance. Therefore, we directly\nuse downstream 8-shot average performance for curve fitting.\nForN= 3e\n10\n, this fit predictsη= 2.2e\n−5\n, which is what we use for the final\nMM1-30B. We initially performed a similar procedure to determine reasonable\nvalues for weight decay, denoted byλ, but ultimately found that the simple rule\nof scaling weight decay by peak learning rate asλ= 0.1ηworked well for all\nmodels. All further training details are described in Appendix B.\nScaling via Mixture-of-Experts (MoE).MoE scales the total number of\nmodel parameters while keeping the activated parameters constant. It enjoys a\nlarger model capacity without sacrificing inference speed significantly. Recently,\nMoE has shown promising results in language [23,29,32,49,136], multimodal [70,\n87] and computer vision [16,25,55,96] tasks.\nIn experiments, we further explore scaling the dense model by adding more\nexperts in the FFN layers of the language model. Our MoE implementation\ngenerally follows GShard [59] and ST-MoE [136]. Specifically, we design two\nMoE models, a 3B-MoE using 64 experts that replaces a dense layer with a\nsparse layer in every-2 layers and a 7B-MoE using 32 experts that replaces a\ndense layer with a sparse layer in every-4 layers. The 3B-MoE contains 64B\nparameters in total and the 7B-MoE contains 47B parameters in total. We adopt\ntop-2 gating with a load balance loss term with a0.01coefficient to encourage a\nbetter expert load balance and adopt a router z-loss term with a0.001coefficient\nto stabilize training. To convert a dense model to MoE, we only replace the dense\nlanguage decoder with an MoE language decoder. The image encoder and the\nvision-language connector are kept the same. To train an MoE, we adopt the\nsame training hyperparameters that are discovered for the dense backbone\n5\nand\nidentical training settings including training data and training tokens.\nMultimodal Pre-training Results.We evaluate pre-trained models on cap-\ntioning and VQA tasks via appropriate prompting.\n6\nWe evaluate zero- and few-\n4\nThe only exception is image encoder, which we downsize to the CLIP\nDFN+VeCap\nViT-L with336×336resolution to reduce compute costs for the grid searches.\n5\nThe dense backbone is defined to be the dense model we use to construct the MoE\nmodel.\n6\nThe models are prompted with “{IMAGE} A photo of” for captioning, and “{IMAGE}\nQuestion: {QUESTION} Short answer:” for VQA. See Appendix C.1 for more de-\ntails on pre-training evaluation.",
    "12B. McKinzie et al.\nModelShot\nCaptioningVisual Question Answering\nCOCO NoCaps TextCaps VQAv2 TextVQA VizWiz OKVQA\nMM1-3B Model Comparisons\nFlamingo-3B [3]\n0\n†\n73.0––49.230.128.941.2\n8\n90.6––55.432.438.444.6\nMM1-3B\n073.5   55.663.346.229.415.626.1\n8\n114.6104.788.863.644.646.448.4\nMM1-7B Model Comparisons\nIDEFICS-9B [58]\n0\n†\n46.0*   36.825.450.925.935.538.4\n897.0*86.863.256.427.540.447.7\nFlamingo-9B [3]\n0\n†\n79.4––51.831.828.844.7\n8\n99.0––58.033.639.450.0\nEmu2-14B [105]\n0\n†\n–––52.9–34.442.8\n8\n–––59.0–43.9–\nMM1-7B\n076.3   61.064.247.828.815.622.6\n8\n116.3106.688.263.646.345.351.4\nMM1-30B Model Comparisons\nIDEFICS-80B [58]\n0\n†\n91.8*   65.056.860.030.936.045.2\n8114.3*105.777.664.835.746.155.1\n16116.6*107.081.465.436.348.356.8\nFlamingo-80B [3]\n0\n†\n84.3––56.335.031.650.6\n8108.8––65.637.344.857.5\n16110.5––66.837.648.457.8\nEmu2-37B [105]\n0–––33.326.240.426.7\n8–––67.849.354.754.1\n16–––68.850.357.057.1\nMM1-30B\n070.3   54.664.948.928.214.524.1\n8123.1111.692.970.949.449.958.3\n16125.3116.097.671.950.657.959.3\nTable 3:Multimodal pre-training evaluations. (*) IDEFICS includes PMD in its train-\ning data (includes COCO). (†) These models include two text-only demonstrations in\ntheir “0” prompt, whereas MM1 does not. For the full table, see Table 6 in Appendix.\nshot, as shown in Table 3, and compare against the few approaches that report\nfew-shot pre-training performance. Note that we only compare our model with\nlarger models,e.g., comparing our 30B model with two 80B models.\nWhen it comes to few-shot performance, MM1 outperforms all published\nprior work for pre-trained MLLMs. We see superior performance at 30B across\ncaptioning benchmarks and the VizWiz-QA benchmark. On VQAv2, TextVQA,\nOKVQA, at that scale we are comparable to Emu2 [105]. For zero-shot perfor-\nmance\n7\n, even without instruction fine-tuning, our models perform favorably on\n7\nWe provide zero-shot results as a reference for the associated few-shot numbers, but\nwe intentionally do not hill-climb on zero-shot metrics as they are mostly indicative\nof how well the pre-training mixture matches the associated evaluation task format.",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training13\nTextCaps across all model sizes, and comparable to Flamingo-3B at small scales\nfor most benchmarks.\n5  Supervised Fine-Tuning\nIn this section, we describe the supervised fine-tuning (SFT) experiments trained\non top of the pre-trained models described in the previous sections.\nSFT Data Mixture.We follow LLaVA-1.5 [74] and LLaVA-NeXT [75], and\ncollect roughly 1.45M SFT examples from a diverse set of datasets, including\n–Instruction-response pairs generated by GPT-4 and GPT-4V, including LLaVA-\nConv and LLaVA-Complex [76] for conversations and complex reasoning, and\nShareGPT-4V [15]\n8\nfor detailed image descriptions;\n–Academic task oriented vision-language (VL) datasets, including (i) VQAv2 [38],\nGQA [46], OKVQA [82], A-OKVQA [98], and COCO Captions [18] for natu-\nral images; (ii) OCRVQA [86], and TextCaps [103] for text-rich images; and\n(iii) DVQA [51], ChartQA [83], AI2D [52], DocVQA [85], InfoVQA [84], and\nSynthdog-En [53] for document and chart understanding.\n–Text-only SFT data: We include an internal text-only dataset to ensure the\nmodel is capable of text-only instruction following.\nThe academic VL datasets are formatted into the instruction-following for-\nmat, following LLaVA-1.5 [74]. More details are provided in Appendix A.3. All\ndatasets are mixed together and randomly sampled during training.\n9\nDuring SFT, we keep both the image encoder and the LLM backboneun-\nfrozen; other SFT training details are provided in Appendix B.2. We evaluate\nour models across 12 benchmarks (see Appendix C.2 for details).\nScaling to Higher Resolutions.Intuitively, higher image resolution leads to\nbetter performance. To support high-resolution SFT, we use two approaches:\nPositional embedding interpolation,e.g., as explored in Qwen-VL [5]\nand BLIP2 [65]. After positional embedding interpolation, the vision transformer\nbackbone is adapted to the new resolution during fine-tuning. Through this\nmethod, we have fine-tuned our model to support image resolutions ranging\nfrom448×448,560×560, to672×672. Note that, for a resolution of672×672,\nwith a patch size of14×14, an image is represented with2,304tokens.\nSub-image decomposition, recently introduced by SPHINX [73], Mon-\nkey [69], and LLaVA-NeXT [75]. Computing self-attention among more than\n2,000image tokens is computationally challenging, limiting further scaling to\neven higher image resolutions. Following SPHINX [73], as shown in Figure 7a,\nfor a high-resolution input image,e.g.,1344×1344, we construct five images\nof672×672, and feed them as independent images into our visual encoder.\n8\nWe also experimented with LVIS-Instruct4V [113], but did not observe better per-\nformance than using ShareGPT-4V [15], thus it is not included in the final mixture.\n9\nWhile some different data mixing strategies were explored, simply mixing these\ndatasets already achieves good performance, similar to observations in Honeybee [12].",
    "14B. McKinzie et al.\nModelVQA\nv2\nVQA\nT\nSQA\nI\nMMMU MathV MME\nP\nMME\nC\nMMB  SEED  POPE LLaVA\nW\nMM-Vet\n3B Model Comparison\nMobileVLM [20]–47.5  61.0–/––   1288.9    –    59.6    –/–84.9––\nLLaVA-Phi [135]71.4   48.6  68.4–/––   1335.1    –    59.8    –/–85.0–28.9\nImp-v1 [99]\n79.45  59.38 69.96–/––   1434.0    –    66.49–88.02–33.1\nTinyLLaVA [133]79.9   59.1  69.1–/––   1464.9    –    66.9    –/–86.4    75.832.0\nBunny [42]79.8–    70.938.2/33.0   –   1488.8  289.3  68.6   62.5/–   86.8––\nGemini Nano-2 [106]67.5   65.9    –32.6/–   30.6–––––––\nMM1-3B-Chat82.071.969.433.9/33.732.01482.5279.367.863.0/68.887.472.143.7\nMM1-3B-MoE-Chat82.572.976.138.6/35.732.61469.4303.170.863.9/69.487.676.842.2\n7B Model Comparison\nInstructBLIP-7B [24]–50.1  60.5–/–25.3––36.0   53.4/––60.926.2\nQwen-VL-Chat-7B [5]78.2   61.5  68.235.9/32.9   –   1487.5  360.7  60.6 58.2/65.4   –––\nLLaVA-1.5-7B [74]\n78.5   58.2  66.8–/––   1510.7  316.1  64.3 58.6/66.1  85.9    63.431.1\nShareGPT4V-7B [15]80.6   60.4  68.4–/––   1567.4  376.4  68.8    –/––72.6–\nLVIS-Ins4V-7B [113]79.6   58.7  68.3–/––   1528.2    –    66.2   60.6/–   86.0    67.031.5\nVILA-7B [71]79.9   64.4  68.2–/––   1531.3    –    68.9   61.1/–   85.5    69.734.9\nSPHINX-Intern2 [36]\n75.5–    70.4–/–35.5  1260.4  294.6  57.9   68.8/–   86.9    57.636.5\nLLaVA-NeXT-7B [75]81.8   64.9  70.135.8/–   34.6   1519   332   67.4   –/70.2  86.53   81.643.9\nMM1-7B-Chat82.872.872.637.0/35.635.91529.3328.972.364.0/69.986.681.542.1\nMM1-7B-MoE-Chat83.473.874.440.9/37.940.91597.4394.672.765.5/70.987.884.745.2\n30B Model Comparison\nEmu2-Chat-37B [105]84.9   66.6    –36.3/34.1   ––––    62.8/–––48.5\nCogVLM-30B [114]83.4   68.1    –32.1/30.1   –––––––56.8\nLLaVA-NeXT-34B [75]\n83.7   69.5  81.851.1/44.7  46.5   1631   397   79.3   –/75.9  87.73   89.657.4\nMM1-30B-Chat83.773.581.044.7/40.339.4\n†\n1637.6431.475.165.9/72.187.689.348.7\nGemini Pro [106]71.274.6–47.9/–45.2–436.7973.6–/70.7––64.3\nGemini Ultra [106]77.882.3–59.4/–53.0–––––––\nGPT4V [1]77.278.0–56.8/55.749.9–517.1475.867.3/69.1––67.6\nTable 4:Comparison with SOTA models on MLLM benchmarks. VQA\nv2\n[38]; VQA\nT\n:\nTextVQA [104]; SQA\nI\n: ScienceQA-IMG [81]; MMMU [128]; MathV: MathVista [80];\nMME\nP/C\n: the Perception/Cognition split of MME [33]; MMB: MMBench [78]; SEED:\nSEED-Bench [62]; POPE [68]; LLaVA\nW\n: LLaVA-Bench (In-the-Wild) [76]; MM-\nVet [127]. The two numbers reported in MMMU denote the performance on the val and\ntest split, respectively. The two numbers reported in SEED denote the performance on\nthe whole SEED-Bench and the image part, respectively. (†) 8-shot prompting: 44.4.\nSpecifically, we first downsample the input image to672×672as a high-level\nrepresentation, and also resize the input image to1344×1344and divide the\nresized image into 4 sub-images of672×672, which preserve more detailed visual\ninformation. Using positional embedding interpolation for each sub-image, we\ncan support image resolution as high as1792×1792in experiments.\n5.1  SFT Results\nComparison with SOTA.Results are summarized in Table 4. We use “-Chat”\nto denote our MM1 models after SFT. First, on average, MM1-3B-Chat and\nMM1-7B-Chat outperforms all listed models of the same size, setting a new\nstate of the art for these model sizes. MM1-3B-Chat and MM1-7B-Chat show\nparticularly strong performance on VQAv2, TextVQA, ScienceQA, and also the\nmore recent benchmarks (MMMU and MathVista).\nSecond, we explore two MoE models: (i) 3B-MoE with 64 experts, and (ii)\n7B-MoE with 32 experts. Our MoE models achieve uniformly better performance",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training15\nImage Encoder (with interpolation) \nSequence of encoded images\nImage Crops\n(a)High resolution image input\nprocessing.\n224336448672896\n13441792\n80\n90\n100\nImage resolution\nAverage evaluation metric\n(b)Impact of image resolution\non SFT performance.\n050100\n150200\n92\n94\n96\n98\n100\nPre-training step (1000s)\nAverage evaluation metric\n(c)Impact of pre-training on\nSFT performance.\nFig. 7:We study the impact of image resolution and pre-training for SFT performance.\nthan the dense counterpart on almost every benchmark. This shows the great\npotential of MoE for further scaling, which is left as future work.\nThird, for the 30B model size, MM1-30B-Chat outperforms Emu2-Chat-\n37B [105] and CogVLM-30B [114] on TextVQA, SEED, and MMMU. Compared\nwith the concurrent LLaVA-NeXT [75], we also achieve competitive performance\nacross the board. However, LLaVA-NeXT does not support multi-image reason-\ning, nor few-shot prompting, as each image is represented as 2,880 tokens sent\nto the LLM, while ours is only 720 in total. This limits certain applications that\ninvolve multiple images.\nImpact of Image Resolution.Figure 7b shows the impact of input image\nresolution on the average performance of the SFT evaluation metrics (defer the\ndetails of how we calculate the meta-average to Appendix C.3). Compared to\na baseline model with an image resolution of 336 pixels, we can achieve a 15%\nrelative increase by supporting an image resolution of1344×1344. Note that\nfor the largest image resolution of1792×1792, average performance decreases\nslightly. This is likely because many of the evaluation images are smaller than this\nresolution, and resizing artifacts may affect the model performance. By default,\nthe results in Table 4 correspond to image resolutions of1344×1344.\nImpact of Pre-training.In contrast to most recent MLLMs, we perform large-\nscale pre-training for our models. To assess the impact of pre-training on the\nfinal model performance, we perform SFT on the same pre-training run, but at\ndifferent checkpoint steps. For an earlier checkpoint step, the model has seen\nless unique data samples than a later checkpoint step, so this is a measure of the\nimportance of the quantity of pre-training data. In Figure 7c, we show that the\nmodel consistently improves as it has seen more pre-training data. Furthermore,\nlarge-scale multimodal pre-training enables strong in-context few-shot learning\nand multi-image reasoning capabilities, while most MLLM benchmarks shown\nin Table 4 focus on zero-shot metrics and single-image reasoning.\nFew-shot Chain-of-Thought Reasoning after SFT.As seen in Section 3.3,\nMM1 gains few-shot capabilities thanks to interleaved data. Even though our",
    "16B. McKinzie et al.\nfine-tuning data includes only single-image examples, we find that MM1-30B-\nChat still exhibits multi-image reasoning. This is shown qualitatively in Figure 2,\nand quantitatively on MathVista [80], where we evaluate few-shot performance\nwith chain-of-thought prompting: 4-shot performance is41.9, which is 2.5 points\nhigher than zero-shot (39.4).\nOur best performing high-resolution SFT model uses 720 tokens per image.\nThis is a challenge when using more than 4 in-context examples due to the con-\ntext length. To allow for more examples, we explore amixed resolution in-context\nexamplesformulation, where we feed some of the examples at a lower resolution\n(see Appendix C.5 for details). Using this formulation with 8 in-context examples\nincreases the performance on MathVista to44.4.\nDo the lessons learned via pre-training transfer to SFT?Yes. We find\nthat (1) pre-training with caption-only data improves SFT metrics, and (2)\ndifferent VL connector architectures have negligible impact on final results. De-\ntailed ablation results are provided in Appendix C.4.\nQualitative Analysis.To better understand MM1, more qualitative examples\nare provided in Appendix D, including single-image and multi-image reasoning,\nand few-shot prompting.\n6  Conclusion\nWe study how to build performant MLLMs. Through carefully ablating modeling\nand data choices, we identify important lessons that yield a pre-trained model\nachieving SOTA results on a range of few-shot evaluations. After SFT, this\nmodel family produces competitive performance on a wide range of benchmarks,\nwhile enabling multi-image reasoning and few-shot prompting. We hope that the\nidentified lessons will help the community in building strong models beyond any\nsingle specific model architecture or data strategy.\nReferences\n1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774 (2023)\n2. Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D.,\nParikh, D., Lee, S., Anderson, P.: Nocaps: Novel object captioning at scale. In:\nICCV (2019)\n3. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,\nMensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han,\nT., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock,\nA., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O.,\nZisserman, A., Simonyan, K.: Flamingo: a visual language model for few-shot\nlearning (2022)\n4. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K.,\nBitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P.W., Ilharco, G.,",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training17\nWortsman, M., Schmidt, L.: Openflamingo: An open-source framework for train-\ning large autoregressive vision-language models. arXiv preprint arXiv:2308.01390\n(2023)\n5. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,\nJ.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966 (2023)\n6. Berant, J., Chou, A., Frostig, R., Liang, P.: Semantic parsing on Freebase from\nquestion-answer pairs. In: EMNLP (2013)\n7. Bisk, Y., Zellers, R., Le bras, R., Gao, J., Choi, Y.: Piqa: Reasoning about physical\ncommonsense in natural language. AAAI (2020)\n8. Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models\nwith reinforcement learning. arXiv preprint arXiv:2305.13301 (2023)\n9. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S.,\nBernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities\nand risks of foundation models. arXiv preprint arXiv:2108.07258 (2021)\n10. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-\nshot learners. NeurIPS (2020)\n11. Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., Kim, S.: Coyo-700m: Image-text\npair dataset.https://github.com/kakaobrain/coyo-dataset(2022)\n12. Cha, J., Kang, W., Mun, J., Roh, B.: Honeybee: Locality-enhanced projector for\nmultimodal llm. arXiv preprint arXiv:2312.06742 (2023)\n13. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. In: CVPR\n(2021)\n14. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-\ning multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195\n(2023)\n15. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.:\nSharegpt4v: Improving large multi-modal models with better captions. arXiv\npreprint arXiv:2311.12793 (2023)\n16. Chen, T., Chen, X., Du, X., Rashwan, A., Yang, F., Chen, H., Wang, Z., Li, Y.:\nAdamv-moe: Adaptive multi-task vision mixture-of-experts. In: ICCV (2023)\n17. Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz,\nC.R., Goodman, S., Wang, X., Tay, Y., et al.: Pali-x: On scaling up a multilingual\nvision and language model. arXiv preprint arXiv:2305.18565 (2023)\n18. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.:\nMicrosoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325 (2015)\n19. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\nguage modeling with pathways. JMLR (2023)\n20. Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang,\nB., Wei, X., et al.: Mobilevlm: A fast, reproducible and strong vision language\nassistant for mobile devices. arXiv preprint arXiv:2312.16886 (2023)\n21. Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,\nDehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416 (2022)\n22. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord,\nO.: Think you have solved question answering? try arc, the ai2 reasoning chal-\nlenge. arXiv preprint arXiv:1803.05457 (2018)",
    "18B. McKinzie et al.\n23. Dai, D., Deng, C., Zhao, C., Xu, R.X., Gao, H., Chen, D., Li, J., Zeng, W.,\nYu, X., Wu, Y., Xie, Z., Li, Y.K., Huang, P., Luo, F., Ruan, C., Sui, Z., Liang,\nW.: Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts\nlanguage models. arXiv preprint arXiv:2401.06066 (2024)\n24. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,\nS.: Instructblip: Towards general-purpose vision-language models with instruction\ntuning (2023)\n25. Daxberger, E., Weers, F., Zhang, B., Gunter, T., Pang, R., Eichner, M., Emmers-\nberger, M., Yang, Y., Toshev, A., Du, X.: Mobile v-moes: Scaling down vision\ntransformers via sparse mixture-of-experts (2023)\n26. Devlin, J.,  Chang,  M.W.,  Lee,  K.,  Toutanova,  K.:  Bert:  Pre-training  of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 (2018)\n27. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n28. Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid,\nA., Tompson, J., Vuong, Q., Yu, T., et al.: PaLM-E: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378 (2023)\n29. Du, N., Huang, Y., Dai, A.M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou,\nY., Yu, A.W., Firat, O., Zoph, B., Fedus, L., Bosma, M.P., Zhou, Z., Wang, T.,\nWang, E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T.,\nDixon, L., Zhang, K., Le, Q., Wu, Y., Chen, Z., Cui, C.: GLaM: Efficient scaling\nof language models with mixture-of-experts. In: ICML (2022)\n30. El-Nouby, A., Klein, M., Zhai, S., Bautista, M.A., Shankar, V., Toshev, A.,\nSusskind, J., Joulin, A.: Scalable pre-training of large autoregressive image mod-\nels. arXiv preprint arXiv:2401.08541 (2024)\n31. Fang, A., Jose, A.M., Jain, A., Schmidt, L., Toshev, A., Shankar, V.: Data filtering\nnetworks. arXiv preprint arXiv:2309.17425 (2023)\n32. Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion param-\neter models with simple and efficient sparsity (2022)\n33. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li,\nK., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal\nlarge language models. arXiv preprint arXiv:2306.13394 (2023)\n34. Fu, T.J., Hu, W., Du, X., Wang, W.Y., Yang, Y., Gan, Z.: Guiding instruction-\nbased image editing via multimodal large language models. arXiv preprint\narXiv:2309.17102 (2023)\n35. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C.,\nGolding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa,\nC., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E.,\nThite, A., Wang, B., Wang, K., Zou, A.: A framework for few-shot language model\nevaluation (12 2023).https://doi.org/10.5281/zenodo.10256836,https://\nzenodo.org/records/10256836\n36. Gao, P., Zhang, R., Liu, C., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S.,\nLin, Z., Jin, P., et al.: Sphinx-x: Scaling data and parameters for a family of\nmulti-modal large language models. arXiv preprint arXiv:2402.05935 (2024)\n37. Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang,\nW., Luo, P., Chen, K.: Multimodal-gpt: A vision and language model for dialogue\nwith humans. arXiv preprint arXiv:2305.04790 (2023)",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training19\n38. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering.\nIn: CVPR (2017)\n39. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham,\nJ.P.: Vizwiz grand challenge: Answering visual questions from blind people. In:\nCVPR (2018)\n40. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders\nare scalable vision learners. In: CVPR (2022)\n41. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n42. He, M., Liu, Y., Wu, B., Yuan, J., Wang, Y., Huang, T., Zhao, B.: Efficient mul-\ntimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530\n(2024)\n43. Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H.,\nBrown, T.B., Dhariwal, P., Gray, S., et al.: Scaling laws for autoregressive gener-\native modeling. arXiv preprint arXiv:2010.14701 (2020)\n44. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E.,\nde Las Casas, D., Hendricks, L.A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,\nMillican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan,\nK., Elsen, E., Rae, J.W., Vinyals, O., Sifre, L.: Training compute-optimal large\nlanguage models (2022)\n45. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mo-\nhammed, O.K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary,\nV., Som, S., Song, X., Wei, F.: Language is not all you need: Aligning perception\nwith language models (2023)\n46. Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In: CVPR (2019)\n47. IDEFICS: Introducing idefics: An open reproduction of state-of-the-art visual\nlanguage model.https://huggingface.co/blog/idefics(2023)\n48. Isik, B., Ponomareva, N., Hazimeh, H., Paparas, D., Vassilvitskii, S., Koyejo, S.:\nScaling laws for downstream task performance of large language models (2024)\n49. Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C.,\nChaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G.,\nLample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian,\nS., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix,\nT., Sayed, W.E.: Mixtral of experts (2024)\n50. Joshi, M., Choi, E., Weld, D.S., Zettlemoyer, L.: Triviaqa: A large scale dis-\ntantly supervised challenge dataset for reading comprehension. arXiv preprint\narXiv:1705.03551 (2017)\n51. Kafle, K., Price, B., Cohen, S., Kanan, C.: Dvqa: Understanding data visualiza-\ntions via question answering. In: CVPR (2018)\n52. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., Farhadi, A.: A\ndiagram is worth a dozen images. In: ECCV (2016)\n53. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han,\nD., Park, S.: Ocr-free document understanding transformer. In: ECCV (2022)\n54. Koh, J.Y., Fried, D., Salakhutdinov, R.: Generating images with multimodal lan-\nguage models. arXiv preprint arXiv:2305.17216 (2023)\n55. Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C.R., Mustafa, B., Ainslie,\nJ., Tay, Y., Dehghani, M., Houlsby, N.: Sparse upcycling: Training mixture-of-\nexperts from dense checkpoints. In: ICLR (2023)",
    "20B. McKinzie et al.\n56. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning\nsegmentation via large language model. arXiv preprint arXiv:2308.00692 (2023)\n57. Lai, Z., Zhang, H., Wu, W., Bai, H., Timofeev, A., Du, X., Gan, Z., Shan, J.,\nChuah, C.N., Yang, Y., et al.: From scarcity to efficiency: Improving clip training\nvia visual-enriched captions. arXiv preprint arXiv:2310.07699 (2023)\n58. Laurençon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A.,\nWang, T., Karamcheti, S., Rush, A.M., Kiela, D., Cord, M., Sanh, V.: Obelics:\nAn open web-scale filtered dataset of interleaved image-text documents (2023)\n59. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer,\nN., Chen, Z.: {GS}hard: Scaling giant models with conditional computation and\nautomatic sharding. In: ICLR (2021)\n60. Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., Liu, Z.: Mimic-\nit: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425\n(2023)\n61. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal\nmodel with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)\n62. Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking\nmultimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125\n(2023)\n63. Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., Gao, J.: Multimodal foun-\ndation models: From specialists to general-purpose assistants. arXiv preprint\narXiv:2309.10020 (2023)\n64. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models (2023)\n65. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597 (2023)\n66. Li, L., Yin, Y., Li, S., Chen, L., Wang, P., Ren, S., Li, M., Yang, Y., Xu, J., Sun,\nX., et al.: M\n3\nit: A large-scale dataset towards multi-modal multilingual instruction\ntuning. arXiv preprint arXiv:2306.04387 (2023)\n67. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple\nand performant baseline for vision and language. arXiv preprint arXiv:1908.03557\n(2019)\n68. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355\n(2023)\n69. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.:\nMonkey: Image resolution and text label are important things for large multi-\nmodal models. arXiv preprint arXiv:2311.06607 (2023)\n70. Lin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Huang, J., Zhang, J., Ning, M.,\nYuan, L.: Moe-llava: Mixture of experts for large vision-language models (2024)\n71. Lin, J., Yin, H., Ping, W., Lu, Y., Molchanov, P., Tao, A., Mao, H., Kautz, J.,\nShoeybi, M., Han, S.: Vila: On pre-training for visual language models. arXiv\npreprint arXiv:2312.07533 (2023)\n72. Lin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona,\nP., Ramanan, D., Doll’a r, P., Zitnick, C.L.: Microsoft COCO: common objects\nin context. arXiv preprint arXiv:1405.0312 (2014)\n73. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W.,\nChen, K., et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings\nfor multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training21\n74. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning.\narXiv preprint arXiv:2310.03744 (2023)\n75. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved\nreasoning, ocr, and world knowledge (January 2024),https://llava-vl.github.\nio/blog/2024-01-30-llava-next/\n76. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023)\n77. Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H.,\nZhu, J., et al.: Llava-plus: Learning to use tools for creating multimodal agents.\narXiv preprint arXiv:2311.05437 (2023)\n78. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J.,\nHe, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player?\narXiv preprint arXiv:2307.06281 (2023)\n79. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. NeurIPS (2019)\n80. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.W.,\nGalley, M., Gao, J.: Mathvista: Evaluating mathematical reasoning of foundation\nmodels in visual contexts. arXiv preprint arXiv:2310.02255 (2023)\n81. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.W., Zhu, S.C., Tafjord, O., Clark,\nP., Kalyan, A.: Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. NeurIPS (2022)\n82. Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In: CVPR (2019)\n83. Masry, A., Long, D.X., Tan, J.Q., Joty, S., Hoque, E.: Chartqa: A benchmark for\nquestion answering about charts with visual and logical reasoning. arXiv preprint\narXiv:2203.10244 (2022)\n84. Mathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, E., Jawahar, C.: Info-\ngraphicvqa. In: WACV (2022)\n85. Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document\nimages. In: WACV (2021)\n86. Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question\nanswering by reading text in images. In: ICDAR (2019)\n87. Mustafa, B., Ruiz, C.R., Puigcerver, J., Jenatton, R., Houlsby, N.: Multimodal\ncontrastive learning with LIMoe: the language-image mixture of experts. In:\nNeurIPS (2022)\n88. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,\nFernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust\nvisual features without supervision. arXiv preprint arXiv:2304.07193 (2023)\n89. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q.N., Bernardi, R., Pezzelle,\nS., Baroni, M., Boleda, G., Fernández, R.: The lambada dataset: Word prediction\nrequiring a broad discourse context (2016)\n90. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-\n2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824 (2023)\n91. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models\nfrom natural language supervision. In: ICML (2021)\n92. Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides,\nJ., Henderson, S., Ring, R., Young, S., et al.: Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint arXiv:2112.11446 (2021)",
    "22B. McKinzie et al.\n93. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR (2020)\n94. Ranasinghe, K., McKinzie, B., Ravi, S., Yang, Y., Toshev, A., Shlens, J.: Percep-\ntual grouping in contrastive vision-language models. In: ICCV (2023)\n95. Rao, Y., Zhao, W., Chen, G., Tang, Y., Zhu, Z., Huang, G., Zhou, J., Lu, J.:\nDenseclip: Language-guided dense prediction with context-aware prompting. In:\nCVPR (2022)\n96. Ruiz, C.R., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Pinto, A.S.,\nKeysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. In:\nBeygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) NeurIPS (2021)\n97. Sakaguchi, K., Bras, R.L., Bhagavatula, C., Choi, Y.: Winogrande: an adversarial\nwinograd schema challenge at scale. Commun. ACM (2021)\n98. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A\nbenchmark for visual question answering using world knowledge. In: ECCV (2022)\n99. Shao, Z., Ouyang, X., Yu, Z., Yu, J.: Imp: An emprical study of multimodal small\nlanguage models (2024),https://huggingface.co/MILVLG/imp-v1-3b\n100. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In: ACL\n(2018)\n101. Sharma, S., El Asri, L., Schulz, H., Zumer, J.: Relevance of unsupervised metrics in\ntask-oriented dialogue for evaluating natural language generation. arXiv preprint\narXiv:1706.09799 (2017)\n102. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., Catanzaro, B.:\nMegatron-lm: Training multi-billion parameter language models using model par-\nallelism. arXiv preprint arXiv:1909.08053 (2019)\n103. Sidorov, O., Hu, R., Rohrbach, M., Singh, A.: Textcaps: a dataset for image\ncaptioning with reading comprehension. In: ECCV (2020)\n104. Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach,\nM.: Towards vqa models that can read. In: CVPR (2019)\n105. Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu,\nJ., Huang, T., et al.: Generative multimodal models are in-context learners. arXiv\npreprint arXiv:2312.13286 (2023)\n106. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R.,\nSchalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805 (2023)\n107. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,\nH.T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models for\ndialog applications. arXiv preprint arXiv:2201.08239 (2022)\n108. Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? explor-\ning the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209\n(2024)\n109. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n110. Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S., Vinyals, O., Hill, F.: Multi-\nmodal few-shot learning with frozen language models. NeurIPS (2021)\n111. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image descrip-\ntion evaluation. arXiv preprint arXiv:1411.5726 (2014)\n112. Wang, F., Mei, J., Yuille, A.: Sclip: Rethinking self-attention for dense vision-\nlanguage inference. arXiv preprint arXiv:2312.01597 (2023)",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training23\n113. Wang, J., Meng, L., Weng, Z., He, B., Wu, Z., Jiang, Y.G.: To see is to be-\nlieve: Prompting gpt-4v for better visual instruction tuning. arXiv preprint\narXiv:2311.07574 (2023)\n114. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,\nL., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv\npreprint arXiv:2311.03079 (2023)\n115. Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou,\nJ., Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder\nfor vision-centric tasks. arXiv preprint arXiv:2305.11175 (2023)\n116. Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai,\nA.M., Le, Q.V.: Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652 (2021)\n117. Welbl, J., Liu, N.F., Gardner, M.: Crowdsourcing multiple choice science ques-\ntions. In: Derczynski, L., Xu, W., Ritter, A., Baldwin, T. (eds.) Proceedings of\nthe 3rd Workshop on Noisy User-generated Text. Association for Computational\nLinguistics (2017)\n118. Wenzek, G., Lachaux, M.A., Conneau, A., Chaudhary, V., Guzmán, F., Joulin,\nA., Grave, E.: Ccnet: Extracting high quality monolingual datasets from web\ncrawl data. arXiv preprint arXiv:1911.00359 (2019)\n119. Wortsman, M., Liu, P.J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes,\nJ.D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-dickstein, J., Xu, K.,\nLee, J., Gilmer, J., Kornblith, S.: Small-scale proxies for large-scale transformer\ntraining instabilities (2023)\n120. Yang, G., Hu, E.J.: Feature learning in infinite-width neural networks. arXiv\npreprint arXiv:2011.14522 (2020)\n121. Yang, G., Hu, E.J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pa-\nchocki, J., Chen, W., Gao, J.: Tensor programs v: Tuning large neural networks\nvia zero-shot hyperparameter transfer (2022)\n122. Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.C., Liu, Z., Wang, L.: The dawn of lmms:\nPreliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421\n(2023)\n123. Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao, C., Xu, G., Li, C., Tian, J.,\net al.: mplug-docowl: Modularized multimodal large language model for document\nunderstanding. arXiv preprint arXiv:2307.02499 (2023)\n124. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,\nShi, Y., et al.: mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178 (2023)\n125. Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou,\nJ.: mplug-owl2: Revolutionizing multi-modal large language model with modality\ncollaboration. arXiv preprint arXiv:2311.04257 (2023)\n126. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,\nYang, Y.: Ferret: Refer and ground anything anywhere at any granularity. In:\nICLR (2024)\n127. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet:\nEvaluating large multimodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490 (2023)\n128. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S.,\nJiang, D., Ren, W., Sun, Y., et al.: Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert agi. arXiv preprint\narXiv:2311.16502 (2023)",
    "24B. McKinzie et al.\n129. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., Choi, Y.: Hellaswag: Can a ma-\nchine really finish your sentence? (2019)\n130. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang,\nL., Li, C., et al.: Llava-grounding: Grounded visual chat with large multimodal\nmodels. arXiv preprint arXiv:2312.02949 (2023)\n131. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,\nDiab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068 (2022)\n132. Zhao, B., Wu, B., Huang, T.: Svit: Scaling up visual instruction tuning. arXiv\npreprint arXiv:2307.04087 (2023)\n133. Zhou, B., Hu, Y., Weng, X., Jia, J., Luo, J., Liu, X., Wu, J., Huang, L.:\nTinyllava: A framework of small-scale large multimodal models. arXiv preprint\narXiv:2402.14289 (2024)\n134. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 (2023)\n135. Zhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., Tang, J.: Llava-phi: Efficient multi-\nmodal assistant with small language model. arXiv preprint arXiv:2401.02330\n(2024)\n136. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., Fedus,\nW.: St-moe: Designing stable and transferable sparse expert models (2022)",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training25\nAppendix\nA  Dataset Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   25\nA.1  Interleaved Image-Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   25\nA.2  Text-Only Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   25\nA.3  Visual Instruction Tuning Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   26\nB  Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   27\nB.1  Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   27\nB.2  Supervised Fine-tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   29\nC  Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30\nC.1  Pre-training Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30\nC.2  SFT Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30\nC.3  SFT Evaluation Meta-Average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30\nC.4  Additional SFT Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   31\nC.5  Implementation Details for Few-shot MM1-30B-Chat . . . . . . . . . .   32\nD Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   33\nE  Author Contributions and Acknowledgements . . . . . . . . . . . . . . . . . . . . .   40\nA  Dataset Details\nA.1  Interleaved Image-Text Data\nFollowing a process similar to OBELICS [58], we construct a dataset of 500M\ninterleaved image-text documents, containing 1B images and 500B text tokens.\nThese 500M documents are built from a collection of 3B HTML files described\nin Sec. A.2. From each of the HTML files, we extract the text body layer and\nall the<img>tags. We remove documents that have no images or more than 30\nimages. We then download the images and insert them at their original positions\nin the text. Finally, we performimage filteringandimage de-duplication\nto remove low-quality and repetitive images.\nDuring image filtering, we remove images that have corrupted bytes and/or\nheader, aspect ratio less than 1/2 or greater than 2, are too small (less than\n100px) or too large (larger than 10,000px), or if their URL containslogo,button,\nicon,pluginorwidget. During image de-duplication, we remove images whose\nURL or MD5 hash have appeared more than 10 times in the dataset. Addition-\nally, when an image appears multiple times on a single page, we only retain its\nfirst appearance.\nA.2  Text-Only Data\nFrom an initial Web corpus of 150B English HTML files, we perform boilerplate\nremoval to arrive at the HTML representing the main content. We then follow\nsimilar processes as GPT-3 [10] and CCNet [118] to filter out documents that\nare too short, contain profanity, or are otherwise considered low-quality doc-\numents. We de-duplicate the data using exact-hash matching and LSH-based\nnear-duplicate detection. Using these methods, we arrive at 3B HTML files.",
    "26B. McKinzie et al.\nDatasetsSizePrompting Strategy\nText-only SFT13k–\nLLaVA-Conv [76]57k\nLLaVA-Complex [76]77k–\nShareGPT-4V [15]102k\nVQAv2 [38]83k\n“Answer the question using a single word or\nphrase.”\nGQA [46]72k\nOKVQA [82]9k\nOCRVQA [86]80k\nDVQA [51]200k\nChartQA [83]18k\nAI2D [52]3k\nDocVQA [85]39k\nInfoVQA [84]24k\nA-OKVQA [98]66k\n“Answer with the option’s letter from the given\nchoices directly.”\nCOCO Captions [18]83kSample from a pre-generated prompt list,e.g.,\n“Provide a brief description of the given image.”TextCaps [103]22k\nSynthDog-EN [53]500k\nSample from a pre-generated prompt list,e.g.,\n“Please transcribe all the text in the picture.”\nTotal1.45M–\nTable 5:List of datasets used for supervised fine-tuning.\nA.3  Visual Instruction Tuning Data\nOur final SFT data mixture contains a variety of datasets, mostly follow LLaVA-\n1.5 [74] and LLaVA-NeXT [75]. Specifically,\n–To encourage the model to provide long-form detailed responses and perform\nconversations, we follow previous work, use the existing GPT-4 generated\ndata (LLaVA-Conv and LLaVA-Complex [76]) and the existing GPT-4V gen-\nerated data (ShareGPT-4V [15]) for model training. We also experimented\nwith LAION-GPT4V, but did not observe further performance improvement,\nthus not included in the final mixture.\n–To enhance the model with better vision-language (VL) understanding capa-\nbility, we use a variety of academic task oriented VL datasets. These datasets\nare either in the form of image captioning, or in the form of VQA with short\nanswers. Specifically,\n•For natural images: VQAv2 [38], GQA [46], OKVQA [82], A-OKVQA [98],\nand COCO Captions [18];\n•For text-rich images: OCRVQA [86], and TextCaps [103];\n•For document and chart understanding: DVQA [51], ChartQA [83], AI2D [52],\nDocVQA [85], InfoVQA [84], and SynthDog-En [53];\n–To enhance the model’s text-only instruction following capability, we also\nblend in a small amount of text-only SFT data.",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training27\nThe academic task oriented image captioning and VQA datasets are formatted\ninto the instruction-following format, following LLaVA-1.5 [74], with detailed\nprompts summarized in Table 5.\nB  Training Details\nB.1  Pre-training\nBatch Size and Composition.For simplicity, all MM1 models are pre-trained\nwith the same batch size of 512 and maximum decoder sequence length of 4096.\nWe allow up to 16 images per input sequence, with each image resulting in 144\ntokens as input to the decoder. Note that this results in roughly 1M text tokens\nand 1M image tokens per batch. Each input sequence is sampled from one of\nthree types of input sources: (1) interleaved, (2) packed image-text pairs, or\n(3) text-only data, with sampling probability 45%, 45%, and 10%, respectively.\nWhen packing image-text pairs or interleaved documents along the sequence\ndimension, we modify the self-attention masks to prevent tokens from attention\nacross example boundaries. For image-text pairs in particular, this was critical\nfor maintaining strong few-shot performance.\nN  Pred.ηPred.λ\n1.2B 8.6e-5  5.0e-6\n2.9B 5.9e-5  3.5e-6\n6.4B 4.2e-5  2.5e-6\n30B 2.2e-5  1.3e-6\nTable 7:Predicted optimal peak\nlearning rateηand weight decayλ\nfor MM1 model sizes.\nNote that our sampling/mixing procedure\nis performed once offline and stored as a fixed\ndeterministicsnapshot of our pre-training\nmixture. This means, with the exception of\nour ablations on the pre-training mixture it-\nself, all models in this paper are trained on\nthe same examples in the same order. We\nfound this was critical to ensure internal re-\nproducibility of our results, as initial experi-\nments showed that different random seeds in\nthe input pipeline could have non-negligible\nimpact on resulting models.\n10M100M1.0B10B\n10\n−6\n10\n−5\n10\n−4\nNum LLM Params\nWeight Decay\nFig. 8:Optimal weight decay as\na function of model size for the\ngrid searches described in Sec. B.1.\nThe x-axis is the number of (non-\nembedding) LLM parameters and\nthe y-axis is weight decay.\nLearning Rate Schedule.For multimodal\npre-training, MM1 employs a standard cosine\nlearning rate decay schedule with an initial\nlinear warmup of 2000 steps. The learning\nrate is then decayed to 10% of its peak value\nover the course of2e5training steps. We per-\nform gradient clipping with max norm 1 and\nuse the AdamW optimizer with an implemen-\ntation that decouples the learning rate and\nweight decay. For MM1-30B, we also add a z-\nloss term with scale 1e-4, as we observed this\nimproves training stability, similar to [119].\nThe predicted optimal (peak) learning\nrates for each of the main LLM sizes studied",
    "28B. McKinzie et al.\nModelShot\nCaptioningVisual Question Answering\nCOCO NoCaps TextCaps VQAv2 TextVQA VizWiz OKVQA\nMM1-3B Model Comparisons\nFlamingo-3B [3]\n0\n†\n73.0––49.230.128.941.2\n485.0––53.232.734.043.3\n890.6––55.432.438.444.6\n1695.3––56.731.843.345.6\nMM1-3B\n073.555.663.346.229.415.626.1\n4\n112.399.784.157.945.338.048.6\n8114.6104.788.863.644.646.448.4\n16\n116.8107.691.660.946.153.850.5\nMM1-7B Model Comparisons\nIDEFICS-9B [58]\n0\n†\n46.0*36.825.450.925.935.538.4\n493.0*81.360.055.427.636.945.4\n897.0*86.863.256.427.540.447.7\n1699.7*89.467.457.027.942.648.4\nFlamingo-9B [3]\n0\n†\n79.4––51.831.828.844.7\n493.1––56.333.634.949.3\n899.0––58.033.639.450.0\n16102.2––59.433.543.050.8\nEmu2-14B [105]\n0\n†\n–––52.9–34.442.8\n4–––58.4–41.3–\n8–––59.0–43.9–\nMM1-7B\n076.361.064.247.828.815.622.6\n4109.896.284.560.644.437.446.6\n8116.3106.688.263.646.345.351.4\n16118.6111.193.165.246.953.252.9\nMM1-30B Model Comparisons\nIDEFICS-80B [58]\n0\n†\n91.8*65.056.860.030.936.045.2\n4110.3*99.672.763.634.440.452.4\n8114.3*105.777.664.835.746.155.1\n16116.6*107.081.465.436.348.356.8\nFlamingo-80B [3]\n0\n†\n84.3––56.335.031.650.6\n4103.2––63.136.539.657.4\n8108.8––65.637.344.857.5\n16110.5––66.837.648.457.8\nEmu2-37B [105]\n0–––33.326.240.426.7\n4–––67.048.254.653.2\n8–––67.849.354.754.1\n16\n–––68.850.357.057.1\nMM1-30B\n070.354.664.948.928.214.524.1\n4117.9103.887.568.848.141.754.9\n8123.1111.692.970.949.449.958.3\n16125.3116.097.671.950.657.959.3\nTable 6:Complete MM1 pre-training few-shot evaluation results. (*) IDEFICS in-\ncludes PMD in its training data (includes COCO). (†) These models included two\ntext-only demonstrations in their “0” prompt, whereas MM1 does not.",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training29\n3.2E-061.0E-053.2E-051.0E-04\n2.2E-05\n4.6E-05\n1.0E-04\n2.2E-04\n4.6E-04\n1.0E-03\n2.2E-03\n9m\n5\n10\n15\n20\n(a)9M\n3.2E-061.0E-053.2E-051.0E-04\n2.2E-05\n4.6E-05\n1.0E-04\n2.2E-04\n4.6E-04\n1.0E-03\n2.2E-03\n85m\n10\n20\n30\n40\n(b)85M\n3.2E-061.0E-053.2E-051.0E-04\n2.2E-05\n4.6E-05\n1.0E-04\n2.2E-04\n4.6E-04\n1.0E-03\n2.2E-03\n302m\n10\n20\n30\n40\n50\n(c)302M\n3.2E-061.0E-053.2E-051.0E-04\n2.2E-05\n4.6E-05\n1.0E-04\n2.2E-04\n4.6E-04\n1.0E-03\n2.2E-03\n1.2b\n30\n40\n50\n60\n(d)1.2B\nFig. 9:8-shot average for grid searches over peak learning rate (y-axis) and weight\ndecay (x-axis) for different LLM sizes. Black cells correspond to settings we did not\nrun a corresponding experiment for.\nin this work are shown in Table 7. For simplicity, for the actual MM1 3B, 7B,\nand 30B models, we usedηequal to 6e-5, 4e-5, and 2e-5, respectively. Finally, we\nfix the peak LR of the randomly initialized vision-language connector of MM1 to\nη=8e-5 for all model sizes. For future versions of MM1, we plan on incorporating\ntechniques similar to [121] to avoid the need to conduct costly hyperparameter\nsearches.\nLearning Rate and Weight Decay Grid Searches.The individual grid\nsearch results corresponding to the final curve fit in Figure 6 are shown in Fig-\nure 9. We train grid search models for5e\n4\nsteps, as [119] found this does not alter\nthe conclusions. We can apply the same procedure that was used for predicting\noptimal learning rate to predict weight decay values, as shown in Figure 8. The\nblue circles correspond to actual data points from the grid search with sampling\nprobability (and darkness of color) proportional to their 8-shot average perfor-\nmance. The corresponding predictions for each of the main model sizes in this\nwork are shown in Table 7.\nB.2  Supervised Fine-tuning (SFT)\nThe model is fine-tuned for 10k steps with batch size 256 and sequence length\n2048. We employ the AdaFactor optimizer with peak learning rate 1e-5 and",
    "30B. McKinzie et al.\ncosine decay to 0. We experimented different learning rates; empirically, the\nvalue of 1e-5 is optimal. During SFT, we keep both the image encoder and\nthe LLMunfrozen, as empirically, we observe that finetuning the whole model\nachieves better performance.\nC  Evaluation Details\nC.1  Pre-training Evaluation\nDataset Evaluation Split\nCOCO   Karpathy test\nNoCapsval\nTextCapsval\nVQAv2testdev\nTextVQAval\nVizWiztestdev\nOKVQAval\nTable 8:Splits used for pre-\ntraining evaluation. Note that,\nunlike the main pre-training\nresults, all pre-training abla-\ntions use the validation splits\nfor VQAv2 and VizWiz.\nFew-shot prompts are randomly sampled per-\ndataset from the training set if available, other-\nwise the validation set (ensuring the query exam-\nple does not appear in any of the shots). Out-\nputs are generated with greedy decoding until the\nmodel emits the EOS token or any additional stop\ntokens that can be specified on a per-task basis.\nThe additional stop token for captioning tasks is\njust the newline character, and for VQA tasks we\nalso include “.”, “,”, and “Question” as valid stop to-\nkens. For postprocessing VQA predictions, we use\nthe same logic as OpenFlamingo\n10\n[4]. For cap-\ntioning tasks, we report CIDEr score [111] using\nthe nlg-eval package [101]. All of our multimodal\npre-training evaluations are implemented in an in-\nternal fork of EleutherAI’s lm-evaluation-harness [35].\nC.2  SFT Evaluation Benchmarks\nWe evaluate our SFT models on a collection of both traditional academic VL\nbenchmarks and recent benchmarks specifically designed for MLLMs. For aca-\ndemic VL benchmarks, we include VQAv2 [38], TextVQA [104], and the image\nsubset of ScienceQA [81]. For recent MLLM benchmarks, we include POPE [68],\nMME [33], MMBench [78], SEED-Bench [62], LLaVA-Bench-in-the-Wild [76],\nMM-Vet [127], MathVista [80], and the recent popular MMMU [128]. For all\nthe benchmarks, we use greedy decoding to generate the responses. For MM-\nVet and LLaVA-Bench-in-the-Wild, which use GPT-4 for evaluation, we run the\nevaluation 3 times, and report the average.\nC.3  SFT Evaluation Meta-Average\nIn the process of SFT ablation, we synthesize all benchmark results into a single\nmeta-average number to simplify comparisons. Because the evaluation metrics\nof different datasets may have different ranges, we normalize with respect to a\nbaseline configuration. This is achieved by initially standardizing the results for\n10\nSpecifcally, the implementation of VQAMetric (commit 60a5fd6).",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training31\nSFT-Metrics\n96\n98\n100\n102\n104\n103\n.\n1\n99\n.\n9\n99\n.\n5\n99\n.\n5\n97\n.\n4\n97\n.\n3\n99\n.\n6\n100\nAverage Performance\n0/100/00/50/5033/33/33\n50/50/050/0/50100/0/0\n42/42/1445/45/9\n(a)Interleaved/Captions/Text\ndata mixture.\n224px\n64tks\n336px\n64tks\n336px\n144tks\n95\n100\n105\n110\n115\n101\n.\n3\n105\n.\n1\n105\n.\n2\n100\n103\n.\n2\n106\n.\n2\n100\n.\n4\n105\n.\n5\n107\n.\n8\nAverage SFT-Metrics\nAvg. PoolAtt. Pool\nC-Abstractor\n(b)Vision-language connector.\n336px\n144tks\n672px\n720tks\n1344px\n720tks\n90\n100\n110\n120\n100\n109\n.\n8\n113\n102\n.\n2\n109\n.\n6\n110\n.\n1\nAverage SFT-Metrics\nUnfrozen Encoder\nFrozen Encoder\n(c)(Un-)freezing image encoder\nduring SFT.\nFig. 10: SFT ablations.(a) The impact of pre-training data mixture on SFT results.\nHere,x/y/zmeans thatx%of the data is interleaved,y%is captions, andz%is pure\ntext. tks: the number of image tokens. (b) The impact of different vision-language\nconnectors on SFT results. For both (a) and (b), we first pre-train MM1-3B with\nthe ablated setting, and then perform SFT on the pre-trained models. (c) Freezing or\nunfreezing the image encoder during SFT.\neach task; that is, we adjust every metric by dividing it by its respective baseline,\nfollowed by averaging across all metrics. To elaborate, we establish our baseline\nusing the performance metrics of a compact MM1 model, which is trained on\n224×224image resolution and employs attention pooling with 64 image queries.\nC.4  Additional SFT Ablations\nIn this section, we perform SFT ablations. This section is analogous to Section 3;\nhere, we perform SFT on the same checkpoints and evaluate if similar lessons\nhold true on SFT evaluations, instead of pre-training evaluations. Furthermore,\nwe also study whether to keep the image encoder frozen or not during SFT. For\nall of these ablations, we train MM1-3B-Chat.\nPre-training data mixture ablations.In Figure 10a, we compare the SFT\nperformance with different weights for pre-training data. We see a similar trend\nwhen comparing with Figure 5 for 0-shot evaluations. Pre-training with caption-\nonly data gives the best performance across the SFT evaluation metrics. This\ncorroboratesData lesson 1: caption data still lifts zero-shot performance for\nSFT evaluations. However, the SFT metrics do not measure few-shot perfor-\nmance, so the impact of the interleaved data is not noticeable in this table.\nVisual-language connector ablations.In Figure 10b, we evaluate different\nvisual-language connector configurations. This figure is similar to Figure 4, ex-\ncept that we evaluate the corresponding SFT models. As can be seen, if a low\nnumber of image tokens is used, average pooling gives similar results as C-\nAbstractor. When the number of image tokens is increased, the C-Abstractor\nconfiguration gives the best results. These trends are not entirely consistent\nwith pre-training results reported in Figure 4. Overall, the impact of the choice",
    "32B. McKinzie et al.\nof visual-language connector appears to not have a very significant impact on\nfinal test performance. Our final models use the C-Abstractor architecture.\nImage encoder ablations.In Figure 10c, we study whether to keep the image\nencoder frozen or not during SFT. The results show that at lower image reso-\nlutions, a frozen image encoder results in better performance than an unfrozen\nimage encoder (+2.2 points). However, at higher resolutions (i.e., 1344px), it\nis beneficial to unfreeze the image encoder (+2.9 points). This is likely because\nthe pre-training is performed at the base resolution without any interpolation\nor image sub-divisions.\nC.5  Implementation Details for Few-shot MM1-30B-Chat\nAs shown in Section 5.1, our fine-tuned model can utilize in-context examples to\nachieve even stronger performance. Interestingly, the performance goes up when\nincreasing the number of examples. We demonstrate this with MM1-30B-Chat.\nOne challenge for few-shot inputs arises due to the use of sub-image decom-\nposition. While this strategy lifts zero-shot performance, it significantly increases\nthe effective number of tokens consumed per image. Using 5 sub-images per input\nimage as MM1-30B-Chat does, processing a 4-shot example where every example\ncontains just one source image already yields 20 effective images. Representing\nevery image with 144 tokens therefore requires 2,880 tokens for images alone,\nquickly exhausting limited language model context. To mitigate this limitation,\nwe propose a newmixed-resolutionapproach. Specifically, forKin-context ex-\namples, we only encode the lastNimages at a high resolution via sub-image\ndecomposition, the remainingK−Nin-context examples are processed at lower\nresolution. This makes in-context examples much less expensive to encode and\nallows to increase the number of in-context examples within a fixed token budget,\nfurther increasing performance. In our implementation, we setN= 3.\nWe demonstrate the effectiveness of our proposed strategy using the Math-\nVista benchmark. Using in-context learning with chain-of-thought, the perfor-\nmance of MM1-30B-Chat improves from 39.4 (0-shot) to 41.9 (4-shot, with all\nin-context examples using full sub-image decomposition). Applying our mixed-\nresolution approach allows to encode additional in-context examples, enabling\nup to 8-shot chain of thought, further improving the performance to 44.4. This\nillustrates that our MM1-Chat model retains in-context learning capabilities in-\nherited from its pre-training regimen, and that our strategy of mixed decompo-\nsition is effective at further increasing few-shot performance within a restricted\ncontext length.",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training33\nD  Qualitative Examples\nIn this section, we share qualitative examples of MM1 predictions.\nUser: Count the number of apples  \nin the image. \nMM1-30B-Chat (Ours): There are seven \napples in the image.\nUser: Count the number of oranges \nin the image. \nMM1-30B-Chat (Ours): There are three \noranges in the image.\nUser: What are all the scene text in the \nimage?\nMM1-30B-Chat (Ours): The earth \nwithout art is just 'eh'\nUser: What are all the scene text in the \nimage?\nMM1-30B-Chat (Ours): 3,642,039,031,055\nUser: Which of these states is farthest north?\nMM1-30B-Chat (Ours): Delaware\nMM1-30B-Chat (Ours): berries and \nflowers, grasses, seeds\nUser: Which of the organisms is the producer \nin this food web?\nUser: What are the other states?\nMM1-30B-Chat (Ours): Arizona, \nOklahoma, North Carolina\nFig. 11:Examples testing MM1 counting, OCR and scientific knowledge capabilities.\nImages and prompts are from COCO 2014 validation set [72] and [122].",
    "34B. McKinzie et al.\nUser: From scale 1 to 10, decide how similar \nthe image is to the text prompt \"a parrot \ndriving a car\". Explain your reasoning.\nMM1-30B-Chat (Ours): I would rate the \nimage a 1 on the scale of 1 to 10 in relation to \nthe text prompt \"a parrot driving a car.\" The \nimage shows a dolphin jumping out of the \nwater, and the text prompt is referring to a \nparrot driving a car. There is no similarity \nbetween the image and the text prompt, \nother than the fact that both contain animals. \nThe dolphin is in the water, while the parrot \nis associated with a car. The actions of the \nanimals are also different, as the dolphin is \njumping out of the water, while the parrot is \nimplied to be driving a car. The environments \nare completely different, with the dolphin \nbeing in a natural aquatic environment and \nthe parrot being in a human-made, terrestrial \nenvironment. \nMM1-30B-Chat (Ours): The image \nportrays a scene that is quite similar to the \ntext prompt \"a parrot driving a car\". In the \nimage, there are two parrots situated inside a \nblue car. The car is parked on a green field \nwith a backdrop of palm trees and a yellow \nsky.  \nOne of the parrots is perched on the steering \nwheel, giving the impression that it is driving \nthe car. The other parrot is comfortably \nseated on the roof of the car, seemingly \nenjoying the view. The colors in the image are \nvibrant, with the blue of the car contrasting \nbeautifully with the green field and the yellow \nsky. The palm trees add a tropical feel to the \nscene. \nThe image is a close representation of the text \nprompt, with the exception of the number of \nparrots and their actions. While the prompt \nmentions only one parrot driving a car, the \nimage shows two parrots, one seemingly \ndriving and the other on the roof of the car. \nThis adds an element of surprise and humor \nto the image. Overall, the image scores a 9 \nout of 10 for its close representation of the \ntext prompt and its creative interpretation.\nUser: From scale 1 to 10, decide how similar \nthe image is to the text prompt \"a parrot \ndriving a car\". Explain your reasoning.\nUser: Can you find the red ball near the gray \ncat in the image? \nMM1-30B-Chat (Ours): No, there is no \nred ball near the gray cat in the image. The \ncat is sitting on a pillow on a couch, and \nthere are no other objects like a red ball \nvisible in the scene.\nUser: Can you see the bright blue feathers on \nthe bird's wing? \nMM1-30B-Chat (Ours): No, the bird in \nthe image does not have bright blue feathers \non its wing. The bird is a large bird of prey, \nsuch as a hawk or an eagle, with typical bird \nof prey feathers.\nFig. 12:Examples testing MM1 against adversarial prompts and image-prompt align-\nment. Images and prompts are from COCO 2014 validation set [72], [122] and [8].",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training35\nUser: <img1> <img2> For each of the two images \ncomment on whether the water is salty.\nUser: <img1> <img2> Which image shows the healthier \nmeal?\nMM1-30B-Chat (Ours): The healthier meal among \nthe given images is the one that features broccoli and a \nbowl of vegetables, along with a plate of meat. \nIncorporating vegetables, like broccoli and those in the \nbowl, provides essential nutrients, vitamins, and fiber, \nwhich contribute to a balanced and healthier diet. The \nmeal with various toppings on a pizza and beer would be \nconsidered less healthy as it may contain more calories, \nfat, and processed ingredients compared to the other \nmeal that emphasizes vegetables and meat.\nUser: After seeing the below image, how people might \nemotionally feel and react?\nMM1-30B-Chat (Ours): Upon witnessing this image, \npeople might feel a mixture of emotions such as awe, \nfascination, and tranquility. The breathtaking view of \nthe airplane window showing the wing of the plane and \na vast, snowy mountain range beneath it offers a sense \nof serenity and the beauty of nature. The magnificent, \norderly pattern of the mountains and their snowy peaks \ncontrasted with the clear blue sky evokes a sense of \nwonder and appreciation for the natural world. This \nview also offers a unique perspective of the world that \none might not experience daily, thus sparking curiosity \nand excitement. Additionally, the image can bring about \nfeelings of calmness and peace, given the untouched and \npristine nature of the landscape depicted.\nUser: After seeing the below image, how people might \nemotionally feel and react?\nMM1-30B-Chat (Ours): The image shows a dark \nstairwell with a creepy atmosphere, which might evoke \nfeelings of unease, fear, or curiosity in people. Some \nindividuals may react by avoiding the stairwell altogether, \nwhile others might be intrigued by the mysterious \nambiance and decide to explore it further. The darkness \nand the shadows in the stairwell can play with people's \nperceptions and trigger their imagination, possibly making \nthem feel as if there is someone or something lurking in \nthe shadows. Overall, the image can provoke a range of \nemotional responses, from apprehension to fascination, \ndepending on each person's disposition and mindset.\nMM1-30B-Chat (Ours): In the first image, there are \nseveral boats floating on a body of water. Based on the \npresence of boats, it's likely this is a harbor, and the \nwater is likely salty as most harbors are typically found \nnear the ocean. However, without more information, it's \nnot possible to confirm the salinity of the water. \nIn the second image, various African animals, including \nelephants and giraffes, are gathered around a watering \nhole in a dry grass field. This body of water is most likely \nfreshwater as animals need fresh water to drink. The \npresence of numerous animals drinking from the watering \nhole confirms that this water is essential for their \nsurvival, and it's highly unlikely they would be able to \ndrink saltwater.\nFig. 13:Examples testing MM1 ability to perceive image aesthetics and compare mul-\ntiple images. Images and prompts are from COCO 2014 validation set [72] and [122].",
    "36B. McKinzie et al.\nUser: First, read the operating menus below \nfor coffee machine buttons. \nMenu 1: \nMenu 2: \nMenu 3: \nNow, let's look at the operating panel of the \ncoffee machine in the image below, which \nbutton should I use for a 6 OZ coffee: left, \nright, top or bottom button?\nMM1-30B-Chat (Ours): To brew a 6 OZ \ncoffee using the coffee machine in the image, \nyou should use the left button. This is \nbecause the buttons on the machine \ncorrespond to the menu icons, and the left \nbutton corresponds to the 6 OZ menu icon. \nThe other buttons correspond to different cup \nsizes, but the left button is the one you need \nto press to brew a 6 OZ coffee.\nFigure: Following [Yang 2023], we use MM1 on task \noriented scenarios such as operating machines and \nnavigating. Images are from [Yang 2023].\nMM1-30B-Chat (Ours): The image \ncaptures a moment from the perspective of a \ncar's dashboard camera. The car is traveling \non a two-lane road, divided by a yellow line, \nsuggesting a rural or less populated area. The \nroad is flanked by trees on both sides, their \ngreen leaves contrasting with the gray, cloudy \nsky above. A speed limit sign displaying \"35\" \nis visible on the right side of the road, \nindicating the maximum speed allowed for \nvehicles. The overall scene suggests a calm \nand serene drive through a lightly wooded \narea.\nUser: Describe the image\nFig. 14:Following [122], we tested MM1 on task-oriented scenarios such as operating\nmachines and navigating. Images and prompts are from [122].",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training37\nUser: Suppose you are a teacher, please use \nthe figure to explain the distinction between \nevaporation and evapotranspiration.\nMM1-30B-Chat (Ours): Evaporation and \nevapotranspiration are two important \nprocesses in the water cycle that are often \nconfused. The image provides a clear \ndistinction between the two. Evaporation is \nthe process where water changes from a liquid \nstate to a gaseous state. This is represented in \nthe image by the sun's heat causing water in \nthe lake and ocean to turn into water vapor \nand rise into the atmosphere. \nOn the other hand, evapotranspiration is the \ncombination of evaporation and plant \ntranspiration. Transpiration is the process \nwhere water absorbed by plants is released \ninto the atmosphere through the plant's \nleaves. In the image, evapotranspiration is \nrepresented by the water absorbed by the \ntrees and grass being released back into the \natmosphere. \nIn summary, evaporation is the process of \nwater turning into vapor and rising into the \natmosphere, while evapotranspiration is the \ncombination of evaporation and plant \ntranspiration, representing the water \nabsorbed by plants and released back into the \natmosphere.\nMM1-30B-Chat (Ours): The image presents a flowchart \nthat outlines the process of how a team member suggests a \nproject. The flowchart begins with a green rectangle that \ncontains the text \"A team member suggests a project\". \nFrom this starting point, the process flows through a series \nof diamond-shaped decision points and rectangular action \npoints, each containing specific text that describes the next \nstep in the process. \nThe process continues with the team discussing the idea in a \nmeeting, followed by a decision point where participants \ndecide if it's a good idea. If it's a good idea, the process \nmoves forward. If not, the process loops back to the point \nwhere the team member conducts more research. \nThe flowchart is color-coded with green rectangles \nrepresenting actions and yellow diamonds representing \ndecisions. The lines connecting these shapes are black, \nindicating the sequence of the process. The flowchart is set \nagainst a white background, which makes the colored shapes \nstand out clearly. \nThe flowchart provides a clear and structured representation \nof the process, making it easy to understand the sequence of \nsteps involved in the project suggestion and decision-making \nprocess. It's a useful tool for visualizing complex processes \nand ensuring that all steps are considered and followed in \nthe correct order.\nUser: Explain the figure.\nFigure: MM1 is able to accurately extract \ninformation from graphics.  Prompts and \nimages from Figure 31 & 35 in [Yang 2023].\nFig. 15:Examples testing MM1 ability at extracting information from graphics. The\nright part shows an example of confusion, highlighted in red. Images and prompts are\nfrom [122].",
    "38B. McKinzie et al.\nUser: Which book has been written by Heller?\nMM1-30B-Chat (Ours): Catch-22\nFigure: Images from MMMU\nUser: And who wrote \"Of Mice & Men\"?\nMM1-30B-Chat (Ours): Steinbeck\nUser: I will show you several pictures and your task is to answer my question. \nA:                      B:                                       C: \nEstimate the temperature in Celsius for each image. Also provide reasons.\nMM1-30B-Chat (Ours):  \n•\nThe image with the umbrella on the beach: 25-30 degrees Celsius. The \numbrella is up, and people are at the beach, suggesting a warm, sunny day. \n•\nThe image with the plane flying over the snow: -20 to -30 degrees Celsius. \nThe plane is flying over a snow-covered area, and there are vehicles on the \nground, indicating a cold winter day. \n•\nThe image with the animals grazing in the field: 15-20 degrees Celsius. The \nanimals are grazing, which usually occurs in milder weather conditions. The \ngrass is green, further suggesting a temperate climate.\nFig. 16:Examples testing MM1 ability at reasoning across images and texts. Images\nare from COCO 2014 validation set [72] and MMMU dev set [128].",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training39\nUser:  \nImage A:\nMM1-30B-Chat (Ours): The first image shows a cake with the number 3 on \nit, and the second image shows a cake with the number 4 on it. The sum is 7.\nFigure: Images from COCO Val set\nImage B:\nFor each image state what's the number shown on the cake. Then \nreturn the sum.\nUser: What is the color of the balloon?\nThe color is ! R_E_D !✨\nThe color is # G_R_E_E_N #✨\nMM1-30B-Chat (Ours):  \nThe color is $ B_L_U_E $✨\nUser: I am showing you an image \nand you need to predict the letter \nor number shown when rotating \nthe image by 180 degrees.\nW\nMM1-30B-Chat (Ours): 9\nFig. 17:Examples testing MM1 ability to follow instructions across multiple images\n(top). Examples testing MM1 at following a style or a task across few shots (bottom).\nImages are from COCO 2014 validation set [72] and from the authors.",
    "40B. McKinzie et al.\nE  Author Contributions and Acknowledgements\nFirst Authors\nBrandon McKinzie: Led the multimodal pre-training design, implementation,\nand experimentation. Co-led design and implementation of multimodal evalua-\ntion infrastructure. Co-set overall direction. Co-wrote paper.\nZhe Gan: Led the SFT design, implementation, and experimentation. Co-\ninitiated effort. Co-set overall direction. Co-wrote paper.\nCore Authors\nJean-Philippe Fauconnier: Co-led design and implementation of multimodal\nevaluation infrastructure, assisted with model evaluations, model implementa-\ntion, multimodal pre-training and SFT experimentation.\nSam Dodge: Assisted with SFT experimentation, data mixtures, and multi-\nmodal evaluation infrastructure.\nBowen Zhang: Co-initiated effort, trained image encoders, assisted with in-\nfrastructure.\nPhilipp Dufter: Assisted with model implementation, evaluations, and exper-\nimentation.\nDhruti Shah: Implemented interleaved SFT, assisted with experimentation.\nXianzhi Du: Implemented and trained MoE for multimodal pre-training, SFT\nand underlying LLM.\nPeter Grasch: Advised and analyzed experiments, co-led design and imple-\nmentation of multimodal evaluation infrastructure, co-wrote paper.\nFurther Authors\nFutang Peng: Data processing and coordination.\nFloris Weers: Led text-based evaluation infrastructure and assisted with mul-\ntimodal evaluation infrastructure.\nHaotian Zhang: Implemented and experimented with MoE models.\nAnton Belyi, Karanjeet Singh, Doug Kang, Ankur Jain: Dataset creation and\nfiltering.\nHongyu Hè: Co-implemented VL connector, assisted with experimentation.\nMax Schwarzer: Implemented support for pre-training on packed image-text\npairs and packed interleaved documents.\nTom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan\nDu, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee: Designed, implemented, and\ntrained the underlying LLMs.\nZirui Wang, Ruoming Pang: Co-initiated effort, designed, implemented, and\ntrained the underlying LLMs.\nSenior Authors\nAlexander Toshev: Co-set overall direction, advised and analyzed experi-\nments, co-wrote paper.\nYinfei Yang: Co-initiated effort, co-set overall direction, advised and analyzed\nexperiments, co-wrote paper.",
    "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training41\nAcknowledgements\nThe authors would like to thank Vaishaal Shankar, Alaa El-Nouby, Yang\nZhao, Shuangfei Zhai, Russ Webb, Hadi Pouransari, Hong-You Chen, Yanghao\nLi, and David Mizrahi for valuable guidance, suggestions, and feedback; Chen\nChen and Qibin Chen for help on instruction tuning; Maitreyi Kunnavakkam\nVinjimur, Megan Maher Welsh, Bhavika Devnani, and David Koski for their\nassistance with input pipelines and data processing; Tom Nickson and Michael\nTu for assistance with the AXLearn infrastructure and early LLM work; Varsha\nMohan Paidi for assistance with dataset creation and filtering; Esteban Gonza-\nlez, Ian Clark, Jack Bailin, David Koski, and in particular Venkata Yerneni for\nassistance with the internal Weights & Biases instance for tracking experiments\nand model evaluations."
  ]
}