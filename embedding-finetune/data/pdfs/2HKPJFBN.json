{
  "key": "2HKPJFBN",
  "url": "https://arxiv.org/pdf/2311.08401v1",
  "metadata": {
    "title": "Fine-tuning Language Models for Factuality",
    "abstract": "  The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.\n",
    "published": "2023-11-14T18:59:15Z"
  },
  "text": [
    "Fine-tuning Language Models for Factuality\nKatherine Tian\n*\n†\n,Eric Mitchell\n*\n†\n,Huaxiu Yao\n†§\n,\nChristopher D. Manning\n†\n,Chelsea Finn\n†\n†\nStanford University\n§\nUNC Chapel Hill\n{kattian,eric.mitchell}@cs.stanford.edu\nAbstract\nThe fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use,\nsometimes even as a replacement for traditional search engines. Yet language models are prone to making\nconvincing but factually inaccurate claims, often referred to as ‘hallucinations.’ These errors can inadver-\ntently spread misinformation or harmfully perpetuate misconceptions.  Further, manual fact-checking of\nmodel responses is a time-consuming process, making human factuality labels expensive to acquire.  In\nthis work, we fine-tune language models to be more factual, without human labeling and targeting more\nopen-ended generation settings than past work.  We leverage two key recent innovations in NLP to do so.\nFirst, several recent works have proposed methods for judging the factuality of open-ended text by measur-\ning consistency with an external knowledge base or simply a large model’s confidence scores. Second, the\ndirect preference optimization algorithm enables straightforward fine-tuning of language models on objec-\ntives other than supervised imitation, using a preference ranking over possible model responses. We show\nthat learning from automatically generated factuality preference rankings, generated either through exist-\ning retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strate-\ngies targeted at factuality. At 7B scale,compared to Llama-2-chat, we observe 58% and 40% reduction\nin factual error ratewhen generating biographies and answering medical questions, respectively.\n1    Introduction\nRecent developments in training large language models (LLMs), particularly methods that learn from rank-\nings over responses such as reinforcement learning from human feedback (RLHF) (Christiano et al., 2017;\nZiegler et al., 2020; Ouyang et al., 2022), have enabled the development of powerful, engaging dialogue\nagents.   State-of-the-art LLMs are pre-trained on a vast amount of knowledge in large datasets (Touvron\net al., 2023a;b) and further fine-tuned to apply this knowledge to follow diverse instructions or complete\nmore specific tasks (Chung et al., 2022; Chen et al., 2021).  However, despite these large language models’\nexposure to diverse datasets, they are prone to confidently generating incorrect claims.  One recent study\nshows that GPT-3.5 (ChatGPT) produces false citations more often than not when asked to provide the au-\nthors of a given study (Agrawal et al., 2023).  Nonetheless, other research has demonstrated that in simple\nquestion-answering settings, large language modelsdoexhibit systematic markers of uncertainty that indi-\ncate their factually unreliable statements (Kadavath et al., 2022; Tian et al., 2023). These results suggest that\nlanguage models internally represent the limits of their knowledge, leading us to ask:Can language models\nbe fine-tuned to leverage this internal awareness, to avoid making untrue statements in the first place?\nA key source of difficulty in training factual models comes in specifying an objective that adequately cap-\ntures factuality. As an example, maximum likelihood, the most common objective for pre-training language\nmodels, does not always encourage factual predictions. Consider the question “Where was Yo-Yo Ma born?”\nA model that continues by near-deterministically producing the text “idk, probably Paris?” is nearly always\ncorrect, but receives extremely high loss if the pre-training data contains any other response to the question.\nOn the other hand, a model that hedges probability mass over many possible phrasings and many possible\nlocations (including incorrect ones,  like Antarctica) will likely receive much lower loss,  as any response\nobserved in the training data will be assigned at leastsomenon-trivial probability. Because the pre-training\nobjective may reward ‘smearing’ probability mass over many possible responses, language models may gen-\n*\nEqual contribution.\n1\narXiv:2311.08401v1  [cs.CL]  14 Nov 2023",
    "Figure 1:  Our approach aims to improve the factuality of language models, specifically focusing on long-\nform generation (e.g.  writing a biography).  We develop two different approaches for estimating factuality\nof a passage (center), each of which allows us to generate a preference dataset (right). We then fine-tune the\nlanguage model to optimize these factuality preferences (far right).\nerate incorrect statements if they underfit the training data or if asked questions that require knowledge not\ncontained in the pre-training data.\nIn principle, reinforcement learning-based objectives can avoid the failures of existing pre-training objectives\nthrough the appropriate choice of a reward function that penalizes factually incorrect statements.  However,\naccurately computing such a reward function can be expensive.   Obtaining human labels of factuality is\ntime-consuming and costly; Min et al. (2023) report that professional fact-checkers took approximately 9\nminutes to fact-check a single model-generated biography of a well-known individual; it cost about $2,000\nto annotate 505 biographies.\nIn light of these challenges, we leverage recent advances in estimating truthfulnesswithout human inter-\nvention: a)reference-basedautomated fact-checking methods that evaluate the extent to which an external\nknowledge base supports the claims in a piece of text (Min et al., 2023; Chern et al., 2023) and b)reference-\nfreetruthfulness evaluations that use a model’s own confidence as a proxy for truthfulness, inspired by Kuhn\net al. (2023).  Using these truthfulness measures and a dataset of unlabeled prompts (e.g., “Write a biogra-\nphy of Yo-Yo Ma.”), we sample pairs of completions from a pre-trained model and annotate them with a\npreference label denoting which has a lower rate of factual errors. Using the recently proposed Direct Pref-\nerence Optimization (Rafailov et al., 2023) algorithm, we can stably and efficiently learn from such data.\nUltimately, this pipeline enables us to fine-tune off-the-shelf language models to produce factual errors less\noften (with or without a reference knowledge base).  See Figure 1 for an overview of our factuality tuning\npipeline.\nOur primary contribution is a straightforward approach to optimizing language models for factuality in long-\nform text generation without human annotation.  We validate this approach on two benchmark datasets for\nevaluating factuality, targeted at generating biographies of popular figures and answering open-ended ques-\ntions about medical conditions.  We find that fine-tuning for factuality outperforms conventional RLHF and\nproduces complementary benefits to LLM decoding strategies that aim to increase factuality.  Further, we\nfind qualitative differences in the result of learning from preference pairs scored with reference-based and\nreference-free  truthfulness  estimation.   Overall,  we  find  that  learning  factuality  from  automatically  con-\nstructed preference pairs is a cost-effective way to increase model factuality without human intervention,\nreducing the error rate for claims generated by Llama models by over 50% for biographies and 20–30% for\nmedical questions.\n2    Preliminaries\nOur approach to fine-tuning directly for improved factuality uses the framework of reinforcement learning\nfrom preferences over candidate actions or responses.  In this section, we provide an overview of reinforce-\nment learning in the context of language models, as well as the specific algorithm we use for preference-based\nRL, direct preference optimization (Rafailov et al., 2023).\nFine-tuning language models with reinforcement learning.Reinforcement learning (RL) has proven to\nbe an effective approach to fine-tuning language models to extract complex, useful behaviors from their pre-\ntrained weights. In the context of RL, a language model policyπ\nθ\n(typically an autoregressive Transformer)\nproduces a conditional distributionπ\nθ\n(y|x)over responsesygiven an input queryx(bothxandyare\ntext sequences). The goal of reinforcement learning is to maximize the average reward of outputs generated\n2",
    "by the policy, where a reward functionr(x,y)assigns a scalar score to an input-output pair that determines\nits  desirability.   However,  past  works  have  observed  that  fine-tuning  language  models  with  an  objective\nof  unconstrained  reward  maximization  can  lead  tooveroptimization(Gao  et  al.,  2022),  that  is,  a  policy\nthat  achieves  high  reward  through  exploitation  of  the  idiosyncrasies  of  the  reward  function  that  are  not\naligned  with  the  intended  behavior.   The  most  commonly-used  objective  in  practice  therefore  combines\nreward maximization with a KL-divergence penalty between the language model and its initialization:\nmax\nπ\nθ\nE\nx∼D\np\n,y∼π\nθ\n(y|x)\n\u0002\nr(x,y)−βlog\nπ\nθ\n(y|x)\nπ\nref\n(y|x)\n\u0003\n(1)\nwhereD\np\nis some dataset of prompts,π\nref\nis the reference model, usually the result of performing some su-\npervised fine-tuning on a pre-trained model using demonstration data, andβis a coefficient that controls the\ntrade-off between reward and divergence (Ouyang et al., 2022; Bai et al., 2022; Stiennon et al., 2020). Opti-\nmizing this objective aligns the model with the reward function without deviating too far from the pre-trained\nreference model, reducing overoptimization.  In practice, the most common algorithm used to optimize this\nobjective for language models is proximal policy optimization (PPO; Schulman et al. (2017)), although some\nvariants exist (Ramamurthy et al., 2022).  However, these algorithms are quite complex to implement and\ntune (Zheng et al., 2023).\nRL from preferences with direct preference optimization (DPO).Most large language models fine-tuned\nwith Eq. 1 optimize a reward function that islearnedfrom a dataset of preference rankings over possible\nmodel outputs.  The DPO algorithm simplifies RL on language models for this special case (Rafailov et al.,\n2023), using a dataset of preference pairsD={x\n(i)\n,y\n(i)\nw\n,y\n(i)\nl\n}\nN\ni=1\nof promptsxand candidate responsesy\nw\nandy\nl\n(typically sampled fromπ\nref\n), wherey\nw\nis preferred overy\nl\n(denotedy\nw\n≻y\nl\n).  The probability of\nobserving a particular preference pair is assumed to follow a Bradley-Terry model (Bradley & Terry, 1952):\np(y\nw\n≻y\nl\n) =σ(r(x,y\nw\n)−r(x,y\nl\n))(2)\nwhereσis the sigmoid function andr(x,y)is an unobserved reward or scoring function.  Rafailov et al.\n(2023)  show  that  the  optimal  policyπ\n∗\nfor  the  problem  in  Eq.  1  can  be  found  by  optimizing  a  simple\nclassification loss computed directly on the preference data:\nL\nDPO\n(π\nθ\n;π\nref\n) =−E\n(x,y\nw\n,y\nl\n)∼D\n\u0014\nlogσ\n\u0012\nβlog\nπ\nθ\n(y\nw\n|x)\nπ\nref\n(y\nw\n|x)\n−βlog\nπ\nθ\n(y\nl\n|x)\nπ\nref\n(y\nl\n|x)\n\u0013\u0015\n(3)\nDPO enables learningπ\nθ\nfrom a fixed dataset of preferences, without fitting an explicit reward function or\nsampling from the policy in the loop of training (as is required in PPO). These advantages make DPO an\nattractive choice for fine-tuning language models for objectives other than imitation.  However, a challenge\nremains in constructing preference pairs that encourage greater factuality.\n3    Constructing Preferences Encouraging Factuality in Long-Form Text\nWhile  existing  preference  learning  algorithms  like  DPO  enable  efficient,  stable  learning  from  objectives\nother than maximum likelihood, they require data in the form of preferences over possible responses to a\nprompt. In this section, we propose two classes of approaches to generating such preferences without human\nlabeling effort. One class leverages existing methods to determine consistency with external reference texts\nas a measure of truthfulness; we propose another, which leverages calibrated model probabilities themselves\nas a proxy for truthfulness.  For both approaches, we are computing an estimatedtruthfulness scoreover\nthe claims in each generated response; the response with higher average truthfulness is taken as the preferred\nresponse.  See Figure 2 for an overview of both procedures for truthfulness scoring.  Note that truthfulness\nscoring is neededonly at training time; at test time, we can sample from the model in the normal manner.\n3.1    Reference-Based Truthfulness Estimation\nAn intuitive approach to estimating truthfulness is by estimating the consistency of a given piece of text with\na reliable reference text or knowledge base.  Several recent works have introduced such evaluation criteria;\nfor  example,  FactScore  (Min  et  al.,  2023)  uses  Wikipedia  as  reference  knowledge,  and  FacTool  (Chern\net al., 2023) uses Google Search Results.  These measures show high agreement with human judgments of\nfactuality, making them attractive sources of truth for preference data construction.  Due to the relatively\n3",
    "Figure  2:  We  estimate  the  factuality  of  a  long-form  generation  by  first  extracting  claims  (left)  and  then\nevaluating the truthfulness of each claim (right).  We consider two approaches for the latter:  areference-\nbased(top right) method that uses a fine-tuned Llama model to check if the fact is supported by Wikipedia\n(Min et al., 2023), and areference-free(bottom right) method that uses the model’s confidence in its most\nlikely answer to estimate its truthfulness.\nconsistent and high quality of Wikipedia articles, we elect to use FactScore as a representative method of\nreference-based truthfulness scoring.\nTo  evaluate  a  piece  of  text,  FactScore  first  extracts  a  list  of  the  atomic  claims  present  in  the  text  using\nGPT-3.5.\n1\nFor each atomic claim, a smaller, more efficient model such as a Llama-1-7B model (Touvron\net al., 2023a) that has been fine-tuned for fact-checking is then used to perform natural language inference\n(MacCartney & Manning, 2008) to determine if a claim is supported by the reference text.  The passage’s\ntruthfulness score is the fraction of the extracted atomic claims that are estimated to be supported by the\nreference text.\nWe note that reference-based truthfulness has the key limitation that it requires access to relevant,  high-\nquality reference texts against which to measure consistency. Such a requirement may limit applicability to\ndomains where ground truth documents are not known and accurate retrieval is difficult, such as in niche\ndomains or less-structured tasks.  Further, reference-based truthfulness estimation requires a reliable model\nto  determine  if  an  atomic  claim  is  supported  by  the  article.   In  light  of  these  limitations,  we  propose  a\nreference-freeapproach to estimating truthfulness of open-ended text, which avoids the need for retrieving\nexternal knowledge and checking consistency.\n3.2    Reference-Free Confidence-Based Truthfulness Estimation\nTo eliminate the need for external knowledge,  we leverage the fact that large language models are well-\ncalibrated (Kadavath et al., 2022; Tian et al., 2023); that is, a large language model’s confidence in a gen-\nerated answer is highly correlated with the probability that the answer is correct.  However, an open-ended\npassage might contain many facts, as well as particular stylistic choices that will have a significant impact\non the total probability a model assigns to the text. Therefore, we first perform a claim extraction step, as in\nreference-based methods, and compute the average confidence of the model over all extracted factual claims\nas the final truthfulness score.  The model used for computing confidence scores essentially takes the place\nof the reference text datastore.\nMore concretely, we first extract atomic claims from the text using GPT-3.5. We then use GPT-3.5 to convert\neach claim to a question testing knowledge of the particular fact.  Careful rephrasing is necessary to ensure\nthat the rephrased question is unambiguous; for example, the claim “Yo-Yo Ma plays the cello” should be\nconverted to the question “What instrument does Yo-Yo Ma play?”  rather than just “What does Yo-Yo Ma\nplay?”  as the latter question admits answers of the wrong type.  If we were to use the second prompt, a\nmodel might assign 50% of its probability on “cello” and 50% of its probability on “basketball.”  However,\n1\nhttps://platform.openai.com/docs/models/gpt-3-5\n4",
    "246\n10\n12\n14\n16\n18\nCorrect facts per response\nStrict improvement vs. SFT\nStrict improvement vs. SFT\nSFT\nITI\nDOLA\nRLHF\nFactTune-FS\nFactTune-MC\nBios\n468\n10\n11\n12\n13\nSFT\nITI\nDOLA\nRLHF\nFactTune-FS\nFactTune-MC\nMedicalQA\nIncorrect facts per response\nFigure 3: Factuality tuning (FactTune FS) is the only method that can produce astrict improvement (shaded\narea)in factuality over the SFT model for the biography generation and medical question-answering prob-\nlems.  That is, only factuality tuning with FactScore-generated preferences (FS) simultaneously increases\nthe number of correct statements and decreases the number of incorrect statements. Other approaches either\nincrease the number of correct statements at the cost of more incorrect statements, or reduce the number\nof incorrect statements at the cost of fewer correct statements.  Factuality tuning with model confidence-\ngenerated preferences (MC) lies just outside the strict improvement region.\nthe model’s low confidence is caused by the ambiguity of the question,notlow confidence in the instrument\nthat Yo-Yo Ma plays. We detail the prompts used for question generation in Appendix A.1.\nAfter each claim is converted to a minimally ambiguous question, we resample an answer 20 times, typically\nfrom the base model (e.g. Llama-1-7B) that is fine-tuned, to estimate the model’s uncertainty over the answer.\nWe use a few-shot prompt to encourage well-formed answers.  We bin these answers by equivalence, using\neither heuristic string matching of the responses or using GPT-3.5 to assess if the answers are semantically\nequivalent,  inspired  by  Kuhn  et  al.  (2023).   Our  heuristic  string  match  checks  whether  the  words  in  the\nanswer,  excluding stop words,  are the same.   We compare these choices in Section 4.4.   The fraction of\nresponses falling into the largest bin is the final truthfulness score used for the fact, essentially representing\nthe model’s confidence for this fact.\nIn Section 4.4 we also evaluate a simpler approach to extracting atomic facts, by simply using named entities\nidentified by a classifier (Honnibal & Montani, 2017). This approach avoids using an external large language\nmodel for claim extraction and question rephrasing; instead, we simply resample the tokens in the original\nnamed entity in the response 20 times,  binning them into buckets with equivalence checking,  and again\nmeasure the fraction of responses falling into the largest bin as the confidence score.\n3.3    Factuality Tuning: Putting it all Together\nGiven a choice of truthfulness estimator, we can now construct a preference dataset for factuality tuning a\ngiven language model from a set of unlabeled prompts.  First, we samplenmultiple candidate responses\nfor each prompt from the model with simple temperature sampling with temperature 1.0 (using few-shot\nprompting for models that have not been fine-tuned).  For each response, we then compute the truthfulness\nscore with the chosen estimator (reference-based or reference-free). Finally, for all\n\u0000\nn\n2\n\u0001\npairs of responses to\neach prompt, we simply choose the response with the higher truthfulness score as the preferred response. For\na set ofmprompts, we ultimately generatem\n\u0000\nn\n2\n\u0001\n−kpreference pairs, wherekis the number of pairs with\nequal scores.  Finally, we fine-tune the model using the DPO pipeline, using all model responses as targets\nfor the SFT stage.\n4    Experiments\nOur experiments evaluate the extent to which factuality can be learned through preference-based reinforce-\nment learning, using the fully automated preference-generation pipeline described in Section 3. We call the\nmodel fine-tuned with our reference-based metric FactTune-FS and the model fine-tuned with our model\n5",
    "Prompts\nper Entity\nResponses\nper Prompt\nExample prompt\nDatasetEntities [train, test]\nBiographies355 [296, 59]110Write a short biography of Mary Wollstonecraft.\nMedical QA200 [150, 50]66What are the common symptoms of a stroke?\nTable 1:Left.Dataset statistics.  In biographies, entities are individuals, and in MedicalQA, entities are\nmedical conditions.  We include 6 questions for each entity in MedicalQA, and we adjust the number of\nresponses per prompt to keep the total number of pairs between thae two datasets roughly similar.Right.An\nexample prompt from each dataset.\nconfidence-based score, which is completely reference-free, FactTune-MC. For all of our experiments, sam-\nples for model confidence are taken from Llama-1-7b.\nDatasets.We conduct our experiments on two tasks: generating biographies and medical question-answering.\nFor biographies, we generated a dataset consisting of 355 diverse well-known individuals (296 train, 59 test)\nwith 10 short-paragraph biographies each. For medical question answering, we used a dataset of 200 diverse\ncommon medical conditions (150 train, 50 test) with 6 questions about each condition and 6 short-paragraph\nanswers  per  question.   The  prompts  were  generated  with  GPT-3.5,  and  the  answers  were  sampled  from\nLlama-1-7B using a few-shot prompt for each dataset. We found that our procedure consistently resulted in\nwell-formed and informative responses, albeit with possible factual errors. Because FactScore uses retrieval\nagainst a given Wikipedia article, we generate data based on individuals and medical conditions that have\nWikipedia pages. See Table 1 for the summary stats and examples from our datasets.\nBaselines.We compare factuality tuning with inference-time intervention (Li et al., 2023, ITI) and decoding\nby contrasting layers (Chuang et al., 2023, DOLA), applied to the SFT model for each task.  For ITI, we\nsupervise the training of the linear probes with FactScore labels:  we take batches of atomic facts extracted\nfrom  the  training  samples  and  bias  the  models’  activations  from  the  incorrect  to  correct  atomic  facts  to\ndetermine the direction of the intervention.   In the case of Llama-2,  we also compare against ‘standard’\nRLHF with human preference labels (Touvron et al., 2023b).\nEvaluation.To evaluate each generated response, we follow the FactScore procedure to extract the number\nof correct and incorrect facts.  Then, to check that the model responses are still relevant and helpful after\nactuality fine-tuning, we also use GPT-3.5 to determine whether each fact is relevant to the question or not\n(using  the  prompt  in  Appendix  A.1).   For  biographies,  we  observed  that  essentially  100%  of  facts  were\nrelevant to the individual, so we skip the relevance computation to save costs. For each dataset, we report the\nnumber of correct and relevant facts (# Correct), the number of inaccuracies (# Incorrect), and the proportion\nof correct relevant facts out of the total number of extracted facts (% Correct). Note that the total number of\nfacts may vary between generations. We validate our evaluation metrics in Sec. 4.5.\n4.1    Fine-Tuning for Factuality Across Domains\nIn this section, we apply our methodology for learning factuality to Llama-1-7b and Llama-2-7b in multiple\ndomains. We show the results in Table 2. Learning from reference-based factuality-scored pairs (FactTune-\nFS) consistently improves factual accuracy compared to RLHF modelsanddecoding-based factuality base-\nlines by at least 23% on biographies and 12% on medical question-answering.  FactTune-FS reduces the\nnumber of factual errors and maintains no more than a slight decrease,  if not increase,  in the amount of\ncorrect information generated. Factuality tuning from model-confidence scores (FactTune-MC) also reduces\nerror  rate  and  improves  the  factuality  of  RLHF  models  on  both  datasets,  without  any  external  reference\ninformation.\nWhile our quantitative metrics demonstrate a clear increase in factual accuracy,  we also wish to investi-\ngate how model generations change qualitatively after factuality fine-tuning.  We observe that FactTune-FS\nand FactTune-MC samples tend to have more objective and direct sentences and less of a conversational or\nstory-telling style compared to the SFT model (for example, see Appendix Table 8).  The FactTune-FS and\nFactTune-MC samples have simpler sentences and lack casual phrases.  As another example (in Appendix\nTable 9) the FactTune-FS and FactTune-MC biographies describe accurate facts, but not in a natural chrono-\nlogical order. GPT-4 rates FactTune-FS as less conversational in tone than the SFT model for 77.5% (n=40)\nof Llama-1 questions and 65.6% (n=32) of Llama-2 samples.\n6",
    "BiographiesMedical QA\nBase ModelMethod# Correct# Incorrect% Correct# Correct# Incorrect% Correct\nLlama-1\nITI11.676.690.6698.915.160.633\nDOLA11.753.840.7548.035.910.576\nSFT13.7812.160.56810.756.310.630\nFactTune-FS (ours)14.813.750.81210.884.500.707\nFactTune-MC (ours)10.592.940.78312.316.880.642\nLlama-2\nITI18.505.750.76010.974.060.730\nDOLA13.415.840.6969.724.380.690\nChat19.036.410.7489.635.500.636\nSFT12.195.190.70111.756.750.635\nFactTune-FS (ours)17.062.000.89512.533.470.783\nFactTune-MC (ours)11.312.060.84611.414.800.704\nTable 2: Factuality tuning from reference-based factuality-scored pairs (FactTune-FS) consistently improves\nfactual  accuracy  compared  to  RLHF  models  and  decoding-based  factuality  baselines,  often  reducing  the\nnumber of factual errorsandincreasing the number of correct facts generated. Factuality tuning from model-\nconfidence scored pairs (FactTune-MC) also outperforms RLHF models and provides a strong reference-free\nalternate method for improving factuality and reducing error.\nBiographiesMedical QA\nBase ModelMethod# Correct# Incorrect% Correct\n# Correct\n# Incorrect\n% Correct\nLlama-2-Chat\n–19.036.410.7489.635.500.636\nDOLA21.005.190.80211.508.250.582\nFactTune-FS (ours)19.944.060.8319.385.250.682\nFactTune-MC (ours)20.914.840.81210.345.690.645\nTable 3:  Factuality tuning a dialogue model (Llama-2-Chat) with both FactScore and model confidence-\nbased truthfulness estimation (FactTune-FS, FactTune-MC) further improves its factual accuracy more than\na baseline method for factuality, DOLA.\n4.2    Fine-tuning Chat Models for Factuality\nMost widely used practical chatbots today are LMs trained with RLHF to follow diverse instructions in a\nway that is helpful to users.  In this section, we investigate the ability of our human-free factuality tuning\nmethod to improve the factuality of RLHF chat models.  Using Llama-2-7b-Chat, we find that fine-tuning\nan RLHF LM with both factuality and semantic entropy-based rewards can further improve its factuality\nwithout significantly decreasing the total number of facts, as shown in Table 3.  In other words,factuality\ntuning can be composed with RLHF to further improve the factuality of chat models.\n4.3    Complementary  Benefits  of  Factuality  Tuning  and  Decoding-Time  Factuality\nInterventions\nBesides fine-tuning for factuality, multiple existing works aim to improve LLM factuality through inference\ntime interventions to either the decoding process or the model parameters themselves. We explore the possi-\nbility of applying both of these types of methods together, i.e., using factuality-boosting decoding methods\non a model fine-tuned with our factuality tuning procedure.  In Table 4 we present the results of stacking\nboth approaches. We find that in most cases, DOLA can even further increase the accuracy of factuality fine-\ntuned models, with one exception for Llama-2 on the biography task. While not a comprehensive evaluation\nof combining methods for improving factuality, this result suggests that different approaches to enhancing\nfactuality may operate through complementary mechanisms.\n4.4    Impact of Design Decisions of Open-Ended Model Confidence Scoring\nWe consider the impacts of different choices for each step in computing a reference-free truthfulness score\nfor factuality tuning: fact extraction, confidence metric, and equivalence matching.\n7",
    "BiographiesMedical QA\nBase ModelMethod#Correct#Incorrect%Correct#Correct#Incorrect%Correct\nLlama-1\nFactTune-FS14.813.750.81210.884.500.707\nFactTune-FS + DOLA12.442.000.86411.473.750.767\nLlama-2\nFactTune-FS17.062.000.89512.533.470.783\nFactTune-FS + DOLA16.222.650.86512.563.440.794\nTable 4:  DOLA factuality decoding frequently composes with factuality fine-tuning, providing an increase\nin average correctness for the majority of combinations of model and dataset.\nBiographiesMedical QA\nFact Ext.EquivMetric#Correct#Incorrect%Correct#Correct#Incorrect%Correct\nEntityHeuristic\nEntropy13.86.310.6939.55.470.660\nMax Conf12.76.310.6939.54.780.673\nAtomicHeuristic\nEntropy10.62.880.81012.65.250.711\nMax Conf12.22.560.84010.25.190.673\nAtomicLLM\nEntropy11.03.220.77811.96.160.661\nMax Conf13.74.160.79411.76.000.668\nTable 5:  Model confidence-based preference construction with atomic question extraction during factual-\nity scoring performs similarly or better than with named entity extraction.  Surprisingly, using GPT-3.5 to\ndetermine equivalence between responses for semantic binning provides worse performance than a simple\nheuristic equivalence check. Note that we used 12 samples for all runs in this table.\nFirst, for the fact extraction step, we consider extracting questions about atomic facts identified by GPT-\n3.5 and sampling answers to each question, compared to extracting named entities for biographies, and noun\nchunks instead for Medical QA, usingnltkand re-sampling the extracted entity. Atomic question extraction\nhas the potential to be more comprehensive and precise, while named entity extraction is a less expensive\nproxy. In Table 5, we observe that atomic question extraction generally outperforms named entity extraction,\nalthough the difference in accuracy on the Medical QA dataset is small.\nNext, we study the choice of confidence metric. The results in Table 5 show that the choice of metric between\nmaximum confidence—the probability of the largest semantic sample bin—or the entropy over the semantic\nbins varies, but maximum confidence provides a noticeable improvement to biographies under the atomic\nquestion setting.\nFinally, when binning samples, we consider replacing the heuristic equivalence match with an equivalence\ncheck by GPT-3.5.  Surprisingly, using GPT-3.5 to determine equivalence between two samples produces\nworse-performingpreference pairs than using a simple string matching heuristic described in Section 3.2.\nWe suspect that this effect can potentially be caused by the following noise in GPT-3.5 equivalence checking:\nour heuristic equivalence match consistently underestimates semantic entropy across all examples,  while\nGPT-3.5 matching could either over or underestimate samples, resulting in noisier preference pairs, even if\nGPT-3.5 equivalence check scores are closer to the true semantic entropy on average.  GPT-4 could reduce\nthis error, but we do not provide results due to its cost.\n4.5    Validating Metrics for Factuality\nOur experiments primarily use counts of correct and incorrect facts computed by FactScore as the main\nevaluation metrics, as FactScore is automated and has been shown to exhibit good agreement with human\nfact-checkers (Min et al., 2023).  Nonetheless, we aim to verify that our results are not specific or overfit to\nthe FactScore criterion.  In this section, we provide an evaluation with (1) human evaluators hired through\nProlific.co\n2\nand (2) GPT-4.\nTo acquire human fact-checking results, we provide each human evaluator with a prompt, a generated re-\nsponse, and the title of the Wikipedia article they should use for fact-checking the response.  We ask the\n2\nHuman evaluators were compensated at an estimated hourly rate of $16-18.\n8",
    "DatasetEvaluationSFTFactTune-FS\nBiographiesHuman0.5820.846\nBiographiesFactScore0.6690.921\nMedQAHuman0.6620.838\nMedQAFactScore0.5340.806\nTable 6:  To validate that our models do not suf-\nfer  from  extreme  reward  overoptimization,  we\nconduct a human evaluation of the Llama-1-7B\nSFT  and  FactTune-FS  models  and  find  that  an\nincrease in FactScore also corresponds to a large\nincrease in human-annotated accuracy.\n0.40.50.60.70.80.91.0\nGPT4-counted # Errors (Scaled)\n2\n4\n6\n8\n10\nFactScore-counted # Errors\nFactuality Metric Agreement\n\n\n\n\n\n\nBio FactTune-FS\nBio FactTune-MC\nBio SFT\nMedQA FactTune-FS\nMedQA FactTune-MC\nMedQA SFT\nFigure 4:  Average FactScore error counts and GPT-4\nerror counts are highly correlated, suggesting that the\nresulting  models  do  not  suffer  from  extreme  reward\noveroptimization (Gao et al., 2022).  We plot the av-\nerage FactScore error count v.s.  the average GPT-4-\ncounted errors, scaling each dataset by the max GPT-\n4-error count in that dataset.\nhuman study participants to count the total number of facts and the number of incorrect facts in the response,\nand we divide these to obtain the human-rated accuracy. We provide the results in Table 6, where on average\nhumans rated our FactTune-FS model for both datasets significantly higher than the SFT model.\nFurther,  we  ask  GPT-4  to  evaluate  the  factuality  of  a  given  response  by  counting  the  number  of  factual\nerrors.  We observe that the GPT-4 model ratings and FactScore model ratings are highly correlated, and\nGPT-4 provides another evaluation metric that demonstrates that FactTune-FS significantly reduces average\nerror compared to the SFT models on both datasets (see Figure 4). Taken together, these results suggest that\nthe improvements in factuality are not the result of exploitation of our evaluation protocol.\n5    Related Work\nMany works have identified reducing factual errors (sometimes called ‘hallucinations’) as a key challenge for\nbuilding more reliable language models (Lewis et al., 2020; Kadavath et al., 2022; Zhang et al., 2023), even\nfor the most powerful language models (Bubeck et al., 2023). Other use of the term ‘hallucination’ refers to\nsummarization or translation system outputs not supported by the reference text (Maynez et al., 2020; Zhang\net al., 2020) even if they are factual (Cao et al., 2022).  Other work uses ‘hallucination’ to describe vision-\nlanguage models producing outputs not grounded in a visual input, e.g., a captioning system describing an\nobject that doesn’t exist in the image (Rohrbach et al., 2018).  In our case, we focus on statements that are\nfactually incorrect (or, inconsistent with a set of ‘authoritative’ texts, such as Wikipedia).\nSeveral works describe methods for detecting likely factual errors through sensitivity to perturbations in the\nprompt (Xu et al., 2023), high diversity of responses under resampling (Kadavath et al., 2022; Mündler et al.,\n2023; Kuhn et al., 2023), or inconsistency with external knowledge sources (Min et al., 2023; Chern et al.,\n2023), or properties of internal activations (Azaria & Mitchell, 2023).  Others go beyond detecting errors,\ncorrecting them after they have been generated (Peng et al., 2023; Gao et al., 2023; Dhuliawala et al., 2023).\nThese approaches typically rely on retrieving relevant data from a trusted knowledge base and use another\nLLM to verify consistency; however, retrieval-based methods face key challenges, namely reliable resolution\nof conflicts between parametric and retrieved knowledge (Longpre et al., 2022; Chen et al., 2022) as well as\nmaintaining improvements in factuality as model size increases (Mallen et al., 2023). Further, retrieval-based\nmethods add significant system complexity; the most common open-source consumer language models thus\nuse purely parametric models (Touvron et al., 2023a).  The FactScore variant of our approach uses retrieval\nonly during training, avoiding inference time complexity.\nMost similar to ours, some approaches attempt to prevent the generation of factual errors in the first place,\nusing prompting strategies (Si et al., 2023) or perturbing the internal representations of the model (Chuang\net al., 2023; Li et al., 2023). Unlike using a fixed heuristic for identifying an internal ‘factuality’ dimension,\nwe optimize directly for the end goal of generating factual statements, which we find shows a greater im-\n9",
    "provement in factuality. Finally, while most past work has focused on short-form NLG tasks like short-form\nquestion-answering (Kadavath et al., 2022), we explore ways to measure model confidence over factual in-\nformation in long-form, unstructured text and estimate truthfulness in a reference-free manner (i.e., don’t\nrequire any external knowledge base or annotations).\n6    Conclusion\nIn this paper, we show a practical, effective strategy to improve a language model’s ability to generate factual\ncontent, specifically focusing on long-form generations.  We develop and study two different approaches to\nestimating the truthfulness of long-form text and optimize for these criteria using preference-based learning.\nIn addition to existingreference-basedtruthfulness estimators that leverage external knowledge to establish\nthe truth of a particular statement, we introduce a novelreference-freeprocedure for estimating truthfulness\nthat uses the language model’s own uncertainty as an indication of factuality.  Our experiments show that\nfine-tuning a language model with either criterion reliably reduces the number of incorrect facts (i.e.  hal-\nlucinations) that the model generates.  Reference-free approaches like the one we have introduced provide\na particularly scalable self-supervision strategy to improve factuality, eliminating the need for a reference\ncorpus of ‘gold’ texts.\nThe experimental results suggest a number of avenues for future work. First, because of the limited research\nand thus the limited benchmarks on the factuality of long-form language model generations, we proposed\ntwo new tasks to benchmark our approach. These tasks are representative of but do not fully cover the range\nof scenarios where we would hope to improve factuality. Furthermore, our experiments provide evidence for\nimproving the factuality of dialogue models that are already fine-tuned with RLHF, but still leave open the\nquestion of how best to combine typical RLHF rewards and approaches with factuality rankings. Similarly,\nexploring additional ways to combine factuality tuning with existing methods for improving factuality, such\nas in our factuality tuning + DOLA experiment, may be a fruitful direction for future research.  Finally, we\nexplore only 7B models in this work.  Scaling up our factuality tuning recipe to larger models (and larger\npreference datasets) may reduce hallucinations even further.\nAcknowledgements\nEM gratefully acknowledges funding from a Knight-Hennessy graduate fellowship and a Stanford Acceler-\nator for Generative AI and Education grant. CF and CDM are CIFAR Fellows.\n10",
    "References\nAyush Agrawal, Mirac Suzgun, Lester Mackey, and Adam Tauman Kalai.  Do language models know when\nthey’re hallucinating references?, 2023. arXiv preprint arxiv:2305.18248. 1\nAmos Azaria and Tom Mitchell. The internal state of an LLM knows when its lying, 2023. 9\nYuntao  Bai,  Andy  Jones,  Kamal  Ndousse,  Amanda  Askell,  Anna  Chen,  Nova  DasSarma,  Dawn  Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom\nConerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott\nJohnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack\nClark,  Sam  McCandlish,  Chris  Olah,  Ben  Mann,  and  Jared  Kaplan.   Training  a  helpful  and  harmless\nassistant with reinforcement learning from human feedback, 2022. 3\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired\ncomparisons.Biometrika, 39(3/4):324–345, 1952. 3\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter\nLee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and\nYi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. 9\nMeng  Cao,  Yue  Dong,  and  Jackie  Cheung.    Hallucinated  but  factual!   inspecting  the  factuality  of  hal-\nlucinations  in  abstractive  summarization.   InProceedings  of  the  60th  Annual  Meeting  of  the  Associ-\nation  for  Computational  Linguistics  (Volume  1:  Long  Papers),  pp.  3340–3354,  Dublin,  Ireland,  May\n2022. Association for Computational Linguistics.  doi:  10.18653/v1/2022.acl-long.236.  URLhttps:\n//aclanthology.org/2022.acl-long.236. 9\nHung-Ting Chen,  Michael Zhang,  and Eunsol Choi.   Rich knowledge sources bring complex knowledge\nconflicts: Recalibrating models to reflect conflicting evidence. InProceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pp. 2292–2307, Abu Dhabi, United Arab Emirates,\nDecember  2022.  Association  for  Computational  Linguistics.   doi:  10.18653/v1/2022.emnlp-main.146.\nURLhttps://aclanthology.org/2022.emnlp-main.146. 9\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards,\nYura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,\nHeidy Khlaaf,  Girish Sastry,  Pamela Mishkin,  Brooke Chan,  Scott Gray,  Nick Ryder,  Mikhail Pavlov,\nAlethea Power,  Lukasz Kaiser,  Mohammad Bavarian,  Clemens Winter,  Philippe Tillet,  Felipe Petroski\nSuch,  David W. Cummings,  Matthias Plappert,  Fotios Chantzis,  Elizabeth Barnes,  Ariel Herbert-Voss,\nWilliam H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike,\nJoshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira\nMurati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and\nWojciech Zaremba.   Evaluating large language models trained on code.ArXiv, abs/2107.03374, 2021.\nURLhttps://api.semanticscholar.org/CorpusID:235755472. 1\nI-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham\nNeubig, and Pengfei Liu. Factool: Factuality detection in generative ai – a tool augmented framework for\nmulti-task and multi-domain scenarios, 2023. 2, 3, 9\nPaul  F  Christiano,  Jan  Leike,  Tom  Brown,  Miljan  Martic,  Shane  Legg,  and  Dario  Amodei.   Deep  rein-\nforcement  learning  from  human  preferences.   In  I.  Guyon,  U.  Von  Luxburg,  S.  Bengio,  H.  Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett (eds.),Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017.  URLhttps://proceedings.neurips.cc/paper_\nfiles/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf. 1\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding\nby contrasting layers improves factuality in large language models, 2023. 6, 9\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason\nWei. Scaling instruction-finetuned language models, 2022. 1\n11",
    "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason\nWeston. Chain-of-verification reduces hallucination in large language models, 2023. 9\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. 3, 9\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y.\nZhao,  Ni Lao,  Hongrae  Lee,  Da-Cheng Juan,  and  Kelvin Guu.   Rarr:  Researching  and revising  what\nlanguage models say, using language models, 2023. 9\nMatthew Honnibal and Ines Montani.  spaCy 2:  Natural language understanding with Bloom embeddings,\nconvolutional neural networks and incremental parsing. To appear, 2017. 5\nSaurav  Kadavath,  Tom  Conerly,  Amanda  Askell,  Tom  Henighan,  Dawn  Drain,  Ethan  Perez,  Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Gan-\nguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse,\nCatherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam\nMcCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022. URL\nhttp://arxiv.org/abs/2207.05221. Arxiv arxiv:2207.05221. 1, 4, 9, 10\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar.  Semantic uncertainty:  Linguistic invariances for uncer-\ntainty estimation in natural language generation, 2023. 2, 5, 9\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nKüttler,  Mike  Lewis,  Wen-tau  Yih,  Tim  Rocktäschel,  Sebastian  Riedel,  and  Douwe  Kiela.   Retrieval-\naugmented generation for knowledge-intensive NLP tasks.   In H. Larochelle,  M. Ranzato,  R. Hadsell,\nM.F.  Balcan,  and  H.  Lin  (eds.),Advances  in  Neural  Information  Processing  Systems,  volume  33,  pp.\n9459–9474. Curran Associates, Inc., 2020.  URLhttps://proceedings.neurips.cc/paper_\nfiles/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf. 9\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.  Inference-time inter-\nvention: Eliciting truthful answers from a language model, 2023. 6, 9\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-\nbased knowledge conflicts in question answering, 2022. 9\nBill MacCartney and Christopher D. Manning.   Modeling semantic containment and exclusion in natural\nlanguage inference.  InProceedings of the 22nd International Conference on Computational Linguistics\n(Coling 2008), pp. 521–528, Manchester, UK, August 2008. Coling 2008 Organizing Committee.  URL\nhttp://www.aclweb.org/anthology/C08-1066. 4\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.  When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories.  In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802–9822, Toronto, Canada, July 2023. Association for Computational Linguistics.   doi:\n10.18653/v1/2023.acl-long.546. URLhttps://aclanthology.org/2023.acl-long.546. 9\nJoshua Maynez,  Shashi Narayan,  Bernd Bohnet,  and Ryan McDonald.   On faithfulness and factuality in\nabstractive summarization.  InProceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics, pp. 1906–1919, Online, July 2020. Association for Computational Linguistics.   doi:\n10.18653/v1/2020.acl-main.173. URLhttps://aclanthology.org/2020.acl-main.173. 9\nSewon Min,  Kalpesh Krishna,  Xinxi Lyu,  Mike Lewis,  Wen tau Yih,  Pang Wei Koh,  Mohit Iyyer,  Luke\nZettlemoyer, and Hannaneh Hajishirzi.  Factscore: Fine-grained atomic evaluation of factual precision in\nlong form text generation, 2023. 2, 3, 4, 8, 9\nNiels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinations of large\nlanguage models: Evaluation, detection and mitigation, 2023. 9\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.  Training\nlanguage models to follow instructions with human feedback, 2022. 1, 3\n12",
    "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden,\nZhou Yu, Weizhu Chen, and Jianfeng Gao.  Check your facts and try again:  Improving large language\nmodels with external knowledge and automated feedback, 2023. 9\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model, 2023. 2, 3\nRajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauck-\nhage, Hannaneh Hajishirzi, and Yejin Choi.  Is reinforcement learning (not) for natural language process-\ning?:  Benchmarks, baselines, and building blocks for natural language policy optimization.  2022.  URL\nhttps://arxiv.org/abs/2210.01241. 3\nAnna  Rohrbach,  Lisa  Anne  Hendricks,  Kaylee  Burns,  Trevor  Darrell,  and  Kate  Saenko.   Object  hallu-\ncination  in  image  captioning.   In  Ellen  Riloff,  David  Chiang,  Julia  Hockenmaier,  and  Jun’ichi  Tsujii\n(eds.),Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\n4035–4045, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1437. URLhttps://aclanthology.org/D18-1437. 9\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-\ntion algorithms, 2017. 3\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan\nWang. Prompting gpt-3 to be reliable, 2023. 9\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul Christiano.  Learning to summarize from human feedback.Neural Information Pro-\ncessing Systems, 18, 2020. 3\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and\nChristopher D. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from\nlanguage models fine-tuned with human feedback, 2023. 1, 4\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a. 1, 4, 9\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-\nrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan,  Marcin  Kardas,  Viktor  Kerkez,  Madian  Khabsa,  Isabel  Kloumann,  Artem  Korenev,  Punit  Singh\nKoura,  Marie-Anne  Lachaux,  Thibaut  Lavril,  Jenya  Lee,  Diana  Liskovich,  Yinghai  Lu,  Yuning  Mao,\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subra-\nmanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic,  Sergey Edunov,  and Thomas Scialom.   Llama 2:  Open foundation and fine-tuned chat\nmodels, 2023b. 1, 6\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat. Understanding\nand Detecting Hallucinations in Neural Machine Translation via Model Introspection.Transactions of the\nAssociation for Computational Linguistics, 11:546–564, 06 2023.  ISSN 2307-387X.  doi:  10.1162/tacl_\na_00563. URLhttps://doi.org/10.1162/tacl_a_00563. 9\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations\ncan snowball.arXiv preprint arXiv:2305.13534, 2023. 9\nYuhao Zhang,  Derek Merck,  Emily Tsai,  Christopher D Manning,  and Curtis Langlotz.   Optimizing the\nfactual correctness of a summary: A study of summarizing radiology reports.  InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics (ACL), 2020. URLhttps://arxiv.\norg/pdf/1911.02541.pdf. 9\n13",
    "Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,\nYuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,\nZhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui,\nQi Zhang, Xipeng Qiu, and Xuanjing Huang.   Secrets of RLHF in large language models part I: PPO,\n2023. 3\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. 1\nA    Appendix\nA.1    Prompts\nTable 7 contains the prompts used with GPT-3.5 to convert statements into questions for model confidence-\nbased truthfulness estimation.\nA.2    Sample Model Generations\nSee Tables 8 and 9 for samples generated by several different models. After factuality tuning, the model does\nproduce somewhat terser responses.\n14",
    "Biography\nAtomic Fact\nto Question\nI will provide a statement containing one atomic fact related to Hillary Clinton or people around\nher.  Please rephrase the following statement into a specific question testing knowledge of the\nkey fact in the statement. For example:\nStatement: Hillary Clinton was born in 1947.\nQuestion: In what year was Hillary Clinton born?\nStatement: Hillary attended the Wellesley College.\nQuestion: What college did Hillary Clinton attend?\nStatement: She married Bill Clinton.\nQuestion: Who did Hillary Clinton marry?\nI will provide a statement containing one atomic fact related to LeBron James or people around\nhim. Please rephrase the following statement into a specific question that testing knowledge of\nthe key fact in the statement. For example:\nStatement: LeBron James is a professional basketball player.\nQuestion: What is LeBron James’ profession?\nStatement: He is one of the best in the NBA.\nQuestion: Where does LeBron James rank among NBA players?\nStatement: James was born in Akron.\nQuestion: In what city was LeBron James born?\nI  will  provide  a  statement  containing  one  atomic  fact  related  to  [NAME]  or  people  around\n[HIM/HER]. Please rephrase the following statement into a specific question testing knowledge\nof the key fact in the statement. For example:\nStatement: [STATEMENT]\nQuestion:\nMedicalQA\nAtomic Fact\nto Question\nI will provide a statement containing one atomic fact about the medical condition menopause.\nPlease rephrase the following statement into a specific question testing knowledge of the key\nfact in the statement. For example:\nStatement: Menopause is a time in a woman’s life.\nQuestion: Menopause is a time in whose life?\nStatement: Menopause is the time when a woman no longer has menstrual periods.\nQuestion: Menopause is the time when a woman no longer has what?\nStatement: There is a decline in the ovarian hormone estrogen.\nQuestion: During menopause there is a decline in what?\nI will provide a statement containing one atomic fact about the medical condition breast cancer.\nPlease rephrase the following statement into a specific question testing knowledge of the key\nfact in the statement. For example:\nStatement:  The signs and symptoms include a lump or thickening in or near the breast or un-\nderarm.\nQuestion: Do the signs and symptoms of breast cancer include a lump or thickening in or near\nthe breast or underarm?\nStatement: The signs and symptoms include a change in the size or shape of the breast.\nQuestion: Do the signs and symptoms of breast cancer include a change in the size or shape of\nthe breast?\nI will provide a statement containing one atomic fact about the medical condition varicose veins.\nPlease rephrase the following statement into a specific question testing knowledge of the key\nfact in the statement. For example:\nStatement: Varicose veins occur when the veins under the skin become enlarged.\nQuestion: Varicose veins occur when what happens to the veins under the skin?\nStatement: Veins in the legs lose their ability to efficiently circulate blood back to the heart.\nQuestion: Varicose veins in the legs lose their ability to efficiently do what?\nI will provide a statement containing one atomic fact about the medical condition [CONDI-\nTION]. Please rephrase the following statement into a specific question testing knowledge of\nthe key fact in the statement. For example:\nStatement: [STATEMENT]\nQuestion:\nTable 7: GPT-3.5 prompts used to convert atomic facts into questions.\n15",
    "SFTGreta Gerwig is an American actress and filmmaker. She was born in and raised in Sacramento,\nCalifornia.  Gerwig became interested in the film industry in 2002, when she attended the San\nFrancisco Art Institute. She got her start in 2006, when she starred in the film ‘The Off Hours’.\nHer other films include Frances Ha,  Mistress America,  and 20th Century Women.   In 2018,\nGerwig directed Lady Bird, and received wide acclaim, a Golden Globe and Tony nomination\nfor directing, and Oscar and BAFTA nominations for the film.\nFactTune-FSGreta Gerwig is an American actress and filmmaker.  Born in Sacramento, California in 1983,\nGerwig started acting at a young age.  She is best known for starring in films such as ‘Frances\nHa’ and ‘Mistress America.’ Gerwig was nominated for an Oscar for Best Actress for her per-\nformance in ‘Lady Bird.’ Gerwig has written and directed a number of acclaimed independent\nfilms, such as ‘Frances Ha’ and ‘Mistress America.’\nFactTune-MCGreta Gerwig is an American actress and filmmaker.   She was born in 1983 in Sacramento,\nCalifornia, and raised in Stanford, California and New York City. Gerwig has appeared in such\nmovies as Greenberg, Frances Ha, and Jackie. She also portrayed the title role in the 2013 film\nof Jackie Kennedy.  Her directorial debut is the 2012 movie, Lady Bird, which was nominated\nfor the Academy Award for Best Picture.\nTable  8:   Samples  for  the  prompt  “Write  a  biography  of  Greta  Gerwig\"  under  the  SFT,  FactTune-FS,\nFactTune-MC Llama-1-7b models.\nSFTMukesh Ambani was born in 1957, in Mumbai, India, to billionaire industrialist Dhirubhai Am-\nbani and Kokilaben Ambani. Mukesh Ambani attended Stanford University, where he received\na Bachelor of Science degree in Chemical Engineering.  He is the chairman and managing di-\nrector of Reliance Industries Limited (RIL), a company owned by the Ambani family of India.\nIn 2002, Ambani became the chairman of Reliance Natural Resources Limited, a company pro-\nducing oil and gas. Reliance has been one of the largest retailers in India since 2006, with 380\nReliance Fresh outlets and 1,250 stores under Reliance Retail.  Reliance is the world’s largest\npolyester fiber manufacturer, operating 98 integrated manufacturing sites worldwide.\nFactTune-FSMukesh Ambani is an Indian business magnate and the chairman, managing director and largest\nshareholder of Reliance Industries Limited (RIL), a Fortune Global 500 company and India’s\nmost valuable company by market value.  Ambani is the richest man in India, with a net worth\nof US$27.4 billion, according to Forbes. Mukesh Ambani was born on April 19, 1957 in Aden,\nYemen.  He is the eldest son of Indian business magnate Dhirubhai Ambani.  Mukesh Ambani\ncurrently lives in Mumbai, India.\nFactTune-MCMukesh Ambani is an Indian businessman and the CEO of Reliance Industries. He is the richest\nperson in India and the 19th richest person in the world.  Ambani was born in 1957 in Aden,\nYemen.  He moved to India with his family in 1958.  Ambani studied chemical engineering in\nthe U.S and worked in the oil company, Reliance, which is the largest Indian company.\nTable 9:  Samples for the prompt “Write a biography of Mukesh Ambani\" under the SFT, FactTune-FS,\nFactTune-MC Llama-1-7b models.\n16"
  ]
}