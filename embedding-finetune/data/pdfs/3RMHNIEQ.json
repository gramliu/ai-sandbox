{
  "key": "3RMHNIEQ",
  "url": "http://arxiv.org/pdf/2208.01626",
  "metadata": {
    "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
    "abstract": "  Recent large-scale text-driven synthesis models have attracted much attention\nthanks to their remarkable capabilities of generating highly diverse images\nthat follow given text prompts. Such text-based synthesis methods are\nparticularly appealing to humans who are used to verbally describe their\nintent. Therefore, it is only natural to extend the text-driven image synthesis\nto text-driven image editing. Editing is challenging for these generative\nmodels, since an innate property of an editing technique is to preserve most of\nthe original image, while in the text-based models, even a small modification\nof the text prompt often leads to a completely different outcome.\nState-of-the-art methods mitigate this by requiring the users to provide a\nspatial mask to localize the edit, hence, ignoring the original structure and\ncontent within the masked region. In this paper, we pursue an intuitive\nprompt-to-prompt editing framework, where the edits are controlled by text\nonly. To this end, we analyze a text-conditioned model in depth and observe\nthat the cross-attention layers are the key to controlling the relation between\nthe spatial layout of the image to each word in the prompt. With this\nobservation, we present several applications which monitor the image synthesis\nby editing the textual prompt only. This includes localized editing by\nreplacing a word, global editing by adding a specification, and even delicately\ncontrolling the extent to which a word is reflected in the image. We present\nour results over diverse images and prompts, demonstrating high-quality\nsynthesis and fidelity to the edited prompts.\n",
    "published": "2022-08-02T17:55:41Z"
  },
  "text": [
    "Prompt-to-Prompt Image Editing\nwith Cross Attention Control\nAmir Hertz\n∗1,2\n, Ron Mokady\n∗1,2\n, Jay Tenenbaum\n1\n, Kfir Aberman\n1\n, Yael Pritch\n1\n, and Daniel Cohen-Or\n∗1,2\n1\nGoogle Research\n2\nThe Blavatnik School of Computer Science, Tel Aviv University\nAbstract\nRecent  large-scale  text-driven  synthesis  models  have  attracted  much  attention  thanks  to\ntheir  remarkable  capabilities  of  generating  highly  diverse  images  that  follow  given  text\nprompts. Such text-based synthesis methods are particularly appealing to humans who are\nused to verbally describe their intent. Therefore, it is only natural to extend the text-driven\nimage synthesis to text-driven image editing.  Editing is challenging for these generative\nmodels, since an innate property of an editing technique is to preserve most of the original\nimage, while in the text-based models, even a small modification of the text prompt often\nleads to a completely different outcome. State-of-the-art methods mitigate this by requiring\nthe users to provide a spatial mask to localize the edit, hence, ignoring the original structure\nand content within the masked region.   In this paper,  we pursue an intuitiveprompt-to-\npromptediting framework,  where the edits are controlled by text only.   To this end,  we\nanalyze a text-conditioned model in depth and observe that the cross-attention layers are\nthe key to controlling the relation between the spatial layout of the image to each word in\nthe prompt. With this observation, we present several applications which monitor the image\nsynthesis by editing the textual prompt only.  This includes localized editing by replacing\na word, global editing by adding a specification, and even delicately controlling the extent\nto which a word is reflected in the image.  We present our results over diverse images and\nprompts, demonstrating high-quality synthesis and fidelity to the edited prompts.\n1  Introduction\nRecently, large-scale language-image (LLI) models, such as Imagen [38], DALL·E 2 [33] and Parti [48], have\nshown phenomenal generative semantic and compositional power, and gained unprecedented attention from\nthe research community and the public eye. These LLI models are trained on extremely large language-image\ndatasets and use state-of-the-art image generative models including auto-regressive and diffusion models.\nHowever, these models do not provide simple editing means, and generally lack control over specific seman-\ntic regions of a given image.  In particular,  even the slightest change in the textual prompt may lead to a\ncompletely different output image.\nTo circumvent this, LLI-based methods [28, 4, 33] require the user to explicitly mask a part of the image to\nbe inpainted, and drive the edited image to change in the masked area only, while matching the background\nof the original image. This approach has provided appealing results, however, the masking procedure is cum-\nbersome, hampering quick and intuitive text-driven editing.  Moreover, masking the image content removes\nimportant structural information,  which is completely ignored in the inpainting process.  Therefore,  some\nediting capabilities are out of the inpainting scope, such as modifying the texture of a specific object.\nIn this paper, we introduce an intuitive and powerfultextual editingmethod to semantically edit images in\npre-trained text-conditioned diffusion models viaPrompt-to-Promptmanipulations.  To do so, we dive deep\ninto the cross-attention layers and explore their semantic strength as a handle to control the generated image.\n*\nPerformed this work while working at Google.\narXiv:2208.01626v1  [cs.CV]  2 Aug 2022",
    "“The boulevards are crowded today.”“Photo of a cat riding on a bicycle.”\n“a cake with decorations.”\na castle next to a river.”\nFigure 1: Our method provides variety ofPrompt-to-Promptediting capabilities. The user can tune the level\nof influence of an adjective word (top-left), replace items in the image (top-right), specify a style for an image\n(bottom-left), or make further refinements over the generated image (bottom-right). The manipulations are in-\nfiltrated through the cross-attention mechanism of the diffusion model without the need for any specifications\nover the image pixel space.\nSpecifically,  we consider the internalcross-attention maps,  which are high-dimensional tensors that bind\npixels and tokens extracted from the prompt text.  We find that these maps contain rich semantic relations\nwhich critically affect the generated image.\nOur key idea is that we can edit images by injecting the cross-attention maps during the diffusion process,\ncontrolling which pixels attend to which tokens of the prompt text during which diffusion steps.  To apply\nour method to various creative editing applications, we show several methods to control the cross-attention\nmaps through a simple and semantic interface (see fig. 1). The first is to change a single token’s value in the\nprompt (e.g., “dog” to “cat”), while fixing the cross-attention maps, to preserve the scene composition.  The\nsecond is to globally edit an image, e.g., change the style, by adding new words to the prompt and freezing the\nattention on previous tokens, while allowing new attention to flow to the new tokens. The third is to amplify\nor attenuate the semantic effect of a word in the generated image.\nOur approach constitutes an intuitive image editing interface through editing only the textual prompt, there-\nfore calledPrompt-to-Prompt.  This method enables various editing tasks, which are challenging otherwise,\nand does not requires model training, fine-tuning, extra data, or optimization.  Throughout our analysis, we\ndiscover even more control over the generation process, recognizing a trade-off between the fidelity to the\nedited prompt and the source image.  We even demonstrate that our method can be applied to real images\nby using an existing inversion process. Our experiments and numerous results show that our method enables\nseamless editing in an intuitive text-based manner over extremely diverse images.\n2  Related work\nImage editing is one of the most fundamental tasks in computer graphics, encompassing the process of mod-\nifying an input image through the use of an auxiliary input,  such as a label,  scribble,  mask,  or reference\nimage.  A specifically intuitive way to edit an image is through textual prompts provided by the user.  Re-\ncently, text-driven image manipulation has achieved significant progress using GANs  [15, 8, 19–21], which\nare known for their high-quality generation, in tandem with CLIP [32], which consists of a semantically rich\njoint image-text representation, trained over millions of text-image pairs. Seminal works [29, 14, 46, 2] which\ncombined these components were revolutionary, since they did not require extra manual labor, and produced\n2",
    "“lemon cake.”“cheese cake.”“apple cake.”“chocolate cake.”\n“pistachio cake.”“jello cake.”“matcha cake.”\n“pumpkin cake.”\n“pepperoni cake.”“fish cake.”“rice cake.”\n“pasta cake.”\n“beet cake.”\n“lego cake.”\n“brick cake.”\n“monster cake.”\n“lego cake.”\n“brick cake.”\n“monster cake.”\n“lemon cake.”“cheese cake.”“apple cake.”“chocolate cake.”\n“pistachio cake.”“jello cake.”“matcha cake.”\n“pumpkin cake.”\n“pepperoni cake.”“fish cake.”“rice cake.”\n“pasta cake.”\n“beet cake.”\nFixed attention maps and random seed\nFixed random seed \nFigure 2: Content modification through attention injection.  We start from an original image generated from\nthe prompt ”lemon cake”, and modify the text prompt to a variety of other cakes. On the top rows, we inject\nthe attention weights of the original image during the diffusion process. On the bottom, we only use the same\nrandom seeds as the original image, without injecting the attention weights. The latter leads to a completely\nnew structure that is hardly related to the original image.\nhighly realistic manipulations using text only. Bau et al. [7] further demonstrated how to use masks provided\nby the user, to localize the text-based editing and restrict the change to a specific spatial region.  However,\nwhile GAN-based image editing approaches succeed on highly-curated datasets [27], e.g., human faces, they\nstruggle over large and diverse datasets.\nTo obtain more expressive generation capabilities, Crowson et al. [9] use VQ-GAN [12], trained over diverse\ndata, as a backbone. Other works [5, 22] exploit the recent Diffusion models [17, 39, 41, 17, 40, 36], which\nachieve state-of-the-art generation quality over highly diverse datasets, often surpassing GANs [10].  Kim\net  al.  [22]  show  how  to  perform  global  changes,  whereas  Avrahami  et  al.  [5]  successfully  perform  local\nmanipulations using user-provided masks for guidance.\nWhile most works that require only text (i.e., no masks) are limited to global editing [9, 23], Bar-Tal et al. [6]\nproposed a text-based localized editing technique without using any mask, showing impressive results.  Yet,\ntheir techniques mainly allow changing textures, but not modifying complex structures, such as changing a\nbicycle to a car. Moreover, unlike our method, their approach requires training a network for each input.\nNumerous  works  [11,  16,  42,  25,  26,  30,  31,  34,  49,  9,  13,  36]  significantly  advanced  the  generation  of\nimages conditioned on plain text, known as text-to-image synthesis.  Several large-scale text-image models\nhave recently emerged, such as Imagen [38], DALL-E2 [33], and Parti [48], demonstrating unprecedented\nsemantic generation. However, these models do not provide control over a generated image, specifically using\ntext guidance only.  Changing a single word in the original prompt associated with the image often leads to\na completely different outcome. For instance, adding the adjective “white” to “dog” often changes the dog’s\nshape.  To overcome this, several works [28, 4] assume that the user provides a mask to restrict the area in\nwhich the changes are applied.\nUnlike previous works,  our method requires textual input only,  by using the spatial information from the\ninternal layers of the generative model itself. This offers the user a much more intuitive editing experience of\nmodifying local or global details by merely modifying the text prompt.\n3",
    "Text to Image Cross Attention\nCross Attenetion Control\nWord SwapAttention Re–weightingAdding a New Phrase\nXX\nφ(z\nt\n)\nPixel featuresPixel QueriesTokens Keys\n(from Prompt)\nAttention\nmaps\nTokens Values\n(from Prompt)\nOutput\nNew weighting\nQKM\nt\nV\n̂\nφ(z\nt\n)\nM\nt\nM\n∗\nt\nM\n∗\nt\n̂\nM\nt\n̂\nM\nt\nFigure 3:  Method overview.  Top:  visual and textual embedding are fused using cross-attention layers that\nproduce spatial attention maps for each textual token. Bottom: we control the spatial layout and geometry of\nthe generated image using the attention maps of a source image.  This enables various editing tasks through\nediting the textual prompt only.  When swapping a word in the prompt, we inject the source image maps\nM\nt\n, overriding the target image mapsM\n∗\nt\n, to preserve the spatial layout. Where in the case of adding a new\nphrase, we inject only the maps that correspond to the unchanged part of the prompt.  Amplify or attenuate\nthe semantic effect of a word achieved by re-weighting the corresponding attention map.\n3  Method\nLetIbe an image which was generated by a text-guided diffusion model [38] using the text promptPand\na random seeds.   Our goal is editing the input image guided only by the edited promptP\n∗\n,  resulting in\nan edited imageI\n∗\n.   For example,  consider an image generated from the prompt “my new bicycle”,  and\nassume that the user wants to edit the color of the bicycle, its material, or even replace it with a scooter while\npreserving the appearance and structure of the original image. An intuitive interface for the user is to directly\nchange the text prompt by further describing the appearance of the bikes, or replacing it with another word.\nAs opposed to previous works, we wish to avoid relying on any user-defined mask to assist or signify where\nthe edit should occur. A simple, but an unsuccessful attempt is to fix the internal randomness and regenerate\nusing the edited text prompt. Unfortunately, as fig. 2 shows, this results in a completely different image with\na different structure and composition.\nOur key observation is that the structure and appearances of the generated image depend not only on the ran-\ndom seed, but also on theinteractionbetween the pixels to the text embedding through the diffusion process.\nBy modifying the pixel-to-text interaction that occurs incross-attentionlayers, we provide Prompt-to-Prompt\nimage editing capabilities. More specifically, injecting the cross-attention maps of the input imageIenables\nus to preserve the original composition and structure.  In section 3.1, we review how cross-attention is used,\nand in section 3.2 we describe how to exploit the cross-attention for editing.  For additional background on\ndiffusion models, please refer to appendix A.\n3.1  Cross-attention in text-conditioned Diffusion Models\nWe use the Imagen [38] text-guided synthesis model as a backbone.  Since the composition and geometry\nare mostly determined at the64×64resolution, we only adapt the text-to-image diffusion model, using the\nsuper-resolution process as is. Recall that each diffusion steptconsists of predicting the noise\u000ffrom a noisy\nimagez\nt\nand text embeddingψ(P)using a U-shaped network [37].  At the final step, this process yields\nthe generated imageI=z\n0\n.  Most importantly, the interaction between the two modalities occurs during\nthe noise prediction, where the embeddings of the visual and textual features are fused using Cross-attention\nlayers that produce spatial attention maps for each textual token.\nMore formally, as illustrated in fig. 3(Top), the deep spatial features of the noisy imageφ(z\nt\n)are projected to\na query matrixQ=`\nQ\n(φ(z\nt\n)), and the textual embedding is projected to a key matrixK=`\nK\n(ψ(P))and\n4",
    "“afurrybearwatching\nt = Tt = 1\nsynthesized image\nbear\nbird\nabird”\nAverage attention maps across all timestamps\nAttention maps for individual  timestamps\nFigure 4:  Cross-attention maps of a text-conditioned diffusion image generation.  The top row displays the\naverage attention masks for each word in the prompt that synthesized the image on the left. The bottom rows\ndisplay the attention maps from different diffusion steps with respect to the words “bear” and “bird”.\na value matrixV=`\nV\n(ψ(P)), via learned linear projections`\nQ\n,`\nK\n,`\nV\n. Theattention mapsare then\nM=Softmax\n(\nQK\nT\n√\nd\n)\n,(1)\nwhere the cellM\nij\ndefines the weight of the value of thej-th token on the pixeli, and wheredis the latent\nprojection dimension of the keys and queries.  Finally, the cross-attention output is defined to be\n̂\nφ(z\nt\n)  =\nMV, which is then used to update the spatial featuresφ(z\nt\n).\nIntuitively, the cross-attention outputMVis a weighted average of the valuesVwhere the weights are the\nattention mapsM, which are correlated to thesimilaritybetweenQandK.  In practice, to increase their\nexpressiveness, multi-head attention [44] is used in parallel, and then the results are concatenated and passed\nthrough a learned linear layer to get the final output.\nImagen [38], similar to GLIDE  [28], conditions on the text prompt in the noise prediction of each diffusion\nstep (see appendix A.2) through two types of attention layers:  i) cross-attention layers.  ii) hybrid attention\nthat acts both as self-attention and cross-attention by simply concatenating the text embedding sequence to\nthe key-value pairs of each self-attention layer. Throughout the rest of the paper, we refer to both of them as\ncross-attention since our method only intervenes in the cross-attention part of the hybrid attention.  That is,\nonly the last channels, which refer to text tokens, are modified in the hybrid attention modules.\n3.2  Controlling the Cross-attention\nWe return to our key observation — the spatial layout and geometry of the generated image depend on the\ncross-attentionmaps.   This  interaction  between  pixels  and  text  is  illustrated  in  fig.  4,  where  the  average\nattention maps are plotted.  As can be seen, pixels are moreattractedto the words that describe them, e.g.,\npixels of the bear are correlated with the word “bear”. Note that averaging is done for visualization purposes,\nand attention maps are kept separate for each head in our method. Interestingly, we can see that the structure\nof the image is already determined in the early steps of the diffusion process.\nSince the attention reflects the overall composition, we can inject the attention mapsMthat were obtained\nfrom the generation with the original promptP, into a second generation with the modified promptP\n∗\n. This\nallows the synthesis of an edited imageI\n∗\nthat is not only manipulated according to the edited prompt, but\nalso preserves the structure of the input imageI.  This example is a specific instance of a broader set of\n5",
    "“A photo of a butterfly on...”\n“...on a flower.”\n“...on a bread.”\n“...on a cup,”“...on grass.”\n“...on a candy.”\n“...on a fruit”“...on the ground.”“...on a computer.”\n“...on a cake.”\n“...on a flute.”“...on a muffin.”“...on a violin,”“...on a present.”\n“...on a table\n“...on the river.”\n“...on a pizza.”\nFigure 5:   Object preservation.  By injecting only the attention weights of the word “butterfly”, taken from\nthe top-left image, we can preserve the structure and appearance of a single item while replacing its context.\nNote how the butterfly sits on top of all objects in a very plausible manner.\nattention-based manipulations leading to different types of intuitive editing. We, therefore, start by proposing\na general framework, followed by the details of the specific editing operations.\nLetDM(z\nt\n,P,t,s)be the computation of a single steptof the diffusion process, which outputs the noisy\nimagez\nt−1\n, and the attention mapM\nt\n(omitted if not used).  We denote byDM(z\nt\n,P,t,s){M←\n̂\nM}the\ndiffusion step where we override the attention mapMwith an additional given map\n̂\nM, but keep the values\nVfrom the supplied prompt. We also denote byM\n∗\nt\nthe produced attention map using the edited promptP\n∗\n.\nLastly, we defineEdit(M\nt\n,M\n∗\nt\n,t)to be a general edit function, receiving as input thet’th attention maps of\nthe original and edited images during their generation.\nOur general algorithm for controlled image generation consists of performing the iterative diffusion process\nfor both prompts simultaneously, where an attention-based manipulation is applied in each step according to\nthe desired editing task.  We note that for the method above to work, we must fix the internal randomness.\nThis is due to the nature of diffusion models, where even for the same prompt, two random seeds produce\ndrastically different outputs. Formally, our general algorithm is:\nAlgorithm 1:Prompt-to-Prompt image editing\n1Input:A source promptP, a target promptP\n∗\n, and a random seeds.\n2Output:A source imagex\nsrc\nand an edited imagex\ndst\n.\n3z\nT\n∼N(0,I)a unit Gaussian random variable with random seeds;\n4z\n∗\nT\n←z\nT\n;\n5fort=T,T−1,...,1do\n6z\nt−1\n,M\nt\n←DM(z\nt\n,P,t,s);\n7M\n∗\nt\n←DM(z\n∗\nt\n,P\n∗\n,t,s);\n8\n̂\nM\nt\n←Edit(M\nt\n,M\n∗\nt\n,t);\n9z\n∗\nt−1\n←DM(z\n∗\nt\n,P\n∗\n,t,s\nt\n){M←\n̂\nM\nt\n};\n10end\n11Return(z\n0\n,z\n∗\n0\n)\nNotice that we can also define imageI, which is generated by promptPand random seeds, as an additional\ninput.  Yet, the algorithm would remain the same.  For editing real images, see section 4.  Also, note that\nwe can skip the forward call in line7by applying the edit function inside the diffusion forward function.\nMoreover, a diffusion step can be applied on bothz\nt−1\nandz\n∗\nt\nin the same batch (i.e., in parallel), and so\nthere is only one step overhead with respect to the original inference of the diffusion model.\nWe now turn to address specific editing operations,  filling the missing definition of theEdit(M\nt\n,M\n∗\nt\n,t)\nfunction. An overview is presented in fig. 3(Bottom).\nWord Swap.In this case, the user swaps tokens of the original prompt with others, e.g.,P=“a big red\nbicycle” toP\n∗\n=“a big red car”.   The main challenge is to preserve the original composition while also\naddressing the content of the new prompt. To this end, we inject the attention maps of the source image into\nthe generation with the modified prompt.  However, the proposed attention injection may over constrain the\n6",
    "“photo of a cat riding on a bicycle.” \nSource image and prompt:\nW.O. attention injectionFull attention injection\nbicycle        car\nbicycle        airplane\nbicycle        motorcycle\nbicycle        train\nFigure 6:  Attention injection through a varied number of diffusion steps.  On the top, we show the source\nimage and prompt. In each row, we modify the content of the image by replacing a single word in the text and\ninjecting the cross-attention maps of the source image ranging from 0% (on the left) to 100% (on the right)\nof the diffusion steps.  Notice that on one hand, without our method, none of the source image content is\nguaranteed to be preserved. On the other hand, injecting the cross-attention throughout all the diffusion steps\nmay over-constrain the geometry, resulting in low fidelity to the text prompt, e.g., the car (3rd row) becomes\na bicycle with full cross-attention injection.\ngeometry, especially when a large structural modification, such as “car” to “bicycle”, is involved. We address\nthis by suggesting a softer attention constrain:\nEdit(M\nt\n,M\n∗\nt\n,t) :=\n{\nM\n∗\nt\nift < τ\nM\nt\notherwise.\nwhereτis a timestamp parameter that determines until which step the injection is applied.  Note that the\ncomposition is determined in the early steps of the diffusion process.  Therefore, by limiting the number of\ninjection steps, we can guide the composition of the newly generated image while allowing the necessary\ngeometryfreedomfor adapting to the new prompt.  An illustration is provided in section 4.  Another natural\nrelaxation for our algorithm is to assign a different number of injection timestamps for the different tokens\nin the prompt.  In case the two words are represented using a different number of tokens, the maps can be\nduplicated/averaged as necessary using an alignment function as described in the next paragraph.\nAdding a New Phrase.In another setting, the user adds new tokens to the prompt, e.g.,P=“a castle next\nto a river” toP\n∗\n=“children drawing of a castle next to a river”.  To preserve the common details, we apply\nthe attention injection only over the common tokens from both prompts.   Formally,  we use an alignment\nfunctionAthat receives a token index from target promptP\n∗\nand outputs the corresponding token index in\nPorNoneif there isn’t a match. Then, the editing function is given by:\n7",
    "“A car on the side of the street.”\nsource image\n“...old car...”\n“...convertibae car...”\n“...crushed car...”\n“...mat black car...”\n“...limousine car...”\n“...at autumn.”\n“...the flooded street.”\n“...the blossom street.”\n“...in Manhattan.”\n“...American car...”\n“...in the snowy street.”“...at evening.”\n“...at sunset.”\n“...in the forset.”\nLocal description\nGlobal description\n“...sport car...”\nFigure 7:  Editing by prompt refinement.  By extending the description of the initial prompt, we can make\nlocal edits to the car (top rows) or global modifications (bottom rows).\n(Edit(M\nt\n,M\n∗\nt\n,t))\ni,j\n:=\n{\n(M\n∗\nt\n)\ni,j\nifA(j) =None\n(M\nt\n)\ni,A(j)\notherwise.\nRecall that indexicorresponds to a pixel value, wherejcorresponds to a text token.  Again, we may set a\ntimestampτto control the number of diffusion steps in which the injection is applied.  This kind of editing\nenables diverse Prompt-to-Prompt capabilities such as stylization, specification of object attributes, or global\nmanipulations as demonstrated in section 4.\nAttention Re–weighting.Lastly,  the user may wish to strengthen or weakens the extent to which each\ntoken is affecting the resulting image. For example, consider the promptP=“a fluffy red ball”, and assume\nwe want to make the ball more or less fluffy. To achieve such manipulation, we scale the attention map of the\nassigned tokenj\n∗\nwith parameterc∈[−2,2], resulting in a stronger/weaker effect. The rest of the attention\nmaps remain unchanged. That is:\n(Edit(M\nt\n,M\n∗\nt\n,t))\ni,j\n:=\n{\nc·(M\nt\n)\ni,j\nifj=j\n∗\n(M\nt\n)\ni,j\notherwise.\nAs described in section 4, the parametercallows fine and intuitive control over the induced effect.\n4  Applications\nOur method, described in section 3, enables intuitive text-only editing by controlling the spatial layout corre-\nsponding to each word in the user-provided prompt.  In this section, we show several applications using this\ntechnique.\nText-Only Localized Editing.We first demonstrate localized editing by modifying the user-provided prompt\nwithout requiring any user-provided mask. In fig. 2, we depict an example where we generate an image using\n8",
    "source image“relaxing photo of...”“... on mars.”“...in the jungle.”“... in the desert.”“dramatic photo of...”\nsource image“charocal...”“watercolor...”“impressionism...”“neo classical...”“futuristic...”\n“drawing of...”“photo of...”\n“photo of...”“painting of...”\n“A waterfall between the mountains.”\nFigure 8: Image stylization. By adding a style description to the prompt while injecting the source attention\nmaps, we can create various images in the new desired styles that preserve the structure of the original image.\nthe prompt “lemon cake”.  Our method allows us to retain the spatial layout, geometry, and semantics when\nreplacing  the  word  “lemon”  with  “pumpkin”  (top  row).   Observe  that  the  background  is  well-preserved,\nincluding the top-left lemons transforming into pumpkins.  On the other hand, naively feeding the synthesis\nmodel with the prompt “pumpkin cake” results in a completely different geometry (3rd row),  even when\nusing the same random seed in a deterministic setting (i.e., DDIM [40]).  Our method succeeds even for a\nchallenging prompt such as “pasta cake.” (2nd row) — the generated cake consists of pasta layers with tomato\nsauce on top. Another example is provided in fig. 5 where we do not inject the attention of the entire prompt\nbut only the attention of a specific word – “butterfly”.  This enables the preservation of the original butterfly\nwhile changing the rest of the content. Additional results are provided in the appendix (fig. 13).\nAs can be seen in fig. 6, our method is not confined to modifying only textures, and it can perform structural\nmodifications, e.g., change a “bicycle” to a “car”.  To analyze our attention injection, in the left column we\nshow the results without cross-attention injection, where changing a single word leads to an entirely different\noutcome. From left to right, we then show the resulting generated image by injecting attention to an increasing\nnumber of diffusion steps. Note that the more diffusion steps in which we apply cross-attention injection, the\nhigher the fidelity to the original image. However, the optimal result is not necessarily achieved by applying\nthe injection throughout all diffusion steps. Therefore, we can provide the user with even better control over\nthe fidelity to the original image by changing the number of injection steps.\nInstead of replacing one word with another, the user may wish to add a new specification to the generated\nimage.   In  this  case,  we  keep  the  attention  maps  of  the  original  prompt,  while  allowing  the  generator  to\naddress the newly added words. For example, see fig. 7 (top), where we add “crushed” to the “car”, resulting\nin the generation of additional details over the original image while the background is still preserved. See the\nappendix (fig. 14) for more examples.\nGlobal editing.Preserving the image composition is not only valuable for localized editing,  but also an\nimportant aspect of global editing.  In this setting, the editing should affect all parts of the image, but still\nretain the original composition, such as the location and identity of the objects. As shown in fig. 7 (bottom),\nwe retain the image content while adding “snow” or changing the lightning.  Additional examples appear in\nfig. 8, including translating a sketch into a photo-realistic image and inducing an artistic style.\nFader Control using Attention Re-weighting.While controlling the image by editing the prompt is very\neffective,  we find that it still does not allow full control over the generated image.   Consider the prompt\n“snowy mountain”.  A user may want to control theamountof snow on the mountain.  However, it is quite\ndifficult to describe the desired amount of snow through text. Instead, we suggest afadercontrol [24], where\nthe user controls the magnitude of the effect induced by a specific word, as depicted in fig. 9.  As described\nin section 3, we achieve such control by re-scaling the attention of the specified word. Additional results are\nin the appendix (fig. 15).\n9",
    "“A photo of a house on a snowy(  ) mountain.”\n“My fluffy(  ) bunny doll.\n“A photo of a birthday(  ) cake next to an apple.”\n“The picnic is ready under a blossom(  ) tree.”\nFigure 9:  Text-based image editing with fader control.  By reducing (top rows) or increasing (bottom) the\ncross-attention of the specified words (marked with an arrow), we can control the extent to which it influences\nthe generated image.\nReal Image Editing.Editing a real image requires finding an initial noise vector that produces the given input\nimage when fed into the diffusion process. This process, known asinversion, has recently drawn considerable\nattention for GANs, e.g., [51, 1, 3, 35, 50, 43, 45, 47], but has not yet been fully addressed for text-guided\ndiffusion models.\nIn the following, we show preliminary editing results on real images, based on common inversion techniques\nfor diffusion models.  First, a rather na\n ̈\nıve approach is to add Gaussian noise to the input image, and then\nperform a predefined number of diffusion steps.  Since this approach results in significant distortions,  we\nadopt an improved inversion approach [10, 40], which is based on the deterministic DDIM model rather than\nthe DDPM model.  We perform the diffusion process in the reverse direction, that isx\n0\n−→x\nT\ninstead of\nx\nT\n−→x\n0\n, wherex\n0\nis set to be the given real image.\nThis inversion process often produces satisfying results, as presented in fig. 10. However, the inversion is not\nsufficiently accurate in many other cases, as in fig. 11. This is partially due to a distortion-editability tradeoff\n[43], where we recognize that reducing the classifier-free guidance [18] parameter (i.e., reducing the prompt\ninfluence) improves reconstruction but constrains our ability to perform significant manipulations.\nTo alleviate this limitation, we propose to restore the unedited regions of the original image using a mask,\ndirectly extracted from the attention maps.  Note that here the mask is generated with no guidance from the\nuser. As presented in fig. 12, this approach works well even using the na\n ̈\nıve DDPM inversion scheme (adding\nnoise followed by denoising). Note that the cat’s identity is well-preserved under various editing operations,\nwhile the mask is produced only from the prompt itself.\n10",
    "real image\n“A black bear is walking in the grass.”\n“Landscape image of trees in a valley...”\nreconstructed“...next to red flowers.”“...when snow comes \ndown.”\n“while another black bear \nis watching.”\n“Oil painting of...”\nreal imagereconstructed“...at fall.”“...at winter.”“...at sunrise.”“...at night.”\nFigure 10: Editing of real images.  On the left, inversion results using DDIM [40] sampling.  We reverse the\ndiffusion process initialized on a given real image and text prompt. This results in a latent noise that produces\nan approximation to the input image when fed to the diffusion process. Afterward, on the right, we apply our\nPrompt-to-Prompt technique to edit the images.\nReal imageReconstructedReal imageReconstructedReal imageReconstructed\nFigure 11: Inversion Failure Cases. Current DDIM-based inversion of real images might result in unsatisfied\nreconstructions.\n5  Conclusions\nIn this work, we uncovered the powerful capabilities of the cross-attention layers within text-to-image dif-\nfusion models. We showed that these high-dimensional layers have an interpretable representation of spatial\nmaps that play a key role in tying the words in the text prompt to the spatial layout of the synthesized image.\nWith this observation, we showed how various manipulations of the prompt can directly control attributes\nin the synthesized image, paving the way to various applications including local and global editing.  This\nwork is a first step towards providing users with simple and intuitive means toeditimages, leveraging textual\nsemantic power.  It enables users to navigate through a semantic, textual, space, which exhibits incremental\nchanges after each step, rather than producing the desired image from scratch after each text manipulation.\nWhile we have demonstrated semantic control by changing only textual prompts, our technique is still subject\nto a few limitations to be addressed in follow-up work. First, the current inversion process results in a visible\ndistortion over some of the test images. In addition, the inversion requires the user to come up with a suitable\nprompt.  This could be challenging for complicated compositions.  Note that the challenge of inversion for\ntext-guided diffusion models is an orthogonal endeavor to our work,  which will be thoroughly studied in\nthe future.  Second, the current attention maps are of low resolution, as the cross-attention is placed in the\nnetwork’s bottleneck.  This bounds our ability to perform even more precise localized editing.  To alleviate\nthis, we suggest incorporating cross-attention also in higher-resolution layers. We leave this for future works\nsince it requires analyzing the training procedure which is out of our current scope.  Finally, we recognize\nthat our current method cannot be used to spatially move existing objects across the image and also leave this\nkind of control for future work.\n11",
    "“Image of cat wearing \na floral shirt.”\nNoised\nResized image + prompt\nDenoised + attention mapBendedSR\n“shirt”\nreal image\u001f...wearing a colorful shirt...\u001eMore dresses\nFigure 12: Mask-based editing. Using the attention maps, we preserve the unedited parts of the image when\nthe inversion distortion is significant. This does not require any user-provided masks, as we extract the spatial\ninformation from the model using our method. Note how the cat’s identity is retained after the editing process.\n6  Acknowledgments\nWe thank Noa Glaser, Adi Zicher, Yaron Brodsky and Shlomi Fruchter for their valuable inputs that helped\nimprove this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with\ntheir support and the pretrained models of Imagen [38].  Special thanks to Yossi Matias for early inspiring\ndiscussion on the problem and for motivating and encouraging us to develop technologies along the avenue\nof intuitive interaction.\nReferences\n[1]  Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan\nlatent space?   InProceedings of the IEEE/CVF International Conference on Computer Vision, pages\n4432–4441, 2019.\n[2]  Rameen Abdal, Peihao Zhu, John Femiani, Niloy J Mitra, and Peter Wonka.  Clip2stylegan:  Unsuper-\nvised extraction of stylegan edit directions.arXiv preprint arXiv:2112.05219, 2021.\n[3]  Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano.  Hyperstyle: Stylegan inversion\nwith hypernetworks for real image editing.  InProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18511–18521, 2022.\n[4]  Omri  Avrahami,  Ohad  Fried,  and  Dani  Lischinski.Blended  latent  diffusion.arXiv preprint\narXiv:2206.02779, 2022.\n[5]  Omri Avrahami, Dani Lischinski, and Ohad Fried.  Blended diffusion for text-driven editing of natural\nimages.   InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 18208–18218, 2022.\n12",
    "[6]  Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel.  Text2live: Text-driven\nlayered image and video editing.arXiv preprint arXiv:2204.02491, 2022.\n[7]  David  Bau,  Alex  Andonian,  Audrey  Cui,  YeonHwan  Park,  Ali  Jahanian,  Aude  Oliva,  and  Antonio\nTorralba. Paint by word, 2021.\n[8]  Andrew Brock, Jeff Donahue, and Karen Simonyan.  Large scale gan training for high fidelity natural\nimage synthesis.arXiv preprint arXiv:1809.11096, 2018.\n[9]  Katherine Crowson,  Stella Biderman,  Daniel Kornis,  Dashiell Stander,  Eric Hallahan,  Louis Castri-\ncato, and Edward Raff.  Vqgan-clip: Open domain image generation and editing with natural language\nguidance.arXiv preprint arXiv:2204.08583, 2022.\n[10]  Prafulla Dhariwal and Alexander Nichol.  Diffusion models beat gans on image synthesis.Advances in\nNeural Information Processing Systems, 34:8780–8794, 2021.\n[11]  Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al.  Cogview:  Mastering text-to-image generation via transformers.Advances\nin Neural Information Processing Systems, 34:19822–19835, 2021.\n[12]  Patrick  Esser,  Robin  Rombach,  and  Bjorn  Ommer.   Taming  transformers  for  high-resolution  image\nsynthesis.   InProceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 12873–12883, 2021.\n[13]  Oran Gafni,  Adam Polyak,  Oron Ashual,  Shelly Sheynin,  Devi Parikh,  and Yaniv Taigman.   Make-\na-scene:  Scene-based text-to-image generation with human priors.arXiv preprint arXiv:2203.13131,\n2022.\n[14]  Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or.   Stylegan-nada:  Clip-\nguided domain adaptation of image generators.arXiv preprint arXiv:2108.00946, 2021.\n[15]  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets.Advances in neural information processing\nsystems, 27, 2014.\n[16]  Tobias Hinz, Stefan Heinrich, and Stefan Wermter.   Semantic object accuracy for generative text-to-\nimage synthesis.IEEE transactions on pattern analysis and machine intelligence, 2020.\n[17]  Jonathan Ho,  Ajay Jain,  and Pieter Abbeel.   Denoising diffusion probabilistic models.Advances in\nNeural Information Processing Systems, 33:6840–6851, 2020.\n[18]  Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. InNeurIPS 2021 Workshop on Deep\nGenerative Models and Downstream Applications, 2021.\n[19]  Tero Karras, Miika Aittala, Samuli Laine, Erik H\n ̈\nark\n ̈\nonen, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila.  Alias-free generative adversarial networks.Advances in Neural Information Processing Systems,\n34:852–863, 2021.\n[20]  Tero Karras, Samuli Laine, and Timo Aila.  A style-based generator architecture for generative adver-\nsarial networks.  InProceedings of the IEEE conference on computer vision and pattern recognition,\npages 4401–4410, 2019.\n[21]  Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.  Analyzing\nand improving the image quality of stylegan. InProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8110–8119, 2020.\n[22]  Gwanghyun  Kim,  Taesung  Kwon,  and  Jong  Chul  Ye.   Diffusionclip:  Text-guided  diffusion  models\nfor robust image manipulation.  InProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2426–2435, 2022.\n[23]  Gihyun Kwon and Jong Chul Ye.  Clipstyler:  Image style transfer with a single text condition.arXiv\npreprint arXiv:2112.00374, 2021.\n[24]  Guillaume  Lample,   Neil  Zeghidour,   Nicolas  Usunier,   Antoine  Bordes,   Ludovic  Denoyer,   and\nMarc’Aurelio Ranzato. Fader networks: Manipulating images by sliding attributes.Advances in neural\ninformation processing systems, 30, 2017.\n13",
    "[25]  Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr.  Controllable text-to-image generation.\nAdvances in Neural Information Processing Systems, 32, 2019.\n[26]  Wenbo  Li,  Pengchuan  Zhang,  Lei  Zhang,  Qiuyuan  Huang,  Xiaodong  He,  Siwei  Lyu,  and  Jianfeng\nGao.  Object-driven text-to-image synthesis via adversarial training.  InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 12174–12182, 2019.\n[27]  Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar Mosseri, Tali Dekel, Daniel Cohen-Or, and\nMichal Irani. Self-distilled stylegan: Towards generation from internet photos. InSpecial Interest Group\non Computer Graphics and Interactive Techniques Conference Proceedings, pages 1–9, 2022.\n[28]  Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models.arXiv preprint arXiv:2112.10741, 2021.\n[29]  Or Patashnik,  Zongze Wu,  Eli Shechtman,  Daniel Cohen-Or,  and Dani Lischinski.   Styleclip:  Text-\ndriven manipulation of stylegan imagery.arXiv preprint arXiv:2103.17249, 2021.\n[30]  Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. Learn, imagine and create: Text-to-image\ngeneration from prior knowledge.Advances in neural information processing systems, 32, 2019.\n[31]  Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.  Mirrorgan:  Learning text-to-image gen-\neration by redescription. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1505–1514, 2019.\n[32]  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.  Learning transferable visual models from\nnatural language supervision.arXiv preprint arXiv:2103.00020, 2021.\n[33]  Aditya  Ramesh,  Prafulla  Dhariwal,  Alex  Nichol,  Casey  Chu,  and  Mark  Chen.    Hierarchical  text-\nconditional image generation with clip latents.arXiv preprint arXiv:2204.06125, 2022.\n[34]  Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. InInternational Conference on Machine Learning,\npages 8821–8831. PMLR, 2021.\n[35]  Daniel Roich, Ron Mokady, Amit H. Bermano, and Daniel Cohen-Or.  Pivotal tuning for latent-based\nediting of real images.ACM Transactions on Graphics (TOG), 2022.\n[36]  Robin  Rombach,  Andreas  Blattmann,  Dominik  Lorenz,  Patrick  Esser,  and  Bj\n ̈\norn  Ommer.High-\nresolution image synthesis with latent diffusion models, 2021.\n[37]  Olaf Ronneberger, Philipp Fischer, and Thomas Brox.  U-net:  Convolutional networks for biomedical\nimage segmentation.  InInternational Conference on Medical image computing and computer-assisted\nintervention, pages 234–241. Springer, 2015.\n[38]  Chitwan Saharia,  William Chan,  Saurabh Saxena,  Lala Li,  Jay Whang,  Emily Denton,  Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans,\nTim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.  Photorealistic text-to-image dif-\nfusion models with deep language understanding.arXiv preprint arXiv:2205.11487, 2022.\n[39]  Jascha Sohl-Dickstein,  Eric Weiss,  Niru Maheswaranathan,  and Surya Ganguli.   Deep unsupervised\nlearning using nonequilibrium thermodynamics.   InInternational Conference on Machine Learning,\npages 2256–2265. PMLR, 2015.\n[40]  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. InInternational\nConference on Learning Representations, 2020.\n[41]  Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution.\nAdvances in Neural Information Processing Systems, 32, 2019.\n[42]  Ming  Tao,  Hao  Tang,  Songsong  Wu,  Nicu  Sebe,  Xiao-Yuan  Jing,  Fei  Wu,  and  Bingkun  Bao.\nDf-gan:   Deep  fusion  generative  adversarial  networks  for  text-to-image  synthesis.arXiv preprint\narXiv:2008.05865, 2020.\n14",
    "[43]  Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or.  Designing an encoder for\nstylegan image manipulation.arXiv preprint arXiv:2102.02766, 2021.\n[44]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin.  Attention is all you need.  InAdvances in Neural Information Processing\nSystems, volume 30, 2017.\n[45]  Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen.  High-fidelity gan inversion for\nimage attribute editing.ArXiv, abs/2109.06590, 2021.\n[46]  Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.  Tedigan:  Text-guided diverse face image\ngeneration and manipulation.   InProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 2256–2265, 2021.\n[47]  Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang.  Gan inver-\nsion: A survey, 2021.\n[48]  Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al.  Scaling autoregressive models for content-rich\ntext-to-image generation.arXiv preprint arXiv:2206.10789, 2022.\n[49]  Zizhao Zhang, Yuanpu Xie, and Lin Yang.  Photographic text-to-image synthesis with a hierarchically-\nnested adversarial network.   InProceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 6199–6208, 2018.\n[50]  Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou.  In-domain gan inversion for real image editing.\narXiv preprint arXiv:2004.00049, 2020.\n[51]  Jun-Yan Zhu, Philipp Kr\n ̈\nahenb\n ̈\nuhl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation\non the natural image manifold.  InEuropean conference on computer vision, pages 597–613. Springer,\n2016.\nA  Background\nA.1  Diffusion Models\nDiffusion Denoising Probabilistic Models (DDPM) [39, 17] are generative latent variable models that aim to\nmodel a distributionp\nθ\n(x\n0\n)that approximates the data distributionq(x\n0\n)and easy to sample from. DDPMs\nmodel a “forward process” in the space ofx\n0\nfrom data to noise.\n†\nThis process is a Markov chain starting\nfromx\n0\n, where we gradually add noise to the data to generate the latent variablesx\n1\n,...,x\nT\n∈X.  The\nsequence of latent variables therefore followsq(x\n1\n,...,x\nt\n|x\n0\n) =\n∏\nt\ni=1\nq(x\nt\n|x\nt−1\n), where a step in the\nforward process is defined as a Gaussian transitionq(x\nt\n|x\nt−1\n) :=N(x\nt\n;\n√\n1−β\nt\nx\nt−1\n,β\nt\nI)parameterized\nby a scheduleβ\n0\n,...,β\nT\n∈(0,1).   WhenTis large enough,  the last noise vectorx\nT\nnearly follows an\nisotropic Gaussian distribution.\nAn interesting property of the forward process is that one can express the latent variablex\nt\ndirectly as the\nfollowing linear combination of noise andx\n0\nwithout sampling intermediate latent vectors:\nx\nt\n=\n√\nα\nt\nx\n0\n+\n√\n1−α\nt\nw, w∼N(0,I),(2)\nwhereα\nt\n:=\n∏\nt\ni=1\n(1−β\ni\n).\nIn  order  to  sample  from  the  distributionq(x\n0\n),  we  define  the  dual  “reverse  process”p(x\nt−1\n|x\nt\n)from\nisotropic Gaussian noisex\nT\nto data by sampling the posteriorsq(x\nt−1\n|x\nt\n).  Since the intractable reverse\nprocessq(x\nt−1\n|x\nt\n)depends on the unknown data distributionq(x\n0\n), we approximate it with a parame-\nterized Gaussian transition networkp\nθ\n(x\nt−1\n|x\nt\n) :=N(x\nt−1\n|μ\nθ\n(x\nt\n,t),Σ\nθ\n(x\nt\n,t)).  Theμ\nθ\n(x\nt\n,t)can be\nreplaced [17] by predicting the noiseε\nθ\n(x\nt\n,t)added tox\n0\nusing equation 2.\nUnder this definition, we use Bayes’ theorem to approximate\nμ\nθ\n(x\nt\n,t) =\n1\n√\nα\nt\n(\nx\nt\n−\nβ\nt\n√\n1−α\nt\nε\nθ\n(x\nt\n,t)\n)\n.(3)\n†\nThis process is called “forward” due to its procedure progressing fromx\n0\ntox\nT\n.\n15",
    "Once we have a trainedε\nθ\n(x\nt\n,t), we can using the following sample method\nx\nt−1\n=μ\nθ\n(x\nt\n,t) +σ\nt\nz, z∼N(0,I).(4)\nWe can controlσ\nt\nof each sample stage, and in DDIMs [40] the sampling process can be made deterministic\nusingσ\nt\n= 0in all the steps. The reverse process can finally be trained by solving the following optimization\nproblem:\nmin\nθ\nL(θ) := min\nθ\nE\nx\n0\n∼q(x\n0\n),w∼N(0,I),t\n‖w−ε\nθ\n(x\nt\n,t)‖\n2\n2\n,\nteaching the parametersθto fitq(x\n0\n)by maximizing a variational lower bound.\nA.2  Cross-attention in Imagen\nImagen [38] consists of three text-conditioned diffusion models:  A text-to-image64×64model, and two\nsuper-resolution models –64×64→256×256and256×256→1024×1024.  These predict the noise\nε\nθ\n(z\nt\n,c,t)via a U-shaped network, fortranging fromTto1. Wherez\nt\nis the latent vector andcis the text\nembedding. We highlight the differences between the three models:\n•64×64– starts from a random noise, and uses the U-Net as in [10].  This model is conditioned on\ntext embeddings via both cross-attention layers at resolutions[16,8]and hybrid-attention layers at\nresolutions[32,16,8]of the downsampling and upsampling within the U-Net.\n•64×64→256×256– conditions on a naively upsampled64×64image. An efficient version of\na U-Net is used, which includes Hybrid attention layers in the bottleneck (resolution of32).\n•256×256→1024×1024– conditions on a naively upsampled256×256image.  An efficient\nversion of a U-Net is used, which only includes cross-attention layers in the bottleneck (resolution\nof64).\nB  Additional results\nWe provide additional examples, demonstrating our method over different editing operations.  fig. 13 show\nword swap results, fig. 14 show adding specification to an image, and fig. 15 show attention re-weighting.\n16",
    "“Photo of a cat riding on a bicycle.”\n“Photo of a house with a flag on a mountain.”\n“A ball between two chairs on the beach.\n“A basket full of apples.”\nsource image\ncat      \ndog\ncat      \nchicken\ncat      \nsquirrel\ncat      \nelephant\nsource image\nhouse\ntent\nhouse\ntree\nhouse\ncar\nhouse\nhotel\nsource image\nball\nturtle\nball\nbucket\nball\nfrisbee\nball\npalace\nsource image\napples\noranges\napples\ncookies\napples\nchocolates\nbasket\nbowl\nbasket\nbox\nbasket\npot\nbasket\nnest\napples\nkittens\napples\nsmoke\nFigure 13: Additional results for Prompt-to-Prompt editing by word swap.\n17",
    "“A photo of a bear wearing sunglasses on and having a drink.”\n“A photo of a butterfly on a flower.”\n“A mushroom in the forest.”\nsource image“...in the wet forest.”“...in the dry forest.”“A neon mushroom...”\n“A plastic mushroom...”“Line art of...”\nsource image“...on a spikey flower.”“...flower made out \nfrom candies.”\n“...wooden flower.”\n“...origami flower.”“...wither flower.”\nsource image“...wearing a squared sunglasses...”“...rounded sunglasses...”“”...coffee drink.”\n“...beer drink.”“...geeky sunglasses...”\nFigure 14: Additional results for Prompt-to-Prompt editing by adding a specification.\n18",
    "“A tiger is sleeping(  ) in a field.”\n“A smiling(  ) teddy bear.”\n“Photo of a cubic(  ) sushi.”\n“My colorful(  ) bedroom.”\n“Photo of a field of poppies at night(  ).”\n“The modern(  ) city.”\nFigure 15: Additional results for Prompt-to-Prompt editing by attention re-weighting.\n19"
  ]
}