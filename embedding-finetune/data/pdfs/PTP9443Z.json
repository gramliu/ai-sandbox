{
  "key": "PTP9443Z",
  "url": "http://arxiv.org/pdf/2004.12832",
  "metadata": {
    "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT",
    "abstract": "  Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.\n",
    "published": "2020-04-27T14:21:03Z"
  },
  "text": [
    "ColBERT: Eicient and Eective Passage Search via\nContextualized Late Interaction over BERT\nOmar Khaab\nStanford University\nokhaab@stanford.edu\nMatei Zaharia\nStanford University\nmatei@cs.stanford.edu\nABSTRACT\nRecent progress in Natural Language Understanding (NLU) is driv-\ning fast-paced advances in Information Retrieval (IR), largely owed\nto ne-tuning deep language models (LMs) for document ranking.\nWhile remarkably eective, the ranking models based on these LMs\nincrease computational cost by orders of magnitude over prior ap-\nproaches, particularly as they must feed each query–document pair\nthrough a massive neural network to compute a single relevance\nscore. To tackle this, we present ColBERT, a novel ranking model\nthat adapts deep LMs (in particular, BERT) for ecient retrieval.\nColBERT introduces alate interactionarchitecture that indepen-\ndently encodes the query and the document using BERT and then\nemploys a cheap yet powerful interaction step that models their\nne-grained similarity.  By delaying and yet retaining this ne-\ngranular interaction, ColBERT can leverage the expressiveness of\ndeep LMs while simultaneously gaining the ability to pre-compute\ndocument representations oine, considerably speeding up query\nprocessing. Beyond reducing the cost of re-ranking the documents\nretrieved by a traditional model, ColBERT’spruning-friendlyin-\nteraction mechanism enables leveraging vector-similarity indexes\nfor end-to-end retrieval directly from a large document collection.\nWe extensively evaluate ColBERT using two recent passage search\ndatasets. Results show that ColBERT’s eectiveness is competitive\nwith existing BERT-based models (and outperforms every non-\nBERT baseline), while executing two orders-of-magnitude faster\nand requiring four orders-of-magnitude fewer FLOPs per query.\nACM Reference format:\nOmar Khaab and Matei Zaharia. 2020. ColBERT: Ecient and Eective Pas-\nsage Search via Contextualized Late Interaction over BERT. InProceedings\nof Proceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, Virtual Event, China, July 25–30,\n2020 (SIGIR ’20),10 pages.\nDOI: 10.1145/3397271.3401075\n1  INTRODUCTION\nOver the past few years, the Information Retrieval (IR) community\nhas witnessed the introduction of a host of neural ranking models,\nincluding DRMM [7], KNRM [4,36], and Duet [20,22]. In contrast\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor prot or commercial advantage and that copies bear this notice and the full citation\non the rst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permied. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specic permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR ’20, Virtual Event, China\n©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n978-1-4503-8016-4/20/07. . .$15.00\nDOI: 10.1145/3397271.3401075\n0.150.200.250.300.350.40\nMRR@10\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nQuery Latency (ms)\nBM25\ndoc2query\nKNRM\nDuet\nDeepCT\nfT+ConvKNRM\ndocTTTTTquery\nBERT-base\nBERT-large\nColBERT (re-rank)\nColBERT (full retrieval)\nBag-of-Words (BoW) Model\nBoW Model with NLU Augmentation\nNeural Matching Model\nDeep Language Model\nColBERT (ours)\nFigure 1: Eectiveness (MRR@10) versus Mean ery La-\ntency (log-scale) for a number of representative ranking\nmodels on MS MARCO Ranking [24]. e gure also shows\nColBERT. Neural re-rankers run on top of the ocial BM25\ntop-1000 results and use a Tesla V100 GPU. Methodology and\ndetailed results are in§4.\nto prior learning-to-rank methods that rely on hand-craed fea-\ntures, these models employ embedding-based representations of\nqueries and documents and directly modellocal interactions(i.e.,\nne-granular relationships) between their contents. Among them,\na recent approach has emerged thatne-tunesdeep pre-trained\nlanguage models (LMs) like ELMo [29] and BERT [5] for estimating\nrelevance.  By computing deeply-contextualized semantic repre-\nsentations of query–document pairs, these LMs help bridge the\npervasive vocabulary mismatch [21,42] between documents and\nqueries [30]. Indeed, in the span of just a few months, a number\nof ranking models based on BERT have achieved state-of-the-art\nresults on various retrieval benchmarks [3,18,25,39] and have\nbeen proprietarily adapted for deployment by Google\n1\nand Bing\n2\n.\nHowever, the remarkable gains delivered by these LMs come\nat a steep increase in computational cost. Hofst\n ̈\naeret al.[9] and\nMacAvaneyet al.[18] observe that BERT-based models in the lit-\nerature are 100-1000×more computationally expensive than prior\nmodels—some of which are arguablynotinexpensive to begin with\n[13]. is quality–cost tradeo is summarized by Figure 1, which\ncompares two BERT-based rankers [25,27] against a representative\nset of ranking models. e gure uses MS MARCO Ranking [24],\na recent collection of 9M passages and 1M queries from Bing’s\nlogs.  It reports retrieval eectiveness (MRR@10) on the ocial\nvalidation set as well as average query latency (log-scale) using a\nhigh-end server that dedicates one Tesla V100 GPU per query for\nneural re-rankers. Following there-rankingsetup of MS MARCO,\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\nre-rank the MS MARCO’s ocial top-1000 documents per query.\n1\nhps://blog.google/products/search/search-language-understanding-bert/\n2\nhps://azure.microso.com/en-us/blog/bing-delivers-its-largest-improvement-\nin-search-experience-using-azure-gpus/\narXiv:2004.12832v2  [cs.IR]  4 Jun 2020",
    "Query\nDocument\nMaxSim\n∑\nMaxSim\nMaxSim\ns\nQuery\nCNN  /  Match Kernels\nCNN  /  Match Kernels / MLP\nMLP\ns\nDocument\n(c) All-to-all Interaction\n(e.g., BERT)\n(b) Query-Document Interaction\n(e.g., DRMM, KNRM, Conv-KNRM)\n(d) Late Interaction\n(i.e., the proposed ColBERT)\n(a) Representation-based Similarity\n(e.g., DSSM, SNRM)\nQuery\nDocument\ns\nQuery\nDocument\ns\nFigure 2: Schematic diagrams illustrating query–document matching paradigms in neural IR. e gure contrasts existing\napproaches (sub-gures (a), (b), and (c)) with the proposed late interaction paradigm (sub-gure (d)).\nOther methods, including ColBERT (full retrieval), directly retrieve\nthe top-1000 results from the entire collection.\nAs the gure shows, BERT considerably improves search preci-\nsion, raising MRR@10 by almost 7% against the best previous meth-\nods; simultaneously, it increases latency by up to tens of thousands\nof milliseconds even with a high-end GPU. is poses a challenging\ntradeo since raising query response times by as lile as 100ms is\nknown to impact user experience and even measurably diminish\nrevenue [17]. To tackle this problem, recent work has started ex-\nploring using Natural Language Understanding (NLU) techniques\nto augment traditional retrieval models like BM25 [32]. For exam-\nple, Nogueiraet al.[26,28] expand documents with NLU-generated\nqueries before indexing with BM25 scores and Dai & Callan [2] re-\nplace BM25’s term frequency with NLU-estimated term importance.\nDespite successfully reducing latency, these approaches generally\nreduce precision substantially relative to BERT.\nTo reconcile eciency and contextualization in IR, we propose\nColBERT, a ranking model based oncontextualizedlate interac-\ntion overBERT. As the name suggests, ColBERT proposes a novel\nlate interactionparadigm for estimating relevance between a query\nqand a documentd. Under late interaction,qanddare separately\nencoded into two sets of contextual embeddings, and relevance is\nevaluated using cheap andpruning-friendlycomputations between\nboth sets—that is, fast computations that enable ranking without\nexhaustively evaluating every possible candidate.\nFigure 2 contrasts our proposed late interaction approach with\nexisting neural matching paradigms. On the le, Figure 2 (a) illus-\ntratesrepresentation-focusedrankers, which independently compute\nan embedding forqand another fordand estimate relevance as\na single similarity score between two vectors [12,41]. Moving to\nthe right, Figure 2 (b) visualizes typicalinteraction-focusedrankers.\nInstead of summarizingqanddinto individual embeddings, these\nrankers model word- and phrase-level relationships acrossqandd\nand match them using a deep neural network (e.g., with CNNs/MLPs\n[22] or kernels [36]). In the simplest case, they feed the neural net-\nwork aninteraction matrixthat reects the similiarity between\nevery pair of words acrossqandd. Further right, Figure 2 (c) illus-\ntrates a more powerful interaction-based paradigm, which models\nthe interactions between wordswithinas well asacrossqanddat\nthe same time, as in BERT’s transformer architecture [25].\nese increasingly expressive architectures are in tension. While\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\nperior for IR tasks [8,21], a representation-focused model—by iso-\nlating the computations amongqandd—makes it possible to pre-\ncompute document representations oine [41], greatly reducing\nthe computational load per query. In this work, we observe that\nthe ne-grained matching of interaction-based models and the pre-\ncomputation of document representations of representation-based\nmodels can be combined by retaining yet judiciouslydelayingthe\nquery–document interaction. Figure 2 (d) illustrates an architec-\nture that precisely does so. As illustrated, every query embedding\ninteracts with all document embeddings via a MaxSim operator,\nwhich computes maximum similarity (e.g., cosine similarity), and\nthe scalar outputs of these operators are summed across query\nterms. is paradigm allows ColBERT to exploit deep LM-based\nrepresentations while shiing the cost of encoding documents of-\nine and amortizing the cost of encoding the query once across\nall ranked documents. Additionally, it enables ColBERT to lever-\nage vector-similarity search indexes (e.g., [1,15]) to retrieve the\ntop-kresults directly from a large document collection, substan-\ntially improvingrecallover models that only re-rank the output of\nterm-based retrieval.\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\nfew hundreds of milliseconds.   For instance,  when used for re-\nranking as in “ColBERT (re-rank)”, it delivers over 170×speedup\n(and requires 14,000×fewer FLOPs) relative to existing BERT-based\nmodels, while being more eective than every non-BERT baseline\n(§4.2 & 4.3).  ColBERT’s indexing—the only time it needs to feed\ndocuments through BERT—is also practical: it can index the MS\nMARCO collection of 9M passages in about 3 hours using a single\nserver with four GPUs (§4.5), retaining its eectiveness with a space\nfootprint of as lile as few tens of GiBs.  Our extensive ablation\nstudy (§4.4) shows that late interaction, its implementation via\nMaxSim operations, and crucial design choices within our BERT-\nbased encoders are all essential to ColBERT’s eectiveness.\nOur main contributions are as follows.\n(1)We proposelate interaction(§3.1) as a paradigm for ecient\nand eective neural ranking.\n(2)We present ColBERT (§3.2 & 3.3), a highly-eective model\nthat employs novel BERT-based query and document en-\ncoders within the late interaction paradigm.",
    "(3)We show how to leverage ColBERT both for re-ranking on\ntop of a term-based retrieval model (§3.5) and for searching\na full collection using vector similarity indexes (§3.6).\n(4)We evaluate ColBERT on MS MARCO and TREC CAR, two\nrecent passage search collections.\n2  RELATED WORK\nNeural Matching Models.Over the past few years, IR researchers\nhave introduced numerous neural architectures for ranking.  In\nthis work, we compare against KNRM [4,36], Duet [20,22], Con-\nvKNRM [4], and fastText+ConvKNRM [10].  KNRM proposes a\ndierentiable kernel-pooling technique for extracting matching\nsignals from an interaction matrix, while Duet combines signals\nfrom exact-match-based as well as embedding-based similarities\nfor ranking.  Introduced in 2018, ConvKNRM learns to matchn-\ngrams in the query and the document. Lastly, fastText+ConvKNRM\n(abbreviated f T+ConvKNRM) tackles the absence of rare words\nfrom typical word embeddings lists by adopting sub-word token\nembeddings.\nIn 2018, Zamaniet al.[41] introduced SNRM, a representation-\nfocused IR model that encodes each query and each document as\na single, sparse high-dimensional vector of “latent terms”. By pro-\nducing a sparse-vector representation for each document, SNRM\nis able to use a traditional IR inverted index for representing docu-\nments, allowing fast end-to-end retrieval. Despite highly promising\nresults and insights, SNRM’s eectiveness is substantially outper-\nformed by the state of the art on the datasets with which it was\nevaluated (e.g., see [18,38]). While SNRM employs sparsity to al-\nlow using inverted indexes, we relax this assumption and compare\na (dense) BERT-based representation-focused model against our\nlate-interaction ColBERT in our ablation experiments in§4.4. For a\ndetailed overview of existing neural ranking models, we refer the\nreaders to two recent surveys of the literature [8, 21].\nLanguage Model Pretraining for IR.Recent work in NLU\nemphasizes the importance pre-training language representation\nmodels in an unsupervised fashion before subsequently ne-tuning\nthem on downstream tasks. A notable example is BERT [5], a bi-\ndirectional transformer-based language model whose ne-tuning\nadvanced the state of the art on various NLU benchmarks. Nogueiraet\nal.[25], MacAvaneyet al.[18], and Dai & Callan [3] investigate\nincorporating such LMs (mainly BERT, but also ELMo [29]) on dif-\nferent ranking datasets. As illustrated in Figure 2 (c), the common\napproach (and the one adopted by Nogueiraet al.on MS MARCO\nand TREC CAR) is to feed the query–document pair through BERT\nand use an MLP on top of BERT’s [CLS] output token to produce a\nrelevance score. Subsequent work by Nogueiraet al.[27] introduced\nduoBERT, which ne-tunes BERT to compare the relevance of a\npairof documents given a query. Relative to their single-document\nBERT, this gives duoBERT a 1% MRR@10 advantage on MS MARCO\nwhile increasing the cost by at least 1.4×.\nBERT Optimizations.As  discussed  in§1,  these  LM-based\nrankers can be highly expensive in practice.  While ongoing ef-\nforts in the NLU literature for distilling [14,33], compressing [40],\nand pruning [19] BERT can be instrumental in narrowing this gap,\nQuery\nDocument\nQuery Encoder, f\nQ\nDocument Encoder, f\nD\nMaxSimMaxSimMaxSim\nscore\nOffline Indexing\nFigure 3: e general architecture of ColBERT given a query\nqand a documentd.\nthey generally achieve signicantly smaller speedups than our re-\ndesigned architecture for IR, due to their generic nature, and more\naggressive optimizations oen come at the cost of lower quality.\nEcient NLU-based Models.Recently, a direction emerged\nthat employs expensive NLU computation oine.  is includes\ndoc2query [28] and DeepCT [2].  e doc2query model expands\neach document with a pre-dened number of synthetic queries\nqueries generated by a seq2seq transformer model that is trained to\ngeneratequeriesgiven a document. It then relies on a BM25 index\nfor retrieval from the (expanded) documents. DeepCT uses BERT\nto produce theterm frequencycomponent of BM25 in a context-\naware manner, essentially representing a feasible realization of the\nterm-independence assumption with neural networks [23]. Lastly,\ndocTTTTTquery [26] is identical to doc2query except that it ne-\ntunes a pre-trained model (namely, T5 [31]) for generating the\npredicted queries.\nConcurrently with our draing of this paper, Hofst\n ̈\naeret al.[11]\npublished their Transformer-Kernel (TK) model. At a high level, TK\nimproves the KNRM architecture described earlier: while KNRM\nemploys kernel pooling on top of word-embedding-based inter-\naction, TK uses a Transformer [34] component for contextually\nencoding queries and documents before kernel pooling. TK estab-\nlishes a new state-of-the-art for non-BERT models on MS MARCO\n(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\nwhile ColBERT reaches up to 36%. Moreover, due to indexing docu-\nment representations oine and employing a MaxSim-based late\ninteraction mechanism, ColBERT is much more scalable, enabling\nend-to-end retrieval which is not supported by TK.\n3  COLBERT\nColBERT prescribes a simple framework for balancing the quality\nand cost of neural IR, particularly deep language models like BERT.\nAs introduced earlier, delaying the query–document interaction can\nfacilitate cheap neural re-ranking (i.e., through pre-computation)\nand even support practical end-to-end neural retrieval (i.e., through\npruning via vector-similarity search). ColBERT addresses how to\ndo so while still preserving the eectiveness of state-of-the-art\nmodels, which condition the bulk of their computations on the\njoint query–document pair.",
    "Even though ColBERT’s late-interaction framework can be ap-\nplied to a wide variety of architectures (e.g., CNNs, RNNs, trans-\nformers, etc.), we choose to focus this work on bi-directional transformer-\nbased encoders (i.e., BERT) owing to their state-of-the-art eective-\nness yet very high computational cost.\n3.1  Architecture\nFigure 3 depicts the general architecture of ColBERT, which com-\nprises: (a) a query encoderf\nQ\n, (b) a document encoderf\nD\n, and (c)\nthe late interaction mechanism. Given a queryqand documentd,\nf\nQ\nencodesqinto a bag of xed-size embeddingsE\nq\nwhilef\nD\nen-\ncodesdinto another bagE\nd\n. Crucially, each embeddings inE\nq\nand\nE\nd\niscontextualizedbased on the other terms inqord, respectively.\nWe describe our BERT-based encoders in§3.2.\nUsingE\nq\nandE\nd\n, ColBERT computes the relevance score be-\ntweenqanddvia late interaction, which we dene as a summation\nof maximum similarity (MaxSim) operators. In particular, we nd\nthe maximum cosine similarity of eachv∈E\nq\nwith vectors inE\nd\n,\nand combine the outputs via summation. Besides cosine, we also\nevaluate squared L2 distance as a measure of vector similarity. In-\ntuitively, this interaction mechanism solysearchesfor each query\ntermt\nq\n—in a manner that reects its context in the query—against\nthe document’s embeddings, quantifying the strength of the “match”\nvia the largest similarity score betweent\nq\nand a document termt\nd\n.\nGiven these term scores, it then estimates the document relevance\nby summing the matching evidence across all query terms.\nWhile more sophisticated matching is possible with other choices\nsuch as deep convolution and aention layers (i.e., as in typical\ninteraction-focused models), a summation of maximum similarity\ncomputations has two distinctive characteristics.  First, it stands\nout as a particularly cheap interaction mechanism, as we examine\nits FLOPs in§4.2.  Second, and more importantly, it is amenable\nto highly-ecient pruning for top-kretrieval, as we evaluate in\n§4.3. is enables using vector-similarity algorithms for skipping\ndocuments without materializing the full interaction matrix or even\nconsidering each document in isolation. Other cheap choices (e.g.,\na summation ofaveragesimilarity scores, instead of maximum) are\npossible; however, many are less amenable to pruning. In§4.4, we\nconduct an extensive ablation study that empirically veries the ad-\nvantage of our MaxSim-based late interaction against alternatives.\n3.2  ery & Document Encoders\nPrior to late interaction, ColBERT encodes each query or document\ninto a bag of embeddings, employing BERT-based encoders.  We\nshare a single BERT model among our query and document en-\ncoders but distinguish input sequences that correspond to queries\nand documents by prepending a special token[Q]to queries and\nanother token[D]to documents.\nery Encoder.Given a textual queryq, we tokenize it into its\nBERT-based WordPiece [35] tokensq\n1\nq\n2\n...q\nl\n. We prepend the token\n[Q]to the query. We place this token right aer BERT’s sequence-\nstart token[CLS]. If the query has fewer than a pre-dened number\nof tokensN\nq\n, we pad it with BERT’s special[mask]tokens up\nto lengthN\nq\n(otherwise, we truncate it to the rstN\nq\ntokens).\nis padded sequence of input tokens is then passed into BERT’s\ndeep transformer architecture, which computes a contextualized\nrepresentation of each token.\nWe denote the padding with masked tokens asquery augmen-\ntation, a step that allows BERT to produce query-based embeddings\nat the positions corresponding to these masks. ery augmentation\nis intended to serve as a so, dierentiable mechanism for learning\nto expand queries with new terms or to re-weigh existing terms\nbased on their importance for matching the query. As we show in\n§4.4, this operation is essential for ColBERT’s eectiveness.\nGiven BERT’s representation of each token, our encoder passes\nthe contextualized output representations through a linear layer\nwith no activations.  is layer serves to control the dimension\nof ColBERT’s embeddings, producingm-dimensional embeddings\nfor the layer’s output sizem.  As we discuss later in more detail,\nwe typically xmto be much smaller than BERT’s xed hidden\ndimension.\nWhile ColBERT’s embedding dimension has limited impact on\nthe eciency of query encoding, this step is crucial for controlling\nthe space footprint of documents, as we show in§4.5. In addition, it\ncan have a signicant impact on query execution time, particularly\nthe time taken for transferring the document representations onto\nthe GPU from system memory (where they reside before processing\na query).  In fact,  as we show in§4.2,  gathering,  stacking,  and\ntransferring the embeddings from CPU to GPU can be the most\nexpensive step in re-ranking with ColBERT. Finally, the output\nembeddings are normalized so each has L2 norm equal to one.\ne result is that the dot-product of any two embeddings becomes\nequivalent to their cosine similarity, falling in the[−1,1]range.\nDocument Encoder.Our document encoder has a very similar\narchitecture. We rst segment a documentdinto its constituent to-\nkensd\n1\nd\n2\n...d\nm\n, to which we prepend BERT’s start token[CLS]fol-\nlowed by our special token[D]that indicates a document sequence.\nUnlike queries, we do not append[mask]tokens to documents. Af-\nter passing this input sequence through BERT and the subsequent\nlinear layer, the document encoder lters out the embeddings corre-\nsponding to punctuation symbols, determined via a pre-dened list.\nis ltering is meant to reduce the number of embeddings per doc-\nument, as we hypothesize that (even contextualized) embeddings\nof punctuation are unnecessary for eectiveness.\nIn summary, givenq=q\n0\nq\n1\n...q\nl\nandd=d\n0\nd\n1\n...d\nn\n, we compute\nthe bags of embeddingsE\nq\nandE\nd\nin the following manner, where\n# refers to the[mask]tokens:\nE\nq\n:=Normalize(CNN(BERT(“[Q]q\n0\nq\n1\n...q\nl\n##...#”)))(1)\nE\nd\n:=Filter(Normalize(CNN(BERT(“[D]d\n0\nd\n1\n...d\nn\n”))))(2)\n3.3  Late Interaction\nGiven the representation of a queryqand a documentd, the rel-\nevance score ofdtoq, denoted asS\nq,d\n, is estimated via late in-\nteraction between their bags of contextualized embeddings.  As\nmentioned before, this is conducted as a sum of maximum sim-\nilarity computations, namely cosine similarity (implemented as\ndot-products due to the embedding normalization) or squared L2\ndistance.",
    "S\nq,d\n:=\n’\ni∈[|E\nq\n|]\nmax\nj∈[|E\nd\n|]\nE\nq\ni\n·E\nT\nd\nj\n(3)\nColBERT is dierentiable end-to-end. We ne-tune the BERT\nencoders and train from scratch the additional parameters (i.e., the\nlinear layer and the [Q] and [D] markers’ embeddings) using the\nAdam [16] optimizer. Notice that our interaction mechanism has\nno trainable parameters.  Given a triple〈q,d\n+\n,d\n−\n〉with queryq,\npositive documentd\n+\nand negative documentd\n−\n, ColBERT is used\nto produce a score for each document individually and is optimized\nvia pairwise somax cross-entropy loss over the computed scores\nofd\n+\nandd\n−\n.\n3.4  Oline Indexing: Computing & Storing\nDocument Embeddings\nBy design, ColBERT isolates almost all of the computations between\nqueries and documents, largely to enable pre-computing document\nrepresentations oine. At a high level, our indexing procedure is\nstraight-forward: we proceed over the documents in the collection\nin batches, running our document encoderf\nD\non each batch and\nstoring the output embeddings per document. Although indexing\na set of documents is an oine process,  we incorporate a few\nsimple optimizations for enhancing the throughput of indexing. As\nwe show in§4.5, these optimizations can considerably reduce the\noine cost of indexing.\nTo begin with, we exploit multiple GPUs, if available, for faster\nencoding of batches of documents in parallel. When batching, we\npad all documents to the maximum length of a documentwithin\nthe batch.\n3\nTo make capping the sequence length on a per-batch\nbasis more eective, our indexer proceeds through documents in\ngroups ofB(e.g.,B=100,000) documents. It sorts these documents\nby length and then feeds batches ofb(e.g.,b=128) documents of\ncomparable length through our encoder. is length-based bucket-\ning is sometimes refered to as aBucketIteratorin some libraries\n(e.g., allenNLP). Lastly, while most computations occur on the GPU,\nwe found that a non-trivial portion of the indexing time is spent on\npre-processing the text sequences, primarily BERT’s WordPiece to-\nkenization. Exploiting that these operations are independent across\ndocuments in a batch, we parallelize the pre-processing across the\navailable CPU cores.\nOnce the document representations are produced, they are saved\nto disk using 32-bit or 16-bit values to represent each dimension.\nAs we describe in§3.5 and 3.6, these representations are either\nsimply loaded from disk for ranking or are subsequently indexed\nfor vector-similarity search, respectively.\n3.5  Top-kRe-ranking with ColBERT\nRecall that ColBERT can be used for re-ranking the output of an-\nother retrieval model, typically a term-based model, or directly\nfor end-to-end retrieval from a document collection.  In this sec-\ntion, we discuss how we use ColBERT for ranking a small set of\nk(e.g.,k=1000) documents given a queryq. Sincekis small, we\nrely on batch computations to exhaustively score each document\n3\ne public BERT implementations we saw simply pad to a pre-dened length.\n(unlike our approach in§3.6). To begin with, our query serving sub-\nsystem loads the indexed documents representations into memory,\nrepresenting each document as a matrix of embeddings.\nGiven a queryq, we compute its bag of contextualized embed-\ndingsE\nq\n(Equation 1) and, concurrently, gather the document repre-\nsentations into a 3-dimensional tensorDconsisting ofkdocument\nmatrices.  We pad thekdocuments to their maximum length to\nfacilitate batched operations, and move the tensorDto the GPU’s\nmemory. On the GPU, we compute a batch dot-product ofE\nq\nand\nD, possibly over multiple mini-batches. e output materializes a\n3-dimensional tensor that is a collection of cross-match matrices\nbetweenqand each document. To compute the score of each docu-\nment, we reduce its matrix across document terms via a max-pool\n(i.e., representing an exhaustive implementation of our MaxSim\ncomputation) and reduce across query terms via a summation. Fi-\nnally, we sort thekdocuments by their total scores.\nRelative to existing neural rankers (especially, but not exclu-\nsively, BERT-based ones), this computation is very cheap that, in\nfact, its cost is dominated by the cost of gathering and transferring\nthe pre-computed embeddings. To illustrate, rankingkdocuments\nvia typical BERT rankers requires feeding BERTkdierent inputs\neach of lengthl=|q|+|d\ni\n|for queryqand documentsd\ni\n, where\naention has quadratic cost in the length of the sequence. In con-\ntrast, ColBERT feeds BERT only a single, much shorter sequence of\nlengthl=|q|. Consequently, ColBERT is not only cheaper, it also\nscales much beer withkas we examine in§4.2.\n3.6  End-to-end Top-kRetrieval with ColBERT\nAs mentioned before, ColBERT’s late-interaction operator is speci-\ncally designed to enable end-to-end retrieval from a large collection,\nlargely to improve recall relative to term-based retrieval approaches.\nis section is concerned with cases where the number of docu-\nments to be ranked is too large for exhaustive evaluation of each\npossible candidate document, particularly when we are only inter-\nested in the highest scoring ones.  Concretely, we focus here on\nretrieving the top-kresults directly from a large document collec-\ntion withN(e.g.,N=10,000,000) documents, wherek\u001cN.\nTo do so, we leverage the pruning-friendly nature of the MaxSim\noperations at the backbone of late interaction.  Instead of apply-\ning MaxSim between one of the query embeddings and all of one\ndocument’s embeddings, we can use fast vector-similarity data\nstructures to eciently conduct this search between the query\nembedding andalldocument embeddings across the full collec-\ntion.  For this, we employ an o-the-shelf library for large-scale\nvector-similarity search, namelyfaiss[15] from Facebook.\n4\nIn par-\nticular, at the end of oine indexing (§3.4), we maintain a mapping\nfrom each embedding to its document of origin and then index all\ndocument embeddings intofaiss.\nSubsequently, when serving queries, we use a two-stage pro-\ncedure to retrieve the top-kdocuments from the entire collection.\nBoth stages rely on ColBERT’s scoring: the rst is an approximate\nstage aimed at ltering while the second is a renement stage. For\nthe rst stage, we concurrently issueN\nq\nvector-similarity queries\n(corresponding to each of the embeddings inE\nq\n) onto ourfaissin-\ndex. is retrieves the top-k\n′\n(e.g.,k\n′\n=k/2) matches for that vector\n4\nhps://github.com/facebookresearch/faiss",
    "over all document embeddings. We map each of those to its docu-\nment of origin, producingN\nq\n×k\n′\ndocument IDs, onlyK≤N\nq\n×k\n′\nof which are unique. eseKdocuments likely contain one or more\nembeddings that are highly similar to the query embeddings. For\nthe second stage, we rene this set by exhaustively re-rankingonly\nthoseKdocuments in the usual manner described in§3.5.\nIn ourfaiss-based implementation, we use anIVFPQindex (“in-\nverted le with product quantization”). is index partitions the\nembedding space intoP(e.g.,P=1000) cells based onk-means clus-\ntering and then assigns each document embedding to its nearest cell\nbased on the selected vector-similarity metric. For serving queries,\nwhen searching for the top-k\n′\nmatches for a single query embed-\nding, only the nearestp(e.g.,p=10) partitions are searched. To\nimprove memory eciency, every embedding is divided intos(e.g.,\ns=16) sub-vectors, each represented using one byte.  Moreover,\nthe index conducts the similarity computations in this compressed\ndomain, leading to cheaper computations and thus faster search.\n4  EXPERIMENTAL EVALUATION\nWe now turn our aention to empirically testing ColBERT, address-\ning the following research questions.\nRQ\n1\n: In a typical re-ranking setup, how well can ColBERT bridge\nthe existing gap (highlighted in§1) between highly-ecient and\nhighly-eective neural models? (§4.2)\nRQ\n2\n: Beyond re-ranking, can ColBERT eectively support end-\nto-end retrieval directly from a large collection? (§4.3)\nRQ\n3\n: What does each component of ColBERT (e.g., late interac-\ntion, query augmentation) contribute to its quality? (§4.4)\nRQ\n4\n:  What are ColBERT’s indexing-related costs in terms of\noine computation and memory overhead? (§4.5)\n4.1  Methodology\n4.1.1  Datasets & Metrics.Similar to related work [2,27,28],\nwe conduct our experiments on the MS MARCO Ranking [24]\n(henceforth, MS MARCO) and TREC Complex Answer Retrieval\n(TREC-CAR) [6] datasets.  Both of these recent datasets provide\nlarge training data of the scale that facilitates training and evaluat-\ning deep neural networks. We describe both in detail below.\nMS MARCO.MS MARCO is a dataset (and a corresponding\ncompetition) introduced by Microso in 2016 for reading compre-\nhension and adapted in 2018 for retrieval. It is a collection of 8.8M\npassages from Web pages, which were gathered from Bing’s results\nto 1M real-world queries.  Each query is associated withsparse\nrelevance judgements of one (or very few) documents marked as\nrelevant and no documents explicitly indicated as irrelevant. Per\nthe ocial evaluation, we use MRR@10 to measure eectiveness.\nWe use three sets of queries for evaluation. e ocial devel-\nopment and evaluation sets contain roughly 7k queries. However,\nthe relevance judgements of the evaluation set are held-out by Mi-\ncroso and eectiveness results can only be obtained by submiing\nto the competition’s organizers. We submied our main re-ranking\nColBERT model for the results in§4.2. In addition, the collection\nincludes roughly 55k queries (with labels) that are provided as ad-\nditional validation data.  We re-purpose a random sample of 5k\nqueries among those (i.e., ones not in our development or training\nsets) as a “local” evaluation set.  Along with the ocial develop-\nment set, we use this held-out set for testing our models as well as\nbaselines in§4.3. We do so to avoid submiing multiple variants\nof the same model at once, as the organizers discourage too many\nsubmissions by the same team.\nTREC CAR.Introduced by Dietz [6]et al.in 2017, TREC CAR\nis a synthetic dataset based on Wikipedia that consists of about\n29M passages. Similar to related work [25], we use the rst four of\nve pre-dened folds for training and the h for validation. is\namounts to roughly 3M queries generated by concatenating the\ntitle of a Wikipedia page with the heading of one of its sections.\nat section’s passages are marked as relevant to the corresponding\nquery. Our evaluation is conducted on the test set used in TREC\n2017 CAR, which contains 2,254 queries.\n4.1.2  Implementation.Our ColBERT models are implemented\nusing Python 3 and PyTorch 1. We use the populartransformers\n5\nlibrary for the pre-trained BERT model. Similar to [25], we ne-tune\nall ColBERT models with learning rate 3×10\n−6\nwith a batch size\n32. We x the number of embeddings per query atN\nq\n=32. We set\nour ColBERT embedding dimensionmto be 128;§4.5 demonstrates\nColBERT’s robustness to a wide range of embedding dimensions.\nFor MS MARCO, we initialize the BERT components of the Col-\nBERT query and document encoders using Google’s ocial pre-\ntrained BERT\nbase\nmodel. Further, we train all models for 200k itera-\ntions. For TREC CAR, we follow related work [2,25] and use a dif-\nferent pre-trained model to the ocial ones. To explain, the ocial\nBERT models were pre-trained on Wikipedia, which is the source\nof TREC CAR’s training and test sets. To avoid leaking test data\ninto train, Nogueira and Cho’s [25] pre-train a randomly-initialized\nBERT model on the Wiki pages corresponding to training subset of\nTREC CAR. ey release their BERT\nlarge\npre-trained model, which\nwe ne-tune for ColBERT’s experiments on TREC CAR. Since ne-\ntuning this model is signicantly slower than BERT\nbase\n, we train\non TREC CAR for only 125k iterations.\nIn our re-ranking results, unless stated otherwise, we use 4 bytes\nper dimension in our embeddings and employ cosine as our vector-\nsimilarity function. For end-to-end ranking, we use (squared) L2\ndistance, as we found ourfaissindex was faster at L2-based re-\ntrieval.  For ourfaissindex, we set the number of partitions to\nP=2,000, and search the nearestp=10 to each query embedding to\nretrievek\n′\n=k=1000 document vectors per query embedding. We\ndivide each embedding intos=16 sub-vectors, each encoded using\none byte. To represent the index used for the second stage of our\nend-to-end retrieval procedure, we use 16-bit values per dimension.\n4.1.3  Hardware & Time Measurements.To evaluate the latency\nof neural re-ranking models in§4.2, we use a single Tesla V100 GPU\nthat has 32 GiBs of memory on a server with two Intel Xeon Gold\n6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\nGiBs of RAM. For the mostly CPU-based retrieval experiments in\n§4.3 and the indexing experiments in§4.5, we use another server\nwith the same CPU and system memory specications but which\nhas four Titan V GPUs aached, each with 12 GiBs of memory.\nAcross all experiments, only one GPU is dedicated per query for\n5\nhps://github.com/huggingface/transformers",
    "MethodMRR@10 (Dev)  MRR@10 (Eval)  Re-ranking Latency (ms)   FLOPs/query\nBM25 (ocial)16.716.5--\nKNRM19.819.83592M (0.085×)\nDuet24.324.522159B (23×)\nfastText+ConvKNRM29.027.72878B (11×)\nBERT\nbase\n[25]34.7-10,70097T (13,900×)\nBERT\nbase\n(our training)36.0-10,70097T (13,900×)\nBERT\nlarge\n[25]36.535.932,900340T (48,600×)\nColBERT (over BERT\nbase\n)34.934.9617B (1×)\nTable 1: “Re-ranking” results on MS MARCO. Each neural model re-ranks the ocial top-1000 results produced by BM25.\nLatency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\nMethodMRR@10 (Dev)  MRR@10 (Local Eval)  Latency (ms)  Recall@50  Recall@200  Recall@1000\nBM25 (ocial)16.7----81.4\nBM25 (Anserini)18.719.56259.273.885.7\ndoc2query21.522.88564.477.989.1\nDeepCT24.3-62(est.)69 [2]82 [2]91 [2]\ndocTTTTTquery27.728.48775.686.994.7\nColBERT\nL2\n(re-rank)34.836.4-75.380.581.4\nColBERT\nL2\n(end-to-end)36.036.745882.992.396.8\nTable 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per querydirectlyfrom the\nentire 8.8M document collection.\nretrieval (i.e., for methods with neural computations) but we use\nup to all four GPUs during indexing.\n4.2  ality–Cost Tradeo: Top-kRe-ranking\nIn this section, we examine ColBERT’s eciency and eectiveness\nat re-ranking the top-kresults extracted by a bag-of-words retrieval\nmodel, which is the most typical seing for testing and deploying\nneural ranking models. We begin with the MS MARCO dataset. We\ncompare against KNRM, Duet, and fastText+ConvKNRM, a repre-\nsentative set of neural matching models that have been previously\ntested on MS MARCO. In addition, we compare against the nat-\nural adaptation of BERT for ranking by Nogueira and Cho [25],\nin particular, BERT\nbase\nand its deeper counterpart BERT\nlarge\n. We\nalso report results for “BERT\nbase\n(our training)”, which is based on\nNogueira and Cho’s base model (including hyperparameters) but\nis trained with the same loss function as ColBERT (§3.3) for 200k\niterations, allowing for a more direct comparison of the results.\nWe report the competition’s ocial metric, namely MRR@10,\non the validation set (Dev) and the evaluation set (Eval). We also\nreport the re-ranking latency, which we measure using a single\nTesla V100 GPU, and the FLOPs per query for each neural ranking\nmodel.  For ColBERT, our reported latency subsumes the entire\ncomputation from gathering the document representations, moving\nthem to the GPU, tokenizing then encoding the query, and applying\nlate interaction to compute document scores.  For the baselines,\nwe measure the scoring computations on the GPU and exclude\nthe CPU-based text preprocessing (similar to [9]).  In principle,\nthe baselines can pre-compute the majority of this preprocessing\n(e.g., document tokenization) oine and parallelize the rest across\ndocuments online, leaving only a negligible cost. We estimate the\nFLOPs per query of each model using the torchprole\n6\nlibrary.\nWe now proceed to study the results, which are reported in Ta-\nble 1.  To begin with, we notice the fast progress from KNRM in\n2017 to the BERT-based models in 2019, manifesting itself in over\n16% increase in MRR@10.  As described in§1, the simultaneous\nincrease in computational cost is dicult to miss. Judging by their\nrather monotonic paern of increasingly larger cost and higher ef-\nfectiveness, these results appear to paint a picture where expensive\nmodels are necessary for high-quality ranking.\nIn contrast with this trend, ColBERT (which employs late inter-\naction over BERT\nbase\n) performs no worse than the original adap-\ntation of BERT\nbase\nfor ranking by Nogueira and Cho [25,27] and\nis only marginally less eective than BERT\nlarge\nand our training\nof BERT\nbase\n(described above). While highly competitive in eec-\ntiveness, ColBERT is orders of magnitude cheaper than BERT\nbase\n,\nin particular, by over 170×in latency and 13,900×in FLOPs. is\nhighlights the expressiveness of our proposed late interaction mech-\nanism, particularly when coupled with a powerful pre-trained LM\nlike BERT. While ColBERT’s re-ranking latency is slightly higher\nthan the non-BERT re-ranking models shown (i.e., by 10s of mil-\nliseconds), this dierence is explained by the time it takes to gather,\nstack, and transfer the document embeddings to the GPU. In partic-\nular, the query encoding and interaction in ColBERT consume only\n13 milliseconds of its total execution time. We note that ColBERT’s\nlatency and FLOPs can be considerably reduced by padding queries\nto a shorter length, using smaller vector dimensions (the MRR@10\nof which is tested in§4.5), employing quantization of the document\n6\nhps://github.com/mit-han-lab/torchprole",
    "vectors, and storing the embeddings on GPU if sucient memory\nexists. We leave these directions for future work.\n0.270.290.310.330.350.37\nMRR@10\n10\n3\n10\n4\n10\n5\n10\n6\n10\n7\n10\n8\n10\n9\nMillion FLOPs (log-scale)\nk=10\n20\n50\n100\n200\n500\n1000\n2000\nk=10\n20\n50\n100\n200\n500\n1000\n2000\nBERT\nbase\n (our training)\nColBERT\nFigure 4: FLOPs (in millions) and MRR@10 as functions\nof the re-ranking depthk. Since the ocial BM25 ranking\nis not ordered, the initial top-kretrieval is conducted with\nAnserini’s BM25.\nDiving deeper into the quality–cost tradeo between BERT and\nColBERT, Figure 4 demonstrates the relationships between FLOPs\nand eectiveness (MRR@10) as a function of the re-ranking depth\nkwhen re-ranking the top-kresults by BM25, comparing ColBERT\nand BERT\nbase\n(our training). We conduct this experiment on MS\nMARCO (Dev). We note here that as the ocial top-1000 ranking\ndoes not provide the BM25 order (and also lacks documents beyond\nthe top-1000 per query), the models in this experiment re-rank the\nAnserini [37] toolkit’s BM25 output. Consequently, both MRR@10\nvalues atk=1000 are slightly higher from those reported in Table 1.\nStudying the results in Figure 4, we notice that not only is Col-\nBERT much cheaper than BERT for the same model size (i.e., 12-\nlayer “base” transformer encoder), it also scales beer with the\nnumber of ranked documents.  In part, this is because ColBERT\nonly needs to process the query once, irrespective of the number of\ndocuments evaluated. For instance, atk=10, BERT requires nearly\n180×more FLOPs than ColBERT; atk=1000, BERT’s overhead\njumps to 13,900×. It then reaches 23,000×atk=2000. In fact, our\ninformal experimentation shows that this orders-of-magnitude gap\nin FLOPs makes it practical to run ColBERT entirely on the CPU,\nalthough CPU-based re-ranking lies outside our scope.\nMethodMAPMRR@10\nBM25 (Anserini)15.3-\ndoc2query18.1-\nDeepCT24.633.2\nBM25 + BERT\nbase\n31.0-\nBM25 + BERT\nlarge\n33.5-\nBM25 + ColBERT31.344.3\nTable 3: Results on TREC CAR.\nHaving studied our results on MS MARCO, we now consider\nTREC CAR, whose ocial metric is MAP. Results are summarized\nin Table 3, which includes a number of important baselines (BM25,\ndoc2query, and DeepCT) in addition to re-ranking baselines that\nhave been tested on this dataset. ese results directly mirror those\nwith MS MARCO.\n4.3  End-to-end Top-kRetrieval\nBeyond cheap re-ranking, ColBERT is amenable to top-kretrieval di-\nrectly from a full collection. Table 2 considers full retrieval, wherein\neach model retrieves the top-1000 documents directly from MS\nMARCO’s 8.8M documents per query. In addition to MRR@10 and\nlatency in milliseconds, the table reports Recall@50, Recall@200,\nand Recall@1000, important metrics for a full-retrieval model that\nessentially lters down a large collection on a per-query basis.\nWe compare against BM25, in particular MS MARCO’s ocial\nBM25 ranking as well as a well-tuned baseline based on the Anserini\ntoolkit.\n7\nWhile many other traditional models exist, we are not\naware of any that substantially outperform Anserini’s BM25 im-\nplementation (e.g., see RM3 in [28], LMDir in [2], or Microso’s\nproprietary feature-based RankSVM on the leaderboard).\nWe also compare against doc2query, DeepCT, and docTTTT-\nTquery.  All three rely on a traditional bag-of-words model (pri-\nmarily BM25) for retrieval. Crucially, however, they re-weigh the\nfrequency of terms per document and/or expand the set of terms\nin each document before building the BM25 index. In particular,\ndoc2query expands each document with a pre-dened number\nof synthetic queries generated by a seq2seq transformer model\n(which docTTTTquery replaced with a pre-trained language model,\nT5 [31]). In contrast, DeepCT uses BERT to produce the term fre-\nquency component of BM25 in a context-aware manner.\nFor the latency of Anserini’s BM25, doc2query, and docTTTT-\nquery, we use the authors’ [26,28] Anserini-based implementation.\nWhile this implementation supports multi-threading, it only utilizes\nparallelism across dierent queries. We thus report single-threaded\nlatency for these models,  noting that simply parallelizing their\ncomputation overshardsof the index can substantially decrease\ntheir already-low latency. For DeepCT, we only estimate its latency\nusing that of BM25 (as denoted by(est.)in the table), since DeepCT\nre-weighs BM25’s term frequency without modifying the index\notherwise.\n8\nAs discussed in§4.1, we use ColBERT\nL2\nfor end-to-\nend retrieval, which employs negative squared L2 distance as its\nvector-similarity function. For its latency, we measure the time for\nfaiss-based candidate ltering and the subsequent re-ranking. In\nthis experiment,faissuses all available CPU cores.\nLooking at Table 2, we rst see Anserini’s BM25 baseline at 18.7\nMRR@10, noticing its very low latency as implemented in Anserini\n(which extends the well-known Lucene system), owing to both\nvery cheap operations and decades of bag-of-words top-kretrieval\noptimizations. e three subsequent baselines, namely doc2query,\nDeepCT, and docTTTTquery, each brings a decisive enhancement\nto eectiveness. ese improvements come at negligible overheads\nin latency, since these baselines ultimately rely on BM25-based\nretrieval.  e most eective among these three, docTTTTquery,\ndemonstrates a massive 9% gain over vanilla BM25 by ne-tuning\nthe recent language model T5.\n7\nhp://anserini.io/\n8\nIn practice, a myriad of reasons could still cause DeepCT’s latency to dier\nslightly from BM25’s. For instance, the top-kpruning strategy employed, if any, could\ninteract dierently with a changed distribution of scores.",
    "Shiing our aention to ColBERT’s end-to-end retrieval eec-\ntiveness, we see its major gains in MRR@10 over all of these end-to-\nend models. In fact, using ColBERT in the end-to-end setup is supe-\nrior in terms of MRR@10 to re-ranking with the same model due\nto the improved recall. Moving beyond MRR@10, we also see large\ngains in Recall@kforkequals to 50, 200, and 1000. For instance,\nits Recall@50 actually exceeds the ocial BM25’s Recall@1000 and\neven all but docTTTTTquery’s Recall@200, emphasizing the value\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\n4.4  Ablation Studies\n0.220.240.260.280.300.320.340.36\nMRR@10\nBERT [CLS]-based dot-product (5-layer)  [A]\nColBERT via average similarity (5-layer)  [B]\nColBERT without query augmentation (5-layer)  [C]\nColBERT (5-layer)  [D]\nColBERT (12-layer)  [E]\nColBERT + e2e retrieval (12-layer)  [F]\nFigure 5: Ablation results on MS MARCO (Dev). Between\nbrackets is the number of BERT layers used in each model.\ne results from§4.2 indicate that ColBERT is highly eective\ndespite the low cost and simplicity of its late interaction mechanism.\nTo beer understand the source of this eectiveness, we examine a\nnumber of important details in ColBERT’s interaction and encoder\narchitecture. For this ablation, we report MRR@10 on the validation\nset of MS MARCO in Figure 5, which shows our mainre-ranking\nColBERT model [E], with MRR@10 of 34.9%.\nDue to the cost of training all models, we train a copy of our\nmain model that retains only the rst 5 layers of BERT out of 12\n(i.e., model [D]) and similarly train all our ablation models for 200k\niterations with ve BERT layers. To begin with, we ask if the ne-\ngranularinteractionin late interaction is necessary.  Model [A]\ntackles this question: it uses BERT to produce a single embedding\nvector for the query and another for the document, extracted from\nBERT’s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equalsN\nq\n×128=32×128).\nRelevance is estimated as the inner product of the query’s and the\ndocument’s embeddings, which we found to perform beer than\ncosine similarity for single-vector re-ranking. As the results show,\nthis model is considerably less eective than ColBERT, reinforcing\nthe importance of late interaction.\nSubsequently, we ask if our MaxSim-based late interaction is bet-\nter than other simple alternatives. We test a model [B] that replaces\nColBERT’s maximum similarity withaveragesimilarity. e results\nsuggest the importance of individual terms in the query paying\nspecial aention to particular terms in the document.  Similarly,\nthe gure emphasizes the importance of our query augmentation\nmechanism: without query augmentation [C], ColBERT has a no-\nticeably lower MRR@10. Lastly, we see the impact of end-to-end\nretrieval not only on recall but also on MRR@10.  By retrieving\ndirectly from the full collection, ColBERT is able to retrieve to the\ntop-10 documents missed entirely from BM25’s top-1000.\n01000020000300004000050000\nThroughput (documents/minute)\nBasic ColBERT Indexing\n+multi-GPU document processing\n+per-batch maximum sequence length\n+length-based bucketing\n+multi-core pre-processing\nFigure 6: Eect of ColBERT’s indexing optimizations on the\noline indexing throughput.\n4.5  Indexing roughput & Footprint\nLastly, we examine the indexing throughput and space footprint\nof ColBERT. Figure 6 reports indexing throughput on MS MARCO\ndocuments with ColBERT and four other ablation seings, which\nindividually enable optimizations described in§3.4 on top of basic\nbatched indexing. Based on these throughputs, ColBERT can index\nMS MARCO in about three hours. Note that any BERT-based model\nmust incur the computational cost of processing each document\nat least once. While ColBERT encodes each document with BERT\nexactly once, existing BERT-based rankers would repeat similar\ncomputations on possibly hundreds of documents for each query.\nSeingDimension(m)Bytes/DimSpace(GiBs)MRR@10\nRe-rank Cosine128428634.9\nEnd-to-end L2128215436.0\nRe-rank L2128214334.8\nRe-rank Cosine4845434.4\nRe-rank Cosine2422733.9\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\nTable 4 reports the space footprint of ColBERT under various\nseings as we reduce the embeddings dimension and/or the bytes\nper dimension. Interestingly, the most space-ecient seing, that\nis, re-ranking with cosine similarity with 24-dimensional vectors\nstored as 2-byte oats, is only 1% worse in MRR@10 than the most\nspace-consuming one, while the former requires only 27 GiBs to\nrepresent the MS MARCO collection.\n5  CONCLUSIONS\nIn this paper, we introduced ColBERT, a novel ranking model that\nemployscontextualized late interactionover deep LMs (in particular,\nBERT) for ecient retrieval. By independently encoding queries\nand documents into ne-grained representations that interact via\ncheap and pruning-friendly computations, ColBERT can leverage\nthe expressiveness of deep LMs while greatly speeding up query\nprocessing. In addition, doing so allows using ColBERT for end-to-\nend neural retrieval directly from a large document collection. Our\nresults show that ColBERT is more than 170×faster and requires\n14,000×fewer FLOPs/query than existing BERT-based models, all\nwhile only minimally impacting quality and while outperforming\nevery non-BERT baseline.\nAcknowledgments.OK was supported by the Eltoukhy Family\nGraduate Fellowship at the Stanford School of Engineering. is\nresearch was supported in part by aliate members and other\nsupporters of the Stanford DAWN project—Ant Financial, Facebook,\nGoogle, Infosys, NEC, and VMware—as well as Cisco, SAP, and the",
    "NSF under CAREER grant CNS-1651570. Any opinions, ndings,\nand conclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reect the views of the\nNational Science Foundation.\nREFERENCES\n[1]Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\nto Index: Optimizing Exact Maximum Inner Product Search. In2019 IEEE 35th\nInternational Conference on Data Engineering (ICDE). IEEE, 1250–1261.\n[2]\nZhuyun Dai and Jamie Callan. 2019.  Context-Aware Sentence/Passage Term\nImportance Estimation For First Stage Retrieval.arXiv preprint arXiv:1910.10687\n(2019).\n[3]Zhuyun Dai and Jamie Callan. 2019.  Deeper Text Understanding for IR with\nContextual Neural Language Modeling.arXiv preprint arXiv:1905.09217(2019).\n[4]Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\nneural networks for so-matching n-grams in ad-hoc search. InProceedings of the\neleventh ACM international conference on web search and data mining. 126–134.\n[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805(2018).\n[6]Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017.  TREC\nComplex Answer Retrieval Overview.. InTREC.\n[7]\nJiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro. 2016. A deep relevance\nmatching model for ad-hoc retrieval. InProceedings of the 25th ACM International\non Conference on Information and Knowledge Management. ACM, 55–64.\n[8]Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani,\nChen Wu, W Bruce Cro, and Xueqi Cheng. 2019.  A deep look into neural\nranking models for information retrieval.arXiv preprint arXiv:1903.06902(2019).\n[9]Sebastian Hofst\n ̈\naer and Allan Hanbury. 2019. Let’s measure run time! Extending\nthe IR replicability infrastructure to include performance aspects.arXiv preprint\narXiv:1907.04614(2019).\n[10]\nSebastian Hofst\n ̈\naer, Navid Rekabsaz, Carsten Eickho, and Allan Hanbury.\n2019. On the eect of low-frequency terms on neural-IR models. InProceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval. 1137–1140.\n[11]\nSebastian Hofst\n ̈\naer, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\nTREC Deep Learning’19–Simple Contextualization for Re-ranking.arXiv preprint\narXiv:1912.01385(2019).\n[12]Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\nHeck. 2013.  Learning deep structured semantic models for web search using\nclickthrough data. InProceedings of the 22nd ACM international conference on\nInformation & Knowledge Management. 2333–2338.\n[13]\nShiyu Ji, Jinjin Shao, and Tao Yang. 2019.  Ecient Interaction-based Neural\nRanking with Locality Sensitive Hashing. Ine World Wide Web Conference.\nACM, 2858–2864.\n[14]\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand n Liu. 2019. Tinybert: Distilling bert for natural language understanding.\narXiv preprint arXiv:1909.10351(2019).\n[15]Je Johnson, Mahijs Douze, and Herv\n ́\ne J\n ́\negou. 2017.  Billion-scale similarity\nsearch with GPUs.arXiv preprint arXiv:1702.08734(2017).\n[16]\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization.arXiv preprint arXiv:1412.6980(2014).\n[17]\nRon Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\n2013. Online controlled experiments at large scale. InSIGKDD.\n[18]\nSean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\nContextualized embeddings for document ranking. InProceedings of the 42nd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. ACM, 1101–1104.\n[19]Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\nBeer than One?. InAdvances in Neural Information Processing Systems. 14014–\n14024.\n[20]Bhaskar Mitra and Nick Craswell. 2019.  An Updated Duet Model for Passage\nRe-ranking.arXiv preprint arXiv:1903.07666(2019).\n[21]Bhaskar Mitra, Nick Craswell, et al.2018. An introduction to neural information\nretrieval.Foundations and Trends®in Information Retrieval13, 1 (2018), 1–126.\n[22]\nBhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\nlocal and distributed representations of text for web search. InProceedings of\nthe 26th International Conference on World Wide Web. International World Wide\nWeb Conferences Steering Commiee, 1291–1299.\n[23]\nBhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\nfor ecient retrieval and ranking using deep neural networks.arXiv preprint\narXiv:1907.03693(2019).\n[24]Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016.   MS MARCO: A Human-Generated MAchine\nReading COmprehension Dataset. (2016).\n[25]Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\narXiv preprint arXiv:1901.04085(2019).\n[26]\nRodrigo Nogueira, Jimmy Lin,  and AI Epistemic. 2019.   From doc2query to\ndocTTTTTquery. (2019).\n[27]Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\nDocument Ranking with BERT.arXiv preprint arXiv:1910.14424(2019).\n[28]\nRodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nExpansion by ery Prediction.arXiv preprint arXiv:1904.08375(2019).\n[29]Mahew E Peters, Mark Neumann, Mohit Iyyer, Ma Gardner, Christopher\nClark, Kenton Lee, and Luke Zelemoyer. 2018.   Deep contextualized word\nrepresentations.arXiv preprint arXiv:1802.05365(2018).\n[30]\nYifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019.   Under-\nstanding the Behaviors of BERT in Ranking.arXiv preprint arXiv:1904.07531\n(2019).\n[31]Colin Rael,  Noam Shazeer,  Adam Roberts,  Katherine Lee,  Sharan Narang,\nMichael Matena,  Yanqi Zhou,  Wei Li,  and Peter J Liu. 2019.   Exploring the\nlimits of transfer learning with a unied text-to-text transformer.arXiv preprint\narXiv:1910.10683(2019).\n[32]Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al. 1995. Okapi at TREC-3.NIST Special Publication(1995).\n[33]\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\n2019. Distilling task-specic knowledge from BERT into simple neural networks.\narXiv preprint arXiv:1903.12136(2019).\n[34]Ashish Vaswani,  Noam Shazeer,  Niki Parmar,  Jakob Uszkoreit,  Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.   Aention is all\nyou need. InAdvances in neural information processing systems. 5998–6008.\n[35]Yonghui Wu, Mike Schuster, Zhifeng Chen, oc V Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016.  Google’s neural machine translation system: Bridging the gap between\nhuman and machine translation.arXiv preprint arXiv:1609.08144(2016).\n[36]\nChenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.\n2017.  End-to-end neural ad-hoc ranking with kernel pooling. InProceedings\nof the 40th International ACM SIGIR conference on research and development in\ninformation retrieval. 55–64.\n[37]Peilin Yang, Hui Fang, and Jimmy Lin. 2018.  Anserini: Reproducible ranking\nbaselines using Lucene.Journal of Data and Information ality (JDIQ)10, 4\n(2018), 1–20.\n[38]Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019.  Critically Examining\nthe” Neural Hype” Weak Baselines and the Additivity of Eectiveness Gains\nfrom Neural Ranking Models. InProceedings of the 42nd International ACM SIGIR\nConference on Research and Development in Information Retrieval. 1129–1132.\n[39]Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\nCross-domain modeling of sentence-level evidence for document retrieval. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). 3481–3487.\n[40]Or Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019.  Q8bert:\nantized 8bit bert.arXiv preprint arXiv:1910.06188(2019).\n[41]Hamed Zamani, Mostafa Dehghani, W Bruce Cro, Erik Learned-Miller, and\nJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\nrepresentation for inverted indexing. InProceedings of the 27th ACM International\nConference on Information and Knowledge Management. ACM, 497–506.\n[42]Le Zhao. 2012.Modeling and solving term mismatch for full-text retrieval. Ph.D.\nDissertation. Carnegie Mellon University."
  ]
}