{
  "key": "UXIQ4RPA",
  "url": "http://arxiv.org/pdf/2309.08872",
  "metadata": {
    "title": "PDFTriage: Question Answering over Long, Structured Documents",
    "abstract": "  Large Language Models (LLMs) have issues with document question answering\n(QA) in situations where the document is unable to fit in the small context\nlength of an LLM. To overcome this issue, most existing works focus on\nretrieving the relevant context from the document, representing them as plain\ntext. However, documents such as PDFs, web pages, and presentations are\nnaturally structured with different pages, tables, sections, and so on.\nRepresenting such structured documents as plain text is incongruous with the\nuser's mental model of these documents with rich structure. When a system has\nto query the document for context, this incongruity is brought to the fore, and\nseemingly trivial questions can trip up the QA system. To bridge this\nfundamental gap in handling structured documents, we propose an approach called\nPDFTriage that enables models to retrieve the context based on either structure\nor content. Our experiments demonstrate the effectiveness of the proposed\nPDFTriage-augmented models across several classes of questions where existing\nretrieval-augmented LLMs fail. To facilitate further research on this\nfundamental problem, we release our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured documents from 10 different\ncategories of question types for document QA. Our code and datasets will be\nreleased soon on Github.\n",
    "published": "2023-09-16T04:29:05Z"
  },
  "text": [
    "PDFTriage: Question Answering over Long, Structured Documents\nJon Saad-Falcon\nStanford University\njonsaadfalcon@stanford.edu\nJoe Barrow\nAdobe Research\njbarrow@adobe.com\nAlexa Siu\nAdobe Research\nasiu@adobe.com\nAni Nenkova\nAdobe Research\nnenkova@adobe.com\nDavid Seunghyun Yoon\nAdobe Research\nsyoon@adobe.com\nRyan A. Rossi\nAdobe Research\nryrossi@adobe.com\nFranck Dernoncourt\nAdobe Research\ndernonco@adobe.com\nAbstract\nLarge Language Models (LLMs) have issues\nwith document question answering (QA) in sit-\nuations where the document is unable to fit in\nthe small context length of an LLM. To over-\ncome this issue, most existing works focus on\nretrieving the relevant context from the docu-\nment, representing them as plain text. However,\ndocuments such as PDFs, web pages, and pre-\nsentations are naturally structured with differ-\nent pages, tables, sections, and so on.  Repre-\nsenting such structured documents as plain text\nis incongruous with the user’s mental model\nof these documents with rich structure. When\na system has to query the document for con-\ntext,  this  incongruity  is  brought  to  the  fore,\nand  seemingly  trivial  questions  can  trip  up\nthe QA system.   To bridge this fundamental\ngap in handling structured documents, we pro-\npose an approach calledPDFTriagethat en-\nables models to retrieve the context based on\neither structure or content.  Our experiments\ndemonstrate the effectiveness of the proposed\nPDFTriage-augmentedmodels across several\nclasses of questions where existing retrieval-\naugmented  LLMs  fail.   To  facilitate  further\nresearch on this fundamental problem, we re-\nlease our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured\ndocuments from 10 different categories of ques-\ntion  types  for  document  QA.  Our  code  and\ndatasets will be released soon on Github.\n1    Introduction\nWhen a document does not fit in the limited con-\ntext window of an LLM, different strategies can\nbe deployed to fetch relevant context. Current ap-\nproaches often rely on a pre-retrieval step to fetch\nthe relevant context from documents (Pereira et al.,\n2023; Gao et al., 2022). These pre-retrieval steps\ntend to represent the document as plain text chunks,\nsharing some similarity with the user query and\npotentially containing the answer. However, many\ndocument types have rich structure, such as web\npages, PDFs, presentations, and so on.  For these\nstructured documents, representing the document\nas plain text is often incongruous with the user’s\nmental model of astructured document. This can\nlead to questions that, to users, may be trivially an-\nswerable, but fail with common/current approaches\nto document QA using LLMs.  For instance, con-\nsider the following two questions:\nQ1\n“Can you summarize the key takeaways from\npages 5-7?”\nQ2\n“What year[in table 3]has the maximum rev-\nenue?”\nIn the first question, document structure isex-\nplicitly referenced(“pages 5-7”).  In the second\nquestion,  document  structure  isimplicitly refer-\nenced(“in table 3”). In both cases, a representation\nof document structure is necessary to identify the\nsalient context and answer the question. Consider-\ning the document as plain text discards the relevant\nstructure needed to answer these questions.\nWe propose addressing this simplification of doc-\numents by allowing models to retrieve the context\nbased on either structure or content. Our approach,\nwhich we refer to asPDFTriage, gives models ac-\ncess to metadata about the structure of the docu-\nment. We leverage document structure by augment-\ning prompts with both document structure meta-\ndata and a set of model-callable retrieval functions\nover various types of structure.  For example, we\nintroduce thefetch_pages(pages: list[int])\nfunction, which allows the model to fetch a list of\npages.  We show that by providing the structure\nand the ability to issue queries over that structure,\nPDFTriage-augmented models can reliably answer\nseveral  classes  of  questions  that  plain  retrieval-\naugmented LLMs could not.\nIn order to evaluate our approach, we construct\na dataset of roughly 900 human-written questions\narXiv:2309.08872v2  [cs.CL]  8 Nov 2023",
    "over 90 documents, representing 10 different cat-\negories of questions that users might ask.  Those\ncategories include “document structure questions”,\n“table reasoning questions”, and “trick questions”,\namong several others. We will release the dataset\nof questions, documents, model answers, and anno-\ntator preferences. In addition, we release the code\nand prompts used.\nThe key contributions of this paper are:\n•  We identify a gap in question answering over\nstructured documents with current LLM ap-\nproaches, namely treating documents as plain\ntext rather than structured objects;\n•We release a dataset of tagged question types,\nalong with model responses, in order to facili-\ntate further research on this topic; and\n•\nWe present a method of prompting the model,\ncalledPDFTriage, that improves the ability\nof an LLM to respond to questions over struc-\ntured documents.\nThe rest of the paper proceeds as follows:  in\nSection 2,  we identify the related works to this\none, and identify the distinguishing features of our\nwork; in Section 3 we outline thePDFTriageap-\nproach, including the document representation, the\nnew retrieval functions, and the prompting tech-\nniques; in Section 4 we outline how we constructed\nthe evaluation dataset of human-written questions;\nin Section 5 we detail the experiments we run to\nsupport the above contributions; in Section 6 we\nlist the key takeaways of those experiments; and,\nlastly, in Section 7 we describe the limitations of\nour current work and future directions.\n2    Related Works\n2.1    Tool and Retrieval Augmented LLMs\nTool-augmented LLMs have become increasingly\npopular  as  a  way  to  enhance  existing  LLMs  to\nutilize  tools  for  responding  to  human  instruc-\ntions (Schick et al., 2023). ReAct (Yao et al., 2022)\nis a few-shot prompting approach that leverages the\nWikipedia API to generate a sequence of API calls\nto solve a specific task. Such task-solving trajecto-\nries are shown to be more interpretable compared\nto baselines.  Self-ask (Press et al., 2022) prompt\nprovides the follow-up question explicitly before\nanswering it, and for ease of parsing uses a specific\nscaffold such as “Follow-up question:” or “So the\nfinal answer is:”. Toolformer (Schick et al., 2023)\nuses self-supervision to teach itself to use tools by\nleveraging the few-shot capabilities of an LM to\nobtain a sample of potential tool uses, which is then\nfine-tuned on a sample of its own generations based\non those that improve the model’s ability to predict\nfuture tokens. TALM (Parisi et al., 2022) augments\nLMs with non-differentiable tools using only text\nalong with an iterative technique to bootstrap per-\nformance using only a few examples.  Recently,\nTaskmatrix (Liang et al., 2023) and Gorilla (Patil\net al., 2023) have focused on improving the ability\nof LLMs to handle millions of tools from a vari-\nety of applications.  There have also been many\nworks focused on benchmarks for tool-augmented\nLLMs (Li et al., 2023; Zhuang et al., 2023). These\ninclude  API-Bank  (Li  et  al.,  2023),  focused  on\nevaluating LLMs’ ability to plan, retrieve, and cor-\nrectly execute step-by-step API calls for carrying\nout various tasks, and ToolQA (Zhuang et al., 2023)\nthat focused on question-answering using external\ntools.\nRetrieval-augmented language models aim to en-\nhance the reasoning capabilities of LLMs using\nexternal knowledge sources for retrieving related\ndocuments  (Asai  et  al.,  2022;  Gao  et  al.,  2022;\nLin et al., 2023; Yu et al., 2023; Zhao et al., 2023;\nFeng et al., 2023). In particular, HyDE (Gao et al.,\n2022) generates a hypothetical document (captur-\ning relevance patterns) by zero-shot instructing an\ninstruction-following LLM, then encodes the doc-\nument into an embedding vector via an unsuper-\nvised contrastively learned encoder, which is used\nto retrieve real documents that are similar to the\ngenerated document.   More recently, Feng et al.\n(2023) proposed InteR that iteratively refines the\ninputs of search engines and LLMs for more ac-\ncurate retrieval.  In particular, InteR uses search\nengines to enhance the knowledge in queries us-\ning LLM-generated knowledge collections whereas\nLLMs improve prompt formulation by leveraging\nthe retrieved documents from the search engine.\nFor further details on augmented language models,\nsee the recent survey (Mialon et al., 2023).\n2.2    Question Answering\nMuch of the existing work in QA does not ground\nthe questions in structured documents, instead pri-\nmarily focusing on extractive QA tasks such as\nGLUE (Wang et al., 2018). For example, text-only\ndocuments in QA datasets, like SQuAD (Rajpurkar\net al., 2016) and NaturalQuestions (Kwiatkowski",
    "Pages\n...\nDocument Metadata Representation\nSection\nTitle: \"2 Related Works\" \nPages: [2, 3]\nSection\nTitle: \"2.1 Tool and Retrieval Augmented LLMs\"\nPages: [2]\nTable\nCaption: \"Table 1: GPTriage functions for Document QA\"\nPages: [4]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n...\nDocument\nSection\nSectionSection\nSection\n...\nH1PULP\nLILI\nQ1: “Can you summarize the key \ntakeaways from pages 5-7?”\nQ2: “What year [in table 3] has \nthe maximum revenue?”\nStep 1:  Generate a structured metadata representation of the document.\nStep 2:  LLM-based Triage \n     (frame selection/filling)\nStep 3:  Question answering \n     with selected context\nQuestion: \"Can you summarize the key \ntakeaways from pages 5-7?\"\nDocument Context:\nAvailable Functions: fetch_pages, fetch_section, search, ...\nfetch_pages(pages: [5, 6, 7])Answer: The key takeaways of ...\nQuestion: \"Can you summarize the key \ntakeaways from pages 5-7?\"\nPage 5:\n...length less than 10 pages, to ensure that \nthere is sufficient but not excessive...\nPage 6:\n...the query embedding. We then feed each \npage’s text as context for answering...\nPage 7:\n...1. The overall quality of the question, \nsuch as its difficulty, clarity,...\nFigure 1:Overview of the PDFTriage technique: PDFTriage leverages a PDF’s structured metadata to implement\na more precise and accurate document question-answering approach. It starts by generating a structured metadata\nrepresentation of the document, extracting information surrounding section text, figure captions, headers, and tables.\nNext, given a query, a LLM-based Triage selects the document frame needed for answering the query and retrieves\nit directly from the selected page, section, figure, or table.  Finally, the selected context and inputted query are\nprocessed by the LLM before the generated answer is outputted.\net al., 2019), don’t contain tables or figures.\nDocument  Question  Answering.Several\ndatasets  have  been  constructed  to  benchmark\ndifferent  aspects  of  document-focused  question-\nanswering.   DocVQA  (Mathew  et  al.,  2021)  is\na visual question-answering dataset focused that\nuses  document  scans.    A  recent  work  by  Lan-\ndeghem et al. (2023) focused on a dataset for docu-\nment understanding and evaluation called DUDE,\nwhich uses both scans and born-digital PDFs. Both\nDUDE  and  DocVQA  have  questions  that  can\nbe answered short-form; DUDE answers average\nroughly 3.35 tokens and DocVQA tokens average\n2.11 tokens.  QASPER (Dasigi et al., 2021) is a\ndataset focused on information-seeking questions\nand their answers from research papers, where the\ndocuments are parsed from raw L\nA\nT\nE\nXsources and\nthe questions are primarily focused on document\ncontents. The PDFTriage evaluation dataset seeks\nto expand on the question types in these datasets,\ngetting questions that can reference the document\nstructure or content, can be extractive or abstractive,\nand can require long-form answers or rewrites.\n3    PDFTriage: Structured Retrieval from\nDocument Metadata\nThePDFTriageapproach consists of three steps to\nanswer a user’s question, shown in Figure 1:\n1.Generate  document  metadata  (Sec.  3.1):\nExtract the structural elements of a document\nand convert them into readable metadata.\n2.LLM-based  triage  (Sec.  3.2):Query  the\nLLM to select the precise content (pages, sec-\ntions, retrieved content) from the document.\n3.Answer using retrieved content (Sec. 3.3):\nBased on the question and retrieved content,\ngenerate an answer.",
    "# of Documents82\n# of Questions908\nEasy Questions393\nMedium Questions144\nHard Questions266\n“Unsure” Questions105\nTable 1: Dataset statistics for the PDFTriage evaluation\ndataset.\nFigure 2: PDFTriage Document Distribution by Word\nCount\n3.1    Document Representation\nWe considerborn-digital PDF documentsas the\nstructured documents that users will be interacting\nwith.  Using the Adobe Extract API, we convert\nthe PDFs into an HTML-like tree, which allows us\nto extract sections, section titles, page information,\ntables,  and figures.\n1\nThe Extract API generates\na hierarchical tree of elements in the PDF, which\nincludes section titles, tables, figures, paragraphs,\nand more. Each element contains metadata, such\nas its page and location.  We can parse that tree\nto identify sections, section-levels, and headings,\ngather all the text on a certain page, or get the text\naround figures and tables. We map that structured\ninformation into a JSON type, that we use as the\ninitial prompt for the LLM. The content is con-\nverted to markdown. An overview of this process\nis shown at the top of Figure 1.\n3.2    LLM Querying of Document\nPDFTriage  utilizes  five  different  functions  in\nthe  approach:fetch_pages,fetch_sections,\n1\nhttps://developer.adobe.com/\ndocument-services/apis/pdf-extract/\nfetch_table\n,fetch_figure, andretrieve. As\ndescribed in Table 2, each function allows the PDF-\nTriage system to gather precise information related\nto the given PDF document, centering around struc-\ntured textual data in headers, subheaders, figures,\ntables, and section paragraphs. The functions are\nused in separate queries by the PDFTriage system\nfor each question, synthesizing multiple pieces of\ninformation to arrive at the final answer. The func-\ntions are provided and called in separate chat turns\nvia the OpenAI function calling API,\n2\nthough it\nwould be possible to organize the prompting in a\nReAct (Yao et al., 2022) or Toolformer (Schick\net al., 2023) -like way.\n3.3    Question Answering\nTo initialize PDFTriage for question-answering, we\nuse the system prompt format of GPT-3.5 to input\nthe following:\nYou are an expert document question answer-\ning system. You answer questions by finding\nrelevant content in the document and answer-\ning questions based on that content.\nDocument:<textual   metadata   of\ndocument>\nUsing user prompting, we then input the query\nwith no additional formatting. Next, the PDFTriage\nsystem uses the functions established in Section 2\nto query the document for any necessary informa-\ntion to answer the question.  In each turn, PDF-\nTriage uses a singular function to gather the needed\ninformation before processing the retrieved context.\nIn the final turn, the model outputs an answer to\nthe question. For all of our experiments, we use the\ngpt-35-turbo-0613model.\n4    Dataset Construction\nTo test the efficacy of PDFTriage, we constructed a\ndocument-focused set of question-answering tasks.\nEach  task  seeks  to  evaluate  different  aspects  of\ndocument question-answering, analyzing reason-\ning across text, tables, and figures within a docu-\nment. Additionally, we wanted to create questions\nranging from single-step answering on an individ-\nual document page to multi-step reasoning across\nthe whole document.\n2\nhttps://platform.openai.com/docs/\napi-reference",
    "FunctionDescription\nfetch_pagesGet the text contained in the pages listed.\nfetch_sectionsGet the text contained in the section listed.\nfetch_figureGet the text contained in the figure caption listed.\nfetch_tableGet the text contained in the table caption listed.\nretrieveIssue a natural language query over the document, and fetch relevant chunks.\nTable 2: PDFTriage Functions for Document QA.\nWe collected questions using Mechanical Turk.\n3\nThe goal of our question collection task was to\ncollect real-world document-oriented questions for\nvarious professional settings. For our documents,\nwe sampled 1000 documents from the common\ncrawl to get visually-rich, professional documents\nfrom various domains, then subsampled 100 docu-\nments based on their reading level (Flesch, 1948).\n4\nBy  collecting  a  broad  set  of  document-oriented\nquestions, we built a robust set of tasks across in-\ndustries for testing the PDFTriage technique.\nIn order to collect a diverse set of questions, we\ngenerated our taxonomy of question types and then\nproceeded to collect a stratified sample across the\ntypes in the taxonomy. Each category highlights a\ndifferent approach to document-oriented QA, cov-\nering multi-step reasoning that is not found in many\nother QA datasets. We asked annotators to read a\ndocument before writing a question.  They were\nthen tasked with writing a salient question in the\nspecified category.\nFor our taxonomy, we consider ten different cat-\negories along with their associated descriptions:\n1.Figure  Questions(6.5%):  Ask  a  question\nabout a figure in the document.\n2.Text  Questions(26.2%):   Ask  a  question\nabout the document.\n3.Table  Reasoning(7.4%):   Ask  a  question\nabout a table in the document.\n4.Structure Questions(3.7%): Ask a question\nabout the structure of the document.\n5.\nSummarization(16.4%): Ask for a summary\nof parts of the document or the full document.\n6.Extraction(21.2%): Ask for specific content\nto be extracted from the document.\n7.\nRewrite(5.2%):  Ask for a rewrite of some\ntext in the document.\n3\nhttps://mturk.com\n4\nhttps://commoncrawl.org/\n8.\nOutside Questions(8.6%):  Ask a question\nthat can’t be answered with just the document.\n9.Cross-page Tasks(1.1%):  Ask a question\nthat needs multiple parts of the document to\nanswer.\n10.Classification(3.7%): Ask about the type of\nthe document.\nIn total, our dataset consists of 908 questions\nacross 82 documents. On average a document con-\ntains 4,257 tokens of text, connected to headers,\nsubheaders, section paragraphs, captions, and more.\nIn Figure 2, we present the document distribution\nby word count.  We provide detailed descriptions\nand examples of each of the classes in the appendix.\n5    Experiments\nWe outline the models and strategies used in our\napproach along with our baselines for comparison.\nThe code and datasets for reproducing our results\nwill be released soon on Github.\n5.1    PDFTriage\nFor our primary experiment, we use our PDFTriage\napproach  to  answer  various  questions  in  the  se-\nlected PDF document dataset. This strategy lever-\nages the structure of PDFs and the interactive sys-\ntem functions capability of GPT-3.5 to extract an-\nswers more precisely and accurately than existing\nnaive approaches.\n5.2    Retrieval Baselines\nPage Retrieval.  For our first baseline, we in-\ndex the pages of each individual document using\ntext-embedding-ada-002embeddings.  Using co-\nsine similarity, we retrieve the pages most similar\nto the query embedding. We then feed each page’s\ntext as context for answering the given question un-\ntil we reach the context window limit for a model.",
    "020406080100\nAnnotator Preferences (%)\nClassification\nExtraction\nText Questions\nSummarization\nCross-page Tasks\nFigure Questions\nOutside Questions\nRewrite\nTable Reasoning\nStructure Questions\n \nOverall\n45.0%25.0%30.0%\n45.6%24.6%29.8%\n46.8%19.2%34.0%\n47.7%34.1%18.2%\n50.0%33.3%16.7%\n51.4%22.9%25.7%\n52.4%33.3%14.3%\n57.1%14.3%28.6%\n62.5%12.5%25.0%\n75.0%20.0%\n50.8%27.1%22.1%\nPDFTriagePage RetrievalChunk Retrieval\nFigure 3:User Preferences between PDFTriage and Alternate Approaches:  Overall, PDFTriage-generated\nanswers were favored the most by the users, claiming 50.8% of the top-ranked answers overall.  Furthermore,\nPDFTriage answers ranked higher on certain multi-page tasks, such as structure questions and table reasoning,\nwhile ranking lower on generalized textual tasks, such as classification and text questions. However, across all the\nquestion categories, PDFTriage beat both the Page Retrieval and Chunk Retrieval approaches on a head-to-head\nranking.\nChunk Retrieval.  In our second baseline, we\nconcatenate all the document’s text before chunk-\ning it into 100-word pieces.  We then index each\nchunk usingtext-embedding-ada-002embeddings\nbefore using cosine similarity calculations to re-\ntrieve the chunks most similar to the query embed-\nding. Finally, we feed each chunk’s textual contents\nas context for answering the given question until\nwe reach the context window limit for a model.\nPrompting. For both retrieval baselines, we use\nthe following prompt to get an answer from GPT-\n3.5:\nYou are an expert document question answer-\ning system. You answer questions by finding\nrelevant content in the document and answer-\ning questions based on that content.\nDocument:<retrieved pages/chunks>\nQuestion:<question>\n5.3    Human Evaluation\nTo measure any difference between PDFTriage and\nthe retrieval baselines, we established a human la-\nbeling study on Upwork. In the study, we hired 12\nexperienced English-speaking annotators to judge\nthe answers generated by each system. Please see\nAppendix A to see the full annotation questions for\neach question-document and its generated answers\n(for the overview, we use a sample question) as well\nas demographic information about the annotators.\nOur questions seek to understand several key\nattributes of each question-document pair as well\nas the associated general questions:\n1.The overall quality of the question, such as its\ndifficulty, clarity, and information needed for\nanswering it.",
    "2.The category of the question, using the taxon-\nomy in section 4.\n3.The ranking of each generated answer for the\ngiven question-document pair.\n4.\nThe   accuracy,   informativeness,   readabil-\nity/understandability, and clarity of each gen-\nerated answer.\n6    Results and Analysis\nIn  Table 1, we present the annotated question dif-\nficulty of each question in our sample.  Overall,\nthe largest group of questions (43.3%) were cate-\ngorized as Easy while roughly a third of questions\nwere categorized as Hard for various reasons.\nIn addition to question difficulty, we asked an-\nnotators to categorize questions by type using the\nsame categories as Section 4. Our annotation frame-\nwork results in a dataset that’s diverse across both\nquestion types and question difficulties, covering\ntextual sections, tables, figures, and headings as\nwell as single-page and multi-page querying. The\ndiversity of questions allows us to robustly evaluate\nmultiple styles of document-centered QA, testing\nthe efficacy of PDFTriage for different reasoning\ntechniques.\n6.1    PDFTriage yields better answers than\nretrieval-based approaches.\nIn our annotation study, we asked the annotators\nto rank PDFTriage compared to our two baselines,\nPage Retrieval and Chunk Retrieval (Section 5).\nIn Figure 3, we found that annotators favored the\nPDFTriage answer over half of the time (50.7%)\nand  favored  the  Chunk  Retrieval  approach  over\nthe  Page  Retrieval  approach.   When  comparing\ndifferent provided answers for the same question,\nPDFTriage performs substantially better than cur-\nrent alternatives, ranking higher than the alternate\napproaches across all the question types.\n6.2    PDFTriage improves answer quality,\naccuracy, readability, and informativeness\nIn  our  annotation  study,  we  also  asked  the  an-\nnotators  to  score  PDFTriage,   Page  Retrieval,\nand  Chunk  Retrieval  answers  across  five  ma-\njor qualities: accuracy, informativeness, readabil-\nity/understandability, and clarity. We hoped to bet-\nter  understand  the  strengths  of  each  answer  for\nusers in document question-answering tasks.  In\nTable 3, we show that PDFTriage answers score\nPDFTriage\nPage\nRetrieval\nChunk\nRetrieval\nReadability\n4.24.14.1\nInformativeness3.93.73.4\nClarity2.02.12.3\nAccuracy3.83.63.4\nOverall Quality3.93.83.6\nTable 3: Answer Quality Scoring\nhigher than Page Retrieval and Chunk Retrieval\nacross all answer qualities except for Clarity. Cru-\ncially, PDFTriage had the highest scores for Overall\nQuality and Answer Accuracy. For annotator agree-\nment,  we  calculated  an  average  Cohen’s  kappa\nscore of 0.584.\nIn Appendix A,  we provide a high-resolution\nbreakdown of annotations for \"Overall Quality\"\nand \"Accuracy\" by question category. We find that\nPDFTriage tends to be stronger for categories like\nsummarization, table reasoning, extraction, and fig-\nure questions which require multi-step reasoning\nacross different parts of a document. Additionally,\nPDFTriage performs similarly to Page Retrieval\nand Chunk Retrieval on other more generalized\nreasoning tasks, such as text questions and classifi-\ncation.\n6.3    PDFTriage requires fewer retrieved\ntokens to produce better answers\nFor the PDF document sample, the average token\nlength of retrieved PDFTriage text is 1568 tokens\n(using the GPT-3.5 tokenizer). The average meta-\ndata length of textual inputs in document JSONs is\n4,257 tokens (using the GPT-3.5 tokenizer).\nWhile PDFTriage utilizes more tokens than Page\nRetrieval (3611 tokens on average) and Chunk Re-\ntrieval (3934 tokens on average),  the tokens are\nretrieved from multiple sections of the document\nthat are non-consecutive. Furthermore, the sections\nused in Page Retrieval and Chunk Retrieval are\noften insufficient for answering the question, as in-\ndicated by lower answer quality scores on average\nfor \"Overall Quality\" and \"Accuracy\".  However,\nsimply concatenating all the document’s text to-\ngether would not ultimately replace PDFTriage due\nto both context window limits and the need to per-\nform multi-hop reasoning for document QA tasks.\nPDFTriage helps overcome this issue through the\nmulti-stage querying of the document, retrieving\nand adding context as needed for different docu-\nment QA tasks.",
    "QuestionPDFTriageChunk Retrieval\nWhich file formats are listed\nunder the “Create Using This\nFormat” section?\nFunction used:fetch_section()\nThe file formats listed under the\n\"Create Using This Format\" section\nare: CSV, JPG, LPT, PDF, PNG,\nPS, RTF, XLS, and XML.\nThe document does not provide\ninformation about the specific\nfile formats listed under the\n“Create Using This Format”\nsection.\nSummarize the second paragraph\nin one long sentence.\nFunction used:fetch_section()\nThe second paragraph explains the\nwriter’s support for H.B. 5540,\nwhich aims to ban guns without\nserial numbers and regulate those\nsold in a form requiring assembly.\nThe writer also supports the...\nIn her testimony, Michele\nMudrick, Legislative Advocate\nfor the Connecticut Conference\nUnited Church of Christ,\nexpresses her support for\nH B 5540 and H B 5542, two\nacts concerning ghost guns, the\npermit application process, and...\nTable 4: A comparison offetch_section()being called successfully and unsuccessfully. Answers highlighted\nin red were considered incorrect. In the second example, both approaches are incorrect; the PDFTriage approach\nfetches the incorrect section, rather than just the first page, the chunk retrieval approach has no knowledge of\ndocument structure and paragraph order.\nFigure 4: PDFTriage Performance compared to Docu-\nment Page Length (uses \"Overall Quality\" scores)\n6.4    PDFTriage performs consistently across\ndocument lengths\nWe also wanted to calculate the correlation between\nPDFTriage performance and the length of the doc-\nument overall. Between the human-annotated PDF-\nTriage answer score for \"Overall Quality\" and doc-\nument length,  we found a Pearson’s correlation\ncoefficient of -0.015. This indicates that document\nlength has a negligible effect on the efficacy of\nPDFTriage, strengthening the generalizability of\nour technique to both short and long documents.\nThe length of different document types seems to\nultimately have no effect on overall performance.\nThe ability of PDFTriage to query specific textual\nsections within the document prevents the need to\ningest documents with excessively large contexts.\nIt allows PDFTriage to connect disparate parts of\na document for multi-page questions such as ta-\nble reasoning, cross-page tasks, figure questions,\nand structure questions, prioritizing relevant con-\ntext and minimizing irrelevant information.  As a\nresult, GPT-3 and other LLMs are better capable\nof handling the reduced context size and ultimately\nutilize less computational and financial resources\nfor document QA tasks.\n7    Future Work & Conclusions\nIn  this  work,  we  present  PDFTriage,  a  novel\nquestion-answering   technique   specialized   for\ndocument-oriented   tasks.We   compare   our\napproach  to  existing  techniques  for  question-\nanswering,  such as page retrieval and chunk re-\ntrieval,  to  demonstrate  the  strengths  of  our  ap-\nproach. We find that PDFTriage offers superior per-\nformance to existing approaches. PDFTriage also\nproves effective across various document lengths\nand contexts used for retrieval. We are considering\nthe following directions for future work:\n1.Developing multi-modal approaches that in-\ncorporate  table  and  figure  information  into\nGPT-4 question-answering for documents.\n2.\nIncorporate question type in PDFTriage ap-\nproach to improve efficiency and efficacy of\nthe approach.",
    "References\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier  Izacard,  Sebastian  Riedel,  Hannaneh  Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions.arXiv preprint arXiv:2211.09260.\nPradeep Dasigi,  Kyle Lo,  Iz Beltagy,  Arman Cohan,\nNoah A Smith, and Matt Gardner. 2021. A dataset of\ninformation-seeking questions and answers anchored\nin research papers.arXiv preprint arXiv:2105.03011.\nJiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen,\nCan Xu, Guodong Long, Dongyan Zhao, and Daxin\nJiang. 2023.  Knowledge refinement via interaction\nbetween search engines and large language models.\narXiv preprint arXiv:2305.07402.\nRudolph Flesch. 1948.   A new readability yardstick.\nJournal of applied psychology, 32(3):221.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022. Precise zero-shot dense retrieval without rele-\nvance labels.arXiv preprint arXiv:2212.10496.\nCaglar  Gulcehre,   Tom  Le  Paine,   Srivatsan  Srini-\nvasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma,  Aditya  Siddhant,  Alex  Ahern,  Miaosen\nWang,  Chenjie  Gu,  Wolfgang  Macherey,  Arnaud\nDoucet,  Orhan Firat,  and Nando de Freitas. 2023.\nReinforced self-training (rest) for language model-\ning.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research.Transactions of the\nAssociation for Computational Linguistics, 7:453–\n466.\nJordy  Landeghem,  Rubén  Tito,  Łukasz  Borchmann,\nMichał  Pietruszka,  Paweł  Józiak,  Rafał  Powalski,\nDawid Jurkiewicz, Mickaël Coustaty, Bertrand Ack-\naert,  Ernest  Valveny,  et  al.  2023.   Document  un-\nderstanding  dataset  and  evaluation  (dude).arXiv\npreprint arXiv:2305.08455.\nMinghao  Li,  Feifan  Song,  Bowen  Yu,  Haiyang  Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-\nbank: A benchmark for tool-augmented llms.arXiv\npreprint arXiv:2304.08244.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan  Xia,  Yu  Liu,  Yang  Ou,  Shuai  Lu,  Lei  Ji,\nShaoguang Mao, et al. 2023.  Taskmatrix. ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis.arXiv preprint arXiv:2303.16434.\nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,\nJimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun\nChen.  2023.   How  to  train  your  dragon:  Diverse\naugmentation towards generalizable dense retrieval.\narXiv preprint arXiv:2302.07452.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawa-\nhar. 2021.  Docvqa: A dataset for vqa on document\nimages. InProceedings of the IEEE/CVF winter con-\nference on applications of computer vision, pages\n2200–2209.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière,  Timo Schick,  Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey.arXiv preprint arXiv:2302.07842.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models.arXiv preprint\narXiv:2205.12255.\nShishir  G  Patil,   Tianjun  Zhang,   Xin  Wang,   and\nJoseph E Gonzalez. 2023.  Gorilla: Large language\nmodel connected with massive apis.arXiv preprint\narXiv:2305.15334.\nJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Ro-\ndrigo Nogueira. 2023.  Visconde:  Multi-document\nqa with gpt-3 and neural reranking.   InEuropean\nConference on Information Retrieval, pages 534–543.\nSpringer.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022.  Measuring\nand narrowing the compositionality gap in language\nmodels.arXiv preprint arXiv:2210.03350.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy  Liang.  2016.    Squad:   100,000+  questions\nfor machine comprehension of text.arXiv preprint\narXiv:1606.05250.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018.  GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. InProceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShunyu Yao,  Jeffrey Zhao,  Dian Yu,  Nan Du,  Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.arXiv preprint arXiv:2210.03629.\nZichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.\n2023. Augmentation-adapted retriever improves gen-\neralization of language models as generic plug-in.\narXiv preprint arXiv:2305.17331.\nRuochen Zhao,  Hailin Chen,  Weishi Wang,  Fangkai\nJiao,  Xuan  Long  Do,  Chengwei  Qin,  Bosheng\nDing, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al.",
    "2023.  Retrieving multimodal information for aug-\nmented  generation:    A  survey.arXiv preprint\narXiv:2303.10868.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang,  Zhanghao Wu,  Yonghao Zhuang,  Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and\nChao Zhang. 2023.  Toolqa: A dataset for llm ques-\ntion answering with external tools.arXiv preprint\narXiv:2306.13304.",
    "A    Appendix\nA.1    Question Categories and Examples\nIn Table 6, and Table 7, we present descriptions\nas well as positive and negative examples for each\nquestion category. Each question category seeks to\ncapture a different document question-answering\ntask  that  is  relevant  across  various  professional\nfields.\nA.2    Annotator Demographic Information\nWe used UpWork to recruit 12 English-speaking\nannotators to judge the answers generated by PDF-\nTriage and the baseline approaches.  We paid all\nthe annotators the same standard rate used for US\nannotators. Here is the demographic breakdown of\nannotators:\n•  4 participants were located in India\n•  2 participants were located in Pakistan\n•  2 participants were located in South Africa\n•  2 participants were located in Australia\n•  2 participants were located in the Phillipines\n•2  participants  were  located  in  the  United\nStates.",
    "CategoryPositive Examples\nFigure Questions\nWhat is the main takeaway of Figure 4?\nWhat is the largest value in Figure 4?\nWhat kind of graph is used on page 5?\nText Questions\nIs 2pm on Wednesday free?\nWhat evidence is used to support the author’s conclusion in section #5?\nTable Reasoning\nCan you convert the minutes column in Table 2 to hours?\nWhat row has the maximum value of the “Accuracy” column?\nStructure Questions\nWhat is the main takeaway from section 5?\nWhat counterexamples are provided in paragraph 3, section #1?\nSummarization\nCan you provide a concise summary of section 2?\nWrite a detailed summary about the main takeaways of the paper.\nExtraction\nFind all the council members mentioned in this document.\nWhat are the three central claims of the author?\nWhat are the main findings?\nRewrite\n- Can you rewrite this in more modern language:\n“The thousand injuries of Fortunato I had borne as best I could.\nBut when he ventured upon insult, I vowed revenge.”\n- Can you simplify this: “In mice, immunoregulatory APCs express\nthe dendritic cell (DC) marker CD11c, and one or more distinctive\nmarkers (CD8, B220, DX5).”\nOutside Questions\n(Closed-book QA)\nWhat other books were written by the novelist author?\nBesides the theory discussed in this document,\nwhat other scientific theories explain the given phenomena?\nCan you explain the term “mitochondria”?\nCross-page TasksDo the results in the conclusions support the claims in the abstract?\nClassification\nIs this document a scientific article?\nIs this document about a residential lease or a commercial lease?\nTrick Question\nA good trick question might:\n(a) be related to the document\n(b) refer to non-existent tables, figures, or sections\n(c) not have enough information to answer it\n(d) not be related to the document at all\nTable 6: Positive Examples for Question Categories",
    "CategoryNegative Examples\nFigure Questions\nWhat is the main takeaway of the second graph.\n(missing reference to page or figure number)\nText Questions\nWhat is the title of subsection #4?\n(too easy to answer)\nTable Reasoning\nWhat value is in the third column, fourth row?\n(too easy to answer)\nStructure Questions\nHow many sections are there in the document?\n(too easy to answer)\nWhat is the title of the document?\n(too easy to answer)\nSummarization\nWhat is a summary of the document?\n(does not specify summary length)\nWrite a short summary.\n(does not specify summary content)\nExtraction\n“How many times does the author\nmention the title character?”\n(not relevant question)\nRewrite\nRemove all typos.\n(too broad, does not refer to specific text)\nOutside Questions\n(Closed-book QA)\nQuestions that are unrelated\nto the document’s content\nCross-page Tasks\nAny task that is answerable in\none place in the document, or\nnot answerable at all.\nClassification\nCategories that are unrelated\nto the document.\nTrick Question\nTable 7: Negative Examples for Question Categories",
    "B    Evaluation Details\nB.1    Human Evaluation Interface\nFigure 5: Annotation Question #1\nFigure 6: Annotation Question #2\nFigure 7: Annotation Question #3\nFigure 8: Annotation Question #4",
    "Figure 9: Annotation Question #5\nFigure 10: Annotation Question #6\nFigure 11: Annotation Question #7\nFigure 12: Annotation Question #8\nB.2    GPT Evaluation and Discussion\nFor each question and document pair in our PDF-\nTriage document sample, we gather the correspond-\ning PDFTriage, Page Retrieval, and Chunks Re-\ntrieval answers for comparison. Next, for automatic\nevaluation, we use thegpt-3.5-turbomodel since\nwe used the same model for our PDFTriage system\nand comparative baselines.  We query the model\nusing the following system prompt:\nGive a score (1-5) for how well the question\nwas answered.  Only provide the numerical\nrating. Do not give any explanation for your\nrating.\nQuestion:<question here>",
    "Answer:<answer here>\nBetween our GPT-4 evaluation scores and the\n\"Overall Quality\" score of the human annotations,\nwe calculated a Cohen’s kappa score of 0.067 and\na Pearson’s correlation coefficient of 0.19 across\nthe entire dataset.  Both these metrics indicate a\nnegligible alignment between the GPT-4 evaluation\nscores and the human annotations.\nTherefore, we believe the automated GPT-4 eval-\nuation requires further instructions or fine-tuning to\nbetter align with human preferences for document\nquestion-answering tasks. Recent work has taken\nsteps towards improving automated LLM evalu-\nation alignment with human preferences (Zheng\net  al.,  2023;  Gulcehre  et  al.,  2023).   For  future\nresearch, it would be worth considering how we\ncan leverage few-shot prompt-tuning to better align\ngenerative LLMs with human preferences in evalu-\nation tasks.\nB.3    Performance vs. Context Window\nTrade-off\nTo better understand the connection between PDF-\nTriage performance and the length of the context\nwindow of the text retrieved from the document,\nwe calculated the correlation between the human\nannotators’ scores for PDFTriage answers and the\nlength of the context retrieved from the document\nmetadata. We found that the Pearson’s correlation\ncoefficient is 0.062, indicating a negligible connec-\ntion between the retrieved context of PDFTriage\nand its overall efficacy.\nInterestingly, it seems like longer context length\ndoes not improve PDFTriage performance, accord-\ning to the human annotations. PDFTriage instead\nneeds to query thepreciseinformation needed for\nanswering different document QA questions, par-\nticularly those like cross-page tasks and structure\nquestions which require multiple stages of query-\ning.  This suggests that full-concatenation of the\ndocument text wouldn’t necessarily improve doc-\nument QA performance since additional text does\nnot correlate with improved accuracy or overall\nquality scores for the answers.\nB.4    Evaluation Breakdown by Question\nCategory\nFigure 13:  Accuracy Annotation Scores by Question\nCategory\nFigure 14: Overall Quality Annotation Scores by Ques-\ntion Category",
    "Figure 15: Informativeness Annotation Scores by Ques-\ntion Category\nFigure 16: Clarity Annotation Scores by Question Cate-\ngory\nFigure 17: Readability Annotation Scores by Question\nCategory"
  ]
}