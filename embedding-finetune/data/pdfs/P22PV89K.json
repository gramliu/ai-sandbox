{
  "key": "P22PV89K",
  "url": "http://arxiv.org/pdf/2405.00200",
  "metadata": {
    "title": "In-Context Learning with Long-Context Models: An In-Depth Exploration",
    "abstract": "  As model context lengths continue to increase, the number of demonstrations\nthat can be provided in-context approaches the size of entire training\ndatasets. We study the behavior of in-context learning (ICL) at this extreme\nscale on multiple datasets and models. We show that, for many datasets with\nlarge label spaces, performance continues to increase with hundreds or\nthousands of demonstrations. We contrast this with example retrieval and\nfinetuning: example retrieval shows excellent performance at low context\nlengths but has diminished gains with more demonstrations; finetuning is more\ndata hungry than ICL but can sometimes exceed long-context ICL performance with\nadditional data. We use this ICL setting as a testbed to study several\nproperties of both in-context learning and long-context models. We show that\nlong-context ICL is less sensitive to random input shuffling than short-context\nICL, that grouping of same-label examples can negatively impact performance,\nand that the performance boosts we see do not arise from cumulative gain from\nencoding many examples together. We conclude that although long-context ICL can\nbe surprisingly effective, most of this gain comes from attending back to\nsimilar examples rather than task learning.\n",
    "published": "2024-04-30T21:06:52Z"
  },
  "text": [
    "Preprint\nIn-Context Learning with Long-Context Models: An In-Depth\nExploration\nAmanda Bertsch\nγ\nabertsch@cs.cmu.edu\nMaor Ivgi\nτ\nmaor.ivgi@cs.tau.ac.il\nUri Alon\nγ∗\nurialon@cs.cmu.edu\nJonathan Berant\nτ\njoberant@cs.tau.ac.il\nMatthew R. Gormley\nγ\nmgormley@cs.cmu.edu\nGraham Neubig\nγ\ngneubig@cs.cmu.edu\nγ\nCarnegie Mellon University\nτ\nTel Aviv University\nAbstract\nAs model context lengths continue to increase, the number of demonstra-\ntions that can be provided in-context approaches the size of entire training\ndatasets. We study the behavior of in-context learning (ICL) at this extreme\nscale on multiple datasets and models. We show that, for many datasets\nwith large label spaces, performance continues to increase with hundreds or\nthousands of demonstrations. We contrast this with example retrieval and\nfinetuning: example retrieval shows excellent performance at low context\nlengths but has diminished gains with more demonstrations; finetuning\nis more data hungry than ICL but can sometimes exceed long-context ICL\nperformance with additional data.  We use this ICL setting as a testbed\nto study several properties of both in-context learning and long-context\nmodels. We show that long-context ICL is less sensitive to random input\nshuffling than short-context ICL, that grouping of same-label examples can\nnegatively impact performance, and that the performance boosts we see do\nnot arise from cumulative gain from encoding many examples together. We\nconclude that although long-context ICL can be surprisingly effective, most\nof this gain comes from attending back to similar examples rather than task\nlearning.\n1\n1    Introduction\nWhen a few examples are provided in-context, large language models can perform many\ntasks with reasonable accuracy. While questions remain about the exact mechanism behind\nthis phenomena (Min et al., 2022b; von Oswald et al., 2023), this paradigm ofin-context\nlearning(ICL) has seen widespread adoption in both academic and industry applications,\nthanks to its ease of implementation, relatively small computational cost, and ability to\nreuse a single model across tasks.\nHowever, most work in this area has focused on short-context models, where the maximum\nnumber of demonstrations is severely limited by context length. As more and more methods\nare developed to adapt language models to extreme context lengths ((Deepmind, 2024; Fu\net al., 2024),inter alia), in-context learning over large quantities of data becomes a potential\nalternative to finetuning.  The properties of ICL in this regime are not well-understood;\nadditionally,  as the cost of inference over many thousands of tokens can be steep,  the\nefficiency and performance tradeoff between many-shot ICL and finetuning on the same\ndata is complex.\n∗\nNow at Google DeepMind\n1\nData and code are available athttps://github.com/abertsch72/long-context-icl\n1\narXiv:2405.00200v1  [cs.CL]  30 Apr 2024",
    "Preprint\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nTREC\nTRECFINE\nNLU\nBanking-77\nClinic-150\n(a) Using randomly selected examples.\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nTREC\nTRECFINE\nNLU\nBanking-77\nClinic-150\n(b) Using retrieved examples.\nFigure 1:  The performance increases with more demonstrations far beyond the context\nwindow of the base Llama-2. Results are on Fu et al. (2024)’s long-context finetuned Llama-\n2-7b model, using a context of up to 80K tokens.\nWe conduct a systemic study of long-context in-context learning.  Namely, we consider:\na) the performance of prompting the base model naively, b) retrieving examples to use\nin-context for each test example, c) comparison to finetuning the base model, and d) using\nmodels trained to adapt to longer contexts. Performance continues to increase past 2000\ndemonstrations (see Figure 1), approaching and sometimesexceedingthe performance of\nmodels finetuned (with LoRA (Hu et al., 2022)) on thousands of examples from the same\ndataset (§ 3).\nWe find that, as the number of demonstrations in-context increases to extreme values, the\nbehavior of ICL shifts (§ 4). In-context learning becomes less sensitive to example order, and\nthe benefits of retrieval over using a random set of demonstrations diminishes– allowing\nthe use of a single set of demonstrations, encoded once through the model and cached,\nrather than re-encoding a custom set of demonstrations for each inference example.  We\ndemonstrate that long-context ICL is strongly impacted by grouping examples of the same\nlabel. We also find that the effectiveness of long-context ICL is not because of the continual\nrefinement of a decision boundary during encoding but because of the retrieval from more\nrelevant examples (§ 5). Our work furthers the understanding of in-context learning and\nsuggests that, in some data regimes, long-context ICL is a strong alternative to retrieval and\nfinetuning.\n2    Experimental setup\nHere we describe the shared setup between our ICL and finetuning experiments.  Each\nsetting is described in more detail as it is introduced.\n2.1    Datasets and models\nWe consider 5 classification datasets: TREC (Hovy et al., 2001), TREC-fine (Hovy et al., 2001),\nNLU (Xingkun Liu & Rieser, 2019), Banking-77 (Casanueva et al., 2020), and Clinic-150\n(Larson et al., 2019). Table 1 contains summary statistics for each dataset, and Appendix E\nshows additional description and examples for each dataset.\nWe compare ICL performance across several variants ofLlama-2-7bTouvron et al. (2023)\nadapted for long context:\n1.\nLlama2(Touvron et al., 2023) is a decoder-only model trained with a 4096 context\nlength. We use the non-instruct (non-chat) variant because it is more commonly used\nas a base model for long-context finetuning and because we observed very similar\nperformance between the chat and non-chat variants in our initial experiments.\n2",
    "Preprint\nDatasetDomain# Labels\nAvg\ndemo\nlength\nTraining\nset size\nExample labels\nTRECquestions622.75,452\nlocation,\nentity\nTREC-finequestions5023.75,452\nabbreviation expansion,\nlocation city\nNLUconversational6820.719,286\ntakeaway query,\niot hue light up\nBanking-77financial7727.410,003\ntop up failed,\nlost or stolen card\nClinic-150multiple15122.315,250\nrollover 401k,\nmeal suggestion\nTable 1:  The datasets we consider in this work span diverse label spaces and domains.\nThe average demonstration length is the average combined length of input, output, and\nformatting tokens per demonstration provided in the context window.\n2.Llama2-32k(TogetherAI, 2023) is a version of Llama-2-7b finetuned by TogetherAI\nfor a 32k context window. We use the non-instruct version.\n3.Llama2-80k(Fu et al., 2024) is a version of Llama-2-7b finetuned with 80k context\nand a carefully designed long-document data mixture.\nTo verify that the trends we observe are not specific to the Llama series, we additionally\nconsiderMistral-7b-v0.2(Jiang et al., 2023). We use the instruct version, as the non-instruct\nmodel is not publicly available. The trained context length of Mistral-7B-Instruct-v0.2 is 32k\ntokens.\nWhile all of these models can extrapolate to inputs longer than their trained context length,\nwe restrict the lengths of inputs to fit within the trained context length; this represents the\nbest case performance without the additional confound of the extrapolation strategy.\n2.2    Constrained decoding\nFor each dataset, we useconstrained decodingto only produce valid labels as output;\n2\nall ICL\nresults in the paper, across all methods, use this constrained decoding. Note that, without\nconstrained decoding, these models may produce invalid labels in the few-shot regimes.\nFor finetuning, we use a classification head so that no invalid outputs may be produced.\n2.3    Evaluation\nFollowing prior work (Zhao et al., 2021; Lu et al., 2022; Han et al., 2022; Ratner et al., 2022),\nwe subsample 250 examples from the test set of each dataset. We release the subsampled test\nset and full prediction outputs for each experiment in the project repository. We evaluate\non each dataset with accuracy and macro-F1, to capture both overall performance and the\nperformance on minority classes\n3\n; as the trends for both metrics are very similar, we report\nprimarily accuracy in the paper for readability.\n2\nFollowing Ratner et al. (2022): at each generation step, we simply multiply the logits of tokens\nthat could lead to a valid label by a large constant.\n3\nWe use the definition of macro-F1 that averages per-class F1; see Opitz & Burst (2021) for discussion\nof the alternative.\n3",
    "Preprint\nDatasetLlama2Llama2-32kLlama2-80kMistral\nRandomly selected\nTREC82.32/ 80.5293.12/93.1290.04/90.0487.28/ 85.00\nTREC-fine61.40/61.4075.56/ 75.0877.20/77.2072.68/ 70.48\nNLU76.88/76.8885.04/ 85.0087.52/ 86.9286.44/86.44\nBanking-7756.36/56.3682.44/82.4488.08/ 87.9686.76/ 86.68\nClinic-15060.92/60.9284.40/84.4089.32/89.3290.56/90.56\nRetrieval\nTREC90.80/ 85.6494.84/ 94.6494.28/ 92.6890.80/90.80\nTREC-fine78.80/78.8083.88/ 81.1283.92/ 81.3680.80/ 79.60\nNLU90.00/ 88.4089.80/89.8089.64/ 89.5290.40/ 89.20\nBanking-7793.20/ 92.4094.32/94.3294.00/ 92.9693.20/93.20\nClinic-15087.60/87.6089.84/89.8493.76/93.7693.20/ 92.40\nTable 2: For all datasets, performance of ICL continues to increase with additional demon-\nstrations. The results are the best accuracy (left) and accuracy at maximum data (right) for\neach model. Bold indicates the best performance for that model/dataset pair.\n3    Long-context ICL\nWe consider three common methods for using a large dataset:  naively sampling a fixed\nsubset to use in-context, retrieving relevant data for each example at inference time, or\nfinetuning on the full dataset.\n3.1    Compared settings\nRandom sampling ICLWe use 10 random shuffles of the training dataset, averaging the\nresults across these shuffles. Across models and across varying numbers of demonstrations\nin-context, we draw the firstnexamples from each shuffle. In this setting, the encoding of\ndemonstrations can be performed once and cached across all inference examples.\nRetrieval ICLA strong alternative for in-context learning is to retrieve a relevant subset\nof examples as demonstrations for each test set example.  In this setting, we use a BM25\n(Robertson & Zaragoza, 2009) retriever with stopwords removed and retrieve the most\nrelevant demonstrations by comparing the test input text to the full demonstration texts.\nWhen doingk-shot prompting, if less thankexamples are retrieved by the retriever,\n4\nwe\nrandomly sample additional examples until we reachk. We compare putting examples in\nthe order they were retrieved and in three random shufflings. Note this is more computa-\ntionally expensive than using a random sample as demonstrations, as a new set of retrieved\ndemonstrations must be encoded for each test set example. However, prior work has found\nthat, in some scenarios, retrieval of good examples can make the difference from near-zero\nto high test accuracy (Levy et al., 2023).\nFinetuningWe finetune Llama2-7b with a classification head on varying amounts of data\nfrom each dataset with several random seeds, and plot performance at convergence on the\nsame held-out test data.  We initialize the classification head from the parameters of the\npretrained language modeling head by subsampling the values of the first token of each\nlabel; this creates a better-than-random initialization for finetuning. For more details on the\nfinetuning procedure, see Appendix D.\n4\nThis occurs when there are less thankexamples with any word overlap with the test example\n(excluding overlap in stopwords).\n4",
    "Preprint\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nRetrieval ICL\nRandom ICL\nFinetuned\n(a) Clinic-150\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nRetrieval ICL\nRandom ICL\nFinetuned\n(b) Trecfine\nFigure 2:  Comparing retrieval ICL, random selection ICL, and finetuning on two repre-\nsentative datasets.  Finetuning sometimes, but not always, exceeds ICL at high numbers\nof demonstrations.  Note that, while retrieval ICL uses the listed number of examples in\ncontext, it assumes access to the larger test set to draw examples from (Perez et al., 2021).\nResults on other datasets are in Appendix C.\n.\n3.2    In-context results\nScaling up ICL to many examples leads to surprisingly strong resultsFigure 1 and\nTable 2 show the performance of models in both in-context learning settings.  Scaling up\nin-context learning from 10 to 1000 demonstrations results in accuracy gains of up to50.8\npoints (an average of 36.8 points across the 5 datasets).\nLonger context lessens the importance of carefully selecting in-context examplesRetriev-\ning relevant examples for each test set example far outperforms using a randomly selected\nsubset in the short-context regime. This is true even if the order of retrieved examples is\nshuffled (rather than ordered by relevance).\n5\nHowever, adding additional examples does\ncontinue to slightly improve performance; this is especially surprising because, after all\nexamples with non-trivial lexical overlap are retrieved the remaining examples are randomly\nselected.\nWhile retrieval continues to outperform random selection on most datasets, the effect is\ndiminished with additional examples. On Banking-77, the dataset where retrieval is most\nbeneficial, the performance gain from retrieval drops from 51.5 points at 1-shot ICL to 4.9\npoints at 1500-shot ICL. This suggests that, as the amount of examples in-context increases,\nthe importance of the selection strategy diminishes. In the long context regime, using the\nmore computationally efficient but less effective strategy of a single randomly selected set\nof demonstrations is more feasible; the performance penalty for doing so is never more than\n5 points of improvement, and as low as 1.8 points (for 2000-shot ICL on TREC).\n3.3    Comparison with finetuning\nWhile we have demonstrated that in-context learning with hundreds or thousands of exam-\nples is effective, this amount of data is also appropriate for finetuning a model. Finetuning\nhas higher upfront cost but allows for reduced inference-time cost.  In this section, we\ncompare in-context learning with the popular parameter-efficient finetuning (PEFT) strategy\nLoRA (Hu et al., 2022).\n5\nWe perform three random shuffles of the retrieved inputs and test for difference in distribution\nfrom the original results. Across all datasets, this shuffling does not significantly change performance\n(2-sided t-test,p<0.05).\n5",
    "Preprint\nPEFT is more data-hungry than ICL– especially ICL with retrievalWhen a relatively\nsmall set of examples is available, ICL generally outperforms LoRA finetuning on the same\nmodel.\n6\nFor most datasets, finetuning performance never exceeds long-context ICL performance\neven with additional examples (e.g. Figure 2a). The exceptions are on TREC and TREC-fine,\nwhere finetuning outperforms ICL at the highest numbers of examples (but continues to\nunderperform at low numbers of examples) (e.g. Figure 2b). Generally, the datasets with\nlarger label spaces show the least strong finetuning performance, likely because these are\nmore open-ended classification problems and require more data to train the classifier.\nFor some datasets, PEFT does win out overall– finetuning with more examples than even\nthe 80k model can fit in-context does result in higher performance. In datasets where PEFT\nperformance never exceeds ICL performance, it nevertheless has dramatically reduced\ninference costs for similar performance; thus, finetuning on 4096 examples may still be\npreferable to prompting with 1000 if efficiency of inference is a major priority. This is because,\neven if demonstration encodings can be cached across inference examples, cross-attention\nto a long context of demonstrations is expensive.\n4    Properties of long-context ICL\nIn this section, we compare the properties of long-context ICL with the known properties\nof short-context ICL. We additionally consider using ICL as a testbed for properties of\nlong-context models in Appendix B.\n10\n1\n10\n2\n10\n3\nNumber of examples in-context\n5\n10\n15\n20\n25\n% of labels flipped\nTREC\nTREC-fine\nNLU\nBanking-77\nClinic-150\nFigure 3: The impact of (randomly) reorder-\ning examples in-context decreases with ad-\nditional demonstrations.\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nOriginal order\nSorted labels\nFigure 4: By contrast, sorting examples by\nlabel has an increasingly negative impact\non performance in longer context regimes.\nResults on Llama2-32k with Clinic-150.\nIs it best to use the entire context?Prior work suggested that, for some simple tasks,\nproviding additional input canreduceperformance (Levy et al., 2024). However, we observe\nmonotonically increasing performance on nearly every dataset; after the performance curve\nbegins to flatten, small variation occurs, but no significantly lower performance occurs at\nhigher example counts. While using the full context window is computationally costly, and\nmay not be necessary to achieve high performance on these datasets, it does not appear to be\nharmful to performance; and the additional cost of more input is minimal, as the key-value\npairs can be cached and reused across test samples.\nSensitivity to example orderPrior work has shown that many models exhibit strong\nsensitivity to example order in-context (Lu et al., 2022).  We examine this by measuring\n6\nNote that some prior results have showed strong PEFT performance in the few-example setting\non different tasks; see Section 6 for more discussion.\n6",
    "Preprint\n(a) Causal attention(b) Blockwise attention\nFigure 5: Normal versus block-causal attention. When performing blockwise attention, we\nallow full attention for the example we are predicting a label for (here: the last 2 tokens).\n.\nthe percentage of predictions that change when the input is reordered; we average this\nover 3 re-shufflings for each set of input examples.  Figure 3 shows that, while there is\nsome sensitivity to example order at all context lengths, this effect weakens substantially\nwith additional context. Across all datasets, the percent of labels flipped by reshuffling in\n1000-shot ICL is less than half the percent of labels flipped when reshuffling in 10-shot ICL.\nLabel sortingWe also consider an adversarial case for example ordering:  we sort the\nexamples so that examples with the same label appear together.   At small numbers of\nexamples, this has very little impact; if the average number of examples per class is low,\nlabel sorting is similar to a random sort. However, as the number of examples grows, label\nsorting begins to have a dramatic impact on performance. Figure 4 shows the performance\nof Llama2-32k on Clinic-150 with and without label sorting. As the number of examples\nin-context increases, the penalty for input sorting increases as well; at 1169-shot ICL, label\nsorting decreases accuracy by 25.7 percentage points. This suggests that contextualization\nof examples withdifferentlabels is important to performance, and that this contextualization\nonly occurs effectively over relatively short distances in the context window.\n5    Why does long-context ICL help?\nTo study the underlying mechanism behind the improved performance of the model at\nlonger context lengths, we consider a modified attention pattern where demonstrations can\nonly attend to a small block of nearby demonstrations. The test example we are predicting a\nlabel for can always attend to all demonstrations. Figure 5 compares this block-attention to\nthe usual causal attention.\nIf improved performance is due predominately to the development of a more fine-grained\ntask understanding from embedding many examples together (e.g. continually refining a de-\ncision boundary, in the way finetuning the model would do), then encoding many examples\nin small blocks would be far worse than encoding them all together. If the improvements\ncome largely from seeing more relevant examples to attend to, then the performance should\nnot be strongly reliant on how many other examples each demonstration is contextualized\nwith. Note that this is distinct from methods that overload the same positions with multiple\nembeddings in order to process longer contexts (e.g. Ratner et al. (2022)); here, we are not\nmodifying any positional information, only restricting attention between demonstrations to\na local context block.\n7",
    "Preprint\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in each attention block\n0\n20\n40\n60\n80\naccuracy\n(a) Banking-77\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in each attention block\n20\n40\n60\n80\naccuracy\n(b) Clinic-150\nFigure 6: Comparing block attention to full attention over the same set of examples with\nLlama2-32k.   Block attention approaches full attention with relatively small block size.\nThe blue dots represent block attention with blocks of that size; the red ‘x’s represent full\nattention over that number of examples. The black line represents the performance of full\nattention over all examples.\nRather than breaking the context window into fixed-length chunks, we fix a number of\nexamples per-block. If the block size is equal to the number of examples in-context, this is\nequivalent to normal attention; if the block size is 1, each example can attend only to itself.\nFigure 6 shows results on Banking-77 and Clinic-150, with a comparison to both full attention\nover all examples and full attention over a single block of examples.\nWhen attending in very small blocks (e.g. for banking,<5 examples), performance isworse\nthan attending to a small set of fixed examples. We hypothesize that this is due to inadequate\ncontextualization  of  each  example  leading  to  less  informative  embeddings.   However,\nperformance quickly climbs; 95% of the performance of full attention is recovered by a block\nof 50 examples in the case of Banking-77 or 75 examples for Clinic-150 (more generally: this\noccurs between 20- and 75-example block sizes in all datasets).  In some dataset/model\npairs, performance of block attention even slightly exceeds performance of full attention.\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in each attention block\n0\n5\n10\n15\n20\n25\n30\n35\naccuracy\nFigure 7: Even when the examples in-context\nare sorted, block attention can recover simi-\nlar performance to full attention on the same\nordering.\nTo determine how much of the benefit of\nencoding multiple examples together is due\nto  task  learning,  we  consider  the  case  of\nICL with examples sorted by label.  In the\nblock attention case, this ensures that most\nblocks have examples with only one label\nrepresented. While sorting examples by la-\nbel is harmful to performance in the block\nattention case as well, it is notmoreharmful\nto the blocked attention model than it is to\nthe full attention model. This suggests that\nmuch of the performance of the model is\nnot due to learning decision boundaries in\neach block and aggregating them, as most\nblocks in the label-sorted case do not see\nmore than one or two labels. This supports\nthe theory that the primary performance im-\nprovement from long-context modeling is\ndue to retrieving from more relevant exam-\nples in-context, rather than learning a better\ntask boundary; this is supported as well by\n8",
    "Preprint\nthe retrieval results in Table 2, where retrieval performance at short contexts is close to\n(though never exceeding) very-long-context ICL performance.\nTasks where long context does not helpConcurrently to our work, Li et al. (2024) identify\na set of tasks where long context is not uniformly helpful. However, we observe that the\ntasks that show this trend either have near-0 performance at short demonstration lengths or\nalso display an inverse performance trend on the short context scale (e.g. for TacRED (Zhang\net al., 2017), we observe that performance decreases from 1 to 10 total demonstrations).\nWhile these are important failure modes of language models, we restrict our analysis to tasks\nwithout these confounding issues. In Banking-77, the one dataset that overlaps between Li\net al. (2024) and this work, both papers observe similar trends of improved performance\nwith additional context.\n6    Related Work\nAugmenting decoder-only models with long contextMany methods for extending the\ncontext of language models have been introduced in the last few years. One series of work\nhas focused on positional embedding extrapolation strategies (Peng et al., 2023; Rozi\n`\nere\net al., 2024; Chen et al., 2023; Liu et al., 2023; Zhu et al., 2024; Xiao et al., 2024; Han et al.,\n2024).  When extrapolating past pretraining length, models also generally benefit from\nadditional finetuning on long-context data (Xiong et al., 2023).  Other methods include\nadding retrieval-based attention (Bertsch et al., 2023; Tworkowski et al., 2023; Yen et al.,\n2024) or hierarchical merging of information (Song et al., 2024; YU et al., 2023).  The two\nlong-context Llama variants we consider in this work are both examples of finetuning for\nlength extrapolation.\nSeparately, methods for longer context for ICL have also been proposed. Parallel context\nwindows (Ratner et al., 2022) and structured prompting (Hao et al., 2022) propose methods\nof re-using the same positional embeddings multiple times to encode more demonstrations;\nthis is quite effective for small numbers of overlaps, albeit with diminishing returns as the\nnumber of overlapping windows increases. Cho et al. (2023) propose a hybrid of ICL and\nlinear prompting which improves beyond few-shot ICL performance.\nSeveral works have also critiqued the efficacy of long context models.  Liu et al. (2024)\ndemonstrate that some long-context models fail to effectively use the middle of the context\nwindow; the models we use were released after this work and have generally high scores\nfor middle-of-context retrieval in their trained context length. Li et al. (2023a) suggest that\nsome long-context models are only effective at utilizing inputs that are shorter than their\ncontext window’s intended supported length; we do not observe this effect strongly, but it\nis a possible contributing factor to the saturation of performance for some models before the\nmaximum number of examples in-context. Li et al. (2023b) show that many models fail at\ntasks that require reasoning over long dependency lengths; this is unlikely to be an issue in\nour setting.\nProperties of in-context learningMilios et al. (2023) study ICL for many-class classification\nwith models up to 4k context length. They find that, when retrieving demonstrations, smaller\n(7b) models show early performance saturation on many tasks. This is consistent with our\nfindings for Llama2-7b with retrieval; however, the same model continues to learn from\ndemonstrations in the random selection case, and the same size model finetuned for longer\ncontext does not show the same performance dropoff and continues to see improvements\nfrom additional context for several tasks. Our results suggest that this failure to use longer\ncontext effectively is not an inherent property of 7b models, but instead a type of shallow\nheuristic used by this particular model when the demonstrations are of sufficiently high\nquality.\nXu et al. (2023) study the impacts of ground-truth label, input distribution, and explanations\non ICL performance; B\n ̈\nol\n ̈\nuc\n ̈\nu et al. (2023) study the impact of example selection in a specific\ndomain. Lin & Lee (2024) argue that ICL occurs in two modes: learning tasks and retrieving\ntasks, and that retrieval of similar-but-not-quite-correct tasks can explain “early ascent”\n9",
    "Preprint\nbehaviors where ICL performance peaks once in a fewshot regime and then performance\nimproves again with a much higher number of examples. Similarly, Pan et al. (2023) argue\nfor a distinction between task recognition and task learning, and suggest that task learning\ncontinues to benefit from additional examples at scale.  von Oswald et al. (2023) suggest\nin-context learning can be viewed as gradient descent, although Deutch et al. (2024) argue\nagainst this interpretation.  Hendel et al. (2023) view in-context learning as compressing\nthe demonstrations into a “task vector” that maps from inputs to outputs; the surprising\neffectiveness of block encoding initially appears contrary to this theory, although it is also\npossible that multiple similar task vectors are learned from the separate blocks and then\nensembled via attention for the final prediction.\nConcurrently to our work, Agarwal et al. (2024) study many-shot prompting of Gemini 1.5\nand show improvements from the fewshot setting across both classification and generation\ntasks. Our work differs in its evaluation of multiple open-source models, our comparison to\nfinetuning the same base model, and our use of ICL as a testbed for analysis of long context\nbehaviors.\nComparing  in-context  learning  and  finetuningMin  et  al.  (2022a)  show  that  models\ntrained on fewshot learning can generalize to perform fewshot learning on new tasks; in\nsome cases, this can outperform finetuning directly on the new task. Mosbach et al. (2023)\ncompare finetuning to ICL more directly; they find that finetuning generally outperforms\nICL with the same number of examples both in-domain and out-of-domain, when comparing\n16-example ICL to finetuning on the same 16 examples.  Their setting differs from ours\nin their choice of model (OPT), the amount of data considered (16 for ICL, 16 or 128 for\nfinetuning), and the use of full finetuning rather than PEFT. Liu et al. (2022) find that PEFT\ngenerally outperforms ICL in their setting, where they finetune an encoder-decoder model\nwith a language modeling objective using their T-few method and 20-70 samples.  Asai\net al. (2023) compare finetuning and ICL for mT5 on cross-lingual transfer and find that\nICL outperforms finetuning in some, but not all, of the tasks studied.  To the best of our\nknowledge, no prior work has considered the relative performance of finetuning and ICL in\nthe many-shot regime, where there are hundreds or thousands of examples in-context.\n7    Conclusion\nWe have demonstrated that ICL with large demonstration sets can be surprisingly effective,\nand shed light on a few surprising properties in its behavior.  Namely, long-context ICL\nexhibits a reduced dependence on example selection, relatively stable performance with\nrespect to example order, and performance often approaching or exceeding parameter-\nefficient finetuning on the same data, all properties that make this an appealing option for\na variety of tasks. We have also shown that long-context ICL’s effectiveness is largely due\nto retrieval from the long context during prediction, rather than cross-attention within the\nlarge demonstration set during encoding.\nOur work also highlights that our understanding of ICL remains incomplete. Though much\nwork has studied the potential mechanisms behind ICL, these works have largely focused\non simple tasks with small (<10 examples) demonstration sets; as our work demonstrates\nthat properties of ICL shift with the scale of the demonstration set, more work is necessary\nto validate hypotheses about ICL at larger scales.\nWhile prior work has focused on two strategies for performing inference on a new task–\neither finetuning on task-specific data or selecting a subset of that data to use in-context–\nour results points to a potential third paradigm: adapting themodelto fit as much of that\ndata in-context as possible, caching and reusing the encoding of the long demonstration set.\nWhile finetuning with full datasets is still a powerful option if the data vastly exceeds the\ncontext length, our results suggest that long-context ICL is an effective alternative– trading\nfinetuning-time cost for increased inference-time compute. As the effectiveness and effiency\nof using very long model context lengths continues to increase, we believe long-context ICL\nwill be a powerful tool for many tasks.\n10",
    "Preprint\nAcknowledgments\nWe would like to thank Vijay Viswanathan, Sewon Min, Akari Asai, Xiang Yue, and Simran\nKhanuja for useful discussions about this work.\nThis work was partially supported by The Yandex Initiative for Machine Learning, the\nLen Blavatnik and the Blavatnik Family foundation, and the European Research Council\n(ERC) under the European Union Horizons 2020 research and innovation programme\n(grant ERC DELPHI 802800).  AB was supported by a grant from the National Science\nFoundation Graduate Research Fellowship Program under Grant No.  DGE2140739.  MI\nalso acknowledges the support of the Israeli Council of Higher Education. Any opinions,\nfindings, and conclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of the sponsors.\nReferences\nRishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand,\nZaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra\nFaust, and Hugo Larochelle. Many-shot in-context learning, 2024.\nAkari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid,\nYulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. Buffet: Benchmarking large\nlanguage models for few-shot cross-lingual transfer, 2023.\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Long-\nrange transformers with unlimited length input. In A. Oh, T. Neumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine (eds.),Advances in Neural Information Processing Systems.\nCurran Associates, Inc., 2023.  URLhttps://proceedings.neurips.cc/paper_files/\npaper/2023/file/6f9806a5adc72b5b834b27e4c7c0df9b-Paper-Conference.pdf.\nStella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Gre-\ngory Anthony, Shivanshu Purohit, and Edward Raff.  Emergent and predictable mem-\norization in large language models.  InThirty-seventh Conference on Neural Information\nProcessing Systems, 2023. URLhttps://openreview.net/forum?id=Iq0DvhB4Kf.\nNecva B\n ̈\nol\n ̈\nuc\n ̈\nu, Maciej Rybinski, and Stephen Wan. impact of sample selection on in-context\nlearning for entity extraction from scientific writing.   In Houda Bouamor, Juan Pino,\nand Kalika Bali (eds.),Findings of the Association for Computational Linguistics:  EMNLP\n2023, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nfindings-emnlp.338. URLhttps://aclanthology.org/2023.findings-emnlp.338.\nI\n ̃\nnigo Casanueva,  Tadas Tem\nˇ\ncinas,  Daniela Gerz,  Matthew Henderson,  and Ivan Vuli\n ́\nc.\nEfficient intent detection with dual sentence encoders.  InProceedings of the 2nd Work-\nshop  on  Natural  Language  Processing  for  Conversational  AI,  Online,  2020.  Association\nfor  Computational  Linguistics.   doi:  10.18653/v1/2020.nlp4convai-1.5.   URLhttps:\n//aclanthology.org/2020.nlp4convai-1.5.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context\nwindow of large language models via positional interpolation, 2023.\nHyunsoo Cho, Hyuhng Joon Kim, Junyeob Kim, Sang-Woo Lee, Sang goo Lee, Kang Min\nYoo, and Taeuk Kim.  Prompt-augmented linear probing:  Scaling beyond the limit of\nfew-shot in-context learners, 2023.\nGoogle Deepmind.  Our next-generation model:  Gemini 1.5, 2024.  URLhttps://blog.\ngoogle/technology/ai/google-gemini-next-generation-model-february-2024/\n#build-experiment.\nGilad Deutch, Nadav Magar, Tomer Bar Natan, and Guy Dar.   In-context learning and\ngradient descent revisited, 2024.\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and\nHao Peng. Data engineering for scaling language models to 128k context, 2024.\n11",
    "Preprint\nChi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang.\nLm-infinite: Zero-shot extreme length generalization for large language models, 2024.\nZhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. Prototypical calibration for\nfew-shot learning of language models, 2022.\nYaru Hao,  Yutao Sun,  Li Dong,  Zhixiong Han,  Yuxian Gu,  and Furu Wei.   Structured\nprompting:  Scaling in-context learning to 1, 000 examples.ArXiv preprint, 2022.  URL\nhttps://arxiv.org/abs/2212.06713.\nRoee Hendel, Mor Geva, and Amir Globerson.  In-context learning creates task vectors.\nIn  Houda  Bouamor,  Juan  Pino,  and  Kalika  Bali  (eds.),Findings  of  the  Association  for\nComputational Linguistics: EMNLP 2023, Singapore, 2023. Association for Computational\nLinguistics.  doi: 10.18653/v1/2023.findings-emnlp.624.  URLhttps://aclanthology.\norg/2023.findings-emnlp.624.\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran.\nToward semantics-based answer pinpointing.   InProceedings of the First International\nConference on Human Language Technology Research, 2001.  URLhttps://aclanthology.\norg/H01-1069.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29,  2022. OpenReview.net,  2022.   URLhttps://openreview.net/forum?id=\nnZeVKeeFYf9.\nMaor  Ivgi,  Uri  Shaham,  and  Jonathan  Berant.   Efficient  long-text  understanding  with\nshort-text models.Transactions of the Association for Computational Linguistics, 2023. doi:\n10.1162/tacla00547. URLhttps://aclanthology.org/2023.tacl-1.17.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\n ́\nelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth\n ́\nee Lacroix, and William El Sayed. Mistral 7b, 2023.\nDamjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora.ArXiv\npreprint, 2023. URLhttps://arxiv.org/abs/2312.03732.\nStefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker\nHill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and\nJason Mars. An evaluation dataset for intent classification and out-of-scope prediction.\nInProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\nHong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nD19-1131. URLhttps://aclanthology.org/D19-1131.\nItay Levy, Ben Bogin, and Jonathan Berant.  Diverse demonstrations improve in-context\ncompositional generalization. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki\n(eds.),Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), Toronto, Canada, 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.78.  URLhttps://aclanthology.org/2023.acl-long.\n78.\nMosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input\nlength on the reasoning performance of large language models, 2024.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang.  How long can context length of open-source LLMs truly\npromise?  InNeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.\nURLhttps://openreview.net/forum?id=LywifFNXV5.\n12",
    "Preprint\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang.  Loogle: Can long-context\nlanguage models understand long contexts?, 2023b.\nTianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle\nwith long in-context learning, 2024.\nXin Li and Dan Roth.  Learning question classifiers.  InCOLING 2002:  The 19th Interna-\ntional Conference on Computational Linguistics, 2002.  URLhttps://aclanthology.org/\nC02-1150.\nZiqian Lin and Kangwook Lee. Dual operating modes of in-context learning, 2024.\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for\nnear-infinite context, 2023.\nHaokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal,\nand Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-\ncontext learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho\n(eds.),Advances in Neural Information Processing Systems, 2022. URLhttps://openreview.\nnet/forum?id=rBCvMG-JsPd.\nNelson  F.  Liu,  Kevin  Lin,  John  Hewitt,  Ashwin  Paranjape,  Michele  Bevilacqua,  Fabio\nPetroni, and Percy Liang. Lost in the Middle: How Language Models Use Long Contexts.\nTransactions of the Association for Computational Linguistics, 2024.  ISSN 2307-387X.  doi:\n10.1162/tacla00638. URLhttps://doi.org/10.1162/tacl_a_00638.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically\nordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.\nInProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), Dublin, Ireland, 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-long.556. URLhttps://aclanthology.org/2022.acl-long.\n556.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and\nBenjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods.https:\n//github.com/huggingface/peft, 2022.\nAristides Milios, Siva Reddy, and Dzmitry Bahdanau. In-context learning for text classifi-\ncation with many labels. In Dieuwke Hupkes, Verna Dankers, Khuyagbaatar Batsuren,\nKoustuv Sinha, Amirhossein Kazemnejad, Christos Christodoulopoulos, Ryan Cotterell,\nand Elia Bruni (eds.),Proceedings of the 1st GenBench Workshop on (Benchmarking) Generali-\nsation in NLP, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.genbench-1.14. URLhttps://aclanthology.org/2023.genbench-1.14.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to\nlearn in context. InProceedings of the 2022 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies, Seattle, United States,\n2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.201.\nURLhttps://aclanthology.org/2022.naacl-main.201.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning\nwork?    InProceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, Abu Dhabi, United Arab Emirates, 2022b. Association for Computational\nLinguistics. URLhttps://aclanthology.org/2022.emnlp-main.759.\nMarius Mosbach,  Tiago Pimentel,  Shauli Ravfogel,  Dietrich Klakow,  and Yanai Elazar.\nFew-shot fine-tuning  vs. in-context  learning:  A fair  comparison and  evaluation.   In\nAnna  Rogers,  Jordan  Boyd-Graber,  and  Naoaki  Okazaki  (eds.),Findings  of  the  Asso-\nciation  for  Computational  Linguistics:   ACL  2023,  Toronto,  Canada,  2023.  Association\nfor Computational Linguistics.   doi:  10.18653/v1/2023.findings-acl.779.   URLhttps:\n//aclanthology.org/2023.findings-acl.779.\n13",
    "Preprint\nJuri Opitz and Sebastian Burst. Macro f1 and macro f1, 2021.\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning “learns” in-\ncontext: Disentangling task recognition and task learning. In Anna Rogers, Jordan Boyd-\nGraber, and Naoaki Okazaki (eds.),Findings of the Association for Computational Linguistics:\nACL 2023, Toronto, Canada, 2023. Association for Computational Linguistics.  doi:  10.\n18653/v1/2023.findings-acl.527. URLhttps://aclanthology.org/2023.findings-acl.\n527.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context\nwindow extension of large language models, 2023.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho.  True few-shot learning with language\nmodels. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and\nJennifer Wortman Vaughan (eds.),Advances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December\n6-14,  2021,  virtual,  2021.   URLhttps://proceedings.neurips.cc/paper/2021/hash/\n5c04925674920eb58467fb52ce4ef728-Abstract.html.\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud D.\nKarpas,  Amnon Shashua,  Kevin Leyton-Brown,  and Yoav Shoham.   Parallel context\nwindows for large language models. InAnnual Meeting of the Association for Computational\nLinguistics, 2022. URLhttps://api.semanticscholar.org/CorpusID:258686160.\nStephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond.Foundations and Trends in Information Retrieval, 2009. doi: 10.1561/1500000019.\nBaptiste Rozi\n`\nere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\n ́\ner\n ́\nemy Rapin, Artyom Kozhevnikov,\nIvan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D\n ́\nefossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis\nMartin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.  Code llama:  Open\nfoundation models for code, 2024.\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models’\nsensitivity to spurious features in prompt design or:  How i learned to start worrying\nabout prompt formatting, 2023.\nWoomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha,\nand Jinwoo Shin.  Hierarchical context merging: Better long context understanding for\npre-trained LLMs. InThe Twelfth International Conference on Learning Representations, 2024.\nURLhttps://openreview.net/forum?id=ulaUJFd96G.\nTogetherAI. Llama-2-7b-32k-instruct - and fine-tuning for llama-2 models with together api,\n2023. URLhttps://www.together.ai/blog/llama-2-7b-32k-instruct.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher,  Cristian  Canton  Ferrer,  Moya  Chen,  Guillem  Cucurull,  David  Esiobu,  Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal,  Anthony Hartshorn,  Saghar Hosseini,  Rui Hou,  Hakan Inan,  Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom.  Llama 2: Open foundation and fine-tuned chat\nmodels, 2023.\n14",
    "Preprint\nSzymon  Tworkowski,   Konrad  Staniszewski,   Mikoł  aj  Pacek,   Yuhuai  Wu,   Henryk\nMichalewski,  and  Piotr  Mił  o\n ́\ns.   Focused  transformer:  Contrastive  training  for  con-\ntext  scaling.In  A.  Oh,  T.  Neumann,  A.  Globerson,  K.  Saenko,  M.  Hardt,  and\nS. Levine (eds.),Advances in Neural Information Processing Systems. Curran Associates,\nInc.,  2023.   URLhttps://proceedings.neurips.cc/paper_files/paper/2023/file/\n8511d06d5590f4bda24d42087802cc81-Paper-Conference.pdf.\nJohannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\n ̃\nao Sacramento, Alexander\nMordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context\nby gradient descent, 2023.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,\nSylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers:\nState-of-the-art natural language processing.   InProceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:  System Demonstrations, Online, 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL\nhttps://aclanthology.org/2020.emnlp-demos.6.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient stream-\ning language models with attention sinks, 2024.\nPawel Swietojanski Xingkun Liu, Arash Eshghi and Verena Rieser. Benchmarking natural\nlanguage understanding services for building conversational agents.  InProceedings of\nthe Tenth International Workshop on Spoken Dialogue Systems Technology (IWSDS), Ortigia,\nSiracusa (SR), Italy, 2019. Springer. URLhttp://www.xx.xx/xx/.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa,\nHan Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,\nSergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of\nfoundation models, 2023.\nPaiheng Xu, Fuxiao Liu, Zongxia Li, and Hyemi Song. Towards understanding in-context\nlearning with contrastive demonstrations and saliency maps, 2023.\nHoward Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel\ncontext encoding, 2024.\nCecilia Ying and Stephen Thomas. Label errors in BANKING77. InProceedings of the Third\nWorkshop on Insights from Negative Results in NLP, Dublin,  Ireland,  2022. Association\nfor Computational Linguistics.   doi:  10.18653/v1/2022.insights-1.19.   URLhttps://\naclanthology.org/2022.insights-1.19.\nLILI YU, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike\nLewis.  MEGABYTE: Predicting million-byte sequences with multiscale transformers.\nInThirty-seventh Conference on Neural Information Processing Systems, 2023.  URLhttps:\n//openreview.net/forum?id=JTmO2V9Xpz.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning.\nPosition-aware  attention  and  supervised  data  improve  slot  filling.   InProceedings  of\nthe 2017 Conference on Empirical Methods in Natural Language Processing,  Copenhagen,\nDenmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1004.\nURLhttps://aclanthology.org/D17-1004.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.  Calibrate before use:\nImproving few-shot performance of language models. In Marina Meila and Tong Zhang\n(eds.),Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, Proceedings of Machine Learning Research. PMLR, 2021. URL\nhttp://proceedings.mlr.press/v139/zhao21c.html.\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose:\nEfficient context window extension of llms via positional skip-wise training, 2024.\n15",
    "Preprint\nA    Saturation\nOne metric we are interested in is the point where the model performancesaturates, which\nwe define informally as the point where adding more examples is unlikely to meaning-\nfully improve performance. More formally, we define the saturation point as the smallest\nnumber of examples tested such that performance reaches 95% of the model’s maximum\nperformance.\nSaturation points vary by dataset.We define saturation as the first point at which perfor-\nmance reaches 95% of the model’s maximum performance on that dataset. Table 3 shows\nthe number of examples at saturation and the maximum number of examples that fit in the\ncontext window for each model. For datasets with larger label spaces, saturation generally\noccurs later; Banking-77 and Clinic-150 do not saturate within the context window of Llama2\n(4096 tokens, which represents between 100-162 in-context examples for these datasets). In\nthe longer-context regime, saturation points generally occur slightly later on Llama2-80k,\nbut in both models occur before the model’s maximum context length.\nThis suggests two things.  First, given a fixed model, it is often not necessary to use the\nfull context length to extract high performance from that model. Second, current models\ndo not make use of the full potential of ICL; models often saturate in performance before\nthe maximum number of examples, despite longer-context versions revealing that further\nperformance improvements are possible.\nThe number of classes has some impact on saturation point– but is not fully explanatory.\nOur results show datasets with more classes benefit from more demonstrations in-context, on\naverage, before saturation. This is to be expected, as the expected number of demonstrations\nnecessary before seeing the correct label increases with the number of total label classes.\nTo test if this is an intrinsic property of these datasets, or truly linked only to the number\nof label classes, we construct subsets of two high-label-space datasets, Banking-77 and\nClinic-150, by randomly selecting half of the labels to exclude from the dataset.  These\nsubsets remain in the same domain, but with a smaller label space; if saturation is tied\nto the number of examples, then this should move the saturation point. Note that this is\ndistinct fromcombininglabels (e.g. TREC vs TREC-FINE), as combining finegrained labels\ninto general labels makes the classification task simpler.  It’s still possible that the subset\nchosen is a simpler task (e.g. by removing one of a pair of frequently confused labels); to\nmoderate the effect of this change, we average results over 3 randomly chosen subsets.\nTable 4 compares the saturation point of the full- and half-label-space runs. For the datasets\nwith the most number of labels, halving the number of labels also reduces the amount of\nexamples that are useful before saturation, albeit not by half. However, the trend is less clear\nfor the tasks with fewer labels; in some cases, reducing the label space actuallyincreases\nthe number of demonstrations before saturation. While the size of the label space clearly\nhas some impact on the saturation point, more investigation is necessary to identify other\nfactors impacting this behavior.\n16",
    "Preprint\nDatasetLlama2Llama2-32kLlama2-80kMistral\nTREC20 (140)100 (1129)75 (2000)50 (1129)\nTREC-fine75 (131)250 (1056)500 (2000)500 (1091)\nNLU100 (162)500 (1309)500 (2000)250 (1309)\nBanking-77- (100)500 (838)750 (1750)500 (860)\nClinic-150- (145)750 (1169)1000 (2000)750 (1212)\nTable 3: We measure the saturation point as the point at which the model reaches 95% of its\nmaximum accuracy on the dataset; “-” in a column indicates that the maximum performance\nis achieved by using the full context window.  The number in parenthesis represents the\nmaximum number of examples that fit in the context window.  As the label space of the\ndataset increases (from top to bottom row), so does the number of examples that can be\nused before saturation.\nDatasetLlama2Llama2-32kLlama2-80kMistral\nTREC14.29 / 24.698.86 / 1.773.75 / 1.714.43 / 6.0\nTREC-fine57.25 / 80.7123.67 / 27.925.0 / 32.3845.83 / 33.33\nNLU61.73 / 80.7138.2 / 17.2125.0 / 26.6719.1 / 36.67\nBanking-77100.0 / 88.059.67 / 37.9142.86 / 28.5758.14 / 35.56\nClinic-150100.0 / 64.0464.16 / 35.1450.0 / 22.8661.88 / 56.67\nTable 4: We compare the saturation point between the full-label-space (left) and half-label-\nspace (right) for each model+dataset pair. Here we represent the label space as a percentage\nof the full context window.\nB    Using ICL as a testbed for long-context model properties\nIn this section, we use in-context learning as a testbed to examine several properties of\nlong-context models.\nHow do long-context models perform in the short-context regime?Llama2-32k and\nLlama2-80k are finetuned variants of Llama2-7b, adapted for longer contexts. We evaluate\nhow these models perform relative to the base model in short-context tasks (e.g. ICL using\nless than 4096 tokens of demonstrations) by testing whether the difference in performance is\nstatistically significant (2-sided t-test,p<0.05). Performance is generally similar, with some\nareas of slight improvement from the base model; Figure 8 shows full results. We observe\ndegradation in performance in some settings for Llama2-32k, highlighting the importance\nof testing for behavior regression when finetuning for additional capabilities.\nInput utilizationWe analyze the performance of all methods on a naturalistic needle-in-\nthe-haystack Ivgi et al. (2023); Liu et al. (2024) style test. If the model is effectively using\nthe context, then it should be able to exactly recover the label for any example it has seen\nin-context. Note that, while a model trained on some set of data is not uniformly capable of\nexact copying from that training data (Biderman et al., 2023), in nearly all of our finetuning\nruns, the model fits the training data with 100% accuracy.\nWe examine this behavior by selecting the same set of examples to use in-context and in\nevaluation; all models should then be able to achieve 100% accuracy.  Table 5 shows the\nresults; while all models achieve very high accuracy on the copied data, no model is able to\nuniformly copy correctly from the input. Surprisingly, performance improves slightly with\nadditional demonstrations for most models, possibly due to additional specification of the\ntask.\n17",
    "Preprint\n1510255075100\nNumber of examples in-context\nClinic-150\nBanking-77\nNLU\nTREC-fine\nTREC\n(a) Llama2-32k\n1510255075100\nNumber of examples in-context\nClinic-150\nBanking-77\nNLU\nTREC-fine\nTREC\n(b) Llama2-80k\nFigure 8:  Short-context behavior of long-context models.  Each model’s performance is\ncompared to the performance of the base model it was finetuned from, on the same amount\nof data.Redrepresents significantly worse performance;bluerepresents significantly better\nperformance (p<0.05).\n.\nNumber of examples15102550100200250\nLlama2100.093.096.597.098.697.95--\nLlama2-32k80.095.097.096.698.498.598.598.9\nLlama2-80k90.094.095.098.298.598.398.097.9\nTable 5:  Copying behavior given the test examples in the context window.  Results are\naveraged over Banking-77 and Clinic-150;  bold indicates the best performance for that\nmodel.\nC    Full ICL results across datasets\nFor space, we show 1-2 representative datasets for each point of analysis in the paper. In\nthis appendix, we present results across all datasets for completeness.\n18",
    "Preprint\nC.1    Random selection ICL across all models\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(a) TREC\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(b) TREC-fine\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(c) NLU\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(d) Banking-77\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(e) Clinic-150\nFigure 9: Performance of random-selection ICL across all models for each dataset. Perfor-\nmance continues to increase with additional examples in-context.\n19",
    "Preprint\nC.2    Retrieval ICL across all models\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(a) TREC\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(b) TREC-fine\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(c) NLU\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(d) Banking-77\n10\n0\n10\n1\n10\n2\n10\n3\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\n4k (base model)\n32k (TogetherAI)\n80k (Fu et al 2024)\nMistral\n(e) Clinic-150\nFigure 10:  Performance of retrieval-based ICL across all models for each dataset.  Short-\ncontext performance here is higher than for random-selection, but performance continues to\nimprove with more examples until a saturation point, where performance flattens out.\n20",
    "Preprint\nC.3    Comparing retrieval, random selection, and finetuning\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nRetrieval ICL\nRandom ICL\nFinetuned\n(a) TREC\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nRetrieval ICL\nRandom ICL\nFinetuned\n(b) TREC-fine\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nRetrieval ICL\nRandom ICL\nFinetuned\n(c) NLU\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nRetrieval ICL\nRandom ICL\nFinetuned\n(d) Banking-77\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of examples in-context\n0\n20\n40\n60\n80\n100\naccuracy\nRetrieval ICL\nRandom ICL\nFinetuned\n(e) Clinic-150\nFigure 11: Performance of retrieval-based ICL, random-selection ICL, and finetuning across\n5 datasets. At small example counts, ICL outperforms finetuning; when several thousand\nexamples are used, finetuning outperforms ICL in some datasets.\n21",
    "Preprint\nD    Finetuning\nTo perform parameter efficient finetuning (PEFT), we used the peft (Mangrulkar et al., 2022)\npackage (version 0.9.0).  We finetune the model for 30 epochs, evaluating it every epoch\non the test set, and ultimately choosing the checkpoint with the highest test accuracy. We\nnote that using the test set to perform model selection presents an unfair advantage to PEFT\n(compared to ICL) and may not be truly indicative of the generalization error. However,\ndoing so provides the advantage of being both comparable to ICL in terms of the data used,\nas well as giving an upper bound on the true generalization accuracy of the finetuned model,\nfurther emphasizing any observed efficacy gap between it and ICL.\nInitialization of the classification headWhile in our default setting we initialize the\nclassification head from the pretrained LM head, subsampled at the representation of the\nfirst token in each label, we investigate the efficacy of this approach by contrasting with a\nrandomly initialized classification head. Figure 12 shows that while in the few-shot regime,\nthis approach has significant advantage, as the training set grows in size the difference\nshrinks to become negligible. In no case was random initialization better than this approach.\nHyperparameter tuningTo remain comparable in terms of compute efficient finetuning,\nwe did not perform extensive hyper-parameter tuning per task, and instead experimented\nwith a good global setting on a single dataset (Banking-77). Specifically, we experimented\nwith different learning rates, different LoRA ranks (r) andα(Hu et al., 2022) and also tried\napplying RSlora (Kalajdzievski, 2023) which sets the scaling factor to\nα\n√\nr\nas some evidence\nsuggest it can outperform the original method. Figure 13 summarizes the results, depicting\naverage test accuracy against training examples with different settings.\nUltimately, we found that using HuggingFace’s (Wolf et al., 2020) default parameters of\nr=8,α=32, LoRA dropout of 0.1 and a learning rate of 1e−3 to work best. In all cases,\nwe used batch sizes of 32 and weight decay of 0.01.\nIt is possible that methods specialized for finetuning in small-data regimes, such as T-few\nLiu et al. (2022), might close the gap between ICL and PEFT in the small-data regimes. We\ndid not consider T-few in our analysis because of its additional pretraining stage, which\nimposes substantial additional cost, and because T-few was developed with a focus on\nencoder-decoder models and we consider only decoder-only models in our setting.\n22",
    "Preprint\n10\n2\n10\n3\n10\n4\n0.2\n0.4\n0.6\n0.8\nTest accuracy\nBanking-77\nInitialization\nlm\nrandom\n10\n2\n10\n3\n10\n4\n0.2\n0.4\n0.6\n0.8\nTest accuracy\nClinic-150\nInitialization\nlm\nrandom\n10\n2\n10\n3\n0.6\n0.7\n0.8\n0.9\nTest accuracy\nTREC\nInitialization\nlm\nrandom\n10\n2\n10\n3\n0.4\n0.6\n0.8\nTest accuracy\nTREC-fine\nInitialization\nlm\nrandom\n10\n2\n10\n3\n10\n4\n0.2\n0.4\n0.6\n0.8\nTest accuracy\nNLU\nInitialization\nlm\nrandom\nFigure 12: Comparing initialization methods of the classification head when finetuning a\nPEFT llama-2-7b model. Averaged (best) test accuracy over 5 random seeds. Initialization\nwithlmsubsamples the pretrained language-modeling head at the first token of the target\nlabel, while random samples random weights.\n10\n2\n10\n3\n10\n4\n(a) Training examples\n0.2\n0.4\n0.6\n0.8\nTest accuracy\n(a) Training examples\nChosen hyperparameters\nBest ablation\n10\n2\n10\n3\n10\n4\n(b) Training examples\n0.2\n0.4\n0.6\n0.8\nTest accuracy\n(b) Training examples\nChosen hyperparameters\nAblation\n10\n5\n10\n4\n10\n3\n10\n2\nLearning Rate\nFigure 13:  Comparing hyperparameters when finetuning a PEFT llama-2-7b model on\nBanking-77. Averaged (best) test accuracy over 3 random seeds. (a) Comparing our fixed\nLoRA configurations to the best alternative configuration (at each scale) we tried.   (b)\nComparing different learning rates.\n23",
    "Preprint\nE    Prompt formatting and examples from datasets\nAs a demonstration of the datasets, we provide an example of 3-shot prompting for each\ndataset with the prompt formatting we used (and with examples drawn from the training\nset of each dataset).\nPrompt formatting and instruction phrasing can have significant impact on performance\n(Sclar et al., 2023); we keep the formatting consistent with prior work (Ratner et al., 2022),\nwith prefixes for the input and output for each exemplar. Because we use predominately\nnon-instruction-tuned models, we do not add an additional instruction or system prompt.\nE.1    TREC\nTREC(Hovy et al., 2001; Li & Roth, 2002) is a question classification dataset with two\ngranularities of labels. We refer to the 6-label coarse classification as TREC.\nFigure 14: The label distribution of TREC. One label is much less frequent than the rest.\nQuestion: How does light travel through the void of space if there is no medium for it to ‘\nwave ’ or ‘ pulse ’ .\nType: description\n==\nQuestion: What cathedral was Thomas Becket murdered in ?\nType: location\n==\nQuestion: The lawyer who represented Randy Craft , what was his name ?\nType: human\n==\nQuestion: What are the rites accompanying the circumcision of a newly born-child in Judaism\ncalled ?\nType:\nFigure 15: Example 3-shot prompt for TREC.\nE.2    TREC-fine\nWe refer to TREC’s 50-label finegrained classification as TREC-fine.\nFigure 16: The label distribution of TREC-fine.\n24",
    "Preprint\nQuestion: How does light travel through the void of space if there is no medium for it to ‘\nwave ’ or ‘ pulse ’ .\nType: description manner\n==\nQuestion: What cathedral was Thomas Becket murdered in ?\nType: location other\n==\nQuestion: The lawyer who represented Randy Craft , what was his name ?\nType: human individual\n==\nQuestion: What are the rites accompanying the circumcision of a newly born-child in Judaism\ncalled ?\nType:\nFigure 17: Example 3-shot prompt for TREC-fine.\nE.3NLU\nFigure 18: The label distribution of NLU.\nNLU(Xingkun Liu & Rieser, 2019) is a 68-way intent classification dataset in the conversa-\ntional domain. The original paper evaluates on 64 of the intents; we use all 68.\nutterance: oh it is nice one, olly.\nintent: general praise\n==\nutterance: nope wrong.\nintent: general negate\n==\nutterance: what events near hear are happening this week\nintent: recommendation events\n==\nutterance: play fishing podcasts that are favorited\nintent:\nFigure 19: Example 3-shot prompt for NLU.\nE.4    Banking-77\nBanking-77(Casanueva et al., 2020) is a 77-way intent classification task in the financial\ndomain. Although the accuracy of some labels in BANKING77 has been criticized (Ying\n25",
    "Preprint\nFigure 20: The label distribution of Banking-77.\n& Thomas, 2022), we report results here on the original dataset for consistency with prior\nwork.\nquery: How long will my payment be pending?\nintent: pending card payment\n==\nquery: My physical card is not working\nintent: card not working\n==\nquery: i cant seem to activate card\nintent: activate my card\n==\nquery: I didn’t set up a direct debit payment on my account.\nintent:\nFigure 21: Example 3-shot prompt for Banking-77.\nE.5    Clinic-150\nFigure 22: The label distribution of Clinic-150. It is balanced except for the “out of scope”\nlabel, which has additional data points in the split we use.\nClinic-150(Larson et al., 2019) is a 151-way, multi-domain intent classification task; examples\nare either labeled with an intent from one of 10 domains or with the catch-all “out-of-scope“\nlabel. We use the “plus“ train split from the original paper, which adds additional “out-of-\nscope” examples to the dataset.\n26",
    "Preprint\nutterance: how much is my comcast bill\nintent: bill balance\n==\nutterance: tell me about yourself\nintent: what is your name\n==\nutterance: how to build up my credit score\nintent: improve credit score\n==\nutterance: are you employed by me\nintent:\nFigure 23: Example 3-shot prompt for Clinic-150.\n27"
  ]
}