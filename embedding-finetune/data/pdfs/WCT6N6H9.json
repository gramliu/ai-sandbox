{
  "key": "WCT6N6H9",
  "url": "http://arxiv.org/pdf/2309.01431",
  "metadata": {
    "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
    "abstract": "  Retrieval-Augmented Generation (RAG) is a promising approach for mitigating\nthe hallucination of large language models (LLMs). However, existing research\nlacks rigorous evaluation of the impact of retrieval-augmented generation on\ndifferent large language models, which make it challenging to identify the\npotential bottlenecks in the capabilities of RAG for different LLMs. In this\npaper, we systematically investigate the impact of Retrieval-Augmented\nGeneration on large language models. We analyze the performance of different\nlarge language models in 4 fundamental abilities required for RAG, including\nnoise robustness, negative rejection, information integration, and\ncounterfactual robustness. To this end, we establish Retrieval-Augmented\nGeneration Benchmark (RGB), a new corpus for RAG evaluation in both English and\nChinese. RGB divides the instances within the benchmark into 4 separate\ntestbeds based on the aforementioned fundamental abilities required to resolve\nthe case. Then we evaluate 6 representative LLMs on RGB to diagnose the\nchallenges of current LLMs when applying RAG. Evaluation reveals that while\nLLMs exhibit a certain degree of noise robustness, they still struggle\nsignificantly in terms of negative rejection, information integration, and\ndealing with false information. The aforementioned assessment outcomes indicate\nthat there is still a considerable journey ahead to effectively apply RAG to\nLLMs.\n",
    "published": "2023-09-04T08:28:44Z"
  },
  "text": [
    "Benchmarking Large Language Models in Retrieval-Augmented Generation\nJiawei Chen\n1,3\n, Hongyu Lin\n1,\n*\n, Xianpei Han\n1,2,\n*\n, Le Sun\n1,2\n1\nChinese Information Processing Laboratory\n2\nState Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3\nUniversity of Chinese Academy of Sciences, Beijing, China\n{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn\nAbstract\nRetrieval-Augmented Generation (RAG) is a promising ap-\nproach  for  mitigating  the  hallucination  of  large  language\nmodels  (LLMs).  However,  existing  research  lacks  rigorous\nevaluation  of  the  impact  of  retrieval-augmented  generation\non different large language models, which make it challeng-\ning  to  identify  the  potential  bottlenecks  in  the  capabilities\nof  RAG  for  different  LLMs.  In  this  paper,  we  systemati-\ncally investigate the impact of Retrieval-Augmented Gener-\nation on large language models. We analyze the performance\nof  different  large  language  models  in  4  fundamental  abili-\nties required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robust-\nness. To this end, we establish Retrieval-Augmented Genera-\ntion Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the afore-\nmentioned fundamental abilities required to resolve the case.\nThen  we  evaluate  6  representative  LLMs  on  RGB  to  diag-\nnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\nof noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to ef-\nfectively apply RAG to LLMs.\nIntroduction\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges includ-\ning factual hallucination (Cao et al. 2020; Raunak, Menezes,\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-\ndating (He, Zhang, and Roth 2022), and the lack of domain-\nspecific expertise (Li et al. 2023c; Shen et al. 2023).\nIncorporating  external  knowledge  via  information  re-\ntrieval,  i.e.,  Retrieval-Augmented  Generation  (RAG),  has\nbeen regarded as a promising way to resolve the above chal-\nlenges.  (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\n*\nCorresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nNoise RobustnessNegative Rejection\nWho was awarded the 2022 Nobel prize in \nliterature?\nThe Nobel Prize in Literature for 2022 is \nawarded to the French author Annie Ernaux, \n“for the courage and clinical acuity ...\nThe Nobel Prize in Literature for 2021 is \nawarded to the novelist Abdulrazak Gurnah, \nborn in Zanzibar and active in ...\nAnnie Ernaux\nQuestion\nExternal documents contain noises\nRetrieval Augmented \nGeneration\nWho was awarded the 2022 Nobel prize in \nliterature?\nThe Nobel Prize in Literature for 2021 is \nawarded to the novelist Abdulrazak Gurnah, \nborn in Zanzibar and active in ...\nThe 2020 Nobel Laureate in Literature, \npoet Louise Glück, has written both poetry \nand essays about poetry. Since her...\nI can not answer the question because of the \ninsufficient information in documents\nQuestion\nExternal documents are all noises\nInformation Integration\nWhen were the ChatGPT app for iOS and \nChatGPT apilaunched?\nOn May 18th, 2023, OpenAI introduced its \nown ChatGPT app for iOS...\nThat changed on March 1, when OpenAI \nannounced the release of API access to \nChatGPT and Whisper,...\nMay 18 and March 1.\nQuestion\nExternal documents contain all answers\nRetrieval Augmented \nGeneration\nCounterfactual Robustness\nWhich city hosted the Olympic games in \n2004?\nThe 2004 Olympic Games returned home to \nNew York, birthplace of the ... \nAfter leading all voting rounds, New York\neasily defeated Rome in the fifth and \nfinal vote ...\nThere are factual errors in the provided \ndocuments. The answer should be Athens. \nQuestion\nCounterfactual external documents\nRetrieval Augmented \nGeneration\nRetrieval Augmented \nGeneration\nFigure  1:  Illustration  of  4  kinds  of  abilities  required  for\nretrieval-augmented generation of LLMs.\n2022; Izacard et al. 2022). With the help of external knowl-\nedge,  LLMs  can  generate  more  accurate  and  reliable  re-\nsponses. The most common method is to use a search engine\nas a retriever such as New Bing. Due to the vast amount of\ninformation available on the Internet, using a search engine\ncan provide more real-time information.\nHowever,  Retrieval-Augmented  Generation  brings  not\nonly positive effects to LLMs (Liu, Zhang, and Liang 2023;\nMaynez  et  al.  2020).  On  one  hand,  there  is  a  significant\namount of noise information even fake news in the content\navailable on the Internet, which poses challenges for search\nengines in accurately retrieving desirable knowledge. On the\nother  hand,  LLMs  suffer  from  unreliable  generation  chal-\nlenge. LLMs can be misled by incorrect information con-\ntained in the context (Bian et al. 2023) and also suffer from\nhallucination  during  the  generation  (Adlakha  et  al.  2023),\nresulting in generating content that goes beyond external in-\narXiv:2309.01431v2  [cs.CL]  20 Dec 2023",
    "formation. These challenges result in LLMs being unable to\nconsistently  generate  reliable  and  accurate  responses.  Un-\nfortunately,  currently  there  lacks  of  comprehensive  under-\nstanding on how these factors can influence RAG, and how\ncould  each  model  survives  from  these  drawbacks  and  im-\nprovement their performance via information retrieval. As a\nresult, there is a pressing need for a comprehensive evalua-\ntion of LLMs on their ability to effectively utilize retrieved\ninformation, as well as their ability to withstand the various\ndrawbacks present in information retrieval.\nTo this end, this paper conducts a comprehensive evalua-\ntion of RAG for current LLMs. Specifically, we create a new\nRetrieval-Augmented Generation Benchmark, namely RGB,\nin both English and Chinese. In order to ensure that the in-\nternal knowledge of LLMs does not introduce bias into the\nevaluation results, RGB chooses to aggregate the latest news\ninformation and constructs queries based on the news infor-\nmation. Then, based on these queries, we use Search API to\nfetch relevant documents and select most relevant snippets\nfrom  the  content  as  external  retrieved  documents.  Finally,\nbased on different compositions of query and document-set\npairs, we expand the corpus and divided it into 4 testbeds to\nevaluate the following basic abilities of LLMs according to\nthe common challenges in RAG, as shown in Figure 1:\n•Noise Robustness, which means a LLM can extract use-\nful information from noisy documents. In this paper, we\ndefine noisy documents as those that are relevant to the\nquestion but do not contain any information of the an-\nswer. For the instance in Figure 1, the noisy documents\nrelated to the question “Who was awarded the 2022 No-\nbel  Prize  in  Literature”  include  reports  about  the  2021\nNobel  Prize  in  Literature.  To  this  end,  the  testbed  for\nnoise robustness contains instances whose external doc-\numents  contain  a  certain  number  of  noisy  documents\nbased on the desired noise ratio.\n•Negative Rejection, which means that a LLM should re-\nject to answer the question when the required knowledge\nis not present in any retrieved document. The testbed for\nnegative rejection contains instances whose external doc-\numents  are  only  with  noisy  documents.  LLMs  are  ex-\npected to indicate “insufficient information” or other re-\njection signals.\n•Information  Integration,   which   evaluates   whether\nLLMs can answer complex questions that require inte-\ngrating information from multiple documents. For the in-\nstance in Figure 1, for the question “When were the Chat-\nGPT app for iOS and ChatGPT api launched?”, LLMs\nare expected to provide information of the launch dates\nfor both the ChatGPT iOS app and ChatGPT API. The\ntestbed  for  information  integration  contains  instances\nthat can only be answered using multiple documents.\n•Counterfactual Robustness,  which  evaluates  whether\nLLMs can identify risks of known factual errors in the\nretrieved documents when the LLMs are given warnings\nabout potential risks in the retrieved information through\ninstruction. The testbed for counterfactual robustness in-\ncludes  instances  that  can  be  answered  directly  by  the\nLLMs, but the external documents contain factual errors.\nBased  on  RGB,  we  conduct  evaluation  on  6  state-of-\nthe-art  large  language  models  including  ChatGPT  (Ope-\nnAI  2022),  ChatGLM-6B  (THUDM  2023a),  ChatGLM2-\n6B  (THUDM  2023b),  Vicuna-7b  (Chiang  et  al.  2023),\nQwen-7B-Chat  (QwenLM  2023),  BELLE-7B  (Yunjie  Ji\n2023). We found that even though RAG can improve the re-\nsponse accuracy of LLMs, they still suffer from the above-\nmentioned  challenges  significantly.  Specifically,  we  found\nthat even though LLMs demonstrate some level of noise ro-\nbustness, they tend to confuse similar information and fre-\nquently generate inaccurate answers when relevant informa-\ntion exists. For example, when faced with a question about\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\nments about the 2021 Nobel Prize in Literature in external\ndocuments, LLMs may become confused and provide inac-\ncurate answers. Besides, LLMs frequently fail to reject an-\nswering  and  generate  incorrect  answers  when  none  of  the\nexternal  documents  contain  relevant  information.  Further-\nmore,  LLMs  lack  the  ability  to  summarize  from  multiple\ndocuments, and therefore if multiple documents are needed\nto answer a question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about po-\ntential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize the retrieved information\nover their own existing knowledge. The experimental results\nmentioned above highlight the need for further resolution of\nimportant issues in the existing RAG method. Therefore, it\nis crucial to exercise caution and carefully design its usage.\nGenerally speaking, the contributions of this paper are\n1\n:\n•  We proposed to evaluate four capabilities for retrieval-\naugmented   generation   of   LLMs   and   created   the\nRetrieval-Augmented Generation Benchmark in both En-\nglish and Chinese. To best of our knowledge, it is the first\nbenchmark designed to assess these four capabilities for\nretrieval-augmented generation of LLMs.\n•  We evaluated the existing LLMs using RGB and found\nthe limitations of them in the four different abilities.\n•  We analyzed the responses of LLMs in RGB and identi-\nfied their current shortcomings as well as suggested di-\nrections for improvement.\nRelated work\nRetrieval-augmented modelsThe  knowledge  stored  in\nlarge language models is commonly out-of-date (He, Zhang,\nand  Roth  2022)  and  they  also  sometimes  generate  hallu-\ncination (Cao et al. 2020; Raunak, Menezes, and Junczys-\nDowmunt  2021;  Ji  et  al.  2023)  i.e.,  they  may  generate  ir-\nrelevant  or  factually  incorrect  contents.  By  using  external\nknowledge  as  guidance,  retrieval-augmented  models  can\ngenerate  more  accurate  and  reliable  responses  (Guu  et  al.\n2020;  Lewis  et  al.  2020;  Borgeaud  et  al.  2022;  Izacard\net  al.  2022;  Shi  et  al.  2023;  Ren  et  al.  2023).  Retrieval-\naugmented models have achieved remarkable results in var-\nious  tasks  such  as  open-domain  QA  (Izacard  and  Grave\n2021;  Trivedi  et  al.  2023;  Li  et  al.  2023a),  dialogue  (Cai\n1\nOur code&data: https://github.com/chen700564/RGB.",
    "News Collection\nThe 2022 Nobel Prize for Physiology and Medicine was awarded on \nMonday to Swedish scientist Svante Pääbofor sequencing the \ngenome of the Neanderthal.\nData adjustment \nand filtering by \nHuman\n{\n“Question”: “Who was awarded the 2022 \nNobel Prize for Physiology and Medicine?”,\n“Answer”: ['Svante Pääbo','Svante Paabo’]\n}\nData generation by \nChatGPT\nRetrieve using \nsearch engine\nRerankby dense \nretrieval model\nWe simulate the process of a user querying and obtaining \ninformation. Suppose the user retrieves a current event news, \nspeculate the event that the user is concerned about and the \nquestion that he/she may want to know, and generate the key \ninformation corresponding to the answer to the question. ...\n...\nNews: The 2022 Nobel Prize for Physiology and Medicine was ...\nRelated event: 2022 Nobel Prize for Physiology and Medicine\nQuestion: Who was awarded the 2022 Nobel Prize for Physiology \nand Medicine?\nKey information: Svante Pääboand Svante Paabo\ngpt-3.5-turbo api\nQuery: Who was awarded the 2022 Nobel Prize for Physiology and \nMedicine?”,\n{\"link\": \"https://www.nobelprize.org/prizes/medicine/\", \"title\": \n\"The Nobel Prize in Physiology or Medicine 2022\", \"snippet\": \"The \nNobel Assembly...\"}, ...\nGoogle Search API\nChun2\nChunk\nWho was awarded the 2022 Nobel \nPrize for Physiology and Medicine?”,\nDense retrieval model\nTop1 Chunk\nTop30 Chunk\nTop2 Chunk\n......\n......\nFigure  2:  The  process  of  data  generation.  Firstly,  we  use\nmodels to extract (event, question, answer) from news ar-\nticles.  Next,  we  utilize  search  engines  to  retrieve  relevant\nweb pages. Finally, a dense retrieval model is employed to\nre-rank the content of these web pages.\net  al.  2019a,b;  Peng  et  al.  2023),  domain-specific  ques-\ntion answering (Cui et al. 2023) and code generation (Zhou\net al. 2023b). Recently, with the development of large mod-\nels, a series of retrieval-enhanced tools and products have\ngained widespread attention, such as ChatGPT retrieval plu-\ngin, Langchain, New Bing, etc. However, in real-world sce-\nnarios,  the  retrieved  text  inevitably  contains  noise.  There-\nfore, in this paper we conducted a systematic evaluation and\nanalysis of retrieval-augmented generation in LLMs.\nEvaluation of LLMsEvaluating LLMs has received sig-\nnificant  attention  due  to  their  remarkable  general  capabil-\nity (Chang et al. 2023). It enables us to gain a deeper under-\nstanding of the specific abilities and limitations of LLMs,\nwhile also providing valuable guidance for future research.\nIn the past, benchmarks such as GLUE (Wang et al. 2019b)\nand SuperCLUE (Wang et al. 2019a) primarily focused on\nevaluating NLP tasks, particularly in natural language un-\nderstanding. However, these evaluations often fail to fully\ncapture the capabilities of LLMs. MMLU (Hendrycks et al.\n2021) was then proposed to measure the knowledge acquired\nby language models when pre-training. Recently, with the\ndevelopment of LLMs, a series of general evaluation bench-\nmarks have emerged, such as AGIEval (Zhong et al. 2023),\nC-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),\nOpenLLM  Leaderboard  (Edward  Beeching  2023),  etc.  In\naddition to general abilities, there are also specific bench-\nmarks that focus on evaluating the capabilities of models.\nFor example, CValues (Xu et al. 2023a) focuses on the safety\nand responsibility of LLMs, M3Exam (Zhang et al. 2023)\nfocuses  on  human  exam  and  ToolBench  (Qin  et  al.  2023)\nevaluates how well LLMs use external tools. Recently, Ad-\nlakha et al. (2023) evaluate the RAG of LLMs in exist QA\ndataset. Different from their work, we focus on 4 required\nabilities of RAG and create Retrieval-Augmented Genera-\ntion Benchmark to evaluate the LLMs.\nRetrieval-Augmented Generation Benchmark\nIn  this  section,  we  first  introduce  the  specific  retrieval-\naugmented generation abilities we aim to evaluate. Next, we\noutline the process of constructing the RAG benchmark for\nevaluation. Lastly, we present the evaluation metrics.\nRequired abilities of RAG\nExternal  knowledge  is  the  key  to  resolving  the  problems\nof  LLMs  such  as  hallucination  and  outdated  knowledge,\nwhich can make LLMs generate more accurate and reliable\nresponses  through  retrieval-augmented  generation  (RAG).\nHowever, LLMs cannot always response as expected with\nRAG.  For  one  thing,  there  are  numerous  irrelevant  docu-\nments and false information on the Internet. Incorporating\nthese external documents into LLMs could have a detrimen-\ntal effect. For anthoer, LLMs suffer from the unreliable gen-\neration challenge. The generation of LLMs is often unpre-\ndictable, and we cannot guarantee that they will utilize the\nuseful information entailed in the external documents. Ad-\nditionally,  LLMs  can  easily  be  misled  by  incorrect  infor-\nmation  in  the  document.  To  this  end,  we  build  Retrieval-\nAugmented Generation Benchmark (RGB) to evaluate the\nretrieval-augmented  generation  of  LLMs,  and  we  concern\nabout 4 specific abilities:\nNoise Robustnessis  the  robustness  of  LLMs  in  noisy\ndocuments. As retrievers are not perfect, the external knowl-\nedge  they  retrieve  often  contains  a  significant  amount  of\nnoise, i.e., documents which are relevant to the question but\ndo not contain any information about the answer. To effec-\ntively answer user questions, LLMs must be able to extract\nthe necessary information from documents despite there are\nnoisy documents.\nNegative Rejectionis a measure of whether LLMs can\ndecline to answer a question when none of the contexts pro-\nvide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the an-\nswers. In these cases, it is important for the model to have\nthe capability to reject recognition and avoid generating mis-\nleading content.\nInformation Integrationis  a  capacity  to  integrate  an-\nswers  from  multiple  documents.  In  many  cases,  the  an-\nswer to a question may be contained in multiple documents.\nFor example, for the question“Who are the champions of\nthe U.S. Open 2022 men’s and women’s singles?”, the two\nchampions may be mentioned in different documents. In or-\nder to provide better answers to complex questions, it is nec-\nessary for LLMs to have the ability to integrate information.\nCounterfactual Robustnessrefers to a capacity to han-\ndle errors in external knowledge. In the real world, there is\nan  abundance  of  false  information  on  the  internet.  Please",
    "note that we only evaluate the situation that LLMs are given\nwarnings about potential risks in the retrieved information\nthrough instruction.\nIn real-world scenarios, it is not possible to obtain per-\nfect documents with all the necessary external knowledge.\nTherefore, evaluating these four abilities of the model be-\ncomes essential in order to measure the RAG of LLMs.\nData construction\nInspired by previous benchmarks for LLMs, RGB utilizes\na question-answering format for evaluation. We evaluate the\nLLMs by judging the retrieval-augmented responses of them\nto the questions. To simulate real-world scenarios, we con-\nstruct question and answer data using actual news articles.\nDue  to  the  abundance  of  knowledge  contained  within  the\nLLMs there is a potential for bias when measuring the first\nthree  abilities.  To  mitigate  this,  the  instances  of  RGB  are\nconstructed by latest news articles. Additionally, we retrieve\nexternal  documents  from  Internet  through  search  engines.\nFinally, we expand the corpus and divided it into 4 testbeds\nto evaluate the above basic abilities of LLMs. The overall\nprocedure of our data construction is illustrated in Figure 2.\nQA instances generation.We first collect latest news ar-\nticles and use prompts to make ChatGPT generate events,\nquestions,  and  answers  for  each  articles.  For  example,  as\nshown in the Figure 2, for a report about “The 2022 Nobel\nPrize”, ChatGPT will generate corresponding event, ques-\ntion and provide key information for answering it. By gen-\nerating events, the model is able to preliminarily filter out\nnews articles that do not contain any events. After genera-\ntion, we manually check the answer and filter out data that\nis difficult to retrieve through search engines.\nRetrieve using search engine.For  each  query,  we  use\nGoogle’s  API  to  fetch  10  relevant  web  pages  and  extract\ncorresponding snippets of text from them. Simultaneously,\nwe read these web pages and convert their textual content\ninto text chunks with a maximum length of 300 tokens. Us-\ning an existing dense retrieval model\n2\n, we select the top-30\ntext chunks that match the query most effectively. These re-\ntrieved text chunks, along with the snippets provided by the\nsearch API, will serve as our external documents. These doc-\numents will be divided into positive documents and negative\ndocuments based on whether they contain the answer.\nTestbeds construction for each ability.We expand the\ncorpus and divided it into 4 testbeds to evaluate the above\nbasic  abilities  of  LLMs.  To  evaluate  the  noise  robustness,\nwe  sample  varying  numbers  of  negative  documents  ac-\ncording  to  the  desired  ratio  of  noises.  For  negative  rejec-\ntion, all the external documents are sampled from negative\ndocuments. For the information integration ability, we fur-\nther construct data based on the above generated questions.\nThis involves expanding or rewriting these questions so that\ntheir answers encompass multiple aspects. For example, the\nquestion  “Who  won  the  MVP  of  Super  Bowl  2023?”  can\nbe  rewrite  as  “Who  won  the  MVPs  of  Super  Bowl  2022\nand  2023?”.  Consequently,  answering  such  questions  re-\n2\nChinese:  https://huggingface.co/moka-ai/m3e-base;  English:\nhttps://huggingface.co/sentence-transformers/all-mpnet-base-v2.\nSystem instruction\nYou are an accurate and reliable AI assistant that can \nanswer questions with the help of external documents. \nPlease note that external documents may contain noisy \nor factually incorrect information. If the information in \nthe document contains the correct answer, you will give \nan accurate answer. If the information in the document \ndoes not contain the answer, you will generate ’I can not \nanswer the question because of the insufficient \ninformation in documents.‘ If there are inconsistencies \nwith the facts in some of the documents, please generate \nthe response 'There are factual errors in the provided \ndocuments.' and provide the correct answer.\nUser input Instruction\nDocument:\\n{DOCS} \\n\\nQuestion:\\n{QUERY}\nSystem instruction\n你是一个准确和可靠的人工智能助手，\n能够借助外部文档回答问题，请注意\n外部文档可能存在噪声事实性错误。\n如果文档中的信息包含了正确答案，\n你将进行准确的回答。如果文档中的\n信息不包含答案，你将生成“文档信\n息不足，因此我无法基于提供的文档\n回答该问题。”如果部分文档中存在\n与事实不一致的错误，请先生成“提\n供文档的文档存在事实性错误。”，\n并生成正确答案。\nUser input Instruction\n文档：\\n{DOCS} \\n\\n问题：\\n{QUERY}\nEnglish\nChinese\nFigure 3: The instructions used in our experiments, which\ninclude a system instruction followed by a user input instruc-\ntion. The “{DOCS}” and “{QUERY}” will be replaced by\nthe external documents and the question.\nquires  utilizing  information  from  various  documents.  Dif-\nferent from the first three abilities, the data of counterfactual\nrobustness is constructed solely based on the internal knowl-\nedge of the model. Based on the aforementioned generated\nquestions mentioned above, we adopt ChatGPT to automat-\nically  generate  its  known  knowledge.  Specifically,  we  use\nprompts to allow the model to generate both questions and\nanswers that are already known. For example, based on the\nquestion “Who was awarded the 2022 Nobel Prize for Phys-\niology and Medicine?”, the model will generate the known\nquestion “Who was awarded the 2021 Nobel Prize in Lit-\nerature?” and answer “Abdulrazak Gurnah”. We then man-\nually  verified  the  generated  answers,  and  retrieve  relevant\ndocuments as described above. In order to make documents\ncontain factual errors, we manually modify the answers and\nreplace the corresponding parts in the document.\nFinally,  we  collect  totally  600  base  questions  in  RGB,\nand 200 additional questions for the information integration\nability and 200  additional questions for counterfactual  ro-\nbustness ability. Half of the instances are in English, and the\nother half are in Chinese.\nEvaluation metrics\nThe core of this benchmark is to evaluate whether LLMs can\nutilize the provided external documents to acquire knowl-\nedge and generate reasonable answers. We evaluate the re-\nsponses of LLMs in order to measure above-mentioned four\nabilities of them.\nAccuracyis used to measure noise robustness and infor-\nmation integration. We employ an exact matching approach\nwhere if the generated text contains an exact match to the\nanswer, it is considered as a correct answer.\nRejection rateis  used  to  measure  negative  rejection.\nWhen  only  noisy  documents  are  provided,  LLMs  should\noutput the specific content – “I can not answer the question\nbecause of the insufficient information in documents.” (We\nuse instructions to inform the model.). If the model gener-\nates this content, it indicates a successful rejection.\nError detection ratemeasures  whether  the  model  can\ndetect the factual errors in the documents for counterfactual\nrobustness.  When  the  provided  documents  contain  factual\nerrors, the model should output the specific content – “There\nare factual errors in the provided documents.” (We use in-",
    "EnglishChinese\nNoise Ratio00.20.40.60.800.20.40.60.8\nChatGPT (OpenAI 2022)96.33  94.67  94.00  90.00  76.0095.67  94.67  91.00  87.67  70.67\nChatGLM-6B (THUDM 2023a)93.6790.6789.3384.6770.6794.3390.6789.0082.3369.00\nChatGLM2-6B (THUDM 2023b)\n91.3389.6783.0077.3357.3386.6782.3376.6772.3354.00\nVicuna-7B-v1.3 (Chiang et al. 2023)87.6783.3386.0082.3360.3385.6782.6777.0069.3349.67\nQwen-7B-Chat (QwenLM 2023)94.3391.6791.0087.6773.6794.0092.3388.0084.3368.67\nBELLE-7B-2M (Yunjie Ji 2023)83.3381.0079.0071.3364.6792.0088.6785.3378.3367.68\nTable 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\nincreasing noise rate poses a challenge for RAG in LLMs.\nLong-distance information.Evidence uncertainty.Concept confusion.\nQuestionWho did Iga Swiatek defeat to win the Qatar Open 2022?What is the name of Apple’s headset?What was Tesla’s revenue in Q1 2022?\nAnswerAnett KontaveitVision Pro18.76 billion\nDocuments\nPositive document\nIn February, Swiatek entered into the Qatar Open ...\nIn the final, she won ...Anett Kontaveit...\nNegative document\nThis time, she defeated Ons Jabeur 6-2, 7-6(5) to win\nthe 2022 US Open, ...\nPositive document\nApple (AAPL.O) on Monday unveiled a costly\naugmented-reality headset called theVision Pro...\nNegative document\n... is what Gurman believes will be called\nApple Reality Pro. ...\nPositive document\nTesla, Inc. (TSLA) reported Q1 FY 2022 earnings results\n... detailed revenues of $18.76 billion...\nNegative document\n...first-quarter earnings for 2022 ...\n...Automotive revenue reached $16.86 billion...\nResponses\nIga Swiatek defeated Ons Jabeur in the second round\nof the Qatar Open 2022 to win the tournament.\nAccording to the document, the name of Apple’s\nheadset is Apple Reality Pro.\nAccording to the financial results provided in the article,\nTesla’s revenue in Q1 2022 was $16.86 billion.\nTable 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses\nare generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,\nwhile the red text highlights the non-matching parts.\nstructions to inform the model.). If the model generates this\ncontent, it indicates that the model has detected erroneous\ninformation in the document.\nError correction ratemeasures whether the model can\nprovide the correct answer after identifying errors for coun-\nterfactual robustness. The model is asked to generate the cor-\nrect answer after identifying the factual errors. If the model\ngenerates the correct answer, it indicates that the model is\ncapable of correcting errors in the document.\nConsidering  that  the  model  may  not  fully  adhere  to  in-\nstructions,  for  rejection  rate  and  error  detection  rate,  we\nalso  use  ChatGPT  to  conduct  additional  evaluation  of  the\nanswers.  Specifically,  we  assess  the  model’s  responses  by\nusing instructions and demonstrations to determine if they\ncan reflect information that is not present in the document or\nidentify any factual errors.\nExperiments\nIn  this  section,  we  evaluate  the  performance  of  various\nLLMs,  analyze  and  discuss  the  results,  summarizing  the\nmain challenges that existing LLMs encounter when using\nexternal knowledge.\nSettings\nTask formats.Due to contextual limitations, we provide 5\nexternal  documents  for  each  question.  In  our  experiments\non  noise  robustness,  we  evaluate  scenarios  with  noise  ra-\ntios ranging from 0 to 0.8. To comprehensively evaluate the\noverall  capabilities,  we  have  adopted  a  unified  instruction\nfor each language, as shown in Figure 3. The experiments\nwere conducted using an NVIDIA GeForce RTX 3090.\nModelsWe   conduct   evaluation   on   6   state-of-the-art\nlarge   language   models   which   can   generate   both   En-\nglish  and  Chinese  including  ChatGPT  (OpenAI  2022)\n3\n,\nChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM\n2023b),  Vicuna-7b-v1.3  (Chiang  et  al.  2023),  Qwen-7B-\nChat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).\nResults on Noise Robustness\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table 1.\nWe can see that:\n(1) RAG can effect improve the responses of LLMs.\nLLMs have shown strong performance even in the presence\nof noise, indicating that RAG is a promising way for LLMs\nto generate accurate and reliable responses.\n(2) The increasing noise rate poses a challenge for\nRAG in LLMs.Specifically, when the noise ratio exceeds\n80%, the accuracy decreases significantly at a significance\nlevel of 0.05. For example, the performance of ChatGPT has\ndecreased from 96.33% to 76.00%, while the performance\nof ChatGLM2-6B has decreased from 91.33% to 57.33%.\nError Analysis.To  better  comprehend  the  negative  im-\npact of noise on model generation, we examined the incor-\nrect answers and found that these errors typically originate\nfrom three reasons, as shown in Table 2.\n(1) Long-distance information.LLMs often face diffi-\nculty in identifying the correct answer from external docu-\nments when the information related to the question is distant\nfrom  the  information  related  to  the  answer.  This  scenario\nis quite common as longer texts are frequently encountered\n3\nWe use gpt-3.5-turbo api in the experiments.",
    "on the internet. In such cases, it is typical for the question’s\ninformation to be initially presented at the start of the doc-\nument and subsequently referred to using pronouns. In Ta-\nble 2, the question information (“Qatar Open 2022”) is only\nmentioned once at the beginning and is far from where the\nanswer text “Anett Kontaveit” appears. This situation may\ncause  LLMs  to  depend  on  information  from  other  docu-\nments and create false impressions, i.e., hallucination.\n(2) Evidence uncertainty.Before  highly  anticipated\nevents,  like  the  release  of  new  Apple  products  or  the  an-\nnouncement  of  the  Oscars,  there  is  often  a  significant\namount of speculative information circulating on the inter-\nnet.  Although  the  relevant  documents  explicitly  state  that\nit is uncertain or speculative content, they can still impact\non the retrieval-augmented generation of LLMs. In Table 2,\nwhen  the  noise  ratio  increases,  the  content  of  erroneous\ndocuments is all about some people’s predictions about the\nname of the headset (“Apple Reality Pro”). Even if there is\na correct answer (“Vision Pro”) in the relevant documents,\nLLMs can still be misled by uncertain evidences.\n(3) Concept confusion.The concepts in external docu-\nments may be similar to, but different from, the concepts in\nthe question. This can cause confusion for LLMs and make\nLLMs generate incorrect answers. In Table 2, the model an-\nswer  focuses  on  the  concept  “automotive  revenue”  in  the\ndocument rather than “revenue” in the question.\nBased on the analysis above, we have identified certain\nlimitations in LLMs regarding retrieval-augmented genera-\ntion. To effectively handle the vast amount of noise present\non the internet, further detailed enhancements are required\nfor the model such as long documents modeling and precise\nconcept comprehension.\nResults on Negative Rejection testbed\nWe evaluated the rejection rate when only noise documents\nwere provided. The results are shown in Table 3. In addi-\ntion to evaluating the rejection rate through exact matching\n(Rej in Table 3), we also utilize ChatGPT to determine if\nthe responses from the LLMs contain any rejection informa-\ntion (Rej\n∗\nin Table 3). We can see that:Negative Rejection\nposes a challenge for RAG in LLMs.The highest rejection\nrates for LLMs in English and Chinese were only 45% and\n43.33%, respectively. This suggests that LLMs can be easily\nmisled by noisy documents, leading to incorrect answers.\nIn addition, through comparing Rej and Rej\n∗\n, we found\nthat LLMs fail to strictly follow instructions, and they often\ngenerate unpredictable responses, which make it hard to use\nthem as state triggers (such as for recognizing rejection).\nWe  conduct  case  studies  in  Table  4.  The  first  error  is\nbecause ofEvidence uncertainty. Although the document\nonly  mentions  contact  with  “Adam  McKay”  and  does  not\nexplicitly  state  that  he  is  the  director  of  the  movie,  the\nmodel  still  concludes  that  he  holds  this  role.  The  first  er-\nror is because ofConcept confusion. The information pro-\nvided in the answer pertains to “the 2018 Winter Olympics”\ninstead of “the 2022 Olympics” mentioned in the question.\nRetrieval-augmented generation poses a greater challenge of\nnegative rejection compared to answer directly as it presents\nrelevant documents that could potentially mislead the LLMs\nLanguagesEnglishChinese\nRejRej\n∗\nRejRej\n∗\nChatGPT24.6745.005.3343.33\nChatGLM-6B\n9.0025.006.3317.00\nChatGLM2-6B10.3341.336.3336.33\nVicuna-7B-v1.3\n17.0033.333.3724.67\nQwen-7B-Chat31.0035.678.6725.33\nBELLE-7B-2M5.6732.335.3313.67\nTable 3: The result of negative rejection. Rej means the re-\njection rate (%) and Rej\n∗\nmeans the rejection rate evaluated\nby ChatGPT. We can see that negative rejection poses a chal-\nlenge for RAG in LLMs.\nQuestionAnswerResponse\nwho will direct\nIrredeemable film?\nJeymes\nSamuel\nThe information provided in the document\nis that Fox has attached “The Big Short”\nwriter-director Adam McKay to its movie\nadaptation of “Irredeemable” from Boom!\nComics. Therefore, the answer is Adam McKay.\nWhich country won the\nmost medals at the\n2022 Winter Olympics?\nNorway\nBased on the information provided in the\ndocument, the country that won the most medals\nat the 2022 Winter Olympics is Germany.\nGermany has won a total of 31 medals, including 14\ngold medals, 14 bronze medals, and 31 silver medals.\nTable  4:  Error  cases  of  negative  rejection  generated  by\nChatGLM2-6B. The red text highlights the error answers.\nand result in incorrect responses. In future developments, it\nwill be crucial for LLMs to enhance their ability to accu-\nrately match questions with the appropriate documents.\nResults on Information Integration testbed\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table 5.\nWhen  comparing  the  model  to  Table  1,  we  observed  that\nit has a weak information integration ability, which in turn\naffects its noise robustness. We can see that:\n(1) Information integration poses a challenge for RAG\nin LLMs.Even without noise, the highest accuracy of LLMs\ncan  only  reach  60%  and  67%  for  English  and  Chinese,\nrespectively.  After  adding  noise,  the  highest  accuracy  de-\ncreases to 43% and 55%. These results suggest that LLMs\nstruggle with integrating information effectively and are not\nwell-suited for directly answering complex questions.\n(2) Complex questions are more challenging for RAG\nwith noisy documents.Performance decline becomes sig-\nnificant when the noise ratio is 0.4, but for simple problems,\na significant decline occurs only at a noise ratio of 0.8 at a\nsignificance level of 0.05. This indicates that complex prob-\nlems  are  more  vulnerable  to  interference  from  noise.  We\nspeculate that this is because solving complex problems re-\nquires integrating information from multiple documents, and\nthis information can be considered as noise to each other,\nmaking it harder for the model to extract relevant informa-\ntion from the documents.\nError Analysis.We   conducted   an   error   analysis   on\nChatGLM2-6B (noise ratio is 0). Apart from the similar er-\nrors founded in the noise robustness experiment (38% of the\ntotal), there are also three types of unique errors. We have\npresented these cases in Table 6.",
    "EnglishChinese\nNoise Ratio00.20.400.20.4\nChatGPT555134635847\nChatGLM-6B453635605352\nChatGLM2-6B343221444332\nVicuna-7B-v1.360   53   43433625\nQwen-7B-Chat\n555037675655\nBELLE-7B-2M403424494138\nTable 5: The experimental result of information integration\nmeasured by accuracy (%) under different noise ratios. We\ncan  see  that  information  integration  poses  a  challenge  for\nRAG in LLMs.\nQuestionAnswerResponseErrors\nWhat groupings are France and\nGermany in World Cup 2022?\nGroup D\nGroup E\nFrance and Germany are in\nGroup D of the 2022 FIFA\nWorld Cup.\nMerging\nError\nWho were the MVP of Super\nBowl 2022 and 2023?\nCooper Kupp\nPatrick\nMahomes\nAccording to the provided\ndocument, the MVP of Super\nBowl LVI was Cooper Kupp\nof the Los Angeles Rams.\nIgnoring\nError\nWhat films won the 2022 and\n2023 Academy Awards for\nBest Picture?\nCODA\nEverything\nEverywhere\nAll at Once\nThe film CODA won the\naward for Best Picture at the\n95th Academy Awards\nceremony held on 2023.\nMisalignment\nError\nTable  6:  Error  cases  of  information  integration,  the  re-\nsponses are generated by ChatGLM2-6B. The blue and red\ntexts represent the answers to two sub-questions.\n(1) Merging Error (28% of the total).The model some-\ntimes merges the answers of the two sub-questions, resulting\nin an error. It mistakenly uses the answer from one question\nto address both two questions. At this point, the model will\ndisregard  any  documents  related  to  one  sub-question.  For\nexample, in Table 6, it incorrectly states that Group D is the\nWorld Cup group for both France and Germany, while in fact\nGermany is actually assigned to Group E.\n(2) Ignoring Error (28% of the total).Sometimes, the\nmodel may ignore one of the sub-questions and only answer\nthe other. This error occurs when the model lacks a complete\nunderstanding of the problem and fails to recognize that it\nconsists  of  multiple  sub-problems.  As  a  result,  the  model\nonly considers relevant documents for one sub-problem in\norder to generate an answer, disregarding the question posed\nby another sub-problem. For example, in Table 6, the model\nonly provides the answer for the MVP of Super Bowl 2022\nand does not consider 2023.\n(3) Misalignment Error (6% of the total).Sometimes,\nthe model incorrectly identifies the documents for one sub-\nquestion as the documents for another sub-question, leading\nto misaligned answers. For example, in Table 6, the third an-\nswer has two errors: an ignoring error and a misalignment er-\nror. Firstly, the model only mentioned the Best Picture of the\n2023 (95th) Academy Awards, completely disregarding the\n2022 awards. Additionally, it incorrectly stated that “CODA”\nis the Best Picture of 2023 when it was actually awarded as\nthe Best Picture in 2022.\nThe errors mentioned above are primarily caused by the\nlimited understanding of complex questions, which hinders\nthe ability to effectively utilize information from different\nsub-problems.  The  key  lies  in  improving  the  model’s  rea-\nsoning capability. One possible solution is to use a chain-of-\nAccAcc\ndoc\nEDED\n∗\nCR\nChatGPT-zh91171333.33\nQwen-7B-Chat-zh77125425.00\nChatGPT-en89987    57.14\nTable 7: The result of counterfactual robustness. ACC is the\naccuracy (%) of LLMs without external documents. ACC\ndoc\nis the accuracy (%) of LLMs with counterfactual documents.\nED  and  ED\n∗\nare  error  detection  rates  evaluated  by  exact\nmatching  and  ChatGPT,  respectively.  CR  is  the  error  cor-\nrection rate.\nthought approach to break down complex problems (Zhou\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-\never, these methods slow down the inference speed and can-\nnot provide timely responses.\nResults on Counterfactual Robustness testbed\nIn order to ensure that LLMs possess relevant knowledge,\nwe assess their performance by directly asking them ques-\ntions. However, we found that most LLMs struggle to an-\nswer  them  correctly.  To  ensure  a  more  reasonable  evalua-\ntion, we only consider LLMs that have an accuracy rate of\nover  70%  as  this  threshold  is  relatively  high  and  encom-\npasses more LLMs. The results are shown in Table 7. We\npresent the following metrics: accuracy without any docu-\nments,  accuracy  with  counterfactual  documents,  error  de-\ntection rates, and error correction rates. We can see that It\nis hard for LLMs to identify and correct factual errors in the\ndocuments. This suggests that the model can be easily mis-\nled by documents containing incorrect facts.\nIt is important to note that retrieval-augmented generation\nis not designed to automatically address factual errors within\na given context, as this contradicts the underlying assump-\ntion that the model lacks knowledge and relies on retrieved\ndocuments for additional information. However, this issue is\ncrucial in practical applications due to the abundance of fake\nnews on the internet. Existing LLMs do not have a safeguard\nto handle inaccurate responses caused by misinformation. In\nfact, they heavily depend on the information they retrieve.\nEven when LLMs contain the internal knowledge about the\nquestions, they often trust false information that is retrieved.\nThis presents significant a challenge for the future develop-\nment of RAG in LLMs.\nConclusion\nIn   this   paper,   we   evaluated   four   abilities   of   retrieval-\naugmented  generation  in  LLMs:  noise  robustness,  nega-\ntive  rejection,  information  integration,  and  counterfactual\nrobustness.  To  conduct  the  evaluation,  we  built  Retrieval-\nAugmented Generation Benchmark (RGB). The instances of\nRGB are generated from latest news articles and the external\ndocuments obtained from search engines. The experimental\nresults suggest that current LLMs have limitations in the 4\nabilities. This indicates that there is still a significant amount\nof work needed to effectively apply RAG to LLMs. To en-\nsure accurate and reliable responses from LLMs, it is crucial\nto exercise caution and carefully design for RAG.",
    "Acknowledgements\nThis  research  work  is  supported  by  the  National  Natural\nScience  Foundation  of China  under  Grants  no.  62122077,\n62106251, 62306303, the CAS Project for Young Scientists\nin Basic Research under Grant No.YSBR-040. Xianpei Han\nis sponsored by CCF- BaiChuan-Ebtech Foundation Model\nFund.\nReferences\nAdlakha, V.; BehnamGhader, P.; Lu, X. H.; Meade, N.; and\nReddy,  S.  2023.   Evaluating  Correctness  and  Faithfulness\nof  Instruction-Following  Models  for  Question  Answering.\narXiv:2307.16877.\nBang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V.; Xu,\nY.;  and  Fung,  P.  2023.   A  Multitask,  Multilingual,  Multi-\nmodal Evaluation of ChatGPT on Reasoning, Hallucination,\nand Interactivity. arXiv:2302.04023.\nBian,  N.;  Liu,  P.;  Han,  X.;  Lin,  H.;  Lu,  Y.;  He,  B.;  and\nSun, L. 2023.  A Drop of Ink Makes a Million Think: The\nSpread  of  False  Information  in  Large  Language  Models.\narXiv:2305.04812.\nBorgeaud,  S.;  Mensch,  A.;  Hoffmann,  J.;  Cai,  T.;  Ruther-\nford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;\nDamoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\nRing, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\nCassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\nO.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\nSifre,  L.  2022.   Improving  language  models  by  retrieving\nfrom trillions of tokens. arXiv:2112.04426.\nCai,  D.;  Wang,  Y.;  Bi,  W.;  Tu,  Z.;  Liu,  X.;  Lam,  W.;  and\nShi,  S.  2019a.Skeleton-to-Response:  Dialogue  Genera-\ntion  Guided  by  Retrieval  Memory.   InProceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),  1219–\n1228.  Minneapolis,  Minnesota:  Association  for  Computa-\ntional Linguistics.\nCai,  D.;  Wang,  Y.;  Bi,  W.;  Tu,  Z.;  Liu,  X.;  and  Shi,  S.\n2019b. Retrieval-guided Dialogue Response Generation via\na  Matching-to-Generation  Framework.   InProceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP),\n1866–1875. Hong Kong, China: Association for Computa-\ntional Linguistics.\nCao, M.; Dong, Y.; Wu, J.; and Cheung, J. C. K. 2020. Fac-\ntual Error Correction for Abstractive Summarization Mod-\nels.   InProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 6251–\n6258. Online: Association for Computational Linguistics.\nChang,  Y.;  Wang,  X.;  Wang,  J.;  Wu,  Y.;  Yang,  L.;  Zhu,\nK.;   Chen,   H.;   Yi,   X.;   Wang,   C.;   Wang,   Y.;   Ye,   W.;\nZhang,  Y.;  Chang,  Y.;  Yu,  P.  S.;  Yang,  Q.;  and  Xie,  X.\n2023.  A Survey on Evaluation of Large Language Models.\narXiv:2307.03109.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023.  Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nCui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023.  Chat-\nLaw: Open-Source Legal Large Language Model with Inte-\ngrated External Knowledge Bases. arXiv:2306.16092.\nDrozdov,  A.;  Sch\n ̈\narli,  N.;  Aky\n ̈\nurek,  E.;  Scales,  N.;  Song,\nX.; Chen, X.; Bousquet, O.; and Zhou, D. 2023.   Compo-\nsitional Semantic Parsing with Large Language Models.  In\nThe Eleventh International Conference on Learning Repre-\nsentations.\nEdward  Beeching,  N.  H.  S.  H.  N.  L.  N.  R.  O.  S.  L.  T.\nT.  W.,  Cl\n ́\nementine  Fourrier.  2023.Open  LLM  Leader-\nboard.https://huggingface.co/spaces/HuggingFaceH4/\nopen\nllmleaderboard.\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y.;\nYue, J.; and Wu, Y. 2023.   How Close is ChatGPT to Hu-\nman Experts? Comparison Corpus, Evaluation, and Detec-\ntion. arXiv:2301.07597.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.\n2020. REALM: Retrieval-Augmented Language Model Pre-\nTraining.  InProceedings of the 37th International Confer-\nence on Machine Learning, ICML’20. JMLR.org.\nHe,  H.;  Zhang,  H.;  and  Roth,  D.  2022.Rethinking\nwith Retrieval: Faithful Large Language Model Inference.\narXiv:2301.00303.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\ntitask Language Understanding. InInternational Conference\non Learning Representations.\nHuang,  Y.;  Bai,  Y.;  Zhu,  Z.;  Zhang,  J.;  Zhang,  J.;  Su,  T.;\nLiu, J.; Lv, C.; Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He,\nJ. 2023.   C-Eval: A Multi-Level Multi-Discipline Chinese\nEvaluation  Suite  for  Foundation  Models.arXiv preprint\narXiv:2305.08322.\nIzacard, G.; and Grave, E. 2021.   Leveraging Passage Re-\ntrieval  with  Generative  Models  for  Open  Domain  Ques-\ntion Answering.  InProceedings of the 16th Conference of\nthe European Chapter of the Association for Computational\nLinguistics: Main Volume, 874–880. Online: Association for\nComputational Linguistics.\nIzacard,  G.;  Lewis,  P.;  Lomeli,  M.;  Hosseini,  L.;  Petroni,\nF.;  Schick,  T.;  Dwivedi-Yu,  J.;  Joulin,  A.;  Riedel,  S.;  and\nGrave,  E.  2022.   Atlas:  Few-shot  Learning  with  Retrieval\nAugmented Language Models. arXiv:2208.03299.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii, E.;\nBang, Y. J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\nlucination in Natural Language Generation.ACM Comput.\nSurv., 55(12).\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; K\n ̈\nuttler, H.; Lewis, M.; Yih, W.-t.; Rockt\n ̈\naschel,\nT.;  Riedel,  S.;  and  Kiela,  D.  2020.   Retrieval-Augmented\nGeneration  for  Knowledge-Intensive  NLP  Tasks.   InPro-\nceedings of the 34th International Conference on Neural\nInformation Processing Systems, NIPS’20. Red Hook, NY,\nUSA: Curran Associates Inc. ISBN 9781713829546.",
    "Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\nVeit,  A.;  Yu,  F.;  and  Kumar,  S.  2023a.    Large  Language\nModels with Controllable Working Memory.  InFindings of\nthe Association for Computational Linguistics: ACL 2023,\n1774–1793.  Toronto,  Canada:  Association  for  Computa-\ntional Linguistics.\nLi,  X.;  Zhang,  T.;  Dubois,  Y.;  Taori,  R.;  Gulrajani,  I.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b.   Al-\npacaEval: An Automatic Evaluator of Instruction-following\nModels. https://github.com/tatsu-lab/alpaca\neval.\nLi, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c.  Are\nChatGPT and GPT-4 General-Purpose Solvers for Financial\nText Analytics? An Examination on Several Typical Tasks.\narXiv:2305.05862.\nLiu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifi-\nability in Generative Search Engines. arXiv:2304.09848.\nMaynez,  J.;  Narayan,  S.;  Bohnet,  B.;  and  McDonald,  R.\n2020.   On Faithfulness and Factuality in Abstractive Sum-\nmarization.  InProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 1906–1919.\nOnline: Association for Computational Linguistics.\nOpenAI. 2022.   Chatgpt: Optimizing language models for\ndialogue. https://openai.com/blog/chatgpt.\nPeng,  B.;  Galley,  M.;  He,  P.;  Cheng,  H.;  Xie,  Y.;  Hu,  Y.;\nHuang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.\nCheck  Your  Facts  and  Try  Again:  Improving  Large  Lan-\nguage  Models  with  External  Knowledge  and  Automated\nFeedback. arXiv:2302.12813.\nQin, Y.; Liang, S.; Ye, Y.; Zhu, K.; Yan, L.; Lu, Y.; Lin, Y.;\nCong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.;\nZhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023.\nToolLLM:  Facilitating  Large  Language  Models  to  Master\n16000+ Real-world APIs. arXiv:2307.16789.\nQwenLM. 2023.   Qwen-7B.   https://github.com/QwenLM/\nQwen-7B.\nRaunak, V.; Menezes, A.; and Junczys-Dowmunt, M. 2021.\nThe  Curious  Case  of  Hallucinations  in  Neural  Machine\nTranslation.  InProceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,  1172–\n1183. Online: Association for Computational Linguistics.\nRen, R.; Wang, Y.; Qu, Y.; Zhao, W. X.; Liu, J.; Tian, H.;\nWu, H.; Wen, J.-R.; and Wang, H. 2023.  Investigating the\nFactual  Knowledge  Boundary  of  Large  Language  Models\nwith Retrieval Augmentation. arXiv:2307.11019.\nShen,  X.;  Chen,  Z.;  Backes,  M.;  and  Zhang,  Y.  2023.   In\nChatGPT We Trust? Measuring and Characterizing the Re-\nliability of ChatGPT. arXiv:2304.08979.\nShi,  W.;  Min,  S.;  Yasunaga,  M.;  Seo,  M.;  James,  R.;\nLewis,  M.;  Zettlemoyer,  L.;  and  tau  Yih,  W.  2023.    RE-\nPLUG: Retrieval-Augmented Black-Box Language Models.\narXiv:2301.12652.\nTHUDM.   2023a.ChatGLM-6B.https://github.com/\nTHUDM/ChatGLM-6B.\nTHUDM.  2023b.ChatGLM2-6B.https://github.com/\nTHUDM/ChatGLM2-6B.\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\nA. 2023. Interleaving Retrieval with Chain-of-Thought Rea-\nsoning for Knowledge-Intensive Multi-Step Questions.   In\nProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n10014–10037. Toronto, Canada: Association for Computa-\ntional Linguistics.\nWang,   A.;   Pruksachatkun,   Y.;   Nangia,   N.;   Singh,   A.;\nMichael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a.Su-\nperGLUE: A Stickier Benchmark for General-Purpose Lan-\nguage Understanding Systems. Red Hook, NY, USA: Curran\nAssociates Inc.\nWang,  A.;  Singh,  A.;  Michael,  J.;  Hill,  F.;  Levy,  O.;  and\nBowman, S. R. 2019b.   GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understanding.\nInInternational Conference on Learning Representations.\nXu,  G.;  Liu,  J.;  Yan,  M.;  Xu,  H.;  Si,  J.;  Zhou,  Z.;  Yi,  P.;\nGao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;\nand Zhou, J. 2023a.  CValues: Measuring the Values of Chi-\nnese Large Language Models from Safety to Responsibility.\narXiv:2307.09705.\nXu,  S.;  Pang,  L.;  Shen,  H.;  Cheng,  X.;  and  Chua,  T.-\nS. 2023b.   Search-in-the-Chain: Towards Accurate, Credi-\nble and Traceable Large Language Models for Knowledge-\nintensive Tasks. arXiv:2304.14732.\nYunjie Ji, Y. G. Y. P. Q. N. B. M. X. L., Yong Deng. 2023.\nBELLE: Bloom-Enhanced Large Language model Engine.\nhttps://github.com/LianjiaTech/BELLE.\nZhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y. K.; and Bing,\nL. 2023.  M3Exam: A Multilingual, Multimodal, Multilevel\nBenchmark for Examining Large Language Models.\nZhong,  W.;  Cui,  R.;  Guo,  Y.;  Liang,  Y.;  Lu,  S.;  Wang,\nY.;  Saied,  A.;  Chen,  W.;  and  Duan,  N.  2023.    AGIEval:\nA  Human-Centric  Benchmark  for  Evaluating  Foundation\nModels. arXiv:2304.06364.\nZhou, D.; Sch\n ̈\narli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V.; and\nChi, E. H. 2023a.  Least-to-Most Prompting Enables Com-\nplex Reasoning in Large Language Models. InThe Eleventh\nInternational Conference on Learning Representations.\nZhou,  S.;  Alon,  U.;  Xu,  F.  F.;  Jiang,  Z.;  and  Neubig,  G.\n2023b.  DocPrompting: Generating Code by Retrieving the\nDocs.  InThe Eleventh International Conference on Learn-\ning Representations."
  ]
}