{
  "key": "X36D5P8H",
  "url": "http://arxiv.org/pdf/2309.15807",
  "metadata": {
    "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a\n  Haystack",
    "abstract": "  Training text-to-image models with web scale image-text pairs enables the\ngeneration of a wide range of visual concepts from text. However, these\npre-trained models often face challenges when it comes to generating highly\naesthetic images. This creates the need for aesthetic alignment post\npre-training. In this paper, we propose quality-tuning to effectively guide a\npre-trained model to exclusively generate highly visually appealing images,\nwhile maintaining generality across visual concepts. Our key insight is that\nsupervised fine-tuning with a set of surprisingly small but extremely visually\nappealing images can significantly improve the generation quality. We pre-train\na latent diffusion model on $1.1$ billion image-text pairs and fine-tune it\nwith only a few thousand carefully selected high-quality images. The resulting\nmodel, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only\ncounterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred\n$68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts\nand our Open User Input benchmark based on the real-world usage of\ntext-to-image models. In addition, we show that quality-tuning is a generic\napproach that is also effective for other architectures, including pixel\ndiffusion and masked generative transformer models.\n",
    "published": "2023-09-27T17:30:19Z"
  },
  "text": [
    "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack\nXiaoliang Dai\n∗\n, Ji Hou\n∗\n, Chih-Yao Ma\n∗\n, Sam Tsai\n∗\n, Jialiang Wang\n∗\n, Rui Wang\n∗\n, Peizhao Zhang\n∗\n,\nSimon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic,\nDhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song,\nRoshan Sumbaly\n†\n, Vignesh Ramanathan\n†\n, Zijian He\n†\n, Peter Vajda\n†\n, Devi Parikh\n†\nGenAI, Meta\n{xiaoliangdai, jihou, cyma, sstsai, jialiangw, ruiw, stzpz}@meta.com\nFigure 1. With quality-tuning, Emu generateshighlyaesthetic images. Prompts: (top) a glass of orange juice; a woman in an apron works\nat a local bar; a coffee mug; (bottom) an egg and a bird made of wheat bread; a corgi; a shake is next to a cake.\nAbstract\nTraining text-to-image models with web scale image-text\npairs enables the generation of a wide range of visual con-\ncepts  from  text.   However,  these  pre-trained  models  often\nface  challenges  when  it  comes  to  generating  highly  aes-\nthetic images. This creates the need for aesthetic alignment\npost pre-training.  In this paper, we propose quality-tuning\nto effectively guide a pre-trained model to exclusively gen-\nerate highly visually appealing images, while maintaining\ngenerality across visual concepts.   Our key insight is that\nsupervised fine-tuning with a set of surprisingly small but\n∗\nCore contributors: equal contribution, alphabetical order.\n†\nEqual last authors.\nextremely  visually  appealing  images  can  significantly  im-\nprove the generation quality. We pre-train a latent diffusion\nmodel on1.1billion image-text pairs and fine-tune it with\nonly a few thousand carefully selected high-quality images.\nThe  resulting  model,  Emu,  achieves  a  win  rate  of82.9%\ncompared with its pre-trained only counterpart. Compared\nto the state-of-the-art SDXLv1.0,  Emu is preferred68.4%\nand71.3%of  the  time  on  visual  appeal  on  the  standard\nPartiPrompts and our Open User Input benchmark based\non the real-world usage of text-to-image models.  In addi-\ntion, we show that quality-tuning is a generic approach that\nis also effective for other architectures, including pixel dif-\nfusion and masked generative transformer models.\n1\narXiv:2309.15807v1  [cs.CV]  27 Sep 2023",
    "1. Introduction\nRecent advances in generative models have enabled them\nto generate various high-quality content, such as text [2,33],\nimage  [21, 25],   music  [16],   video  [32],   and  even  3D\nscenes  [19, 22, 34],  which  has  fundamentally  revolution-\nized generative artificial intelligence (AI). In this paper, we\npresent  a  recipe  we  have  found  to  be  effective  for  train-\ninghighlyaesthetic text-to-image models.  It involves two\nstages:  a  knowledge  learning  stage,  where  the  goal  is  to\nacquire the ability to generate virtually anything from text,\nwhich typically involves pre-training on hundreds of mil-\nlions of image-text pairs; and a quality learning stage, which\nis necessary to restrict the output to a high-quality and aes-\nthetically pleasing domain.  We refer to the process of fine-\ntuning for the purpose of improving quality and promoting\naesthetic alignment asquality-tuningfor short.\nOur  key  insight  is  that  to  effectively  perform  quality-\ntuning, a surprisingly small amount – a couple of thousand\n– exceptionally high-quality images and associated text is\nenough  to  make  asignificantimpact  on  the  aesthetics  of\nthe generated imageswithoutcompromising the generality\nof the model in terms of visual concepts that can be gen-\nerated.  Although having more data while maintaining the\nsame level of quality may be helpful, any attempts to priori-\ntize quantity over quality may result in a compromise of the\nquality of generated images.\nThis  is  an  interesting  finding  in  the  broader  landscape\nof  fine-tuning  generative  models.   Quality-tuning  for  vi-\nsual  generative  models  can  be  thought  of  as  analogous\nto  instruction-tuning  for  large  language  models  (LLMs)\nin  terms  of  improving  generation  quality.    First,  before\ninstruction-tuning, language models are more prone to gen-\nerating low-quality text, which may be inconsistent in tone,\noverly  verbose  or  concise,  or  simply  unhelpful  or  even\ntoxic  [6, 7, 13, 20, 35];  ChatGPT-level  [2]  performance  is\nachieved with effective instruction-tuning [20].  Similarly,\nwe find that quality-tuning significantly improves the gen-\neration quality of text-to-image models. Second, the recipe\nfor  effectively  performing  instruction-tuning  and  quality-\ntuning is similar:  use high-quality data,  even if the quan-\ntity has to be small to maintain quality.   Llama2 [33] has\nbeen  fine-tuned  on27K  high-quality  prompts,  which  can\nbe  considered  a  very  small  quantity  compared  to  the  bil-\nlions or trillions of pre-training tokens.  Similarly, we find\nthat strong text-to-image performance can be achieved by\nfine-tuning with even less data – a few thousand carefully\nselected images.  Lastly, the knowledge obtained from pre-\ntraining is mostly retained after both instruction-tuning and\nquality-tuning. Like instruction-tuned LLMs, quality-tuned\ntext-to-image models retain their generality in terms of the\nvisual  concepts  that  can  be  generated.    These  post  pre-\ntraining  stages  align  the  knowledge  to  downstream  user\nvalue  –  improving  text  quality  and  following  instructions\nin the case of LLMs, and promoting aesthetic alignment in\nthe case of text-to-image models.\nConcretely, we pre-train a latent diffusion model (LDM)\non1.1billion image-text pairs and quality-tune the model\non a few thousand hand-picked exceptionally high-quality\nimages  selected  from  a  large  corpus  of  images.    By  its\nnature,  the  selection  criterion  is  subjective  and  culturally\ndependent.   We  follow  some  common  principles  in  pho-\ntography,  including  but  not  limited  to  composition,  light-\ning,  color,  effective  resolution,  focus,  and  storytelling  to\nguide  the  selection  process.   With  a  few  optimizations  to\nthe latent diffusion architecture, we start with a strong pre-\ntrained model and dramatically improve the visual appeal\nof  our  generated  images  through  quality-tuning.   In  fact,\nit significantly outperform a state-of-the-art publicly avail-\nable model SDXLv1.0 [21] on visual appeal.  We call our\nquality-tuned  LDM  Emu.   We  show  example  generations\nfrom Emu in Figure 1 and Figure 2.\nFurthermore,  we  show  that  quality-tuning  is  a  generic\napproach  that  is  also  effective  for  pixel  diffusion  and\nmasked generative transformer models.\nOur main contributions are:\n•  We build Emu, a quality-tuned latent diffusion model\nthat  significantly  outperforms  a  publicly  available\nstate-of-the-art model SDXLv1.0 on visual appeal.\n•  To the best of our knowledge, this is the first work to\nemphasize the importance of a good fine-tuning recipe\nfor aesthetic alignment of text-to-image models.  We\nprovide insights and recommendations for a key ingre-\ndient of this recipe – supervised fine-tuning with a sur-\nprisingly  small  amount  of  exceptionally  high-quality\ndata can have a significant impact on the quality of the\ngenerated images. Image quality should always be pri-\noritized over quantity.\n•  We show that quality-tuning is a generic approach that\nalso works well for other popular model architectures\nbesides  LDM,  including  pixel  diffusion  and  masked\ngenerative transformer models.\n2. Related Work\nText-to-Image Models.Generating  an  image  from  a\ntextual  description  has  been  explored  using  various  ap-\nproaches.  Diffusion-based methods learn a denoising pro-\ncess  to  gradually  generate  images  from  pure  Gaussian\nnoise [14].  The denoising process can occur either in pixel\nspace or latent space, resulting in pixel diffusion models [5,\n25, 30]  or  latent  diffusion  models  [21, 27],  which  feature\nhigher  efficiency  by  reducing  the  size  of  spatial  features.\nGenerative  transformer  methods  usually  train  autoregres-\nsive [4, 10, 11, 26, 37, 38] or non-autoregressive (masked) [9]\ntransformers in discrete token space to model the generation\n2",
    "Figure 2.Selected Examples.Selected images generated by our quality-tuned model, Emu.\n3",
    "process. Generative adversarial network [17, 31] also show\nremarkable capability of generating realistic images. While\nthese models exhibit unprecedented image generation abil-\nity, they do notalwaysgeneratehighlyaesthetic images.\nFine-Tuning Text-to-Image Models.Given a pre-trained\ntext-to-image  model,  different  methods  have  been  devel-\noped  to  enable  specific  tasks.    A  number  of  techniques\nhave been developed to personalize or adapt text-to-image\nmodels  to  a  new  subject  or  style  [12, 15, 28].    Control-\nNet [39] provides additional control to the generation pro-\ncess  by  additionally  conditioning  on  pose,  sketch,  edges,\ndepth,  etc.  InstructPix2Pix [8] makes text-to-image mod-\nels follow editing instructions by fine-tuning them on a set\nof  generated  image  editing  examples.   To  the  best  of  our\nknowledge, this is the first work highlighting fine-tuning for\ngenerically promoting aesthetic alignment for a wide range\nof visual domains.\nFine-Tuning Language Models.Fine-tuning has become\na  critical  step  in  building  high-quality  LLMs  [1, 20, 33].\nIt   generically   improves   output   quality   while   enabling\ninstruction-following capabilities.  Effective fine-tuning of\nLLMs  can  be  achieved  with  a  relatively  small  but  high-\nquality fine-tuning dataset,e.g.,using27K prompts in [33].\nIn this work, we show that effective fine-tuning of text-to-\nimage models can be also achieved with asmallbuthigh-\nqualityfine-tuning  dataset.   This  finding  shows  an  inter-\nesting connection between fine-tuning vision and language\nmodels in generative AI.\n3. Approach\nAs discussed earlier, our approach involves a knowledge\nlearning stage followed by a quality-tuning stage. This may\nseem like a well-known recipe when mapped to pre-training\nand fine-tuning. That said, the key insights here are: (i) the\nfine-tuning dataset can be surprisingly small, on the order of\na couple of thousand images, (ii) the quality of the dataset\nneeds to be very high, making it difficult to fully automate\ndata curation,  requiring manual annotation,  and (iii) even\nwith a small fine-tuning dataset, quality-tuning not only sig-\nnificantly improves the aesthetics of the generated images,\nbut does so without sacrificing generality as measured by\nfaithfulness to the input prompt.  Note that the stronger the\nbase pre-trained model, the higher the quality of the gener-\nated images after quality-tuning. To this end, we made sev-\neral modifications to the latent diffusion architecture [27] to\nfacilitate high-quality generation.  That said, quality-tuning\nis general enough and can be applied to a variety of archi-\ntectures.\nIn this section, we first introduce the latent diffusion ar-\nchitecture we use.  Then, we discuss the pre-training stage,\nFigure 3.Autoencoder.The visual quality of the reconstructed\nimages for autoencoders with different channel sizes. While keep-\ning all other architecture layers the same, we only change the latent\nchannel size. We show that the original 4-channel autoencoder de-\nsign [27] is unable to reconstruct fine details.  Increasing channel\nsize leads to much better reconstructions.  We choose to use a 16-\nchannel autoencoder in our latent diffusion model.\nfollowed by the protocol for collecting the high-quality fine-\ntuning dataset, and finally the quality-tuning stage. Later in\nSection 4, we demonstrate that quality-tuning is not limited\nto  latent  diffusion  models  but  also  improve  other  models\nsuch as pixel diffusion [30] and masked generative trans-\nformer [9] models.\n3.1. Latent Diffusion Architecture\nWe design a latent diffusion model that outputs1024×\n1024resolution images. Following standard latent diffusion\narchitecture design, our model has an autoencoder (AE) to\nencode an image to latent embeddings and a U-Net to learn\nthe denoising process.\nWe find that the commonly used 4-channel autoencoder\n(AE-4) architecture often results in a loss of details in the\nreconstructed images due to its high compression rate. The\nissue is especially noticeable in small objects. Intuitively, it\ncompresses the image resolution by64×with three2×2\ndownsampling  blocks  but  increases  the  channel  size  only\n4",
    "from 3 (RGB) to 4 (latent channels). We find that increasing\nthe channel size to 16 significantly improves reconstruction\nquality (see Table 1).   To further improve the reconstruc-\ntion performance,  we use an adversarial loss and apply a\nnon-learnable pre-processing step to RGB images using a\nFourier Feature Transformto lift the input channel dimen-\nsion from 3 (RGB) to a higher dimension to better capture\nfine structures.  See Figure 3 for qualitative results of au-\ntoencoders of different channel size.\nmodelchannelSSIMPSNRFID\n40.8028.640.35\nAE80.8630.950.19\n160.9234.000.06\nFourier-AE160.9334.190.04\nTable 1. While keeping all other architecture design choices fixed,\nwe first only change the latent channel size and report their recon-\nstruction metrics on ImageNet [29].  We see that AE-16 signifi-\ncantly improves over AE-4 on all reconstruction metrics.  Adding\na Fourier Feature Transform and an adversarial loss further im-\nproves the reconstruction performance.\nWe  use  a  large  U-Net  with  2.8B  trainable  parameters.\nWe increase the channel size and number of stacked residual\nblocks in each stage for larger model capacity. We use text\nembeddings from both CLIP ViT-L [23] and T5-XXL [24]\nas the text conditions.\n3.2. Pre-training\nWe  curate  a  large  internal  pre-training  dataset  consist-\ning  of1.1billion  images  to  train  our  model.   The  model\nis trained with progressively increasing resolutions, similar\nto [21].  This progressive approach allows the model to ef-\nficiently learn high-level semantics at lower resolutions and\nimprove finer details at the highest resolutions We also use\na noise-offset [3] of 0.02 in the final stage of pre-training.\nThis facilitates high-contrast generation, which contributes\nto the aesthetics of the generated images.\n3.3. High-Quality Alignment Data\nAs  discussed  before,  in  order  to  align  the  model  to-\nwards highly aesthetic generations – quality matters signif-\nicantly  more  than  quantity  in  the  fine-tuning  dataset  (see\nSection  4.3  for  an  ablation  study  on  quality  vs  quantity).\nAs also discussed, the notion of aesthetics is highly subjec-\ntive.   Here  we  discuss  in  detail  what  aesthetics  we  chose\nand how we curated our fine-tuning dataset by combining\nboth automated filtering and manual filtering.  The general\nquality-tuning strategy will likely apply to other aesthetics\nas well.\nAutomatic Filtering.Starting from an initial pool of bil-\nlions  of  images,  we  first  use  a  series  of  automatic  filters\nto  reduce  the  pool  to  a  few  hundreds  of  millions.   These\nFigure 4.Visual Appealing Data. Examples of visually appealing\ndata that can meet our human filtering criterion.\nfilters  include  but  are  not  limited  to  offensive  content  re-\nmoval,  aesthetic  score  filter,  optical  character  recognition\n(OCR) word count filter to eliminate images with too much\noverlaying text on them, and CLIP score filter to eliminate\nsamples  with  poor  image-text  alignment,  which  are  stan-\ndard pre-filtering steps for sourcing large datasets. We then\nperform additional automated filtering via image size and\naspect  ratio.   Lastly,  to  balance  images  from  various  do-\nmains and categories, we leverage visual concept classifi-\ncation  [36] to  source  images  from specific  domains  (e.g.,\nportrait, food, animal, landscape, car, etc). Finally, with ad-\nditional quality filtering based on proprietary signals (e.g.,\nnumber of likes), we can further reduce the data to200K.\nHuman Filtering.Next,  we perform a two-stage human\nfiltering process to only retain highly aesthetic images.  In\nthe first stage, we traingeneralistannotators to downselect\nthe image pool to 20K images.   Our primary goal during\nthis  stage  is  to  optimize  recall,  ensuring  the  exclusion  of\nmedium and low quality that may have passed through the\nautomatic  filtering.   In  the  second  stage,  we  engagespe-\ncialistannotators who have a good understanding of a set\nof  photography  principles.   Their  task  is  to  filter  and  se-\nlect images of the highest aesthetic quality (see Figure 4 for\nexamples).  During this stage, we focus on optimizing pre-\ncision, meaning we aim to select only the very best images.\nA brief annotation guideline for photorealistic images is as\nfollows.  Our hypothesis is that following basic principles\nof high quality photography leads to generically more aes-\nthetic images across a variety of styles, which is validated\n5",
    "via human evaluation.\n1.Composition.The  image  should  adhere  to  certain\nprinciples  of  professional  photography  composition,\nincluding the “Rule Of Thirds”, “Depth and Layering”,\nand more.  Negative examples may include imbalance\nin visual weight,  such as when all focal subjects are\nconcentrated on one side of the frame,  subjects cap-\ntured from less flattering angles, or instances where the\nprimary subject is obscured, or surrounding unimpor-\ntant objects are distracting from the subject.\n2.Lighting.We are looking for dynamic lighting with\nbalanced  exposure  that  enhances  the  image,  for  ex-\nample, lighting that originates from an angle, casting\nhighlights on select areas of the background and sub-\nject(s). We try to avoid artificial or lackluster lighting,\nas well as excessively dim or overexposed light.\n3.Color and Contrast.We prefer images with vibrant\ncolors and strong color contrast. We avoid monochro-\nmatic images or those where a single color dominates\nthe entire frame.\n4.Subject and Background.The image should have a\nsense of depth between the foreground and background\nelements.  The background should be uncluttered but\nnot  overly  simplistic  or  dull.   The  focused  subjects\nmust be intentionally placed within the frame, ensur-\ning that all critical details are clearly visible without\ncompromise.   For instance,  in a portrait,  the primary\nsubject of image should not extend beyond the frame\nor be obstructed.  Furthermore,  the level of detail on\nthe foreground subject is extremely important.\n5.Additional Subjective Assessments.Furthermore,\nwe  request  annotators  to  provide  their  subjective  as-\nsessments to ensure that only images ofexceptionally\naesthetic quality are retained by answering a couple of\nquestions, such as: (i) Does this image convey a com-\npelling story? (ii) Could it have been captured signifi-\ncantly better? (iii) Is this among the best photos you’ve\never seen for this particular content?\nThrough this filtering process, we retained a total of 2000\nexceptionally high-quality images.  Subsequently, we com-\nposed  ground-truth  captions  for  each  of  them.   Note  that\nsome of these handpicked images are below our target reso-\nlution of1024×1024. We trained a pixel diffusion upsam-\npler inspired by the architecture proposed in [30] to upsam-\nple these images when necessary.\n3.4. Quality-Tuning\nWe can think of the visually stunning images (like the\n2000  images  we  collected)  as  a  subset  of  all  images  that\nshare  some  common  statistics.   Our  hypothesis  is  that  a\nstrongly  pre-trained  model  is  already  capable  of  generat-\ning highly aesthetic images,  but the generation process is\nnot properly guided towards always producing images with\nthese statistics.  Quality-tuning effectively restricts outputs\nto a high-quality subset.\nWe fine-tune the pre-trained model with a small batch\nsize of64.  We use a noise-offset of0.1at this stage.  Note\nthat  early  stopping  is  important  here  as  fine-tuning  on  a\nsmall dataset for too long will result in significant overfit-\nting and degradation in generality of visual concepts.  We\nfine-tune for no more than15K iterations despite the loss\nstill decreasing.  This total iteration number is determined\nempirically.\n4. Experiments\nWe compare our quality-tuned model to our pre-trained\nmodel  to  demonstrate  the  effectiveness  of  quality-tuning.\nTo place the visual appeal of our generated images in con-\ntext with a current state-of-the-art model, we compare our\nmodel to SDXLv1.0 [21]. Due to lack of access to training\ndata  of  SDXL  and  their  underlying  model,  we  leveraged\ntheir corresponding APIs for our comparison. Note that un-\nlike SDXL, we use a single stage architecture, and do not\nuse  a  subsequent  refinement  stage.   As  stated  earlier,  we\nalso show that quality-tuning is not specific to LDMs, and\ncan be applied to other architectures – pixel diffusion and\nmasked generative transformer models.\n4.1. Evaluation Setting\nPrompts.We evaluate on two large sets of prompts: 1600\nPartiPrompts  [37]  which  is  commonly  used  for  text-to-\nimage generation benchmarking, and our 2100 Open User\nInput (OUI) Prompts. The OUI prompt set is based on real-\nworld user prompts.   It captures prompts that are popular\nwith text-to-image models,  reflecting visual concepts that\nare relevant to real-world use cases (Figure 5), paraphrased\nby  LLMs  to  be  closer  to  how  regular  users  might  input\nprompts (as opposed to being highly prompt engineered).\nThe overall motivation was to capture the creativity and in-\ntricacies  of  popular  prompts  for  text-to-image  models  so\nwe are pushing the capabilities of models, while also being\ngrounded in likely real world use cases.\nMetrics.We use two separate evaluation metrics:  visual\nappeal  and  text  faithfulness.   Visual  appeal  refers  to  the\noverall aesthetic quality of a generated image.  It combines\nvarious visual elements such as color,  shape,  texture,  and\ncomposition that creates a pleasing and engaging look. The\nconcept of visual appeal is subjective and varies from per-\nson to person, as what may be aesthetically pleasing to one\nperson may not be to another. Therefore, we ask five anno-\ntators to rate each sample.  Concretely, we show annotators\n6",
    "Animals\nFood\nArt & Fashion\nCulture\nTransportation\nNature\nEmotion\nActivity\nArchitecture\nObjects\nHuman\nLocation\nFigure 5.Prompt distributions. The distribution of different con-\ncepts in our Open User Input prompts. We cover a comprehensive\nlist of common concepts people typically use to generate images.\ntwo images A and B, side-by-side, each generated by a dif-\nferent model using the same caption.  The text captions are\nnot displayed.  Annotators choose which image is more vi-\nsually appealing by selecting “A”, “B” or “Tie”.\nText  faithfulness  refers  to  the  degree  of  similarity  be-\ntween a generated image and a text caption.   In this task,\nwe again display two generated images A and B, side-by-\nside, but with the caption alongside the images. The annota-\ntors are asked to ignore the visual appeal of the images and\nchoose which ones best describe the caption with choices\n“A ”, “B”, “Both”, and “Neither”, where “Both” and “Nei-\nther” are considered as “Tie”.  In this task, we have three\nannotators to annotate each sample pair.\nWe do not report “standard” metrics such as FID scores.\nAs argued in many recent papers (e.g.,[18, 21]), FID scores\ndo not correlate well with human assessment of the perfor-\nmance of generative models.\n4.2. Results\nEffectiveness of Quality-Tuning.First, we compare our\nquality-tuned model, Emu, with the pre-trained model. See\nFigure 7 for random (not cherry-picked) qualitative exam-\nples before and after quality-tuning.  Note the highly aes-\nthetic non-photorealistic image as well, validating our hy-\npothesis  that  following  certain  photography  principles  in\ncurating the quality-tuning dataset leads to improved aes-\nthetics for a broad set of styles. We show more examples of\ngenerated images using Emu in Figure 8 and Figure 9.\nQuantitatively, as shown in Figure 6 (top), after quality-\ntuning, Emu is preferred in both visual appeal and text faith-\nfulness by a significant margin.  Specifically,  Emu is pre-\nferred82.9%and91.2%of the time for visual appeal, and\n36.7%and47.9%of the time for text faithfulness on Par-\ntiPrompts and OUI Prompts, respectively.  In contrast, the\n(a) All Prompts\n(b) Stylized Prompts\nFigure 6.Quality-Tuning vs Pre-training. Human evaluation on\nboth the PartiPrompts and Open User Input prompts shows that\nour quality-tuned model, Emu, significantly outperforms the pre-\ntrained model on visual appeal, without loss of generality of visual\nconcepts or styles that can be generated.\npre-trained model is preferred only15.4%and7.9%of the\ntime for visual appeal, and21.0%and18.5%of the time for\ntext faithfulness on PartiPrompts and OUI Prompts. The re-\nmaining cases result in ties. From the two large sets of eval-\nuation data that covers various domains and categories, we\ndid not observe degradation of generality across visual con-\ncepts.  In fact, as seen, text-faithfulness also improved.  We\nhypothesize this is because the captions of the 2000 quality-\ntuning images were manually written, while the captions in\nthe pre-training dataset tend to be noisy.  Finally, we ana-\nlyze the results on non-photorealistic stylized prompts (e.g.,\nsketches,  cartoons,  etc.).   We  find  that  the  improvements\nbroadly apply to a variety of styles, see Figure 6 (bottom).\nVisual Appeal in the Context of SoTA.To place the vi-\nsual appeal of our generated images in the context of current\nstate-of-the-art, we compare Emu with SDXLv1.0 [21]. As\nshown in Table 2, our model is preferred over SDXLv1.0 in\nvisual appeal by a significant margin – including on stylized\n(non-photorealistic) prompts.\n7",
    "Figure 7.Qualitative Comparison. a comparison of images gen-\nerated by the pre-trained and quality-tuned model.\nQuality-Tuning Other Architectures.Next,  we  show\nour quality-tuning can also improve other popular architec-\ntures, such as pixel diffusion and masked generative trans-\nformer models.  Specifically, we re-implement and re-train\nfrom scratch a pixel diffusion model, similar to Imagen [30]\nEval datawin (%)tie (%)lose (%)\nParti (All)68.42.129.5\nOUI (All)71.31.227.5\nParti (Stylized)81.71.916.3\nOUI (Stylized)75.51.423.1\nTable 2.Emu vs SDXL on Visual Appeal.  Our model is pre-\nferred over SDXL by a large margin, including on stylized, non-\nphotorealistic prompts.\narchitecture,  and a masked generative transformer model,\nsimilar to Muse [9] architecture, and then quality-tune them\non  2000  images.   We  evaluate  both  quality-tuned  models\non1/3randomly sampled PartiPrompts.  As shown in Fig-\nure 10, both architectures show significant improvement af-\nter quality-tuning on both visual appeal and text faithfulness\nmetrics.\n4.3. Ablation Studies\nWe do ablation studies on the fine-tuning dataset with a\nfocus on visual appeal.  We first investigate the impact of\nthe dataset size. We report results of quality-tuning on ran-\ndomly sampled subsets of sizes 100, 1000 and 2000 in Ta-\nble 3. With even just 100 fine-tuning images, the model can\nalready be guided towards generating visually-pleasing im-\nages, jumping from a win rate of24.8%to60%compared\nwith SDXL.\nfine-tune data\nwin (%)tie (%)lose (%)\nw/o quality-tuning24.81.473.9\n10060.31.538.2\n100063.21.935.0\n200067.02.630.4\nTable 3.Visual Appeal by Fine-Tuning Dataset Size.All the\nnumbers are against SDXL as baseline.   With merely 100 high-\nquality  images  as  fine-tuning  data,  our  quality-tuned  model  can\nalready outperform SDXL in visual appeal.   Our model’s visual\nappeal further improves as more images are used.\n5. Limitation\nLimitation of Human Evaluation.Relative to most pub-\nlished works, the prompt sets we evaluate on are reasonably\nlarge (1600 Parti prompts and our 2100 Open User Input\nprompts),  under  a  multi-review  annotation  setting.   Even\nthen, the evaluation results may not fully reflect real-world\nusage of the models. In addition, human evaluation of text-\nto-image models, especially when it comes to aesthetics, is\ninherently subjective and noisy.  As a result, evaluation on\na different set of prompts or with different annotators and\nguidelines may lead to different results.\nLimitations of Small-Scale Fine-Tuning.The  role  of\nquality-tuning is to restrict the output distribution to a high-\nquality domain.  However, issues rooted from pre-training\n8",
    "Figure 8.Generated Examples.Images generated by our quality-tuned model, Emu.\n9",
    "Figure 9.More Examples.Images generated by our quality-tuned model, Emu.\n10",
    "Figure 10.Quality-Tuning vs Pre-training on Pixel Diffusion\nand Masked Generative Transformer.   We  adapt  our  quality-\ntuning to other popular text-to-image model architectures. Our re-\nsults indicate that the success of quality-tuning can be transferred\nto other architectures, beyond latent diffusion models.\nmay still persist.  For instance, the models may struggle to\ngenerate  certain  objects  that  were  not  sufficiently  learned\nduring pre-training.\nLimitations of Text-to-Image Models in General.Like\nother  text-to-image  models,  our  models  may  generate  bi-\nased,  misleading,  or  offensive  outputs.   We’ve  invested  a\nsignificant  amount  in  fairness  and  safety  of  our  models  -\nstarting with balanced dataset construction,  creating dedi-\ncated evaluation sets for these risk categories and investing\nmultiple hours of redteaming.\n6. Conclusion\nIn this paper, we demonstrated that manually selecting\nhigh  quality  images  that  are  highly  aesthetically-pleasing\nis one of the most important keys to improving the aesthet-\nics of images generated by text-to-image generative models.\nWe showed that with just a few hundred to thousand fine-\ntuning images, we were able to improve the visual appeal\nof generated images without compromising on the general-\nity of visual concepts depicted.  With this finding, we build\nEmu, a LDM for high-quality image synthesis. On the com-\nmonly used PartiPrompts and our Open User Input Prompts,\nwe  carefully  evaluate  our  model  against  a  publicly  avail-\nable state-of-the-art text-to-image model (SDXLv1.0 [21])\nas well as our pre-trained LDM. We also show that quality-\ntuning  not  only  improves  LDMs,  but  also  pixel  diffusion\nand masked generative transformer models.\n7. Acknowledgement\nThis  work  would  not  have  been  possible  without  a\nlarge  group  of  collaborators  who  helped  with  the  un-\nderlying  infrastructure,  data,  privacy,  and  the  evaluation\nframework.We  extend  our  gratitude  to  the  following\npeople  for  their  contributions  (alphabetical  order):   Eric\nAlamillo, Andr\n ́\nes Alvarado, Giri Anantharaman, Stuart An-\nderson,  Snesha Arumugam,  Chris Bray,  Matt Butler,  An-\nthony  Chen,  Lawrence  Chen,  Jessica  Cheng,  Lauren  Co-\nhen, Jort Gemmeke, Freddy Gottesman, Nader Hamekasi,\nZecheng  He,  Jiabo  Hu,  Praveen  Krishnan,  Carolyn  Krol,\nTianhe  Li,  Mo  Metanat,  Vivek  Pai,  Guan  Pang,  Albert\nPumarola, Ankit Ramchandani, Stephen Roylance, Kalyan\nSaladi,  Artsiom Sanakoyeu,  Dev Satpathy,  Alex Schneid-\nman,  Edgar  Schoenfeld,  Shubho  Sengupta,  Hardik  Shah,\nShivani  Shah,  Yaser  Sheikh,  Karthik  Sivakumar,  Lauren\nSpencer, Fei Sun, Ali Thabet, Mor Tzur, Mike Wang, Mack\nWard,  Bichen  Wu,  Seiji  Yamamoto,  Licheng  Yu,  Hector\nYuen, Luxin Zhang, Yinan Zhao, and Jessica Zhong.\nFinally,  thank you Connor Hayes,  Manohar Paluri and\nAhmad Al-Dahle for your support and leadership.\nReferences\n[1]  https://cdn.openai.com/papers/gpt-4.pdf.\n[2]  https://openai.com/blog/chatgpt/.\n[3]  https://www.crosslabs.org/blog/diffusion-with-offset-noise/.\n[4]  Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir\nKarpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Man-\ndar Joshi, Gargi Ghosh, Mike Lewis, et al.  Cm3:  A causal\nmasked  multimodal  model  of  the  internet.\narXivpreprint\narXiv:2201.07520, 2022.\n[5]  Yogesh  Balaji,  Seungjun  Nah,  Xun  Huang,  Arash  Vahdat,\nJiaming  Song,  Karsten  Kreis,  Miika  Aittala,  Timo  Aila,\nSamuli  Laine,  Bryan  Catanzaro,  et  al.ediffi:   Text-to-\nimage diffusion models with an ensemble of expert denois-\ners.\narXivpreprintarXiv:2211.01324, 2022.\n[6]  Emily M Bender, Timnit Gebru, Angelina McMillan-Major,\nand  Shmargaret  Shmitchell.   On  the  dangers  of  stochastic\nparrots:  Can language models be too big?   InProceedings\nofthe2021ACMconferenceonfairness,accountability,and\ntransparency, pages 610–623, 2021.\n[7]  Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette  Bohg,  Antoine  Bosselut,  Emma  Brunskill,  et  al.\nOn the opportunities and risks of foundation models.\narXiv\npreprintarXiv:2108.07258, 2021.\n[8]  Tim Brooks, Aleksander Holynski, and Alexei A Efros.  In-\nstructpix2pix: Learning to follow image editing instructions.\nIn\nProceedingsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition, pages 18392–18402, 2023.\n[9]  Huiwen Chang,  Han Zhang,  Jarred Barber,  AJ Maschinot,\nJose  Lezama,  Lu  Jiang,  Ming-Hsuan  Yang,  Kevin  Mur-\nphy, William T Freeman, Michael Rubinstein, et al.  Muse:\nText-to-image generation via masked generative transform-\ners.\narXivpreprintarXiv:2301.00704, 2023.\n[10]  Ming  Ding,   Zhuoyi  Yang,   Wenyi  Hong,   Wendi  Zheng,\nChang  Zhou,  Da  Yin,  Junyang  Lin,  Xu  Zou,  Zhou  Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image gen-\neration via  transformers.\nAdvancesinNeuralInformation\nProcessingSystems, 34:19822–19835, 2021.\n[11]  Oran  Gafni,  Adam  Polyak,  Oron  Ashual,  Shelly  Sheynin,\nDevi  Parikh,  and  Yaniv  Taigman.    Make-a-scene:   Scene-\nbased  text-to-image  generation  with  human  priors.In\n11",
    "EuropeanConferenceonComputerVision,  pages  89–106.\nSpringer, 2022.\n[12]  Rinon   Gal,   Yuval   Alaluf,   Yuval   Atzmon,   Or   Patash-\nnik,  Amit  H  Bermano,  Gal  Chechik,  and  Daniel  Cohen-\nOr.    An  image  is  worth  one  word:   Personalizing  text-to-\nimage  generation  using  textual  inversion.\narXivpreprint\narXiv:2208.01618, 2022.\n[13]  Samuel  Gehman,  Suchin  Gururangan,  Maarten  Sap,  Yejin\nChoi, and Noah A Smith.  Realtoxicityprompts:  Evaluating\nneural toxic degeneration in language models.\narXivpreprint\narXiv:2009.11462, 2020.\n[14]  Jonathan Ho, Ajay Jain, and Pieter Abbeel.  Denoising dif-\nfusion probabilistic models.Advancesinneuralinformation\nprocessingsystems, 33:6840–6851, 2020.\n[15]  Edward J Hu,  Yelong Shen,  Phillip Wallis,  Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv\npreprintarXiv:2106.09685, 2021.\n[16]  Qingqing Huang,  Daniel S Park,  Tao Wang,  Timo I Denk,\nAndy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,\nJiahui  Yu,  Christian  Frank,  et  al.Noise2music:   Text-\nconditioned music generation with diffusion models.\narXiv\npreprintarXiv:2302.03917, 2023.\n[17]  Minguk  Kang,  Jun-Yan  Zhu,  Richard  Zhang,  Jaesik  Park,\nEli  Shechtman,  Sylvain  Paris,  and  Taesung  Park.Scal-\ning up gans for text-to-image synthesis.   InProceedingsof\ntheIEEE/CVFConferenceonComputerVisionandPattern\nRecognition, pages 10124–10134, 2023.\n[18]  Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana,  Joe  Penna,  and  Omer  Levy.    Pick-a-pic:   An  open\ndataset  of  user  preferences  for  text-to-image  generation.\narXivpreprintarXiv:2305.01569, 2023.\n[19]  Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui  Zeng,  Xun  Huang,  Karsten  Kreis,  Sanja  Fidler,\nMing-Yu Liu, and Tsung-Yi Lin.  Magic3d: High-resolution\ntext-to-3d content creation. InProceedingsoftheIEEE/CVF\nConferenceonComputerVisionandPatternRecognition,\npages 300–309, 2023.\n[20]  Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll  Wainwright,  Pamela  Mishkin,  Chong  Zhang,  Sand-\nhini  Agarwal,  Katarina  Slama,  Alex  Ray,  et  al.    Training\nlanguage  models  to  follow  instructions  with  human  feed-\nback.\nAdvancesinNeuralInformationProcessingSystems,\n35:27730–27744, 2022.\n[21]  Dustin   Podell,Zion   English,Kyle   Lacey,Andreas\nBlattmann,  Tim  Dockhorn,  Jonas  M\n ̈\nuller,  Joe  Penna,  and\nRobin  Rombach.    Sdxl:   Improving  latent  diffusion  mod-\nels  for  high-resolution  image  synthesis.arXivpreprint\narXiv:2307.01952, 2023.\n[22]  Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.   Dreamfusion:  Text-to-3d  using  2d  diffusion.arXiv\npreprintarXiv:2209.14988, 2022.\n[23]  Alec  Radford,   Jong  Wook  Kim,   Chris  Hallacy,   Aditya\nRamesh,  Gabriel  Goh,  Sandhini  Agarwal,  Girish  Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable  visual  models  from  natural  language  supervi-\nsion. In\nInternationalconferenceonmachinelearning, pages\n8748–8763. PMLR, 2021.\n[24]  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu.   Exploring the limits of transfer learning with\na unified text-to-text transformer.\nTheJournalofMachine\nLearningResearch, 21(1):5485–5551, 2020.\n[25]  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand  Mark  Chen.   Hierarchical  text-conditional  image  gen-\neration with clip latents.arXivpreprintarXiv:2204.06125,\n2022.\n[26]  Aditya   Ramesh,   Mikhail   Pavlov,   Gabriel   Goh,   Scott\nGray,   Chelsea   Voss,   Alec   Radford,   Mark   Chen,   and\nIlya  Sutskever.Zero-shot  text-to-image  generation.In\nInternationalConferenceonMachineLearning, pages 8821–\n8831. PMLR, 2021.\n[27]  Robin  Rombach,   Andreas  Blattmann,   Dominik  Lorenz,\nPatrick  Esser,  and  Bj\n ̈\norn  Ommer.    High-resolution  image\nsynthesis  with  latent  diffusion  models.   In\nProceedingsof\ntheIEEE/CVFconferenceoncomputervisionandpattern\nrecognition, pages 10684–10695, 2022.\n[28]  Nataniel  Ruiz,  Yuanzhen  Li,  Varun  Jampani,  Yael  Pritch,\nMichael Rubinstein, and Kfir Aberman.  Dreambooth:  Fine\ntuning  text-to-image  diffusion  models  for  subject-driven\ngeneration. 2022.\n[29]  Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya  Khosla,  Michael  Bernstein,  et  al.    Imagenet  large\nscale visual recognition challenge.\nInternationaljournalof\ncomputervision, 115:211–252, 2015.\n[30]  Chitwan  Saharia,   William  Chan,   Saurabh  Saxena,   Lala\nLi,  Jay  Whang,  Emily  L  Denton,  Kamyar  Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage  understanding.AdvancesinNeuralInformation\nProcessingSystems, 35:36479–36494, 2022.\n[31]  Axel  Sauer,  Tero  Karras,  Samuli  Laine,  Andreas  Geiger,\nand  Timo  Aila.   Stylegan-t:  Unlocking  the  power  of  gans\nfor fast large-scale text-to-image synthesis.arXivpreprint\narXiv:2301.09515, 2023.\n[32]  Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang  Zhang,  Qiyuan  Hu,  Harry  Yang,  Oron  Ashual,\nOran Gafni, et al.  Make-a-video:  Text-to-video generation\nwithout text-video data.arXivpreprintarXiv:2209.14792,\n2022.\n[33]  Hugo  Touvron,   Thibaut  Lavril,   Gautier  Izacard,   Xavier\nMartinet, Marie-Anne Lachaux, Timoth\n ́\nee Lacroix, Baptiste\nRozi\n`\nere,  Naman  Goyal,  Eric  Hambro,  Faisal  Azhar,  et  al.\nLlama:   Open  and  efficient  foundation  language  models.\narXivpreprintarXiv:2302.13971, 2023.\n[34]  Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion.arXivpreprintarXiv:2305.16213, 2023.\n[35]  Laura Weidinger, John Mellor, Maribeth Rauh, Conor Grif-\nfin,  Jonathan  Uesato,  Po-Sen  Huang,  Myra  Cheng,  Mia\nGlaese, Borja Balle, Atoosa Kasirzadeh, et al.   Ethical and\nsocial risks of harm from language models.\narXivpreprint\narXiv:2112.04359, 2021.\n12",
    "[36]  I Zeki Yalniz, Herv\n ́\ne J\n ́\negou, Kan Chen, Manohar Paluri, and\nDhruv Mahajan.  Billion-scale semi-supervised learning for\nimage classification.\narXivpreprintarXiv:1905.00546, 2019.\n[37]  Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei  Yang,  Burcu  Karagol  Ayan,  et  al.   Scaling  autoregres-\nsive models for content-rich text-to-image generation.\narXiv\npreprintarXiv:2206.10789, 2(3):5, 2022.\n[38]  Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al.  Scaling autoregressive multi-\nmodal  models:   Pretraining  and  instruction  tuning.\narXiv\npreprintarXiv:2309.02591, 2023.\n[39]  Lvmin Zhang and Maneesh Agrawala.  Adding conditional\ncontrol  to  text-to-image  diffusion  models.arXivpreprint\narXiv:2302.05543, 2023.\n13"
  ]
}