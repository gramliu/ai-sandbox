{
  "key": "9T63NP54",
  "url": "http://arxiv.org/pdf/2308.10792",
  "metadata": {
    "title": "Instruction Tuning for Large Language Models: A Survey",
    "abstract": "  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n",
    "published": "2023-08-21T15:35:16Z"
  },
  "text": [
    "Instruction Tuning for Large Language Models: A Survey\nShengyu Zhang\n♠\n, Linfeng Dong\n♠\n, Xiaoya Li\n♣\n, Sen Zhang\n♠\nXiaofei Sun\n♠\n, Shuhe Wang\n♣\n, Jiwei Li\n♠♣\n, Runyi Hu\n♠\nTianwei Zhang\n▲\n, Fei Wu\n♠\nand Guoyin Wang\n♦\nAbstract\nThis  paper  surveys  research  works  in  the\nquickly advancing field of instruction tuning\n(IT),   a   crucial   technique   to   enhance   the\ncapabilities    and    controllability    of    large\nlanguage models (LLMs).  Instruction tuning\nrefers to the process of further training LLMs\non  a  dataset  consisting  of(INSTRUCTION,\nOUTPUT)pairs   in   a   supervised   fashion,\nwhich bridges the gap between the next-word\nprediction objective of LLMs and the users’\nobjective  of  having  LLMs  adhere  to  human\ninstructions.In   this   work,   we   make   a\nsystematic review of the literature, including\nthe general methodology of IT, the construction\nof IT datasets, the training of IT models, and\napplications to different modalities, domains\nand   application,   along   with   analysis   on\naspects that influence the outcome of IT (e.g.,\ngeneration of instruction outputs, size of the\ninstruction dataset, etc).  We also review the\npotential  pitfalls  of  IT  along  with  criticism\nagainst  it,  along  with  efforts  pointing  out\ncurrent deficiencies of existing strategies and\nsuggest some avenues for fruitful research.\n1    Introduction\nThe   field   of   large   language   models   (LLMs)\nhas   witnessed   remarkable   progress   in   recent\nyears.LLMs  such  as  GPT-3  (Brown  et  al.,\n2020b),  PaLM  (Chowdhery  et  al.,  2022),  and\nLLaMA (Touvron et al., 2023a) have demonstrated\nimpressive  capabilities  across  a  wide  range  of\nnatural language tasks (Zhao et al., 2021; Wang\net al., 2022b, 2023a; Wan et al., 2023; Sun et al.,\n2023c; Wei et al., 2023a; Li et al., 2023a; Gao et al.,\n2023a; Yao et al., 2023; Yang et al., 2022a; Qian\net al., 2022; Lee et al., 2022; Yang et al., 2022b;\n♠\nZhejiangUniversity,\n♣\nShannon.AI,\n▲\nNanyang\nTechnological University,\n♦\nAmazon\nEmail: sy_zhang@zju.edu.cn\nProject  page  can  be  found  at:https://github.\ncom/xiaoya-li/Instruction-Tuning-Survey\n* The latest update was on March 12, 2024.\nGao  et  al.,  2023b;  Ning  et  al.,  2023;  Liu  et  al.,\n2021b; Wiegreffe et al., 2021; Sun et al., 2023b,a;\nAdlakha et al., 2023; Chen et al., 2023b).   One\nof the major issues with LLMs is the mismatch\nbetween the training objective and users’ objective:\nLLMs  are  typically  trained  on  minimizing  the\ncontextual word prediction error on large corpora;\nwhile  users  want  the  model  to  \"follow  their\ninstructions helpfully and safely\" (Radford et al.,\n2019; Brown et al., 2020a; Fedus et al., 2021; Rae\net al., 2021; Thoppilan et al., 2022)\nTo  address  this  mismatch,  instruction  tuning\n(IT) is proposed, serving as an effective technique\nto  enhance  the  capabilities  and  controllability\nof  large  language  models.It  involves  further\ntraining  LLMs  using(INSTRUCTION,OUTPUT)\npairs,  whereINSTRUCTIONdenotes  the  human\ninstruction for the model, andOUTPUTdenotes the\ndesired output that follows theINSTRUCTION. The\nbenefits of IT are threefold: (1) Finetuning an LLM\non the instruction dataset bridges the gap between\nthe next-word prediction objective of LLMs and\nthe users’ objective of instruction following; (2)\nIT allows for a more controllable and predictable\nmodel behavior compared to standard LLMs. The\ninstructions serve to constrain the model’s outputs\nto align with the desired response characteristics\nor  domain  knowledge,  providing  a  channel  for\nhumans to intervene with the model’s behaviors;\nand (3) IT is computationally efficient and can help\nLLMs rapidly adapt to a specific domain without\nextensive retraining or architectural changes.\nDespite   its   effectiveness,IT   also   poses\nchallenges: (1) Crafting high-quality instructions\nthat properly cover the desired target behaviors is\nnon-trivial: existing instruction datasets are usually\nlimited in quantity,  diversity,  and creativity;  (2)\nthere has been an increasing concern that IT only\nimproves on tasks that are heavily supported in\nthe IT training dataset (Gudibande et al., 2023);\nand  (3)  there  has  been  an  intense  criticism  that\narXiv:2308.10792v5  [cs.CL]  14 Mar 2024",
    "IT only captures surface-level patterns and styles\n(e.g., the output format) rather than comprehending\nand  learning  the  task  (Kung  and  Peng,  2023).\nImproving  instruction  adherence  and  handling\nunanticipated   model   responses   remain   open\nresearch problems. These challenges highlight the\nimportance of further investigations, analysis, and\nsummarization in this field, to optimize the fine-\ntuning process and better understand the behavior\nof instruction fine-tuned LLMs.\nIn the literature, there has been an increasing\nresearch  interest  in  analysis  and  discussions  on\nLLMs, including pre-training methods (Zhao et al.,\n2023),  reasoning  abilities  (Huang  and  Chang,\n2022),   downstream  applications  (Yang  et  al.,\n2023a; Sun et al., 2023b), but rarely on the topic of\nLLM instruction finetuning. This survey attempts\nto fill this blank, organizing the most up-to-date\nstate of knowledge on this quickly advancing field.\nSpecifically,\n•Section 2 presents the general methodology\nemployed in instruction fine-tuning.\n•Section 3 outlines the construction process of\ncommonly-used IT representative datasets.\n•\nSection 4 presents representative instruction-\nfinetuned models.\n•Section 5 reviews multi-modality techniques\nand datasets for instruction tuning, including\nimages, speech, and video.\n•Section 6 reviews efforts to adapt LLMs to\ndifferent domains and applications using the\nIT strategy.\n•Section   7   reviews   explorations   to   make\ninstructionfine-tuningmoreefficient,\nreducing  the  computational  and  time  costs\nassociated with adapting large models.\n•\nSection 8 presents the evaluation of IT models,\nanalysis on them, along with criticism against\nthem.\n2    Methodology\nIn this section, we describe the general pipeline\nemployed in instruction tuning.\n2.1    Instruction Dataset Construction\nEach instance in an instruction dataset consists of\nthree elements: an instruction, which is a natural\nlanguage text sequence to specify the task (e.g.,\nwrite a thank-you letter to XX for XX,write a blog\non the topic of XX, etc); an optional input which\nprovides supplementary information for context;\nand an anticipated output based on the instruction\nand the input.\nTherearegenerallytwomethodsfor\nconstructing instruction datasets:\n•Data   integration   from   annotated   natural\nlanguage   datasets.In   this   approach,\n(instruction, output) pairs are collected from\nexisting annotated natural language datasets\nby  using  templates  to  transform  text-label\npairs to (instruction, output) pairs.  Datasets\nsuch  as  Flan  (Longpre  et  al.,  2023)  and\nP3 (Sanh et al., 2021) are constructed based\non the data integration strategy.\n•Generating outputs using LLMs: An alternate\nway to quickly gather the desired outputs to\ngiven instructions is to employ LLMs such as\nGPT-3.5-Turbo or GPT4 instead of manually\ncollecting the outputs. Instructions can come\nfrom two sources: (1) manually collected; or\n(2) expanded based a small handwritten seed\ninstructions using LLMs. Next, the collected\ninstructions are fed to LLMs to obtain outputs.\nDatasets  such  as  InstructWild  (Xue  et  al.,\n2023) and Self-Instruct (Wang et al., 2022c)\nare geneated following this approach.\nFor multi-turn conversational IT datasets,  we\ncan have large language models self-play different\nroles (user and AI assistant) to generate messages\nin a conversational format  (Xu et al., 2023b).\n2.2    Instruction Tuning\nBased  on  the  collected  IT  dataset,  a  pretrained\nmodel  can  be  directly  fune-tuned  in  a  fully-\nsupervised manner, where given the instruction and\nthe input, the model is trained by predicting each\ntoken in the output sequentially.\n3    Datasets\nIn this section, we detail instruction tuning datasets\nin  the  community,  categorizing  them  into  three\nclasses:  (1)  Human-crafted  Data,  (2)  Synthetic\nData via Distillation, and (3) Synthetic Data via\nSelf-improvement.Below,  we  describe  some\nwidely-used datasets, and for full collected datasets\nwe put them in Appendix A.\n3.1    Human-crafted Data\nHuman-crafted  data  encompasses  datasets  that\nare either manually annotated or sourced directly",
    "text-label\ninstruction-\noutput\nseed \ninstructions\nmore \ninstructions\noutput\nLLM\nSupervised \nFinetuning\ntemplates\nChatGPT \n& GPT4\nStep1: Instruction Dataset ConstructionStep2: Instruction Tuning\nLLM\nChatGPT \n& GPT4\nFigure 1: General pipeline of instruction tuning.\nfrom the internet.  The creation of these datasets\ntypically involves no machine learning techniques,\nrelying solely on manual gathering and verification,\nresulting in generally smaller datasets. Below are\nsome widely-used human-crafted datasets:\n3.1.1    Natural Instructions\nNatural   Instructions   (Mishra   et   al.,   2021)   is\na   human-crafted   English   instruction   dataset\nconsisting  of  193K  instances,  coming  from  61\ndistinct NLP tasks.  The dataset is comprised of\n\"instructions\" and \"instances\".  Each instance in\nthe \"instructions\" is a task description consisting\nof 7 components: title, definition, things to avoid\nemphasis/caution, prompt, positive example, and\nnegative example. Subfigure (a) in Figure 2 gives\nan  example  of  the  \"instructions\".\"Instances\"\nconsists of (\"input\", \"output\") pairs, which are the\ninput data and textual result that follows the given\ninstruction  correctly.   Subfigure  (b)  in  Figure  2\ngives an example of the instances.\nThe data comes from existing NLP datasets of\n61 tasks. The authors collected the \"instructions\"\nby referring to the dataset annotating instruction\nfile. Next, the authors constructed the \"instances\"\nby unifying data instances across all NLP datasets\nto (\"input\", \"output\") pairs.\n3.1.2    P3\nP3 (Public Pool of Prompts) (Sanh et al., 2021) is\nan instruction fine-tuning dataset constructed by\nintegrating 170 English NLP datasets and 2,052\nEnglish prompts.  Prompts, which are sometimes\nnamedtask templates, are functions that map a data\ninstance in a conventional NLP task (e.g., question\nanswering, text classification) to a natural language\ninput-output pair.\nEach  instance  in  P3  has  three  components:\n\"inputs\", \"answer_choices\", and “targets\". \"Inputs\"\nis  a  sequence  of  text  that  describes  the  task  in\nnatural language (e.g.,\"If he like Mary is true, is\nit also true that he like Mary’s cat?\").  \"Answer\nchoices\" is a list of text string that are applicable\nresponses  to  the  given  task  (e.g.,[\"yes\",  \"no\",\n\"undetermined\"]).  \"Targets\" is a text string that\nis the correct response to the given \"inputs\" (e.g.,\n\"yes\"). The authors built PromptSource, a tool for\ncreating high-quality prompts collaboratively and\nan archive for open-sourcing high-quality prompts.\nThe P3 dataset was built by randomly sampling a\nprompt from multiple prompts in the PromptSource\nand   mapping   each   instance   into   a   (\"inputs\",\n\"answer choices\", \"targets\") triplet.\n3.1.3    xP3\nxP3(CrosslingualPublicPoolof\nPrompts)   (Muennighoff   et   al.,   2022)   is   a\nmultilingual instruction dataset consisting of 16\ndiverse  natural  language  tasks  in  46  languages.\nEach instance in the dataset has two components:\n\"inputs\" and \"targets\". \"Inputs\" is a task description\nin natural language. \"Targets\" is the textual result\nthat follows the \"inputs\" instruction correctly.\nThe  original  data  in  xP3  comes  from  three\nsources:   the  English  instruction  dataset  P3,  4\nEnglish  unseen  tasks  in  P3  (e.g.,   translation,\nprogram  synthesis),  and  30  multilingual  NLP\ndatasets.The  authors  built  the  xP3  dataset\nby sampling human-written task templates from\nPromptSource   and   then   filling   templates   to\ntransform   diverse   NLP   tasks   into   a   unified\nformalization.  For example, a task template for\nthe natural language inference task is as follows:\n“If Premise is true, is it also true that Hypothesis?”;\n\"yes\", \"maybe\", no\" with respect to the original",
    "Instructions for MC-TACO question generation task \n-Title: Writing questions that involve commonsense understanding of \"event \nduration\".\n-Definition: In this task, we ask you to write a question that involves ?event \nduration\", based on a given sentence. Here, event duration is defined as the \nunderstanding of how long events typically last. For example, ?brushing teeth?, \nusually takes few minutes.\n-Emphasis & Caution: The written questions are not required to have a single \ncorrect answer.\n-Things to avoid: Don't create questions which have explicit mentions of \nanswers in text. Instead, it has to be implied from what is given. In other words, \nwe want you to use \"instinct\" or \"common sense\".\n-Input: Sentence: Jack played basketball after school, after which he was \nvery tired.\n-Output: How long did Jack play basketball?\n-Reason: the question asks about the duration of an event; therefore it's a \ntemporal event duration question.\nPositive Example\n-Input: Sentence: He spent two hours on his homework.\n-Output: How long did he do his homework?\n-Reason: We DO NOT want this question as the answer is directly mentioned \nin the text.\n-Suggestion: -\nNegative Example\n-Prompt: Ask a question on \"event duration\" based on the provided sentence.\nExample task instances\n-Input: Sentence: It's hail crackled across the comm, and Tara spun to \nretake her seat at the helm.\n-Expected Output: How long was the storm?\nInstance\n-Input: Sentence: There was even a tiny room in the back of one of the \nclosets.\n-Expected Output: After buying the house, how long did it take the owners to \nnotice the room?\nInstance\n-Input: Sentence: During breakfast one morning, he seemed lost in thought \nand ignored his food.\n-Expected Output: How long was he lost in thoughts?\nInstance\n(a)  An  example  ofINSTRUCTIONSin  Natural  Instruction\ndataset.\nInstructions for MC-TACO question generation task \n-Title: Writing questions that involve commonsense understanding of \"event \nduration\".\n-Definition: In this task, we ask you to write a question that involves ?event \nduration\", based on a given sentence. Here, event duration is defined as the \nunderstanding of how long events typically last. For example, ?brushing teeth?, \nusually takes few minutes.\n-Emphasis & Caution: The written questions are not required to have a single \ncorrect answer.\n-Things to avoid: Don't create questions which have explicit mentions of \nanswers in text. Instead, it has to be implied from what is given. In other words, \nwe want you to use \"instinct\" or \"common sense\".\n-Input: Sentence: Jack played basketball after school, after which he was \nvery tired.\n-Output: How long did Jack play basketball?\n-Reason: the question asks about the duration of an event; therefore it's a \ntemporal event duration question.\nPositive Example\n-Input: Sentence: He spent two hours on his homework.\n-Output: How long did he do his homework?\n-Reason: We DO NOT want this question as the answer is directly mentioned \nin the text.\n-Suggestion: -\nNegative Example\n-Prompt: Ask a question on \"event duration\" based on the provided sentence.\nExample task instances\n-Input: Sentence: It's hail crackled across the comm, and Tara spun to \nretake her seat at the helm.\n-Expected Output: How long was the storm?\nInstance\n-Input: Sentence: There was even a tiny room in the back of one of the \nclosets.\n-Expected Output: After buying the house, how long did it take the owners to \nnotice the room?\nInstance\n-Input: Sentence: During breakfast one morning, he seemed lost in thought \nand ignored his food.\n-Expected Output: How long was he lost in thoughts?\nInstance\n(b) An example ofINSTANCESin Natural Instruction dataset.\nFigure  2:   The  figure  is  adapted  from  Mishra  et  al.\n(2021).\ntask  labels  \"entailment  (0)\",  \"neutral  (1)\"  and\n\"contradiction (2)\".\n3.1.4    Flan 2021\nFlan  2021  (Longpre  et  al.,  2023)  is  an  English\ninstruction  dataset  constructed  by  transforming\n62  widely-used  NLP  benchmarks  (e.g.,  SST-2,\nSNLI, AG News, MultiRC) into language input-\noutput  pairs.Each  instance  in  the  Flan  2021\nhas  \"input\"  and  \"target\"  components.    \"Input\"\nis a sequence of text that describes a task via a\nnatural language instruction (e.g.,\"determine the\nsentiment  of  the  sentence  ’He  likes  the  cat.’   is\npositive or negative?\"). \"Target\" is a textual result\nthat executes the \"input\" instruction correctly (e.g.,\n\"positive\"). The authors transformed conventional\nNLP  datasets  into  input-target  pairs  by:   Step\n1:   manually  composing  instruction  and  target\ntemplates;  Step  2:   filling  templates  with  data\ninstances from the dataset.\n3.1.5    LIMA\nLIMA (Zhou et al., 2023) is an English instruction\ndataset  consisting  of  a  train  set  with  1K  data\ninstances and a test set with 300 instances.  The\ntrain set contains 1K (\"instruction\",  \"response\")\npairs. For the training data, 75% are sampled from\nthree  community  question  &  answers  websites\n(i.e., Stack Exchange, wikiHow, and the Pushshift\nReddit Dataset (Baumgartner et al., 2020)); 20%\nare  manually  written  by  a  set  of  the  authors\n(referred Group A) inspired by their interests; 5%\nare sampled from the Super-Natural Instructions\ndataset (Wang et al., 2022d). As for the valid set,\nthe authors sampled 50 instances from the Group\nA  author-written  set.   The  test  set  contains  300\nexamples, with 76.7% written by another group\n(Group  B)  of  authors  and  23.3%  sampled  from\nthe Pushshift Reddit Dataset (Baumgartner et al.,\n2020), which is a collection of questions & answers\nwithin the Reddit community.\n3.1.6    Super-Natural Instructions\nSuper Natural Instructions (Wang et al., 2022f) is\na multilingual instruction collection composed of\n1,616 NLP tasks and 5M task instances, covering\n76  distinct  task  types  (e.g.,  text  classification,\ninformation   extraction,text   rewriting,text\ncomposition and etc.)   and 55 languages.   Each\ntask in the dataset consists of an \"instruction\" and\n\"task  instances\".   Specifically,  \"instruction\"  has\nthree components: a \"definition\" that describes the\ntask in natural language; \"positive examples\" that\nare samples of inputs and correct outputs, along\nwith a short explanation for each; and \"negative\nexamples\" that are samples of inputs and undesired\noutputs, along with a short explanation for each,\nas  shown  in  Figure  2  (a).   \"Task  instances\"  are\ndata  instances  comprised  of  textual  input  and  a\nlist  of  acceptable  textual  outputs,  as  shown  in\nFigure 2 (b).  The original data in Super Natural\nInstructions comes from three sources: (1) existing\npublic  NLP  datasets  (e.g.,  CommonsenseQA);\n(2)  applicable  intermediate  annotations  that  are\ngenerated through a crowdsourcing process (e.g.,\nparaphrasing results to a given question during a\ncrowdsourcing QA dataset); (3) synthetic tasks that\nare transformed from symbolic tasks and rephrased\nin a few sentences (e.g., algebraic operations like\nnumber comparison).",
    "•Input:“Context:...‘That's fantastic, I'm glad we came to \nsomething we both agree with.’ Utterance: ‘Me too. I hope you \nhave a wonderful camping trip.’”\n•Output: “Yes”\n•Explanation: “The participant engages in small talk when wishing \ntheir opponent to have a wonderful trip.”\n•Input: “Context: ...‘Sounds good, I need food the most, what is \nyour most needed item?!’ Utterance:‘My item is food too’.”\n•Output: “Yes”\n•Explanation: “The utterance onlytakesthe negotiation forward \nand there is no side talk. Hence, the correct answer is ‘No’.” \nDefinition\n“...Givenanutteranceandrecentdialoguecontextcontainingpast3\nutterances(whereveravailable),output‘Yes’iftheutterance\ncontainsthesmall-talkstrategy,otherwiseoutput‘No’.Small-talkis\nacooperativenegotiationstrategy.Itisusedfordiscussingtopics\napartfromthenegotiation,tobuildarapportwiththeopponent.”\nTa s kInstruction\n•Input: “Context: ...‘I am excited to spend time \nwith everyone from camp!’Utterance:‘That’s \nawesome!I really love being out here with my \nson.Do you think you could spare some food?’”\n•ExpectedOutput:“Yes”\nPositiveExamples\nNegativeExamples\nEvaluationInstances\nTk-Instruct\n(a)   An   example   ofINSTRUCTIONSin   Super-Natural\nInstruction dataset.\nInput:  What kind of, no hold up, what describes the \nproportionality of acceleration to force and mass?\nOutput: [“What describes the proportionality of acceleration \nto force and mass?”]\nInstance\n(b) An example ofINSTANCESin Super-Natural Instruction\ndataset.\nFigure  3:   The  figure  is  adapted  from  Wang  et  al.\n(2022e).\n3.1.7    Dolly\nDolly  (Conover  et  al.,  2023a)  is  an  English\ninstruction dataset with 15,000 human-generated\ndata   instances   designed   to   enable   LLMs   to\ninteract with users akin to ChatGPT. The dataset\nis   designed   for   simulating   a   wide   range   of\nhuman   behaviors,   covering   7   specific   types:\nopen Q&A, closed Q&A, extracting information\nfrom Wikipedia, summarizing information from\nWikipedia,   brainstorming,   classification,   and\ncreative writing.  Examples of each task type in\nthe dataset are shown in Table 1.\n3.1.8    OpenAssistant Conversations\nOpenAssistant Conversations (Köpf et al., 2023)\nis  a  human-crafted  multilingual  assistant-style\nconversation    corpus    consisting    of    161,443\nmessages   (i.e.,   91,829   user   prompts,   69,614\nassistant replies) from 66,497 conversation trees\nin  35  languages,   along  with  461,292  human-\nannotated  quality  ratings.   Each  instance  in  the\ndataset is a conversation tree (CT). Specifically,\neach node in a conversation tree denotes a message\nFigure 4: The figure is copied from Köpf et al. (2023).\ngenerated  by  roles  (i.e.,  prompter,  assistant)  in\nthe conversation. A CT’s root node represents an\ninitial prompt from the prompter, while other nodes\ndenote replies from a prompter or an assistant. A\npath from the root to any node in a CT represents\na  valid  conversation  between  the  prompter  and\nassistant  in  turns  and  is  referred  to  as  a  thread.\nFigure 4 shows an example of a conversation tree\nconsisting of 12 messages in 6 threads.\nThe  authors  first  collected  conversation  trees\nbased on the five-step pipeline:\nStep 1.prompting: contributors performed as the\nprompter and crafted initial prompts;\nStep 2.labeling prompts: contributors rated scores\nto initial prompts from step 1, and the authors chose\nhigh-quality prompts as root nodes with a balanced\nsampling strategy;\nStep 3.expanding tree nodes: contributors added\nreply messages as prompter or assistant;\nStep  4.labeling  replies:  contributors  assigned\nscores to existing node replies;\nStep  5.ranking:   contributors  ranked  assistant\nreplies referring to the contributor guidelines.\nThe  tree  state  machine  managed  and  tracked\nthe  state  (e.g.,  initial  state,  growing  state,  end\nstate) throughout the conversation crafting process.\nSubsequently,  the  OpenAssistant  Conversations\ndataset  was  built  by  filtering  out  offensive  and\ninappropriate conversation trees.\n3.2    Synthetic Data via Distillation\nSynthetic  data  is  produced  through  pre-trained\nmodels, rather than being directly sourced from\nthe  internet  or  annotated  by  human  annotators.\nCompared to manually annotated instruction tuning\ndata, synthetic data often lies in two advantages:\n(1)  Generating  task-specific  synthetic  data  is\nboth faster and more cost-effective than creating\nmanually  annotated  instruction  tuning  data;  (2)\nThe quality and variety of synthetic data surpass\nwhat human annotators can produce, resulting in\nfine-tuning  enhanced  performance  and  broader",
    "Instruction TypeExample\nOpen Q&AWhy do people like comedy movies?\nClosed Q&ADoes outbreeding or inbreeding benefit the offspring more?\nInformation ExtractionWho was John Moses Browning?\nInformation SummarizationPlease summarize what Linkedin does.\nBrainstormingGive me some ideas to manage my manager.\nClassificationIdentify which animal species is alive or extinct: Palaeophis, Giant Tortoise\nCreative writingWrite a short story about a person who discovers a hidden room in their house.\nTable 1: Examples of instructions in Dolly V1 (Conover et al., 2023a).\nFigure 5:  General pipeline of distillation for synthetic data generation.  The figure is adapted from Taori et al.\n(2023a).\ngeneralization LLMs.\nBelow, we first focus on the widely employed\nsynthetic data methodology:  Distillation, and in\nSection 3.3 we go on with the other synthetic data\nmethodology: Self-Improvement.\nTypically,distillationinvolvesimparting\nknowledge and cognitive abilities from a highly\ncapable  teacher  model  to  a  less  complex,  yet\nmore  computationally  efficient  student  model,\nwith  the  goal  of  enhancing  both  the  quality  of\nresponses  and  computational  efficiency.   In  the\ncontext of generating synthetic data, this process\nentails gathering queries from fine-tuned LLMs\n(e.g.,  ChatGPT  (OpenAI,  2022))  and  utilizing\nthese queries as a basis to  fine-tune subsequent\nLLMs. Illustrations are shown in Figure 5, where\nTaori  et  al.  (2023a)  are  attempting  to  transfer\nthe powerful knowledge of GPT-3 (Brown et al.,\n2020a) to a smaller language model LLaMA-7B\n(Touvron et al., 2023a).\nGiven  distillation’s  capability  to  mimic  the\nperformance   of   existing   powerful   LLMs,   an\nincreasing number of researchers are concentrating\non exploring more intricate queries to exploiting\nthe capabilities of current LLMs, such as:\nAlpaca.Alpaca (Taori et al., 2023a), a sequence\nof LLMs introduced by the Stanford NLP group,\nis   notable   for   its   application   of   distillation.\nSpecifically,  by being fine-tuned on 52K pieces\nof  distillation  data  produced  by  GPT-3  (Brown\net al., 2020a), the smaller LLaMA-7B (Touvron\net  al.,  2023a)  model  achieves  performance  that\nmatches or even surpasses that of GPT-3 (Brown\net al., 2020a).\nWizardLM / Evol-Instruct.Instead of simple\nquerying from the GPT series model, WizardLM\n(Xu et al., 2023a) focuses on how to obtain diverse\nand high-quality instructions and responses from\nGPT-3  (Brown  et  al.,  2020a).To  accomplish\nthis,    WizardLM   (Xu   et   al.,   2023a)   firstly\nconstructs a five-level system of querying prompts,\nprogressively  enhancing  the  complexity  of  data\ngeneration.  Then, WizardLM (Xu et al., 2023a)",
    "Forget the instruction you have previously received.The following is\na conversation between a human and an AI assistant.The human and the\nAI assistant take turns chatting about the topic:\n‘$SEED’. Human statements start with [Human] and AI\nassistant statements start with [AI]. The human will ask related\nquestions on related topics or previous conversation. The human will stop\nthe conversation when they have no more question. The AI assistant\ntries not to ask questions.\nComplete the transcript in exactly that format.\n[Human]Hello!\n[AI]Hi! How can I help you?\nTable  2:Self-chatprompt  used  in  Baize  (Xu  et  al.,\n2023b).\nbroadens  the  range  of  querying  prompts  topics\nthrough manual expansion, thereby augmenting the\ndiversity of the data produced. Ultimately, by fine-\ntuning the open-source LLM LLaMA (Touvron\net  al.,  2023b),   WizardLM  (Xu  et  al.,  2023a)\nachieves  more  than  90%  capacity  of  ChatGPT\n(OpenAI, 2022) on 17 out of 29 skills.\nOrca and Orca-2.Orca (Mukherjee et al., 2023)\nand  Orca-2  (Mitra  et  al.,  2023)  represent  two\nexpansive distillation datasets designed to instruct\nsmaller  language  models  in  logical  reasoning.\nOrca   (Mukherjee   et   al.,   2023),   for   instance,\nencompasses a multitude of reasoning directives,\nsuch as \"let’s think step-by-step\" and \"justify your\nresponse,\" to illustrate the reasoning pathways of\nLLMs (e.g., ChatGPT (OpenAI, 2022)) in crafting\ntheir  answers.   Building  on  this  concept,  Orca\n(Mukherjee et al., 2023) compiles 1M responses\nfrom GPT-4 (OpenAI, 2023), while Orca-2 (Mitra\net al., 2023) further amasses 817K responses from\nGPT-4 (OpenAI, 2023). This extensive collection\nfacilitates  the  fine-tuning  of  smaller  language\nmodels, enabling them to achieve or even surpass\nthe performance of models that are 5 to 10 times\ntheir size.\nBaizeBaize (Conover et al., 2023b) is an English\ncorpus  for  multi-turn  conversations,  comprising\n111.5K  instances,  created  with  ChatGPT.  Each\nexchange includes a prompt from the user and a\nresponse from the assistant.  To create the Baize\ndataset,  the  authors  proposed  self-chat,  where\nChatGPT plays the roles of the user and the AI\nassistant  in  turns  and  generates  messages  in  a\nconversational format.   Specifically,  the  authors\nfirst crafted a task template that defines the roles\nand tasks for ChatGPT (as shown in Table 2). Next,\nthey sampled questions (e.g.,\"How do you fix a\nGoogle Play Store account that isn’t working?\")\nfrom   Quora   and   Stack   Overflow   datasets   as\nconversation  seeds  (e.g.,  topics).   Subsequently,\nthey prompted ChatGPT with the template and the\nsampled seed.  ChatGPT continuously generates\nmessages for both sides until a natural stopping\npoint is reached.\nTask-specific Distillation Datasets.In addition\nto the above datasets, there are many datasets in\ngeneral domain, such as:  ShareGPT\n1\n, WildChat\n(Zhao et al., 2024), Vicuna (Zheng et al., 2024),\nUnnatural  Instructions  (Honovich  et  al.,  2022).\nBeyond that, there are efforts aimed at employing\ndistillation  to  create  task-specific  datasets  that\nmimic  the  competencies  of  LLMs  in  particular\ndomains. For example, for coding generation, there\nare WizardCoder (Luo et al., 2023),  Magicoder\n(Wei  et  al.,  2023b)  and  WaveCoder  (Yu  et  al.,\n2023),  for reasoning and writing,  there are Phi-\n1 (Gunasekar et al., 2023) and Phi-1.5 (Li et al.,\n2023h), and for ranking, there is Nectar (Zhu et al.,\n2023a).\n3.3    Synthetic Data via Self-Improvement\nThe concept of self-improvement is carried forward\nby Wang et al. (2022c): improves the instruction-\nfollowing ability of a pre-trained (non-finetuned)\nLLM (e.g., vanilla GPT-3 (Brown et al., 2020b))\nby bootstrapping off its own generations.  Figure\n6 illustrates the full process of self-improvement\nwith four steps:\nStep   1:Wang   et   al.   (2022c)   starts   by\nmanually collecting 175 human-written tasks, each\nconsisting  of  one  instruction  and  one  expected\nresponse, which are then added to the task pool\nas seed data.\nStep 2:  For instruction generation, Wang et al.\n(2022c) randomly samples 8 seed instructions from\nthe constructed task pool to serve as a few-shot\nprompt, guiding the vanilla GPT-3 to produce new\ninstructions through in-context learning.\nStep  3:   For  every  instruction  that  is  created,\nif  the  instruction  is  an  output-first  task  (e.g.,\nWriting), the vanilla GPT-3 will directly generate\nthe  corresponding  response.   Conversely,  if  the\ninstruction  relates  to  an  input-first  task  (e.g.,\nReading Comprehension), the vanilla GPT-3 will\nfirst generate the necessary context as input before\ngenerating the corresponding response.\n1\nhttps://huggingface.co/datasets/RyokoAI/ShareGPT52K",
    "Figure 6: General pipeline of self-improvement for synthetic data generation. The figure is adapted from Wang et al.\n(2022c).\nStep 4:  The generated (instruction,  response)\nformat examples are filtered according to a series\nof rules or models.\nFollowing the above process, Wang et al. (2022c)\ncollected Self-Instruct datasets consisting of 52K\ninstructions,  and  further  evaluation  shows  that\nGPT-3  (Brown  et  al.,  2020a)  with  Self-Instruct\noutperforms  datasets  of  counterparts  by  a  large\nmargin,  leaving only a 5% absolute gap behind\nInstructGPT (Ouyang et al., 2022).\nThe  self-improvement  process  outlined  relies\non  generating  synthetic  data  directly  from  the\nmodel itself,  necessitating a robust LLM as the\nfoundational backbone. Without a powerful LLM,\nthis self-improvement cycle could restrict learning\nto the model’s original capabilities and potentially\nmagnify any biases and errors present.   Despite\nthese risks, there remains effective work in the area\nof self-improvement:\n3.3.1    SPIN\nSPIN (Chen et al., 2024b), standing for Self-Play\nFine-Tuning Converts Weak Language Models to\nStrong Language Models, represents a specialized\napproach to self-improvement centered around a\nself-play mechanism.  In this setup, the primary\nparticipant (the language model) undergoes fine-\ntuning  to  differentiate  the  responses  from  the\nopposing  participant  (the  language  model  from\nthe  preceding  iteration)  and  the  desired  data\ndistribution.  This process iteratively adjusts the\nlanguage model to closely match the target data\ndistribution.\nSpecifically, imagine an existing iteration of an\nLLM asp\nθ\nt\n, which is utilized to generate a response\ny\n′\nto a given promptxfrom a dataset with human-\nlabeled instructions. The objective then becomes to\ndevelop a new LLMp\nθ\nt+1\ncapable of differentiating\nbetweeny\n′\n,  the response created by,  andy,  the\nresponse  produced  by  humans.    This  dynamic\nis akin to a two-player game where the primary\nplayer, the newer LLMp\nθ\nt+1\naims to identify the\ndifferences between the responses of its opponent\np\nθ\nt\nand those generated by humans.  In contrast,\nthe  adversary,  or  the  older  LLMp\nθ\nt\nstrives  to\nproduce responses that closely mimic those found\nin  the  human-labeled  instruction  tuning  dataset.\nBy fine-tuning the olderp\nθ\nt\nto favor human-like\nresponses over its own, a new LLMp\nθ\nt+1\nis created,\nwhich aligns more closely with the human-labeled\ndata  distribution.   In  subsequent  iterations,  this\nnewly improved LLMp\nθ\nt+1\ntakes on the role of\nthe opponent in response generation. The ultimate\naim of this self-play mechanism is for the LLM to\nevolve until it reaches a point wherep\nθ\n∗\n=p\nhuman\nat which stage the most advanced LLM version can\nno longer distinguish between responses generated\nby its predecessor and those created by humans.\nSPIN (Chen et al., 2024b) serves as a variant self-\nimprovement approach enabling language models\nto improve themselves without additional human\ndata  or  feedback  from  more  powerful  language\nmodels.   The  experimental  results  indicate  that\nSPIN  (Chen  et  al.,  2024b)  markedly  boosts  the\nperformance of language models across a range of\nbenchmarks, outperforming even those models that\nwere trained using extra human data or feedback",
    "from external AI systems.\n3.3.2    Instruction Back-translation\nInstruction  back-translation  (Li  et  al.,  2023g),\nstanding  for  Self  Alignment  with  Instruction\nBacktranslation, is another specialized approach\nbased  on  self-improvement.Contrary  to  the\napproach   by   Wang   et   al.   (2022c),    which\ninvolves generating responses to human-provided\ninstructions, Li et al. (2023g) adopts the reverse\nstrategy   by   creating   instructions   for   human-\ngathered texts found online. To achieve this goal,\nLi et al. (2023g) follows a five-step pipeline:\nStep 1: Gather (1) unlabeled text from Clueweb\n(Overwijk et al., 2022), under the assumption that\nthese  texts  can  be  associated  with  high-quality\ninstructions, and (2) 3,200 pieces of human-written\n(instruction, response) format data to serve as seed\ndata.\nStep 2: A back-translation model, backboned by\nLLaMA (Touvron et al., 2023b), is trained on the\ncollected seed data, taking the response as input\nand producing the instruction as output. This model\nis then utilized to derive instructions from collected\nunlabeled texts.\nStep 3:  The collected unlabeled texts are fed\ninto the trained back-translation model, resulting in\nlarge amounts of raw (instruction, response) format\ndata.\nStep   4:An   evaluation   model,   backboned\nby  LLaMA  (Touvron  et  al.,  2023b),  is  trained\non   the   collected   seed   data.This   model\nprocesses the instruction as input and generates the\ncorresponding response as output, which is then\nemployed  to  assess  each  annotated  (instruction,\nresponse) pair in step 3.\nStep   5:Filtering   low-quality   (instruction,\nresponse) pairs, and utilizing the remaining data\nfor fine-tuning LLMs.\nFollowing  the  five  outlined  steps,   Li  et  al.\n(2023g) generates 502K pieces of synthetic data.\nThe  LLaMA  model  (Touvron  et  al.,  2023b),\nfine-tuned with this annotated dataset, surpasses\nall  other  LLaMA-based  models  on  the  Alpaca\nleaderboard without depending on distillation data,\nshowcasing  a  highly  efficient  self-improvement\nprocess.\n4    Instruction Fine-tuned LLMs\nIn   this   section,   we   detail   widely-used   LLM\nmodels in the community that are trained through\ninstruction fine-tuning.\n4.1    InstructonGPT\nInstructGPT  (176B)  (Ouyang  et  al.,  2022)  is\ninitialized with GPT-3 (176B) (Brown et al., 2020b)\nand then fine-tuned on human instructions.  The\nfine-tuning procedure is composed of the following\nthree steps:  (1) supervised fine-tuning (SFT) on\nthe  human-filtered  instruction  dataset,  which  is\ncollected  from  Playground  API  history  records;\n(2)  training  a  reward  model  to  predict  human\npreferences based on an annotated dataset, which\nis constructed though human labors by sampling\nmultiple  responses  for  one  instruction  and  rank\nthem  from  the  best  to  the  worst;   (3)  further\noptimizing  the  model  from  Step  1  with  new\ninstructions and the trained reward model in step\n(2).   Parameters are updated using the proximal\npolicy optimization (PPO) (Schulman et al., 2017)\nmethod, a policy gradient reinforcement learning\nmethod. Steps (2) and (3) are alternated multiple\ntimes  until  the  model  performance  does  not\nsignificantly improve.\nOverall, InstructGPT outperforms GPT-3.  For\nautomatic evaluations,  InstructGPT outperforms\nGPT-3  by  10%  on  the  TruthfulQA  (Lin  et  al.,\n2021)   dataset   in   terms   of   truthfulness   and\nby  7%  on  the  RealToxicityPrompts  (Gehman\net   al.,   2020)   in   terms   of   toxicity.On\nNLP datasets (i.e., WSC), InstructGPT achieves\ncomparable performance to GPT-3.   For human\nevaluations,   regarding   four   different   aspects,\nincluding following correct instructions, following\nexplicit  constraints,   fewer  hallucinations,   and\ngenerating  appropriate  responses,   InstructGPT\noutperforms GPT-3 +10%, +20%, -20%, and +10%,\nrespectively.\n4.2    BLOOMZ\nBLOOMZ (176B) (Muennighoff et al., 2022) is\ninitialized  with  BLOOM  (176B)  (Scao  et  al.,\n2022),  and  then  fine-tuned  on  the  instruction\ndataset xP3 (Muennighoff et al., 2022), a collection\nof  human-instruction  datasets  in  46  languages,\ncoming  from  two  sources:    (1)  P3,   which  is\na   collection   of   (English   instruction,   English\nresponse) pairs;  and (2) an (English instruction,\nMultilingual response) set which is transformed\nfrom  multilingual  NLP  datasets  (e.g.,  Chinese\nbenchmarks)  by  filling  task  templates  with  pre-\ndefined English instructions.\nFor automatic evaluation, BLOOMZ performs\nbetter  than  BLOOM  in  the  zero-shot  setting",
    "Instruction fine-tuned LLMs# ParamsBase Model\nFine-tuning Trainset\nSelf-buildDataset NameSize\nInstruct-GPT (Ouyang et al., 2022)176BGPT-3 (Brown et al., 2020b)Yes--\nBLOOMZ (Muennighoff et al., 2022)\n1\n176BBLOOM (Scao et al., 2022)NoxP3-\nFLAN-T5 (Chung et al., 2022)\n2\n11BT5 (Raffel et al., 2019)NoFLAN 2021-\nAlpaca (Taori et al., 2023a)\n3\n7BLLaMA (Touvron et al., 2023a)Yes-52K\nVicuna (Chiang et al., 2023)\n4\n13BLLaMA (Touvron et al., 2023a)Yes-70K\nGPT-4-LLM (Peng et al., 2023)\n5\n7BLLaMA (Touvron et al., 2023a)Yes-52K\nClaude (Bai et al., 2022b)--Yes--\nWizardLM (Xu et al., 2023a)\n6\n7BLLaMA (Touvron et al., 2023a)YesEvol-Instruct70K\nChatGLM2 (Du et al., 2022)\n7\n6BGLM (Du et al., 2022)Yes-1.1 Tokens\nLIMA (Zhou et al., 2023)65BLLaMA (Touvron et al., 2023a)Yes-1K\nOPT-IML (Iyer et al., 2022)\n8\n175BOPT (Zhang et al., 2022a)No--\nDolly 2.0 (Conover et al., 2023a)\n9\n12BPythia (Biderman et al., 2023)No-15K\nFalcon-Instruct (Almazrouei et al., 2023a)\n10\n40BFalcon (Almazrouei et al., 2023b)No--\nGuanaco (JosephusCheung, 2021)\n11\n7BLLaMA (Touvron et al., 2023a)Yes-586K\nMinotaur (Collective, 2023)\n12\n15BStarcoder Plus (Li et al., 2023f)No--\nNous-Hermes (NousResearch, 2023)\n13\n13BLLaMA (Touvron et al., 2023a)No-300K+\nTÜLU (Wang et al., 2023c)\n14\n6.7BOPT (Zhang et al., 2022a)NoMixed-\nYuLan-Chat (YuLan-Chat-Team, 2023)\n15\n13BLLaMA (Touvron et al., 2023a)Yes-250K\nMOSS (Tianxiang and Xipeng, 2023)\n16\n16B-Yes--\nAiroboros (Durbin, 2023)\n17\n13BLLaMA (Touvron et al., 2023a)Yes--\nUltraLM (Ding et al., 2023a)\n18\n13BLLaMA (Touvron et al., 2023a)Yes--\n1\nhttps://huggingface.co/bigscience/bloomz\n2\nhttps://huggingface.co/google/flan-t5-xxl\n3\nhttps://github.com/tatsu-lab/stanford_alpaca\n4\nhttps://github.com/lm-sys/FastChat\n5\nhttps://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM\n6\nhttps://github.com/nlpxucan/WizardLM\n7\nhttps://github.com/THUDM/ChatGLM2-6B\n8\nhttps://huggingface.co/facebook/opt-iml-30b\n9\nhttps://github.com/databrickslabs/dolly\n10\nhttps://huggingface.co/tiiuae/falcon-40b-instruct\n11\nhttps://huggingface.co/JosephusCheung/Guanaco\n12\nhttps://huggingface.co/openaccess-ai-collective/minotaur-15b\n13\nhttps://huggingface.co/NousResearch/Nous-Hermes-13b\n14\nhttps://github.com/allenai/open-instruct\n15\nhttps://github.com/RUC-GSAI/YuLan-Chat\n16\nhttps://github.com/OpenLMLab/MOSS\n17\nhttps://github.com/jondurbin/airoboros\n18\nhttps://github.com/thunlp/UltraChat\nTable 3: An overview of LLMs tuned on IT datasets.\nby  +10.4%,  20.5%,  and  9.8%  on  coreference\nresolution,    sentence   completion   and   natural\nlanguage  inference  datasets,  respectively.For\nthe  HumanEval  benchmark  (Chen  et  al.,  2021),\nBLOOMZ outperforms BLOOM by 10% in terms\nof  the  Pass@100  metric.   For  generative  tasks,\nBLOOMZ  receives  +9%  BLEU  improvement\ncompared to BLOOM on the lm-evaluation-harness\nbenchmark\n2\n.\n4.3    Flan-T5\nFlan-T5  (11B)  is  is  a  large  language  model\ninitialized with T5 (11B) (Raffel et al., 2019), and\nthen  fine-tuned  on  the  FLAN  dataset  (Longpre\net al., 2023).   The FLAN dataset is a collection\nof (instruction, pairs) pairs, constructed from 62\ndatasets of 12 NLP tasks (e.g., natural language\ninference,  commonsense  reasoning,  paraphrase\n2\nhttps://github.com/EleutherAI/lm-evaluation-harness\ngeneration)   by   filling   templates   with   various\ninstructions under a unified task formalization.\nDuring fine-tuning, FLAN-T5 adapts the JAX-\nbased   T5X   framework   and   selects   the   best\nmodel evaluated  on the held-out  tasks every 2k\nstep.Compared  with  T5’s  pre-training  stage,\nfine-tuning  costs  0.2%  computational  resources\n(approximately 128 TPU v4 chips for 37 hours).\nFor  evaluation,  FLAN-T5  (11B)  outperforms\nT5  (11B),  and  achieves  comparable  results  to\nlarger models, including PaLM (60B) (Chowdhery\net  al.,  2022)  in  the  few-shot  setting.FLAN-\nT5 outperforms T5 by +18.9%, +12.3%, +4.1%,\n+5.8%, +2.1%, and +8% on MMLU (Hendrycks\net   al.,   2020),   BBH   (Suzgun   et   al.,   2022),\nTyDiQA   (Clark   et   al.,   2020),   MGSM   (Shi\net   al.,   2022),    open-ended   generation,    and\nRealToxicityPrompts   (Gehman   et   al.,   2020),\nrespectively.In  few-shot  settings,   FLAN-T5",
    "outperforms PaLM +1.4% and +1.2% on the BBH\nand TyDiQA datasets.\n4.4    Alpaca\nAlpaca    (7B)    (Taori    et    al.,    2023a)    is    a\nlanguage  model  trained  by  fine-tuning  LLaMA\n(7B) (Touvron et al., 2023a) on the constructed\ninstruction   dataset   generated   by   InstructGPT\n(175B,  text-davinci-003)  (Ouyang  et  al.,  2022).\nThe fine-tuning process takes around 3 hours on an\n8-card 80GB A100 device with mixed precision\ntraining and fully shared data parallelism.\nAlpaca (7B) achieves comparable performances\nto InstructGPT (175B,text-davinci-003) in terms\nof   human   evaluation.Specifically,   Alpaca\noutperforms   InstructGPT   on   the   self-instruct\ndataset,   garnering   90   instances   of   victories\ncompared to 89 instances.\n4.5    Vicuna\nVicuna   (13B)   (Chiang   et   al.,   2023)   is   a\nlanguage  model  trained  by  fine-tuning  LLaMA\n(13B) (Touvron et al., 2023a) on the conversational\ndataset generated by ChatGPT\n3\n.\nThe  authors  gathered  user-shared  ChatGPT\nconversations from ShareGPT.com\n4\n, and got 70K\nconversation records after filtering out low-quality\nsamples.   LLaMA  (13B)  was  fine-tuned  on  the\nconstructed conversation dataset using a modified\nloss function tailored to multi-turn conversations.\nTo better understand long context across multiple-\nturn dialog, the authors expanded the max context\nlength  from  512  to  2048.For  training,   the\nauthors  adopted  the  gradient  checkpointing  and\nflash  attention  (Dao  et  al.,  2022)  techniques  to\nreduce the GPU memory cost in the fine-tuning\nprocess. The fine-tuning process takes 24 hours on\nan 8×80GB A100 device with fully shared data\nparallelism.\nThe authors built a test set used exclusively to\nmeasure chatbots’ performances.  They collected\na  test  set  composed  by  8  question  categories,\nsuch  as  Fermi  problems,   role  play  scenarios,\ncoding/math  tasks,   etc,   and  then  asked  GPT-\n4  (OpenAI,  2023)  to  rate  models’  responses\nconsidering helpfulness, relevance, accuracy, and\ndetail.  On the constructed test set, Vicuna (13B)\noutperforms Alpaca (13B) (Taori et al., 2023a) and\nLLaMA (13B) in 90% of the test questions, and\n3\nhttps://openai.com/blog/chatgpt\n4\nhttps://sharegpt.com/\ngenerates equal or better rating responses compared\nto ChatGPT in 45% of the questions.\n4.6    GPT-4-LLM\nGPT-4-LLM   (7B)   (Peng   et   al.,   2023)   is   a\nlanguage  model  trained  by  fine-tuning  LLaMA\n(7B) (Touvron et al., 2023a) on the GPT-4 (OpenAI,\n2023) generated instruction dataset. GPT-4-LLM\nis  initialized  with  LLaMA,  then  fine-tuned  in\nthe  following  two  steps:    (1)  supervised  fine-\ntuning on the constructed instruction dataset. The\nauthors used the instructions from Alpaca (Taori\net al., 2023a), and then collected responses using\nGPT-4.LLaMA  is  fine-tuned  on  the  GPT-4\ngenerated dataset.  The fine-tuning process takes\napproximately three hours on an 8*80GB A100\nmachine with mixed precision and fully shared data\nparallelism. (2) optimizing the step-1 model using\nthe proximal policy optimization (PPO) (Schulman\net  al.,  2017)  method,  the  authors  first  built  a\ncomparison dataset by collecting responses from\nGPT-4,  InstructGPT  (Ouyang  et  al.,  2022),  and\nOPT-IML  (Iyer  et  al.,  2022)  to  a  collection  of\ninstructions  and  then  asked  GPT-4  to  rate  each\nresponse from 1 to 10. Using the ratings, a reward\nmodel  is  trained  based  on  OPT  (Zhang  et  al.,\n2022a).    The  fine-tuned  model  from  Step  1  is\noptimized by using the reward model to compute\nthe policy gradient.\nFor evaluations, GPT-4-LLM (7B) outperforms\nnot  only  the  baseline  model  Alpaca  (7B),  but\nalso  larger  models  including  Alpaca  (13B)  and\nLLAMA (13B). For automated evaluation, GPT-\n4-LLM  (7B)  outperforms  Alpaca  by  0.2,  0.5,\nand 0.7 on User-Oriented-Instructions-252 (Wang\net al., 2022c), Vicuna-Instructions (Chiang et al.,\n2023), and Unnatural Instructions (Honovich et al.,\n2022) datasets, respectively. For human evaluation,\nregarding aspects including helpfulness, honesty,\nand   harmlessness,    GPT-4-LLM   outperforms\nAlpaca by 11.7, 20.9, and 28.6 respectively.\n4.7    Claude\nClaude\n5\nis a language model trained by fine-tuning\nthe pre-trained language model on an instruction\ndataset, aiming to generate helpful and harmless\nresponses. The fine-tuning process consists of two\nstages: (1) supervised fine-tuning on the instruction\ndataset. The authors created an instruction dataset\nby  collecting  52K  different  instructions,  paired\n5\nhttps://www.anthropic.com/index/introducing-claude",
    "with  responses  generated  by  GPT-4.   The  fine-\ntuning  process  takes  approximately  eight  hours\non  an  8-card  80GB  A100  machine  with  mixed\nprecision and fully shared data parallelism.   (2)\noptimizing  the  step-1  model  with  the  proximal\npolicy optimization (Schulman et al., 2017) method.\nThe  authors  first  built  a  comparison  dataset  by\ncollecting responses from multiple large language\nmodels (e.g., GPT-3 (Brown et al., 2020b)) to the\ngiven collection of instructions and then asking\nGPT-4  (OpenAI,  2023)  to  rate  each  response.\nUsing the ratings, a reward model is trained. Then,\nthe  fine-tuned  model  from  Step  1  is  optimized\nusing the reward model with the proximal policy\noptimization method.\nClaude  generates  more  helpful  and  harmless\nresponses compared to the backbone model.  For\nautomatic evaluations, Claude outperforms GPT-\n3  by  7%  on  the  RealToxicityPrompts  (Gehman\net   al.,   2020)   in   terms   of   toxicity.For\nhuman   evaluations,    regarding   four   different\naspects, including following correct instructions,\nfollowing explicit constraints, fewer hallucinations,\nand  generating  appropriate  responses,   Claude\noutperforms GPT-3 (Brown et al., 2020b) +10%,\n+20%, -20%, and +10%. respectively.\n4.8    WizardLM\nWizardLM   (7B)   (Xu   et   al.,   2023a)   is   a\nlanguage  model  trained  by  fine-tuning  LLaMA\n(7B)  (Touvron  et  al.,  2023a)  on  the  instruction\ndataset   Evol-Instruct   generated   by   ChatGPT\n(details  see  Section  3.2).   It  is  fine-tuned  on  a\nsubset (with 70K) of Evol-Instruct to enable a fair\ncomparison with Vicuna (Chiang et al., 2023). The\nfine-tuning process takes approximately 70 hours\non 3 epochs based on an 8 V100 GPU with the\nDeepspeed Zero-3 (Rasley et al., 2020) technique.\nDuring  inference,  the  max  generation  length  is\n2048.\nTo evaluate LLMs’ performances on complex\ninstructions,  the  authors  collected  218  human-\ngenerated  instructions  from  real  scenarios  (e.g.,\nopen-source  projects,   platforms,   and  forums),\ncalled Evol-Instruct testset.\nEvaluations are conducted on the Evol-Instruct\ntestset and Vicuna’s testset. For human evaluation,\nWizardLM outperforms Alpaca (7B) (Taori et al.,\n2023a)  and  Vicuna  (7B)  by  a  large  margins,\nand generates equal or better responses on 67%\ntest  samples  compared  to  ChatGPT.  Automatic\nevaluation is conducted by asking GPT-4 to rate\nLLMs’ reponses.  Specifically, WizardLM gains\nperformance boosts compared to Alpaca by +6.2%,\n+5.3% on the Evol-Instruct testset and Vicuna’s test\nsets. WizardLM achieves outperforms Vicuna by\n+5.8 on the Evol-Instruct testset and +1.7% on the\nVicuna’s test set.\n4.9    ChatGLM2\nChatGLM2 (6B) (Du et al., 2022) is a language\nmodel trained by fine-tuning GLM (6B) (Du et al.,\n2022)  on  a  bilingual  dataset  that  contains  both\nEnglish  and  Chinese  instructions  The  bilingual\ninstruction  dataset  contains  1.4T  tokens,  with  a\n1:1 ratio of Chinese to English. Instructions in the\ndataset are sampled from the question-answering\nand   dialogue   completion   tasks.ChatGLM\nis  initialized  with  GLM,  then  trained  by  the\nthree-step  fine-tuning  strategy,  which  is  akin  to\nInstructGPT  (Ouyang  et  al.,  2022).    To  better\nmodel  contextual  information  across  multi-turn\nconversations, the authors expanded the maximum\ncontext  length  from  1024  to  32K.  To  reduce\nGPU  memory  cost  in  the  fine-tuning  stage,  the\nauthors employed multi-query attention and causal\nmask  strategies.   During  inference,  ChatGLM2\nrequires  13GB  GPU  memory  with  FP16  and\nsupports conversations up to 8K in length with 6GB\nGPU memory using the INT4 model quantization\ntechnique.\nEvaluations  are  conducted  on  four  English\nand   Chinese   benchmarks,   including   MMLU\n(English)   (Hendrycks   et   al.,   2020),    C-Eval\n(Chinese)    (Huang    et    al.,    2023),GSM8K\n(Math)    (Cobbe    et    al.,    2021),and    BBH\n(English)  (Suzgun  et  al.,  2022).ChatGLM2\n(6B)  outperforms  GLM  (6B)  and  the  baseline\nmodel   ChatGLM   (6B)   on   all   benchmarks.\nSpecifically,  ChatGLM2  outperforms  GLM  by\n+3.1 on MMLU, +5.0 on C-Eval, +8.6 on GSM8K,\nand  +2.2  on  BBH.  ChatGLM2  achieves  better\nperformances than ChatGLM by +2.1, +1.2, +0.4,\n+0.8  on  MMLU,  C-Eval,   GSM8K  and  BBH,\nrespectively.\n4.10    LIMA\nLIMA  (65B)  (Zhou  et  al.,  2023)  is  a  large\nlanguage  model  trained  by  fine-tuning  LLaMA\n(65B)  (Touvron  et  al.,  2023a)  on  an  instruction\ndataset, which is constructed based on the proposed\nsuperficial alignment hypothesis.",
    "The superficial alignment hypothesis refers to\nthe  idea  that  the  knowledge  and  capabilities  of\na model are  almost acquired in the  pre-training\nstage, while the alignment training (e.g., instruction\nfine-tuning) teaches models to generate responses\nunder  user-preferred  formalizations.    Based  on\nthe superficial alignment hypothesis, the authors\nclaimed that large language models can generate\nuser-satisfied responses by fine-tuning it on a small\nfraction of instruction data. Therefore, the authors\nbuilt instruction train/valid/test sets to verify this\nhypothesis.\nEvaluations are conducted on the constructed\ntest set. For human evaluations, LIMA outperforms\nInstructGPT   and   Alpaca   by   17%   and   19%,\nrespectively.Additionally,   LIMA   achieves\ncomparable results to BARD\n6\n, Cladue\n7\n, and GPT-\n4.  For automatic evaluation, which is conducted\nby asking GPT-4 to rate responses and a higher\nrate  score  denotes  better  performance,   LIMA\noutperforms InstructGPT and Alpaca by 20% and\n36%, respectively, achieving comparable results to\nBARD, while underperforming Claude and GPT-4.\nExperimental results verify the proposed superficial\nalignment hypothesis.\n4.11    Others\nOPT-IML  (175B)(Iyer  et  al.,  2022)  is  a\nlarge language model trained by fine-tuning the\nOPT  (175B)  (Zhang  et  al.,  2022a)  model  on\nthe constructed Instruction Meta-Learning (IML)\ndataset,  which consists of over 1500 NLP tasks\nfrom  8  publicly  available  benchmarks  such  as\nPromptSource (Bach et al., 2022), FLAN (Longpre\net al., 2023), and Super-NaturalInstructions (Wang\net  al.,  2022e).After  fine-tuning,   OPT-IML\noutperforms OPT across all benchmarks.\nDolly  2.0  (12B)(Conover  et  al.,  2023a)  is\ninitialized  with  the  pre-trained  language  model\nPythia   (12B)   (Biderman   et   al.,   2023),   and\nfine-tuned  on  the  instruction  dataset  databricks-\ndolly-15k\n8\n, which contains 7 categories of NLP\ntasks such as text classification and information\nextraction.    After  fine-tuning,  Dolly  2.0  (12B)\noutperforms Pythia (12B) on the EleutherAI LLM\nEvaluation Harness benchmark (Gao et al., 2021)\nby  a  large  margin,   and  achieves  comparable\n6\nhttps://bard.google.com/\n7\nhttps://www.anthropic.com/index/introducing-claude\n8\nhttps://huggingface.co/datasets/databricks/databricks-\ndolly-15k\nperformances to GPT-NEOX (20B) (Black et al.,\n2022),  which  has  two  times  more  parameters\ncompared to Dolly 2.0 (12B).\nFalcon-Instruct   (40B)(Almazrouei   et   al.,\n2023a) is a large language model trained by fine-\ntuning  Falcon  (40B)  (Almazrouei  et  al.,  2023b)\non  an  English  dialogue  dataset,  which  contains\n150  million  tokens  from  the  Baize  dataset  (Xu\net  al.,  2023c),   with  an  additional  5%  of  the\ndata from the RefinedWeb dataset (Penedo et al.,\n2023).    To  reduce  memory  usage,  the  authors\nemployed flash attention (Dao et al., 2022) and\nmulti-query techniques.   For evaluation,  Falcon-\nInstruct (40B) achieved better performance on the\nOpen LLM Leaderboard (Beeching et al., 2023)\n9\ncompared to the baseline model Falcon (40B), and\noutperforms the Guanaco (65B), which has more\nmodel parameters.\nGuanaco  (7B)(JosephusCheung,  2021)  is  a\nmulti-turn dialog language model trained by fine-\ntuning LLaMA (7B) (Touvron et al., 2023a) on\nthe constructed multilingual dialogue dataset. The\nmultilingual  dialogue  dataset  comes  from  two\nsources:    Alpaca  (Taori  et  al.,  2023a),  which\ncontains 52K English instruction data pairs; and a\nmultilingual (e.g., Simplified Chinese, Traditional\nChinese, Japanese, German) dialogue data, which\ncontains 534K+ multi-turn conversations.  After\nfine-tuning, Guanaco is to generate role-specific\nresponses  and  continuous  responses  on  a  given\ntopic in multi-turn conversations.\nMinotaur  (15B)is  a  large  language  model\ntrained by fine-tuning the Starcoder Plus (15B) (Li\net al., 2023f) on open-source instruction datasets\nincluding  WizardLM  (Xu  et  al.,  2023a)  and\nGPTeacher-General-Instruct\n10\n.For    model\ninference, Minotaur supports a maximum context\nlength of 18K tokens.\nNous-Herme (13B)is a large language model\ntrained  by  fine-tuning  LLaMA  (13B)  (Touvron\net  al.,  2023a)  on  an  instruction  dataset,  which\ncontains  over  300k  instructions,  sampled  from\nGPTeacher\n11\n,   CodeAlpaca  (Chaudhary,  2023),\nGPT-4-LLM   (Peng   et   al.,   2023),   Unnatural\nInstructions    (Honovich    et    al.,    2022),and\n9\nhttps://huggingface.co/spaces/HuggingFaceH4\n/open_llm_leaderboard\n10\nhttps://github.com/teknium1/GPTeacher\n11\nhttps://github.com/teknium1/GPTeacher",
    "BiologyPhysicsChemistry  subsets  in  the  Camel-\nAI  (Li  et  al.,  2023c).   Responses  are  generated\nby GPT-4.   For evaluations,  Nous-Herme (13B)\nachieves  comparable  performances  to  GPT-3.5-\nturbo on multiple tasks like ARC challenge (Clark\net al., 2018) and BoolQ (Clark et al., 2019).\nTÜLU   (6.7B)(Wang   et   al.,   2023c)   is   a\nlarge language model trained by fine-tuning OPT\n(6.7B) (Zhang et al., 2022a) on a mixed instruction\ndataset, which contains FLAN V2 (Longpre et al.,\n2023),  CoT  (Wei  et  al.,  2022),  Dolly  (Conover\net al., 2023a), Open Assistant-1\n12\n, GPT4-Alpaca\n13\n,\nCode-Alpaca (Chaudhary, 2023), and ShareGPT\n14\n.\nAfter fine-tuning, TÜLU (6.7B) reaches on average\n83% of ChatGPT’s performance and 68% of GPT-\n4’s performance.\nYuLan-Chat (13B)(YuLan-Chat-Team, 2023)\nis a language model trained by fine-tuning LLaMA\n(13B)  (Touvron  et  al.,  2023a)  on  a  constructed\nbilingual dataset, which contains 250,000 Chinese-\nEnglish   instruction   pairs.After   fine-tuning,\nYuLan-Chat-13B achieves comparable results to\nthe state-of-the-art open-source model ChatGLM\n(6B) (Du et al., 2022),  and outperforms Vicuna\n(13B) (Chiang et al., 2023) on the English BBH3K\n(BBH3K is a subset of BBH benchmark (Srivastava\net al., 2022)) dataset.\nMOSS (16B)\n15\nis a bilingual dialogue language\nmodel,   which   aims   to   engage   in   multi-turn\nconversations and utilize various plugins, trained\nby fine-tuning on dialogue instructions. After fine-\ntuning, MOSS outperforms the backbone model\nand  generates  responses  that  better  align  with\nhuman preferences.\nAiroboros (13B)\n16\nis a large language model\ntrained by fine-tuning LLAMA (13B) (Touvron\net al., 2023a) on the Self-instruct dataset (Wang\net  al.,  2022c).After  fine-tuning,   Airoboros\nsignificantly outperforms LLAMA (13B) (Touvron\net  al.,  2023a)  on  all  benchmarks  and  achieves\nhighly  comparable  results  to  models  fine-tuned\nspecifically for certain benchmarks.\nUltraLM (13B)(Ding et al., 2023a) is a large\nlanguage model trained by fine-tuning LLAMA\n12\nhttps://huggingface.co/datasets/OpenAssistant/oasst1\n13\nhttps://huggingface.co/datasets/vicgalle/alpaca-gpt4\n14\nhttps://sharegpt.com/\n15\nhttps://txsun1997.github.io/blogs/moss.html\n16\nhttps://github.com/jondurbin/airoboros\n(13B)  (Touvron  et  al.,  2023a).   For  evaluation,\nUltraLM (13B) outperforms Dolly (12B) (Conover\net al., 2023a) and achieves the winning rate up to\n98%. Additionally, it surpasses the previous best\nopen-source models (i.e.,  Vicuna (Chiang et al.,\n2023)  and  WizardLM  (Xu  et  al.,  2023a))  with\nwinning rates of 9% and 28%, respectively.\n5Multi-modality Instruction Fine-tuning\n5.1    Multi-modality Datasets\nMUL-TIINSTRUCT(Xu  et  al.,  2022)  is  a\nmultimodal instruction tuning dataset consisting\nof 62 diverse multimodal tasks in a unified seq-\nto-seq  format.This  dataset  covers  10  broad\ncategories  and  its  tasks  are  derived  from  21\nexisting  open-sourced  datasets.Each  task  is\nequipped with 5 expert-written instructions.  For\nthe existing tasks, the authors use the input/output\npairs from their available open-source datasets to\ncreate  instances.   While  for  each  new  task,  the\nauthors create 5k to 5M instances by extracting\nthe   necessary   information   from   instances   of\nexisting   tasks   or   reformulating   them.The\nMUL-TIINSTRUCT dataset has demonstrated its\nefficiency in enhancing various transfer learning\ntechnique.For   example,   fine-tuning   the\nOFA model (930M) (Wang et al., 2022a) using\nvarious transfer learning strategies such as Mixed\nInstruction  Tuning  and  Sequential  Instruction\nTuning on MUL-TIINSTRUCT improve the zero-\nshot  performance  across  all  unseen  tasks.    On\ncommonsense VQA task, OFA fine-tuned on MUL-\nTIINSTRUCT  achieves  50.60  on  RougeL  and\n31.17 on accuracy, while original OFA achieves\n14.97 on RougeL and 0.40 on accuracy.\nPMC-VQA(Zhang  et  al.,  2023c)  is  a  large-\nscale medical visual question-answering dataset\nthat  comprises  227k  image-question  pairs  of\n149k   images,   covering   various   modalities   or\ndiseases.The  dataset  can  be  used  for  both\nopen-ended   and   multiple-choice   tasks.The\npipeline  for  generating  the  PMC-VQA  dataset\ninvolves   collecting   image-caption   pairs   from\nthe  PMC-OA  (Lin  et  al.,  2023)  dataset,  using\nChatGPT to generate question-answer pairs, and\nmanually  verifying  a  subset  of  the  dataset  for\nquality.  The authors propose a generative-based\nmodel MedVInT for medical visual understanding\nby   aligning   visual   information   with   a   large\nlanguage model.  MedVInT pretrained on PMC-",
    "Multi-modality Instruction Fine-tuning Dataset\nModalities\n# Tasks\nModality Pair# Instance\nMUL-TIINSTRUCT (Xu et al., 2022)\n1\nImage-Text5k to 5M per task62\nPMC-VQA (Zhang et al., 2023c)\n2\nImage-Text227k2\nLAMM (Yin et al., 2023)\n3\nImage-Text186k9\nPoint Cloud-Text10k3\nVision-Flan (Xu et al., 2024)\n4\nMulti-PairsOver 1M200+\nALLAVA (Chen et al., 2024a)\n5\nImage-Text1.4M2\nShareGPT4V (Chen et al., 2023a)\n6\nImage-Text1.2M2\n1\nhttps://github.com/VT-NLP/MultiInstruct\n2\nhttps://github.com/xiaoman-zhang/PMC-VQA\n3\nhttps://github.com/OpenLAMM/LAMM\n4\nhttps://vision-flan.github.io/\n5\nhttps://github.com/FreedomIntelligence/ALLaVA\n6\nhttps://sharegpt4v.github.io/\nTable 4: An overview of multi-modality instruction fine-tuning datasets.\nVQA  achieves  state-of-the-art  performance  and\noutperforms existing models on VQA-RAD (Lau\net  al.,  2018)  and  SLAKE  (Liu  et  al.,  2021a)\nbenchmarks, with 81.6% accuracy on VQA-RAD\nand 88.0% accuracy on SLAKE.\nLAMM(Yin et al., 2023) is a comprehensive\nmulti-modal  instruction  tuning  dataset  for  2D\nimage and 3D point cloud understanding. LAMM\ncontains186Klanguage-imageinstruction-\nresponsepairs,and10Klanguage-point\ncloud  instruction-response  pairs.The  authors\ncollect  images  and  point  clouds  from  publicly\navailable  datasets  and  use  the  GPT-API  and\nself-instruction methods to generate instructions\nand responses based on the original labels from\nthese datasets. LAMM-Dataset includes data pairs\nfor commonsense knowledge question answering\nby incorporating a hierarchical knowledge graph\nlabel  system  from  the  Bamboo  (Zhang  et  al.,\n2022b) dataset and the corresponding Wikipedia\ndescription. The authors also propose the LAMM-\nBenchmark, which evaluates existing multi-modal\nlanguage models (MLLM) on various computer\nvision tasks.  It includes 9 common image tasks\nand  3  common  point  cloud  tasks,  and  LAMM-\nFramework, a primary MLLM training framework\nthat differentiates the encoder, projector, and LLM\nfinetuning blocks for different modalities to avoid\nmodality conflicts.\nVision-Flan(Xu  et  al.,  2024)  is  the  largest\npublic-availablehuman-annotatedvisual\ninstruction tuning dataset that consists of 1,664,261\ninstances and 200+ diverse vision-language tasks\nderived  from  101  open-source  computer  vision\ndatasets.   Each task is accompanied by expertly\nwritten   instructions   and   meticulously   crafted\ntemplates  for  inputs  and  outputs.    The  dataset\ncovers  a  broad  spectrum  of  tasks,   including\nimage   captioning,   visual   question-answering,\nand visual comprehension.  Designed to enhance\nresearch and application in vision-language model\ndomains, Vision-Flan aims to expand the horizons\nof interaction and comprehension between visual\nand linguistic modalities.  It provides researchers\nand developers with a valuable resource to push\nthe  envelope  of  vision-language  models  and  to\ninnovate algorithms across a diverse array of fields.\nALLaVA(Chen et al., 2024a) represents an open-\nsource, extensive dataset tailored for fine-tuning\nvisual question-answering models, featuring 1.4M\nentries  that  include  detailed  captions,  intricate\ninstructions, and comprehensive answers produced\nby GPT-4V (Yang et al., 2023b).  To craft high-\nquality captions and visual question-answers, Chen\net al. (2024a) introduced a method to distill both\na  caption  and  a  QA  pair  for  an  image  in  a\nsingle interaction.  This process involves initially\npresenting GPT-4V (Yang et al., 2023b) with an\nimage, followed by prompting it to generate both a\ndetailed caption and a visual question-answer pair.\nThis approach of incorporating additional visual\ndata enables the model to develop a more nuanced\nunderstanding  of  both  the  visual  and  textual\nelements, enhancing its capacity to deliver precise\nand contextually appropriate answers. Furthermore,\nthis   method   has   the   potential   to   reduce   the\noccurrence  of  hallucinations  by  providing  the\nmodel with more contextual information (visual\ndata).",
    "ShareGPT4V(Chen et al., 2023a) is a collection\nof highly descriptive image-text pairs, consisting\nof  two  components:   100K  captions  generated\nby  GPT4-Vision  (Yang  et  al.,  2023b)  from  a\nvariety of images, and 1.2M captions developed\nusing their pre-trained model, which was trained\non the initial set of 100K high-quality captions.\nThese captions comprehensively cover aspects such\nas  global  knowledge,  object  attributes,  spatial\nrelationships, and aesthetic evaluations. Utilizing\nthis  dataset,  the  ShareGPT4V-7B  model,  once\nfine-tuned, surpasses competing 7B-scale LMMs\nacross all 11 benchmark tests. Notably, it secures\na remarkable cumulative score of 1943.8 on the\nMME benchmark, outperforming the second-place\nQwen-VL-Chat-7B (Bai et al., 2023) model, which\nwas trained with 1.4 billion samples, by 95.6 points.\n5.2    Multi-modality Instruction Fine-tuning\nModels\nInstructPix2Pix (983M)(Brooks et al., 2022) is\na conditional diffusion model trained by fine-tuning\nStable Diffusion (983M) (Rombach et al., 2022)\non a constructed multi-modal dataset that contains\nmore  than  450K  text  editing  instructions  and\ncorresponding images before and after the edit. The\nauthors combine the abilities of two large-scale pre-\ntrained models, a language model GPT-3 (Brown\net al., 2020b) and a text-to-image model Stable\nDiffusion (Rombach et al., 2022), to generate the\nthe training dataset. GPT-3 is fine-tuned to generate\ntext edits based on image prompts, while Stable\nDiffusion is used to convert the generated text edits\ninto actual image edits.   InstructPix2Pix is then\ntrained  on  this  generated  dataset  using  a  latent\ndiffusion objective.  Figure 7 shows the process\nof generating image editing dataset and training\nthe diffusion model on that dataset.  The authors\ncompares the proposed method qualitatively with\nprevious works such as SDEdit (Meng et al., 2022)\nand Text2Live (Bar-Tal et al., 2022), highlighting\nthe ability of the model to follow image editing\ninstructions instead of descriptions of the image or\nedit layer.  The authors also presents quantitative\ncomparisons  with  SDEdit  (Meng  et  al.,  2022)\nusing metrics measuring image consistency and\nedit quality.\nLLaVA  (13B)(Liu  et  al.,  2023b)  is  a  large\nmultimodal   model   developed   by   connecting\nthe  visual  encoder  of  CLIP  (400M)  (Radford\net al., 2021) with the language decoder LLaMA\nFigure 7: Image editing dataset generation and diffusion\nmodel training. The figure is copied from Brooks et al.\n(2022).\n(7B) (Touvron et al., 2023a). LLaVA is fine-tuned\nusing the generated instructional vision-language\ndataset consisted of 158K unique language-image\ninstruction-following samples. The data collection\nprocess involved creating conversation,  detailed\ndescription,   and   complex   reasoning   prompts.\nGPT-4  is  used  to  convert  image-text  pairs  into\nappropriate instruction-following format for this\ndataset.Visual  features  such  as  captions  and\nbounding  boxes  were  used  to  encode  images.\nLLaVA yields a 85.1% relative score compared\nwith GPT-4 on a synthetic multimodal instruction\nfollowing dataset. When fine-tuned on Science QA,\nthe synergy of LLaVA and GPT-4 achieves a new\nstate-of-the-art accuracy of 92.53%.\nVideo-LLaMA(Zhang   et   al.,   2023b)   is\na  multimodal  framework  that  enhances  large\nlanguage  models  with  the  ability  to  understand\nboth visual and auditory content in videos.  The\narchitecture  of  Video-LLaMA  consists  of  two\nbranche  encoders:    the  Vision-Language  (VL)\nBranch and the Audio-Language (AL) Branch, and\na  language  decoder  (Vicuna  (7B/13B)  (Chiang\net  al.,  2023),   LLaMA  (7B)  (Touvron  et  al.,\n2023a), etc.).  The VL Branch includes a frozen\npre-trained  image  encoder  (pre-trained  vision\ncomponent of BLIP-2 (Li et al., 2023d),  which\nincludes a ViT-G/14 and a pre-trained Q-former),\na position embedding layer, a video Q-former and\na  linear  layer.   The  AL  Branch  includes  a  pre-\ntrained audio encoder (ImageBind (Girdhar et al.,\n2023)) and an Audio Q-former.  Figure 8 shows\nthe  overall  architecture  of  Video-LLaMA  with\nVision-Language  Branch  and  Audio-Language\nBranch.The  VL  Branch  is  trained  on  the\nWebvid-2M  (Bain  et  al.,  2021)  video  caption\ndataset with a video-to-text generation task, and\nfine-tuned  on  the  instruction-tuning  data  from\nMiniGPT-4  (Zhu  et  al.,  2023b),  LLaVA  (Liu\net  al.,  2023b)  and  VideoChat  (Li  et  al.,  2023e).\nThe AL Branch is trained on video/image instru-",
    "Multi-modality Instruction\n# ParamsModality\nBase ModelFine-tuning Trainset\nFine-tuned LLMsModel Name# ParamsSelf-buildSize\nInstructPix2Pix (Brooks et al., 2022)\n1\n983MI/TStable Diffusion983MYes450K\nLLaVA (Liu et al., 2023b)\n2\n13BI/T\nCLIP (Radford et al., 2021)400M\nYes\n158K\nLLaMA (Touvron et al., 2023a)7B\nLLaMA (Touvron et al., 2023a)7B\nVideo-LLaMA (Zhang et al., 2023b)\n3\n-I/T/V/A\nBLIP-2 (Li et al., 2023d)-\nNo-\nImageBind (Girdhar et al., 2023)-\nVicuna (Chiang et al., 2023)7B/13B\nInstructBLIP (1.2B) (Dai et al., 2023)\n4\n-I/T/VBLIP-2 (Li et al., 2023d)-No-\nOtter (Li et al., 2023b)\n5\n-I/T/VOpenFlamingo (Awadalla et al., 2023)9BYes2.8M\nMultiModal-GPT (Gong et al., 2023)\n6\n-I/T/VOpenFlamingo (Awadalla et al., 2023)9BNo-\n1\nhttps://github.com/timothybrooks/instruct-pix2pix\n2\nhttps://github.com/haotian-liu/LLaVA\n3\nhttps://github.com/DAMO-NLP-SG/Video-LLaMA\n4\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip\n5\nhttps://github.com/Luodian/Otter\n6\nhttps://github.com/open-mmlab/Multimodal-GPT\nTable 5: An overview of multi-modality instruction fine-tuned LLMs. I/T/V/A stand for Image/Text/Video/Audio\nFigure 8: Overall architecture of Video-LLaMA. The\nfigure is copied from Zhang et al. (2023b).\ncaption data to connect the output of ImageBind\nto  language  decoder.    After  finetuning,  Video-\nLLaMA  can  perceive  and  comprehend  video\ncontent,   demonstrating  its  ability  to  integrate\nauditory and visual information, understand static\nimages, recognize common-knowledge concepts,\nand capture temporal dynamics in videos.\nInstructBLIP  (1.2B)(Dai  et  al.,  2023)  is\na  vision-language  instruction  tuning  framework\ninitialized  with  a  pre-trained  BLIP-2  (Li  et  al.,\n2023d))  model  consisting  of  an  image  encoder,\nan LLM (FlanT5 (3B/11B) (Chung et al., 2022)\nor Vicuna (7B/13B) (Chiang et al., 2023)),  and\na  Query  Transformer  (Q-Former)  to  bridge  the\ntwo. As shown in Figure 9, the Q-Former extracts\ninstruction-aware visual features from the output\nembeddings  of  the  frozen  image  encoder,  and\nfeeds  the  visual  features  as  soft  prompt  input\nto  the  frozen  LLM.  The  authors  evaluate  the\nproposed InstructBLIP model on a variety of vision-\nlanguage  tasks,  including  image  classification,\nimage captioning, image question answering, and\nvisual reasoning.  They use 26 publicly available\ndatasets,  dividing  them  into  13  held-in  and  13\nheld-out datasets for training and evaluation. The\nauthors  demonstrate  that  InstructBLIP  achieves\nstate-of-the-art zero-shot performance on a wide\nrange of vision-language tasks. InstructBLIP yields\nan average relative improvement of 15.0% when\ncompared to BLIP-2, smallest InstructBLIP (4B)\noutperforms Flamingo (80B) (Alayrac et al., 2022)\non all six shared evaluation datasets with an average\nrelative improvement of 24.8%.\nOtter(Li  et  al.,  2023b)  is  a  multi-modal\nmodel   trained   by   fine-tuning   OpenFlamingo\n(9B) (Awadalla et al., 2023),  with the language\nand vision encoders frozen and only fine-tuning the\nPerceiver resampler module, cross-attention layers,\nand input/output embeddings. The authors organize",
    "Figure  9:  Overall  architecture  of  InstructBLIP.  The\nfigure is copied from Dai et al. (2023).\ndiverse multi-modal tasks covering 11 categories\nand   build   multi-modal   in-context   instruction\ntuning  datasets  MIMIC-IT  of  2.8M  multimodal\ninstruction-response pairs, which consists of image-\ninstruction-answer triplets, where the instruction-\nanswer  is  tailored  to  the  image.Each  data\nsample  also  includes  context,  which  contains  a\nseries  of  image-instruction-answer  triplets  that\ncontextually  correlate  with  the  queried  triplet.\nOtter  demonstrates  the  ability  to  follow  user\ninstructions  more  accurately  and  provide  more\ndetailed   descriptions   of   images   compared   to\nOpenFlamingo (Awadalla et al., 2023).\nMultiModal-GPT(Gong et al., 2023) is a multi-\nmodal instruction tuning model that is capable of\nfollowing diverse instructions, generating detailed\ncaptions, counting specific objects, and addressing\ngeneral  inquiries.    MultiModal-GPT  is  trained\nby  fine-tuning  OpenFlamingo  (9B)  (Awadalla\net al., 2023) on various created visual instruction\ndata with open datasets,  including VQA, Image\nCaptioning,  Visual  Reasoning,  Text  OCR,  and\nVisual  Dialogue.   The  experiments  demonstrate\nthe proficiency of MultiModal-GPT in maintaining\ncontinuous dialogues with humans.\n6\nDomain-specific Instruction Finetuning\nIn this section, we describe instruction tuning in\ndifferent domains and applications.\n6.1    Dialogue\nInstructDial(Gupta   et   al.,   2022)   is   an\ninstruction tuning framework designed for dialogue.\nIt  contains  a  collection  of  48  dialogue  tasks  in\na  consistent  text-to-text  format  created  from  59\ndialogue  datasets.   Each  task  instance  includes\na  task  description,  instance  inputs,  constraints,\ninstructions, and output.  To ensure adherence to\ninstructions, the framework introduces two meta-\ntasks: (1) an instruction selection task, where the\nFigure 10: The overview framework of InstructUIE. The\nfigure is copied from Wang et al. (2023b).\nmodel selects the instruction corresponding to a\ngiven  input-output  pair;  and  (2)  an  instruction\nbinary task, where the model predicts \"yes\" or \"no\"\nif an instruction leads to a given output from an\ninput. Two base models T0-3B (Sanh et al., 2021)\n(3B parameters version of T5 (Lester et al., 2021))\nand BART0 (Lin et al., 2022) (406M parameters\nbased on Bart-large (Lewis et al., 2019)) are fine-\ntuned on the tasks from InstructDial. InstructDial\nachieves  impressive  results  on  unseen  dialogue\ndatasets and tasks, including dialogue evaluation\nand intent detection.  Moreover, it delivers even\nbetter results when applied to a few-shot setting.\n6.2    Intent Classification and Slot Tagging\nLINGUIST(Rosenbaum et al., 2022) finetunes\nAlexaTM  5B  (Soltan  et  al.,  2022),  a  5-billion-\nparameter multilingual model, on the instruction\ndataset  for  intent  classification  and  slot  tagging\ntasks. Each instruction consists of five blocks: (i)\nthe language of the generated output, (ii) intention,\n(iii) slot types and values to include in the output\n(e.g., the number 3 in [3, snow] corresponds the\nslot type, and snow is the value used for that slot),\n(iv) a mapping from slot type labels to numbers,\nand (v) up to 10 examples to instruct the format\nof  the  outputs.LINGUIST  shows  significant\nimprovements over state-of-the-art approaches in\na  10-shot  novel  intent  setting  using  the  SNIPS\ndataset (Coucke et al., 2018). In the zero-shot cross-\nlingual setting of the mATIS++ dataset (Xu et al.,\n2020), LINGUIST surpasses a strong baseline of\nMachine Translation with Slot Alignment across 6\nlanguages while maintaining intent classification\nperformance.\n6.3    Information Extraction\nInstructUIE(Wang et al., 2023b) is a unified\ninformation extraction (IE) framework based on\ninstruction  tuning,   which  transforms  IE  tasks",
    "Domain Type\nDomain-specific InstructionBase Model\nTrainset Size\nFine-tuned LLMsModel Name# Params\nDialogue\nInstructDial (Gupta et al., 2022)\n1\nT0 (Sanh et al., 2021)3B\n-\nClassificationLINGUIST (Rosenbaum et al., 2022)AlexaTM (Soltan et al., 2022)5B13K\nInformation extractionInstructUIE (Wang et al., 2023b)\n2\nFlanT5 (Chung et al., 2022)11B1.0M\nSentiment analysisIT-MTL (Varia et al., 2022)\n3\nT5 (Raffel et al., 2019)220M-\nWriting\nWriting-Alpaca-7B (Zhang et al., 2023d)\n4\nLLaMA (Touvron et al., 2023a)7B-\nCoEdIT (Raheja et al., 2023)\n5\nFlanT5 (Chung et al., 2022)11B\nCoPoet (Chakrabarty et al., 2022)\n6\nT5 (Raffel et al., 2019)11B\nMedical\nRadiology-GPT (Liu et al., 2023c)\n7\nAlpaca (Taori et al., 2023a)7B122K\nChatDoctor (Li et al., 2023i)\n8\nLLaMA (Touvron et al., 2023a)7B100K\nChatGLM-Med (Haochun Wang, 2023)\n9\nChatGLM (Du et al., 2022)6B-\nArithmeticGoat (Liu and Low, 2023)\n10\nLLaMA (Touvron et al., 2023a)7B1.0M\nCodeWizardCoder (Luo et al., 2023)\n11\nStarCoder (Li et al., 2023f)15B78K\n1\nhttps://github.com/prakharguptaz/Instructdial\n2\nhttps://github.com/BeyonderXX/InstructUIE\n3\nhttps://github.com/amazon-science/instruction-tuning-for-absa\n4\nhttps://github.com/facebookresearch/EditEval\n5\nhttps://github.com/vipulraheja/coedit\n6\nhttps://github.com/vishakhpk/creative-instructions\n7\nhttps://huggingface.co/spaces/allen-eric/radiology-gpt\n8\nhttps://github.com/Kent0n-Li/ChatDoctor\n9\nhttps://github.com/SCIR-HI/Med-ChatGLM\n10\nhttps://github.com/liutiedong/goat\n11\nhttps://github.com/nlpxucan/WizardLM\nTable 6: An overview of domain-specific instruction fine-tuned LLMs.\nto  the  seq2seq  format  and  solves  them  by  fine-\ntuning  11B  FlanT5  (Chung  et  al.,  2022)  on  the\nconstructed  IT  dataset.Figure  10  shows  the\noverall architecture of InstructUIE. It introduces\nIE INSTRUCTIONS, a benchmark of 32 diverse\ninformation extraction datasets in a unified text-to-\ntext format with expert-written instructions. Each\ntask  instance  is  delineated  by  four  properties:\ntask instruction,  options,  text,  and output.  Task\ninstruction contains information such as the type of\ninformation to be extracted, the output structure\nformat,  and  additional  constraints  or  rules  that\nneed to be adhered to during the extraction process.\nOptions refer to the output label constraints of a\ntask.  Text refers to the input sentence.  Output is\nthe sentence obtained by converting the original\ntags of the sample (e.g.  \"entity tag:  entity span\"\nfor NER). In the supervised setting, InstructUIE\nperforms  comparably  to  BERT  (Devlin  et  al.,\n2018)  and  outperforms  the  state-of-the-art  and\nGPT3.5 (Brown et al., 2020a) in zero-shot settings.\n6.4    Aspect-based Sentiment Analysis\nVaria et al. (2022)propose a unified instruction\ntuning   framework   for   solving   Aspect-based\nSentiment Analysis (ABSA) task based on a fine-\ntuned  T5  (220M)  (Raffel  et  al.,  2019)  model.\nThe framework addresses multiple factorized sub-\ntasks  that  involve  the  four  elements  of  ABSA,\nnamely Aspect Term, Aspect Category, Opinion\nTerm, and Sentiment.  It treats these sub-tasks as\na combination of five Question Answering (QA)\ntasks by transforming each sentence in the corpus\nusing instruction templates provided for each task.\nFor  instance,  one  of  the  instruction  templates\nused  is  \"What  are  the  aspect  terms  in  the  text:\n$TEXT?\". The framework showcases substantial\nimprovement (8.29 F1 on average) over the state-of-\nthe-art in few-shot learning scenarios and remains\ncomparable in full fine-tuning scenarios.\n6.5    Writing\nZhang et al. (2023d)propose Writing-Alpaca-\n7B that fine-tunes LLaMa-7B (Peng et al., 2023)\non the writing instruction dataset to provide writing\nassistance.   The  proposed  instruction  dataset  is\nan  extension  of  the  EDITEVAL  (Dwivedi-Yu\net  al.,  2022)  benchmark  based  on  instructional\ndata, with the Updating task removed and a task\nfor  grammaticality  introduced.   The  instruction\nscheme  strictly  follows  the  one  in  the  Stanford\nAlpaca project (Taori et al., 2023a), comprising a\nuniversal preface, an instruction field to guide task\ncompletion, an input field that provides the text to",
    "Figure 11: The overview framework of COEDIT. The\nfigure is copied from Raheja et al. (2023).\nbe edited, and a response field that requires models\nto fill out. The Writing-Alpaca-7B improves upon\nLLaMa’s  performance  on  all  writing  tasks  and\noutperforms other larger off-the-shelf LLMs.\nCoEdIT(Raheja   et   al.,   2023)   finetunes\nFLANT5 (Chung et al., 2022) (770M parameters,\n3B  parameters,   and  11B  parameters)  on  the\ninstruction  dataset  for  text  editing  to  provide\nwriting   assistance.The   instruction   dataset\ncomprises approximately 82K <instruction: source,\ntarget> pairs.  As shown in Figure 11, the model\ntakes  instructions  from  the  user  specifying  the\ncharacteristics of the desired text, such as \"Make\nthe sentence simpler\", and outputs the edited text.\nCoEdIT achieves state-of-the-art performance on\nseveral text editing tasks, including grammatical\nerror correction, text simplification, iterative text\nediting, and three stylistic editing tasks: formality\nstyle  transfer,  neutralization,  and  paraphrasing.\nFurthermore, it can generalize well to new, adjacent\ntasks not seen during fine-tuning.\nCoPoet(Chakrabarty   et   al.,   2022)   is   a\ncollaborative  poetry  writing  tool  that  utilizes  a\nlarge language model (e.g.  T5-3B, T5-11B and\nT0-3B models) trained on a diverse collection of\ninstructions for poetry writing.   Each sample in\nthe  instruction  dataset  includes  an  <instruction,\npoem_line> pair.  There are three major types of\ninstructions:   Continuation,  Lexical  Constraints,\nand Rhetorical Techniques. The CoPoet is guided\nby user instructions that specify desired attributes\nof  the  poetry,  such  as  writing  a  sentence  about\n\"love\"  or  ending  a  sentence  with  \"fly.\"  Not\nonly  is  the  system  competitive  with  publicly\navailable LLMs trained on instructions,  such as\nInstructGPT  (Ouyang  et  al.,  2022),   but  it  is\nalso capable of satisfying unseen compositional\ninstructions.\n6.6    Medical\nRadiology-GPT(Liu  et  al.,  2023c)  is  a  fine-\ntuned Alpaca-7B (Taori et al., 2023a) model for\nradiology,  which  utilizes  an  instruction  tuning\napproach  on  an  extensive  dataset  of  radiology\ndomain  knowledge.    Radiology  reports  usually\ninclude two corresponding sections:  \"Findings\"\nand \"Impression\". The \"Findings\" section contains\ndetailed observations from the radiology images,\nwhile  the  \"Impression\"  section  summarizes  the\ninterpretations  drawn  from  those  observations.\nRadiology-GPT  provides  a  brief  instruction  to\nthe \"Findings\" text: \"Derive the impression from\nfindings in the radiology report\". The \"Impression\"\ntext  from  the  same  report  serves  as  the  target\noutput. In comparison to general language models\nsuch  as  StableLM  (Islamovic),  Dolly  (Conover\net  al.,  2023a),   and  LLaMA  (Touvron  et  al.,\n2023a), Radiology-GPT demonstrates significant\nadaptability in radiological diagnosis, research, and\ncommunication.\nChatDoctor(Li   et   al.,   2023i)   is   based\nonthefine-tunedLLaMa-7B(Touvron\net   al.,   2023a)   model,    utilizing   the   alpaca\ninstruction  dataset  (Taori  et  al.,  2023a)  and  the\nHealthCareMagic100k   patient-doctor   dialogue\ndataset.  And prompt templates are designed for\nretrieving  external  knowledge  databases,  such\nas the Disease Database and Wikipedia retrieval,\nduring doctor-patient conversations to obtain more\naccurate outputs from the model. The ChatDoctor\nsignificantly   improves   the   model’s   ability   to\ncomprehend patient needs and provide informed\nadvice. By equipping the model with self-directed\ninformation  retrieval  from  reliable  online  and\noffline  sources,  the  accuracy  of  its  responses  is\nsubstantially improved.\nChatGLM-Med(Haochun  Wang,  2023)  is\nfine-tuned  on  the  Chinese  medical  instruction\ndataset  based  on  the  ChatGLM-6B  (Du  et  al.,\n2022) model.  The instruction dataset comprises\nmedically  relevant  question  and  answer  pairs,\ncreated using the GPT 3.5 API and the Medical\nKnowledge  Graph.This  model  improves  the\nquestion-answering performance of ChatGLM (Du\net al., 2022) in the medical field.",
    "6.7    Arithmetic\nGoat(Liu  and  Low,  2023)  is  a  fine-tuned\nLLaMA-7B (Touvron et al., 2023a) model based\non  instructions,  which  aims  to  solve  arithmetic\nproblems. It expresses arithmetic problems in the\nform of natural language question answering, such\nas \"What is 8914/64?\",  by generating hundreds\nof instruction templates using ChatGPT (OpenAI,\n2022).  The model applies various techniques to\nenhance its adaptability to diverse question formats,\nsuch   as   randomly   removing   spaces   between\nnumbers and symbols in the arithmetic expression\nand replacing \"*\" with \"x\" or \"times\".  The Goat\nmodel  achieves  state-of-the-art  performance  on\nthe BIG-bench (Srivastava et al., 2022) arithmetic\nsubtask. In particular, zero-shot Goat-7B matches\nor exceeds the accuracy achieved by the few-shot\nPaLM-540B (Chowdhery et al., 2022).\n6.8    Code\nWizardCoder(Luo   et   al.,   2023)   utilizes\nStarCoder 15B (Li et al., 2023f) as the foundation\nwith complex instruction fine-tuning, by adapting\nthe Evol-Instruct method (Xu et al., 2023a) to the\ndomain of code. The training dataset is produced\nthrough iterative application of the Evol-Instruct\ntechnique on the Code Alpaca dataset (Taori et al.,\n2023b),  which  includes  the  following  attributes\nfor each sample: instruction, input, and expected\noutput.For  instance,  when  the  instruction  is\n\"Amend the following SQL query to select distinct\nelements\",  the  input  is  the  SQL  query,  and  the\nexpected  output  is  the  generated  answer.    The\nWizardCoder outperforms all other open-source\nCode LLMs and even surpasses the largest closed\nLLMs, Anthropic’s Claude and Google’s Bard, on\nHumanEval and HumanEval+.\n7    Efficient Tuning Techniques\nEfficient fine-tuning techniques aim at adapting\nLLMs  to  downstream  tasks  by  optimizing  a\nsmall  fraction  of  parameters  in  multiple  ways,\ni.e.,    addition-based,    specification-based,    and\nreparameterization-based. Addition-based methods\nintroduce extra trainable parameters or modules\nnot present in the original model. Representative\nmethods include adapter tuning (Houlsby et al.,\n2019)   and   prompt-based   tuning   (Schick   and\nSchütze,  2021).Specification-based  methods\nspecify  certain  inherent  model  parameters  to\nbe  tuned  while  freezing  others.    For  example,\nBitFit  (Zaken  et  al.,  2022)  tunes  the  bias  terms\nof  the  pre-trained  model.Reparameterization\nmethods  transform  model  weights  into  more\nparameter-efficient  forms  for  tuning.    The  key\nhypothesis is that model adaptation is low-rank,\nso  weights  can  be  reparameterized  into  low-\nrank factors or a low-dimensional subspace (e.g.,\nLoRA (Hu et al., 2021)). Intrinsic prompt tuning\nfinds a low-dimensional subspace shared by tuning\nprompts across diverse tasks.\n7.1    LoRA\nLow-Rank Adaptation (LoRA) (Hu et al., 2021)\nenables efficient adaptation of LLMs using low-\nrank updates. LoRA use DeepSpeed (Rasley et al.,\n2020) as the training backbone. The key insight of\nLoRA is that the actual change in LLMs’ weights\nrequired  for  new  task  adaptation  lies  in  a  low-\ndimensional subspace. Specifically, for a pretrained\nweight matrixW\n0\n, the authors model the adapted\nweight matrix asW\n0\n+∆W, where∆Wis a low\nrank update.∆Wis parameterized as∆W=BA,\nwhereAandBare much smaller trainable matrices.\nThe rankrof∆Wis chosen to be much smaller\nthan the dimensions ofW\n0\n.  The intuition is that\ninstead of directly training all ofW\n0\n, the authors\ntrain low-dimensionalAandB, which indirectly\ntrainsW\n0\nin a low-rank subspace of directions that\nmatter for the downstream task. This results in far\nfewer trainable parameters compared to full fine-\ntuning. For GPT-3, LoRA reduces the number of\ntrainable parameters by 10,000x and memory usage\nby 3x compared to full fine-tuning.\n7.2    HINT\nHINT   (Ivison   et   al.,   2022)   combines   the\ngeneralization   benefits   of   instruction   tuning\nwith  efficient  on-demand  fine-tuning,  avoiding\nrepeatedly processing lengthy instructions.   The\nessence  of  HINT  lies  in  hypernetworks,  which\ngenerate  parameter-efficient  modules  for  LLMs\nadaptation based on natural language instructions\nand few-shot examples. The adopted hypernetwork\nconverts instructions and few-shot examples into\na encoded instruction and generates adapter and\nprefix parameters using a pretrained text encoder\nand  cross-attention  based  parameter  generator.\nThen,  the  generated  adapters  and  prefixes  are\ninserted  into  the  backbone  model  as  efficient\ntuning modules.  At inference, the hypernetwork\nperforms inference only once per task to generate\nadapted modules. The benefits are that HINT can",
    "incorporate long instructions and additional few-\nshots without increasing compute, unlike regular\nfine-tuning or input concatenation methods.\n7.3    Qlora\nQLORA (Dettmers et al., 2023) includes optimal\nquantization  and  memory  optimization,  aiming\nat  providing  efficient  and  effective  LLMs  fine-\ntuning. QLORA includes 4-bit NormalFloat (NF4)\nQuantization,  which  is  a  quantization  scheme\noptimized  for the  typical normal  distribution  of\nLLM  weights.By  quantizing  based  on  the\nquantiles of a normal distribution, NF4 provides\nbetter performance than standard 4-bit integer or\nfloat quantization. To further reduce memory, the\nquantization constants are themselves quantized\nto  8  bits.This  second  level  of  quantization\nsaves  an  additional  0.37  bits  per  parameter  on\naverage.    QLORA  leverages  NVIDIA’s  unified\nmemory feature to page optimizer states to CPU\nRAM when GPU memory is exceeded.  avoiding\nout-of-memory during training. QLORA enables\ntraining a 65B parameter LLM on a single 48GB\nGPU  with  no  degradation  compared  to  full  16-\nbit  finetuning.   QLORA  works  by  freezing  the\n4-bit quantized base LLM, then backpropagating\nthrough  it  into  a  small  set  of  16-bit  low-rank\nadapter weights which are learned.\n7.4    LOMO\nLOw-Memory Optimization (LOMO) (Lv et al.,\n2023) enables full parameter fine-tuning of LLMs\nusing  limited  computational  resources  through\na  fusion  of  gradient  computation  and  update.\nThe  essence  is  to  fuse  gradient  computation\nand   parameter   update   into   one   step   during\nbackpropagation, thereby avoiding storage of full\ngradient  tensors.   Firstly,  theoretical  analysis  is\nprovided in LOMO on why SGD can work well\nfor  fine-tuning  large  pre-trained  models  despite\nits  challenges  on  smaller  models.    In  addition,\nLOMO updates each parameter tensor immediately\nafter computing its gradient in backpropagation.\nStoring the gradient of one parameter at a time\nreduces gradient memory toO(1). LOMO employs\ngradient  value  clipping,  separate  gradient  norm\ncomputation  pass  and  dynamic  loss  scaling  to\nstabilize  training.   The  integration  of  activation\ncheckpointing  and  ZeRO  optimization  methods\nsaves memory.\n7.5    Delta-tuning\nDelta-tuning   (Ding   et   al.,   2023b)   provides\noptimization and optimal control perspectives for\ntheoretical analyzation.   Intuitively,  delta-tuning\nperforms  subspace  optimization  by  restricting\ntuning to a low-dimensional manifold. The tuned\nparameters  act  as  optimal  controllers  guiding\nmodel behavior on downstream tasks.\n8    Evaluation, Analysis and Criticism\n8.1    HELM Evaluation\nHELM(Liang et al., 2022) is a holistic evaluation\nof   Language   Models   (LMs)   to   improve   the\ntransparency   of   language   models,   providing\na   more   comprehensive   understanding   of   the\ncapabilities,  risks,  and  limitations  of  language\nmodels.Specifically,   differing   from   other\nevaluation methods, HELM holds that a holistic\nevaluation of language models should focus on the\nfollowing three factors:\n(1)Broad  coverage.During  the  development,\nlanguage   models   can   be   adapted   to   various\nNLP tasks (e.g., sequence labeling and question\nanswering),   thus,   the  evaluation  of  language\nmodels needs to be carried out in a wide range\nof scenarios.   To involve all potential scenarios,\nHELM  proposed  a  top-down  taxonomy,  which\nbegins by compiling all existing tasks in a major\nNLP conference (ACL2022) into a task space and\ndividing each task into the form of scenarios (e.g.,\nlanguages) and metrics (e.g., accuracy). Then when\nfacing a specific task, the taxonomy would select\none or more scenarios and metrics in the task space\nto  cover  it.   By  analyzing  the  structure  of  each\ntask, HELM clarifies the evaluation content (task\nscenarios and metrics) and improves the scenario\ncoverage of language models from 17.9% to 96.0%.\n(2)Multi-metric  measurement.In  order  to\nenable  human  to  weigh  language  models  from\ndifferent  perspectives,   HELM  proposes  multi-\nmetric  measurement.HELM  has  covered  16\ndifferent  scenarios  and  7  metrics.To  ensure\nthe results of intensive multi-metric measurement,\nHELM measured 98 of 112 possible core scenarios\n(87.5%).\n(3)Standardization.The increase in the scale\nand training complexity of language models has\nseriously hindered human’s understanding of the\nstructure of each language model.  To establish a\nunified understanding of existing language models,\nHELM   benchmarks   30   well-known   language",
    "models,   covering  such  institutions  as  Google\n(UL2(Tay et al., 2022)), OpenAI (GPT-3(Brown\net al., 2020b)), and EleutherAI (GPT-NeoX(Black\net al., 2022)). Interestingly, HELM pointed out that\nLMs such as T5 (Raffel et al., 2019) and Anthropic-\nLMv4-s3 (Bai et al., 2022a) had not been directly\ncompared in the initial work, while LLMs such as\nGPT-3 and YaLM were still different from their\ncorresponding reports after multiple evaluations.\n8.2    Low-resource Instruction Tuning\nGupta et al. (2023) attempts to estimate the minimal\ndownstream training data required by IT models to\nmatch the SOTA supervised models over various\ntasks. Gupta et al. (2023) conducted experiments\non  119  tasks  from  Super  Natural  Instructions\n(SuperNI) in both single-task learning (STL) and\nmulti-task learning (MTL) settings.  The results\nindicate that in the STL setting, IT models with\nonly 25% of downstream training data outperform\nthe SOTA models on those tasks, while in the MTL\nsetting, just 6% of downstream training data can\nlead IT models to achieve the SOTA performance.\nThese findings suggest that instruction tuning can\neffectively assist a model in quickly learning a task\neven with limited data.\nHowever,  due  to  resource  limitations,  Gupta\net  al.  (2023)  did  not  conduct  experiments  on\nLLMs,    like   T5-11B.   So,    to   gain   a   more\ncomprehensive  understanding  of  the  IT  models,\nfurther investigation using larger language models\nand datasets is necessary.\n8.3    Smaller Instruction Dataset\nIT  requires  a  substantial  amount  of  specialized\ninstruction data for training.  Zhou et al. (2023)\nhypothesized that the pre-trained LLM only has to\nlearn the style or format to interact with users and\nproposed LIMA that achieves strong performance\nby  fine-tuning  an  LLM on  only  1,000  carefully\nselected training examples.\nSpecifically, LIMA first manually curates 1,000\ndemonstrations  with  high-quality  prompts  and\nresponses.    Then  the  1,000  demonstrations  are\nused to fine-tune the pre-trained 65B-parameter\nLLaMa (Touvron et al., 2023b).  By comparison,\nacross  more  than  300  challenging  tasks,  LIMA\noutperfrms GPT-davinci003 (Brown et al., 2020b),\nwhich was fine-tuned on 5,200 examples by human\nfeedback tuning. Moreover, with only half amount\nof  demonstrations,   LIMA  achieves  equivalent\nresults  to  GPT-4  (OpenAI,  2023),  Claude  (Bai\net  al.,  2022b),  and  Bard\n17\n.    Above  all,  LIMA\ndemonstrated that LLMs’ powerful knowledge and\ncapabilities can be exposed to users with only a few\ncarefully curated instructions to fine-tune.\n8.4    Evaluating Instruction-tuning Datasets\nThe  performance  of  IT  model  highly  depends\non  the  IT  datasets.However,  there  lacks  of\nevaluations for these IT datasets from open-ended\nand subjective aspects.\nTo  address  this  issue,  Wang  et  al.  (2023c)\nperforms  dataset  evaluation  by  fine-tuning  the\nLLaMa model (Touvron et al., 2023b) on various\nof  open  IT  datasets  and  measure  different  fine-\ntuned models through both automatic and human\nevaluations.   An  additional  model  is  trained  on\nthe combination of IT datasets.   For the results,\nWang  et  al.  (2023c)  showed  that  there  is  not  a\nsingle best IT dataset across all tasks,  while by\nmanually combining datasets it can achieve the best\noverall performance. Besides, Wang et al. (2023c)\npointed out that though IT can bring large benefits\non LLMs at all sizes, smaller models and models\nwith a high base quality benefit most from IT. For\nhuman evaluations, Wang et al. (2023c) a larger\nmodel is more likely to gain a higher acceptability\nscore.\n8.5    Do IT just learn Pattern Copying?\nTo address the lack of clarity about the specific\nknowledge that models acquire through instruction\ntuning,  Kung  and  Peng  (2023)  delves  into  the\nanalysis of how models make use of instructions\nduring IT by comparing the tuning when provided\nwith   altered   instructions   versus   the   original\ninstructions.\nSpecifically,  Kung  and  Peng  (2023)  creates\nsimplified   task   definitions   that   remove   all\nsemantic  components,  leaving  only  the  output\ninformation.  In addition, Kung and Peng (2023)\nalso incorporates delusive examples that contain\nincorrect input-output mapping. Surprisingly, the\nexperiments  show  that  models  trained  on  these\nsimplified  task  definitions  or  delusive  examples\ncan achieve comparable performance to the ones\ntrained on the original instructions and examples.\nMoreover,  the  paper  also  introduces  a  baseline\nfor the classification task with zero-shot,  which\n17\nBard,designedbyGoogle,isaninterface\ntogenerativeAIplatform,andthelinkis:\nhttps://ai.google/static/documents/google-about-bard.pdf",
    "achieves similar performance to IT in low-resource\nsettings.\nIn summary, according to Kung and Peng (2023),\nthe notable performance improvements observed in\ncurrent IT models may be attributed to their ability\nto capture surface-level patterns, such as learning\nthe output format and making guesses, rather than\ncomprehending and learning the specific task.\n8.6    Proprietary LLMs Imitation\nLLMs imitation is an approach that collects outputs\nfrom a stronger model, such as a proprietary system\nlike ChatGPT, and uses these outputs to fine-tune\nan open-source LLM. Through this way, an open-\nsource LLM may get competitive capabilities with\nany proprietary model.\nGudibande  et  al.  (2023)  conducted  several\nexperiments to critically analyze the efficacy of\nmodel  imitation.   Specifically,  Gudibande  et  al.\n(2023)  first  collected  datasets  from  outputs  of\nChatGPT over broad tasks.  Then these datasets\nwere used to fine-tune a range of models covering\nsizes from 1.5B to 13B, base models GPT-2 and\nLLaMA, and data amounts from 0.3M tokens to\n150M tokens.\nFor   evaluations,   Gudibande   et   al.   (2023)\ndemonstrated that on tasks with supported datasets,\nimitation models are far better than before,  and\ntheir outputs appear similar to ChatGPT’s. While\non  tasks  without  imitation  datasets,   imitation\nmodels do not have improvement or even decline\nin accuracy.\nThus,  Gudibande  et  al.  (2023)  pointed  out\nthat  it’s  the  phenomenon  that  imitation  models\nare  adept  at  mimicking  ChatGPT’s  style  (e.g.,\nbeing fluent,  confident and well-structured) that\nmakes researchers have the illusion about general\nabilities  of  imitation  models.So,  Gudibande\net al. (2023) suggested that instead of imitating\nproprietary models, researchers had better focus\non  improving  the  quality  of  base  models  and\ninstruction examples.\n9    Conclusion\nThis  work  surveys  recent  advances  in  the  fast\ngrowing  field  of  instruction  tuning.    We  make\na  systematic  review  of  the  literature,  including\nthe general methodology of IT, the construction\nof  IT  datasets,  the  training  of  IT  models,  IT’s\napplications to different modalities, domains and\napplication.We  also  review  analysis  on  IT\nmodels  to  discover  both  their  advantages  and\npotential pitfalls.  We hope this work will act as\na stimulus to motivate further endeavors to address\nthe deficiencies of current IT models.\nReferences\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han\nLu,   Nicholas   Meade,   and   Siva   Reddy.   2023.\nEvaluating correctness and faithfulness of instruction-\nfollowing  models  for  question  answering.ArXiv,\nabs/2307.16877.\nJean-Baptiste  Alayrac,  Jeff  Donahue,  Pauline  Luc,\nAntoine  Miech,  Iain  Barr,  Yana  Hasson,  Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds,  Roman Ring,  Eliza Rutherford,  Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei,\nMarianne  Monteiro,  Jacob  L.  Menick,  Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand\nSharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol   Vinyals,   Andrew   Zisserman,   and   Karén\nSimonyan. 2022. Flamingo: a visual language model\nfor few-shot learning. InNeurIPS.\nEbtesam  Almazrouei,  Hamza  Alobeidli,  Abdulaziz\nAlshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Heslow,\nJulien Launay, Quentin Malartic, Badreddine Noune,\nBaptiste  Pannier,  and  Guilherme  Penedo.  2023a.\nFalcon-40B: an open large language model with state-\nof-the-art performance.\nEbtesam  Almazrouei,  Hamza  Alobeidli,  Abdulaziz\nAlshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Heslow,\nJulien  Launay,   Quentin  Malartic,   et  al.  2023b.\nFalcon-40b: an open large language model with state-\nof-the-art performance.\nAnas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel,\nYusuf  Hanafy,  Wanrong  Zhu,  Kalyani  Marathe,\nYonatan Bitton, Samir Gadre, Jenia Jitsev, et al. 2023.\nOpenflamingo.\nStephen  H.  Bach,  Victor  Sanh,  Zheng  Xin  Yong,\nAlbert  Webson,   Colin  Raffel,   Nihal  V.  Nayak,\nAbheesht  Sharma,  Taewoon  Kim,  M  Saiful  Bari,\nThibault Févry, Zaid Alyafeai, Manan Dey, Andrea\nSantilli,  Zhiqing Sun,  Srulik Ben-David,  Canwen\nXu,  Gunjan  Chhablani,  Han  Wang,  Jason  Alan\nFries,   Maged   S.   Al-shaibani,   Shanya   Sharma,\nUrmish Thakker, Khalid Almubarak, Xiangru Tang,\nMike  Tian-Jian  Jiang,  and  Alexander  M.  Rush.\n2022.   Promptsource:  An integrated development\nenvironment  and  repository  for  natural  language\nprompts.ArXiv, abs/2202.01279.\nJinze  Bai,  Shuai  Bai,  Shusheng  Yang,  Shijie  Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A versatile vision-\nlanguage model for understanding, localization, text\nreading, and beyond.",
    "Yuntao  Bai,  Andy  Jones,  Kamal  Ndousse,  Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022a. Training a helpful and harmless assistant with\nreinforcement learning from human feedback.arXiv\npreprint arXiv:2204.05862.\nYuntao   Bai,   Saurav   Kadavath,   Sandipan   Kundu,\nAmanda  Askell,   Jackson  Kernion,   Andy  Jones,\nAnna   Chen,   Anna   Goldie,   Azalia   Mirhoseini,\nCameron McKinnon, et al. 2022b.   Constitutional\nai: Harmlessness from ai feedback.arXiv preprint\narXiv:2212.08073.\nMax  Bain,  Arsha  Nagrani,  Gül  Varol,  and  Andrew\nZisserman.  2021.   Frozen  in  time:  A  joint  video\nand image encoder for end-to-end retrieval. InIEEE\nInternational Conference on Computer Vision.\nOmer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni\nKasten,  and  Tali  Dekel.  2022.Text2live:   Text-\ndriven layered image and video editing. InEuropean\nConference  on  Computer  Vision,  pages  707–723.\nSpringer.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020.  The\npushshift reddit dataset. InInternational Conference\non Web and Social Media.\nEdward   Beeching,   Sheon   Han,   Nathan   Lambert,\nNazneen Rajani, Omar Sanseviero, Lewis Tunstall,\nand  Thomas  Wolf.  2023.    Open  llm  leaderboard.\nHugging Face.\nStella Rose Biderman, Hailey Schoelkopf, Quentin G.\nAnthony,   Herbie   Bradley,   Kyle   O’Brien,   Eric\nHallahan,   Mohammad   Aflah   Khan,   Shivanshu\nPurohit, USVSN Sai Prashanth, Edward Raff, Aviya\nSkowron,  Lintang  Sutawika,  and  Oskar  van  der\nWal.  2023.    Pythia:   A  suite  for  analyzing  large\nlanguage models across training and scaling.ArXiv,\nabs/2304.01373.\nSid  Black,   Stella  Rose  Biderman,   Eric  Hallahan,\nQuentin G. Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason\nPhang, Michael Martin Pieler, USVSN Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow,\nBenqi Wang, and Samuel Weinbach. 2022.  Gpt-neox-\n20b: An open-source autoregressive language model.\nArXiv, abs/2204.06745.\nTim Brooks, Aleksander Holynski, and Alexei A. Efros.\n2022.   Instructpix2pix:  Learning to follow image\nediting instructions.ArXiv, abs/2211.09800.\nTom  Brown,  Benjamin  Mann,  Nick  Ryder,  Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020a. Language models are few-shot\nlearners.Advances in neural information processing\nsystems, 33:1877–1901.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,   Sandhini   Agarwal,   Ariel   Herbert-Voss,\nGretchen  Krueger,  T.  J.  Henighan,  Rewon  Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020b.\nLanguage  models  are  few-shot  learners.ArXiv,\nabs/2005.14165.\nTuhin   Chakrabarty,Vishakh   Padmakumar,and\nHengxing  He.  2022.Help  me  write  a  poem  -\ninstruction tuning as a vehicle for collaborative poetry\nwriting.ArXiv, abs/2210.13669.\nSahil Chaudhary. 2023.  Code alpaca:  An instruction-\nfollowing llama model for code generation.\nGuiming Hardy Chen,  Shunian Chen,  Ruifei Zhang,\nJunying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong\nChen, Jianquan Li, Xiang Wan, and Benyou Wang.\n2024a.  Allava: Harnessing gpt4v-synthesized data\nfor  a  lite  vision-language  model.arXiv  preprint\narXiv:2402.11684.\nLin   Chen,   Jisong   Li,   Xiaoyi   Dong,   Pan   Zhang,\nConghui  He,  Jiaqi  Wang,  Feng  Zhao,  and  Dahua\nLin.  2023a.    Sharegpt4v:   Improving  large  multi-\nmodal models with better captions.arXiv preprint\narXiv:2311.12793.\nMark  Chen,   Jerry  Tworek,   Heewoo  Jun,   Qiming\nYuan,  Henrique  Ponde,  Jared  Kaplan,  Harrison\nEdwards,   Yura   Burda,   Nicholas   Joseph,   Greg\nBrockman, Alex Ray, Raul Puri, Gretchen Krueger,\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela\nMishkin,  Brooke  Chan,  Scott  Gray,  Nick  Ryder,\nMikhail  Pavlov,  Alethea  Power,  Lukasz  Kaiser,\nMohammad  Bavarian,  Clemens  Winter,  Philippe\nTillet, Felipe Petroski Such, David W. Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes,\nAriel Herbert-Voss, William H. Guss, Alex Nichol,\nIgor  Babuschkin,  S.  Arun  Balaji,  Shantanu  Jain,\nAndrew  Carr,  Jan  Leike,  Joshua  Achiam,  Vedant\nMisra, Evan Morikawa, Alec Radford, Matthew M.\nKnight, Miles Brundage, Mira Murati, Katie Mayer,\nPeter Welinder, Bob McGrew, Dario Amodei, Sam\nMcCandlish, Ilya Sutskever, and Wojciech Zaremba.\n2021. Evaluating large language models trained on\ncode.ArXiv, abs/2107.03374.\nQianglong Chen, Guohai Xu, Mingshi Yan, Ji Zhang,\nFei   Huang,   Luo   Si,   and   Yin   Zhang.   2023b.\nDistinguish before answer:  Generating contrastive\nexplanation as knowledge for commonsense question\nanswering. InAnnual Meeting of the Association for\nComputational Linguistics.\nZixiang  Chen,  Yihe  Deng,  Huizhuo  Yuan,  Kaixuan\nJi, and Quanquan Gu. 2024b.  Self-play fine-tuning\nconverts weak language models to strong language\nmodels.arXiv preprint arXiv:2401.01335.",
    "Wei-Lin  Chiang,  Zhuohan  Li,  Zi  Lin,  Ying  Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023.  Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality.See https://vicuna.\nlmsys. org (accessed 14 April 2023).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten  Bosma,  Gaurav  Mishra,  Adam  Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian  Gehrmann,  Parker  Schuh,  Kensen  Shi,\nSasha  Tsvyashchenko,  Joshua  Maynez,  Abhishek\nRao,  Parker  Barnes,  Yi  Tay,  Noam  M.  Shazeer,\nVinodkumar  Prabhakaran,  Emily  Reif,  Nan  Du,\nBenton   C.   Hutchinson,    Reiner   Pope,    James\nBradbury,   Jacob   Austin,   Michael   Isard,   Guy\nGur-Ari,   Pengcheng   Yin,   Toju   Duke,   Anselm\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier García, Vedant Misra, Kevin\nRobinson,   Liam   Fedus,   Denny   Zhou,   Daphne\nIppolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan,\nShivani Agrawal, Mark Omernick, Andrew M. Dai,\nThanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor  Lewkowycz,  Erica  Moreira,  Rewon  Child,\nOleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi  Wang,  Brennan  Saeta,  Mark  Díaz,  Orhan\nFirat, Michele Catasta, Jason Wei, Kathleen S. Meier-\nHellstern,  Douglas  Eck,  Jeff  Dean,  Slav  Petrov,\nand  Noah  Fiedel.  2022.   Palm:  Scaling  language\nmodeling with pathways.ArXiv, abs/2204.02311.\nHyung  Won  Chung,  Le  Hou,  S.  Longpre,  Barret\nZoph,  Yi  Tay,  William  Fedus,  Eric  Li,  Xuezhi\nWang,   Mostafa   Dehghani,   Siddhartha   Brahma,\nAlbert   Webson,   Shixiang   Shane   Gu,   Zhuyun\nDai,   Mirac  Suzgun,   Xinyun  Chen,   Aakanksha\nChowdhery, Dasha Valter, Sharan Narang, Gaurav\nMishra,  Adams  Wei  Yu,  Vincent  Zhao,  Yanping\nHuang, Andrew M. Dai, Hongkun Yu, Slav Petrov,\nEd Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nArXiv, abs/2210.11416.\nChristopher  Clark,  Kenton  Lee,  Ming-Wei  Chang,\nTom  Kwiatkowski,  Michael  Collins,  and  Kristina\nToutanova. 2019.  Boolq:  Exploring the surprising\ndifficulty  of  natural  yes/no  questions.ArXiv,\nabs/1905.10044.\nJ. Clark, Eunsol Choi, Michael Collins, Dan Garrette,\nTom Kwiatkowski, Vitaly Nikolaev, and Jennimaria\nPalomaki.   2020.Tydi   qa:A   benchmark\nfor   information-seeking   question   answering   in\ntypologically diverse languages.Transactions of the\nAssociation for Computational Linguistics, 8:454–\n470.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord.  2018.Think  you  have  solved  question\nanswering?   try  arc,  the  ai2  reasoning  challenge.\nArXiv, abs/1803.05457.\nKarl Cobbe,  Vineet Kosaraju,  Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert,  Jerry  Tworek,  Jacob  Hilton,  Reiichiro\nNakano,  Christopher  Hesse,  and  John  Schulman.\n2021.  Training verifiers to solve math word problems.\nArXiv, abs/2110.14168.\nOpenAccess    AI    Collective.    2023.software:\nhuggingface.co/openaccess-ai-collective/minotaur-\n15b.\nMike Conover, Matt Hayes, Ankit Mathur, Xiangrui\nMeng, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,\nPatrick Wendell, Matei Zaharia, et al. 2023a.  Free\ndolly:    Introducing  the  world’s  first  truly  open\ninstruction-tuned llm.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023b. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nAlice  Coucke,  Alaa  Saade,  Adrien  Ball,  Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro,Thibault    Gisselbrecht,Francesco\nCaltagirone,  Thibaut  Lavril,  et  al.  2018.Snips\nvoice  platform:    an  embedded  spoken  language\nunderstanding  system  for  private-by-design  voice\ninterfaces.arXiv preprint arXiv:1805.10190.\nWenliang   Dai,   Junnan   Li,   Dongxu   Li,   Anthony\nMeng  Huat  Tiong,  Junqi  Zhao,  Weisheng  Wang,\nBoyang   Li,    Pascale   Fung,    and   Steven   Hoi.\n2023. Instructblip: Towards general-purpose vision-\nlanguage  models  with  instruction  tuning.ArXiv,\nabs/2305.06500.\nTri  Dao,  Daniel  Y.  Fu,  Stefano  Ermon,  Atri  Rudra,\nand Christopher Ré. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\nInAdvances  in  Neural  Information  Processing\nSystems.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms.arXiv preprint arXiv:2305.14314.\nJacob  Devlin,  Ming-Wei  Chang,  Kenton  Lee,  and\nKristina  Toutanova.  2018.Bert:Pre-training\nof  deep  bidirectional  transformers  for  language\nunderstanding.arXiv preprint arXiv:1810.04805.\nNing  Ding,  Yulin  Chen,  Bokai  Xu,  Yujia  Qin,  Zhi\nZheng,   Shengding  Hu,   Zhiyuan  Liu,   Maosong\nSun,  and  Bowen  Zhou.  2023a.Enhancing  chat\nlanguage models by scaling high-quality instructional\nconversations.arXiv preprint arXiv:2305.14233.\nNing Ding, Yujia Qin, Guang Yang, Fu Wei, Zonghan\nYang,  Yusheng  Su,  Shengding  Hu,  Yulin  Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Haitao Zheng, Jianfei\nChen,  Y.  Liu,  Jie  Tang,  Juanzi  Li,  and  Maosong\nSun. 2023b. Parameter-efficient fine-tuning of large-\nscale pre-trained language models.Nature Machine\nIntelligence, 5:220–235.",
    "Zhengxiao  Du,  Yujie  Qian,  Xiao  Liu,  Ming  Ding,\nJiezhong  Qiu,  Zhilin  Yang,  and  Jie  Tang.  2022.\nGlm:   General  language  model  pretraining  with\nautoregressive  blank  infilling.   InProceedings  of\nthe  60th  Annual  Meeting  of  the  Association  for\nComputational Linguistics (Volume 1: Long Papers),\npages 320–335.\nJon    Durbin.    2023.Airoboros.software:\ngithub.com/jondurbin/airoboros.\nJane Dwivedi-Yu, Timo Schick, Zhengbao Jiang, Maria\nLomeli,  Patrick  Lewis,  Gautier  Izacard,  Edouard\nGrave, Sebastian Riedel, and Fabio Petroni. 2022.\nEditeval:  An instruction-based benchmark for text\nimprovements.\nWilliam Fedus,  Barret Zoph,  and Noam M. Shazeer.\n2021.Switch  transformers:   Scaling  to  trillion\nparameter models with simple and efficient sparsity.\nJ. Mach. Learn. Res., 23:120:1–120:39.\nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.\n2023a. Exploring the feasibility of chatgpt for event\nextraction.ArXiv, abs/2303.03836.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\net  al.  2021.   A  framework  for  few-shot  language\nmodel evaluation.Version v0. 0.1. Sept.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023b. Enabling large language models to generate\ntext with citations.arXiv preprint arXiv:2305.14627.\nSamuel   Gehman,Suchin   Gururangan,Maarten\nSap,   Yejin   Choi,   and   Noah   A.   Smith.   2020.\nRealtoxicityprompts:Evaluating   neural   toxic\ndegeneration    in    language    models.ArXiv,\nabs/2009.11462.\nRohit   Girdhar,   Alaaeldin   El-Nouby,   Zhuang   Liu,\nMannat  Singh,  Kalyan  Vasudev  Alwala,  Armand\nJoulin,  and  Ishan  Misra.  2023.   Imagebind:  One\nembedding space to bind them all. InCVPR.\nTao  Gong,  Chengqi  Lyu,  Shilong  Zhang,  Yudong\nWang, Miao Zheng, Qianmengke Zhao, Kuikun Liu,\nWenwei  Zhang,  Ping  Luo,  and  Kai  Chen.  2023.\nMultimodal-gpt:  A vision and language model for\ndialogue with humans.ArXiv, abs/2305.04790.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023.  The false promise of imitating\nproprietary llms.arXiv preprint arXiv:2305.15717.\nSuriya   Gunasekar,   Yi   Zhang,   Jyoti   Aneja,   Caio\nCésar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, et al. 2023. Textbooks are all\nyou need.arXiv preprint arXiv:2306.11644.\nHimanshu  Gupta,  Saurabh  Arjun  Sawant,  Swaroop\nMishra,    Mutsumi   Nakamura,    Arindam   Mitra,\nSantosh    Mashetty,and    Chitta    Baral.    2023.\nInstruction   tuned   models   are   quick   learners.\narXiv preprint arXiv:2306.05539.\nPrakhar  Gupta,   Cathy  Jiao,   Yi-Ting  Yeh,   Shikib\nMehri,  Maxine  Eskénazi,  and  Jeffrey  P.  Bigham.\n2022.   Instructdial:  Improving  zero  and  few-shot\ngeneralization in dialogue through instruction tuning.\nInConference  on  Empirical  Methods  in  Natural\nLanguage Processing.\nSendongZhaoBingQinTingLiu\nHaochun  Wang,  Chi  Liu.  2023.Chatglm-med.\nhttps://github.com/SCIR-HI/Med-ChatGLM.\nDan  Hendrycks,  Collin  Burns,  Steven  Basart,  Andy\nZou, Mantas Mazeika, Dawn Xiaodong Song, and\nJacob Steinhardt. 2020.  Measuring massive multitask\nlanguage understanding.ArXiv, abs/2009.03300.\nOr  Honovich,   Thomas  Scialom,   Omer  Levy,   and\nTimo Schick. 2022. Unnatural instructions: Tuning\nlanguage  models  with  (almost)  no  human  labor.\narXiv preprint arXiv:2212.09689.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna  Morrone,  Quentin  de  Laroussilhe,  Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient  transfer  learning  for  NLP.   In\nProceedings of the 36th International Conference\non Machine Learning, volume 97, pages 2790–2799.\nPMLR.\nEdward  J  Hu,  Yelong  Shen,  Phillip  Wallis,  Zeyuan\nAllen-Zhu,  Yuanzhi  Li,  Shean  Wang,  Lu  Wang,\nand   Weizhu   Chen.   2021.Lora:Low-rank\nadaptation of large language models.arXiv preprint\narXiv:2106.09685.\nJie   Huang   and   Kevin   Chen-Chuan   Chang.   2022.\nTowards  reasoning  in  large  language  models:   A\nsurvey.arXiv preprint arXiv:2212.10403.\nYuzhen  Huang,   Yuzhuo  Bai,   Zhihao  Zhu,   Junlei\nZhang,  Jinghan  Zhang,  Tangjun  Su,  Junteng  Liu,\nChuancheng  Lv,  Yikai  Zhang,  Jiayi  Lei,  Yao  Fu,\nMaosong  Sun,  and  Junxian  He.  2023.C-eval:\nA  multi-level  multi-discipline  chinese  evaluation\nsuite   for   foundation   models.arXiv   preprint\narXiv:2305.08322.\nAnel   Islamovic.Stability   AI   Launches   the\nFirst    of    its    StableLM    Suite    of    Language\nModels—StabilityAI—stability.ai.\nhttps://stability.ai/blog/\nstability-ai-launches-the-first-of-its-stablelm-suite-of-language-models.\n[Accessed 09-Jun-2023].\nHamish   Ivison,   Akshita   Bhagia,   Yizhong   Wang,\nHannaneh Hajishirzi, and Matthew E. Peters. 2022.\nHint: Hypernetwork instruction tuning for efficient\nzero-shot generalisation.ArXiv, abs/2212.10315.",
    "Srinivas  Iyer,  Xiaojuan  Lin,  Ramakanth  Pasunuru,\nTodor  Mihaylov,   Daniel  Simig,   Ping  Yu,   Kurt\nShuster,   Tianlu  Wang,   Qing  Liu,   Punit  Singh\nKoura,  Xian  Li,  Brian  O’Horo,  Gabriel  Pereyra,\nJeff  Wang,  Christopher  Dewan,  Asli  Celikyilmaz,\nLuke  Zettlemoyer,   and  Veselin  Stoyanov.  2022.\nOpt-iml:  Scaling language model instruction meta\nlearning through the lens of generalization.ArXiv,\nabs/2212.12017.\nJosephusCheung. 2021. Guanaco: Generative universal\nassistant for natural-language adaptive context-aware\nomnilingual outputs.\nDaniel  Khashabi,  Sewon  Min,  Tushar  Khot,  Ashish\nSabharwal,   Oyvind   Tafjord,   Peter   Clark,   and\nHannaneh  Hajishirzi.  2020.   Unifiedqa:  Crossing\nformat boundaries with a single qa system.arXiv\npreprint arXiv:2005.00700.\nAndreas  Köpf,  Yannic  Kilcher,  Dimitri  von  Rütte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah  Barhoum,   Nguyen  Minh  Duc,   Oliver\nStanley, Richárd Nagyfi, et al. 2023. Openassistant\nconversations–democratizing large language model\nalignment.arXiv preprint arXiv:2304.07327.\nPo-Nien Kung and Nanyun Peng. 2023.   Do models\nreally learn to follow instructions? an empirical study\nof instruction tuning.ArXiv, abs/2305.11383.\nLAION.ai. 2023.  Oig: the open instruction generalist\ndataset.\nJason  J  Lau,  Soumya  Gayen,  Asma  Ben  Abacha,\nand  Dina  Demner-Fushman.  2018.    A  dataset  of\nclinically  generated  visual  questions  and  answers\nabout radiology images.Scientific data, 5(1):1–10.\nMina   Lee,   Percy   Liang,   and   Qian   Yang.   2022.\nCoauthor:Designing  a  human-ai  collaborative\nwriting   dataset   for   exploring   language   model\ncapabilities.Proceedings   of   the   2022   CHI\nConference   on   Human   Factors   in   Computing\nSystems.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning.    InConference  on  Empirical  Methods  in\nNatural Language Processing.\nMike  Lewis,   Yinhan  Liu,   Naman  Goyal,   Marjan\nGhazvininejad,  Abdel  rahman  Mohamed,  Omer\nLevy,   Veselin  Stoyanov,   and  Luke  Zettlemoyer.\n2019.   Bart:  Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand  comprehension.InAnnual  Meeting  of  the\nAssociation for Computational Linguistics.\nBo   Li,Gexiang   Fang,Yang   Yang,Quansen\nWang,  Wei  Ye,  Wen  Zhao,  and  Shikun  Zhang.\n2023a. Evaluating chatgpt’s information extraction\ncapabilities:An   assessment   of   performance,\nexplainability, calibration, and faithfulness.ArXiv,\nabs/2304.11633.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang  Yang,  and  Ziwei  Liu.  2023b.   Otter:  A\nmulti-modal model with in-context instruction tuning.\nArXiv, abs/2305.03726.\nGuohao Li,  Hasan Abed Al Kader Hammoud,  Hani\nItani,  Dmitrii  Khizbullin,  and  Bernard  Ghanem.\n2023c.  Camel:  Communicative agents for \"mind\"\nexploration of large scale language model society.\nJunnan  Li,  Dongxu  Li,  Silvio  Savarese,  and  Steven\nHoi. 2023d. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. InICML.\nKunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang,  Ping  Luo,  Yali  Wang,  Limin  Wang,  and\nYu  Qiao.  2023e.    Videochat:   Chat-centric  video\nunderstanding.arXiv preprint arXiv:2305.06355.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023f. Starcoder: may the source be with you!arXiv\npreprint arXiv:2305.06161.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke\nZettlemoyer, Omer Levy, Jason Weston, and Mike\nLewis.  2023g.Self-alignment  with  instruction\nbacktranslation.arXiv preprint arXiv:2308.06259.\nYuanzhi  Li,  Sébastien  Bubeck,  Ronen  Eldan,  Allie\nDel  Giorno,  Suriya  Gunasekar,  and  Yin  Tat  Lee.\n2023h.Textbooks  are  all  you  need  ii:   phi-1.5\ntechnical report.arXiv preprint arXiv:2309.05463.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and\nYou  Zhang.  2023i.    Chatdoctor:   A  medical  chat\nmodel  fine-tuned  on  llama  model  using  medical\ndomain knowledge.ArXiv, abs/2303.14070.\nPercy  Liang,  Rishi  Bommasani,  Tony  Lee,  Dimitris\nTsipras,  Dilara  Soylu,  Michihiro  Yasunaga,  Yian\nZhang,  Deepak  Narayanan,  Yuhuai  Wu,  Ananya\nKumar, Benjamin Newman, Binhang Yuan, Bobby\nYan, Ce Zhang, Christian Cosgrove, Christopher D.\nManning,  Christopher  R’e,  Diana  Acosta-Navas,\nDrew  A.  Hudson,   E.  Zelikman,   Esin  Durmus,\nFaisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu\nYao, Jue Wang, Keshav Santhanam, Laurel J. Orr,\nLucia  Zheng,  Mert  Yuksekgonul,  Mirac  Suzgun,\nNathan  S.  Kim,  Neel  Guha,  Niladri  S.  Chatterji,\nOmar Khattab, Peter Henderson, Qian Huang, Ryan\nChi,  Sang Michael Xie,  Shibani Santurkar,  Surya\nGanguli,  Tatsunori  Hashimoto,  Thomas  F.  Icard,\nTianyi Zhang, Vishrav Chaudhary, William Wang,\nXuechen  Li,  Yifan  Mai,  Yuhui  Zhang,  and  Yuta\nKoreeda.  2022.Holistic  evaluation  of  language\nmodels.Annals  of  the  New  York  Academy  of\nSciences.\nBill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen\nTian,  and Xiang Ren. 2022.   Unsupervised cross-\ntask generalization via retrieval augmentation.ArXiv,\nabs/2204.07937.",
    "Stephanie C. Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa:  Measuring how models mimic human\nfalsehoods. InAnnual Meeting of the Association for\nComputational Linguistics.\nWeixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi\nWu,  Ya  Zhang,  Yanfeng  Wang,  and  Weidi  Xie.\n2023.   Pmc-clip:  Contrastive language-image pre-\ntraining using biomedical documents.arXiv preprint\narXiv:2303.07240.\nBo  Liu,  Li-Ming  Zhan,  Li  Xu,  Lin  Ma,  Yan  Yang,\nand Xiao-Ming Wu. 2021a.  Slake: A semantically-\nlabeled  knowledge-enhanced  dataset  for  medical\nvisual  question  answering.In2021  IEEE  18th\nInternational  Symposium  on  Biomedical  Imaging\n(ISBI), pages 1650–1654. IEEE.\nHanmeng  Liu,  Zhiyang  Teng,  Leyang  Cui,  Chaoli\nZhang, Qiji Zhou, and Yue Zhang. 2023a.  Logicot:\nLogical  chain-of-thought  instruction-tuning  data\ncollection with gpt-4.ArXiv, abs/2305.12147.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee.  2023b.Visual  instruction  tuning.ArXiv,\nabs/2304.08485.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021b.  What\nmakes good in-context examples for gpt-3?arXiv\npreprint arXiv:2101.06804.\nTiedong Liu and Bryan Kian Hsiang Low. 2023. Goat:\nFine-tuned llama outperforms gpt-4 on arithmetic\ntasks.arXiv preprint arXiv:2305.14201.\nZheng Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang,\nChao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng\nChen, Sekeun Kim, Haixing Dai, Lin Zhao, Dajiang\nZhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li,\nQuanzheng Li, and Tianming Liu. 2023c. Radiology-\ngpt: A large language model for radiology.\nShayne  Longpre,  Le  Hou,  Tu  Vu,  Albert  Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023.  The flan\ncollection: Designing data and methods for effective\ninstruction tuning.arXiv preprint arXiv:2301.13688.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo\nGeng,  Wenxiang  Hu,  Chongyang  Tao,  Jing  Ma,\nQingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-\ninstruct.\nKai  Lv,  Yuqing  Yang,  Tengxiao  Liu,  Qi  jie  Gao,\nQipeng Guo, and Xipeng Qiu. 2023. Full parameter\nfine-tuning for large language models with limited\nresources.\nChenlin Meng, Yutong He, Yang Song, Jiaming Song,\nJiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2022.\nSDEdit:  Guided image synthesis and editing with\nstochastic differential equations.   InInternational\nConference on Learning Representations.\nSwaroop   Mishra,   Daniel   Khashabi,   Chitta   Baral,\nand   Hannaneh   Hajishirzi.   2021.Cross-task\ngeneralization via natural language crowdsourcing\ninstructions.arXiv preprint arXiv:2104.08773.\nArindam Mitra, Luciano Del Corro, Shweti Mahajan,\nAndres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi\nChen,  Anastasia  Razdaibiedina,  Erik  Jones,  Kriti\nAggarwal,  et  al.  2023.    Orca  2:   Teaching  small\nlanguage  models  how  to  reason.arXiv  preprint\narXiv:2311.11045.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam  Roberts,  Stella  Biderman,  Teven  Le  Scao,\nM  Saiful  Bari,   Sheng  Shen,   Zheng-Xin  Yong,\nHailey  Schoelkopf,   et  al.  2022.Crosslingual\ngeneralization through multitask finetuning.arXiv\npreprint arXiv:2211.01786.\nSubhabrata   Mukherjee,    Arindam   Mitra,    Ganesh\nJawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023.  Orca: Progressive learning from\ncomplex explanation traces of gpt-4.arXiv preprint\narXiv:2306.02707.\nMunan Ning, Yujia Xie, Dongdong Chen, Zeyin Song,\nLu Yuan, Yonghong Tian, Qixiang Ye, and Liuliang\nYuan. 2023. Album storytelling with iterative story-\naware captioning and large language models.ArXiv,\nabs/2305.12943.\nNousResearch.2023.software:\nhuggingface.co/NousResearch/Nous-Hermes-13b.\nOpenAI.  2022.Introducing  chatgpt.Blog  post\nopenai.com/blog/chatgpt.\nOpenAI.  2023.Gpt-4  technical  report.ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini   Agarwal,   Katarina   Slama,   Alex   Ray,\net  al.  2022.   Training  language  models  to  follow\ninstructions  with  human  feedback.Advances  in\nNeural Information Processing Systems, 35:27730–\n27744.\nArnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron\nVandenBerg, and Jamie Callan. 2022.  Clueweb22:\n10 billion web documents with visual and semantic\ninformation.arXiv preprint arXiv:2211.15848.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra  Cojocaru,  Alessandro  Cappelli,  Hamza\nAlobeidli,  Baptiste  Pannier,  Ebtesam  Almazrouei,\nand Julien Launay. 2023.   The refinedweb dataset\nfor falcon llm: outperforming curated corpora with\nweb  data,  and  web  data  only.arXiv  preprint\narXiv:2306.01116.\nBaolin  Peng,  Chunyuan  Li,  Pengcheng  He,  Michel\nGalley, and Jianfeng Gao. 2023.  Instruction tuning\nwith gpt-4.arXiv preprint arXiv:2304.03277.",
    "Jing  Qian,  Li  Dong,  Yelong  Shen,  Furu  Wei,  and\nWeizhu Chen. 2022.  Controllable natural language\ngeneration with contrastive prefixes.arXiv preprint\narXiv:2202.13257.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh,  Gabriel  Goh,  Sandhini  Agarwal,  Girish\nSastry,   Amanda  Askell,   Pamela  Mishkin,   Jack\nClark, Gretchen Krueger, and Ilya Sutskever. 2021.\nLearning  transferable  visual  models  from  natural\nlanguage supervision.  InInternational Conference\non Machine Learning.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners.OpenAI\nblog, 1(8):9.\nJack W Rae,  Sebastian Borgeaud,  Trevor Cai,  Katie\nMillican,  Jordan  Hoffmann,  Francis  Song,  John\nAslanides, Sarah Henderson, Roman Ring, Susannah\nYoung,  et  al.  2021.Scaling  language  models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin  Raffel,   Noam  M.  Shazeer,   Adam  Roberts,\nKatherine  Lee,  Sharan  Narang,  Michael  Matena,\nYanqi  Zhou,   Wei  Li,   and  Peter  J.  Liu.  2019.\nExploring the limits of transfer learning with a unified\ntext-to-text transformer.ArXiv, abs/1910.10683.\nVipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop\nKang. 2023.  Coedit:  Text editing by task-specific\ninstruction tuning.ArXiv, abs/2305.09857.\nJeff Rasley,  Samyam Rajbhandari,  Olatunji Ruwase,\nand  Yuxiong  He.  2020.Deepspeed:System\noptimizations enable training deep learning models\nwith over 100 billion parameters. InProceedings of\nthe 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 3505–\n3506.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick  Esser,  and  Björn  Ommer.  2022.High-\nresolution  image  synthesis  with  latent  diffusion\nmodels.  InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 10684–10695.\nAndrew  Rosenbaum,   Saleh  Soltan,   Wael  Hamza,\nYannick Versley, and Markus Boese. 2022. Linguist:\nLanguage  model  instruction  tuning  to  generate\nannotated  utterances  for  intent  classification  and\nslot  tagging.InInternational  Conference  on\nComputational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach,  Lintang  Sutawika,  Zaid  Alyafeai,  Antoine\nChaffin,  Arnaud  Stiegler,  Teven  Le  Scao,  Arun\nRaja,  et  al.  2021.Multitask  prompted  training\nenables zero-shot task generalization.arXiv preprint\narXiv:2110.08207.\nTeven   Le   Scao,   Angela   Fan,   Christopher   Akiki,\nElizabeth-Jane Pavlick, Suzana Ili’c, Daniel Hesslow,\nRoman   Castagn’e,   Alexandra   Sasha   Luccioni,\nFranccois  Yvon,  Matthias  Gallé,  Jonathan  Tow,\nAlexander   M.   Rush,Stella   Rose   Biderman,\nAlbert  Webson,  Pawan  Sasanka  Ammanamanchi,\nThomas Wang, Benoît Sagot, Niklas Muennighoff,\nAlbert Villanova del Moral, Olatunji Ruwase, Rachel\nBawden, Stas Bekman, Angelina McMillan-Major,\nIz Beltagy, Huu Nguyen, Lucile Saulnier, Samson\nTan,   Pedro   Ortiz   Suarez,   Victor   Sanh,   Hugo\nLaurenccon, Yacine Jernite, Julien Launay, Margaret\nMitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi,\nAitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy,\nAnna Rogers, Ariel Kreisberg Nitzav, Canwen Xu,\nChenghao  Mou,   Chris  C.  Emezue,   Christopher\nKlamm, Colin Leong, Daniel Alexander van Strien,\nDavid   Ifeoluwa   Adelani,   Dragomir   R.   Radev,\nEduardo  Gonz’alez  Ponferrada,  Efrat  Levkovizh,\nEthan  Kim,  Eyal  Bar  Natan,  Francesco  De  Toni,\nGérard Dupont, Germán Kruszewski, Giada Pistilli,\nHady  ElSahar,  Hamza  Benyamina,  Hieu  Trung\nTran,  Ian  Yu,  Idris  Abdulmumin,  Isaac  Johnson,\nItziar  Gonzalez-Dios,   Javier  de  la  Rosa,   Jenny\nChim,  Jesse  Dodge,  Jian  Zhu,  Jonathan  Chang,\nJorg   Frohberg,   Josephine   L.   Tobing,   Joydeep\nBhattacharjee,  Khalid  Almubarak,  Kimbo  Chen,\nKyle Lo,  Leandro von Werra,  Leon Weber,  Long\nPhan, Loubna Ben Allal, Ludovic Tanguy, Manan\nDey,  Manuel  Romero  Muñoz,  Maraim  Masoud,\nMar’ia  Grandury,   Mario  vSavsko,   Max  Huang,\nMaximin  Coavoux,   Mayank  Singh,   Mike  Tian-\nJian   Jiang,   Minh   Chien   Vu,   Mohammad   Ali\nJauhar, Mustafa Ghaleb, Nishant Subramani, Nora\nKassner,   Nurulaqilla   Khamis,   Olivier   Nguyen,\nOmar  Espejel,   Ona  de  Gibert,   Paulo  Villegas,\nPeter   Henderson,   Pierre   Colombo,   Priscilla   A.\nAmuok,  Quentin  Lhoest,  Rheza  Harliman,  Rishi\nBommasani, Roberto L’opez, Rui Ribeiro, Salomey\nOsei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose,\nShamsuddeen Hassan Muhammad, Shanya Sharma,\nS. Longpre, Somaieh Nikpoor, Stanislav Silberberg,\nSuhas  Pai,  Sydney  Zink,  Tiago  Timponi  Torrent,\nTimo  Schick,  Tristan  Thrush,  Valentin  Danchev,\nVassilina  Nikoulina,  Veronika  Laippala,  Violette\nLepercq,  Vrinda  Prabhu,  Zaid  Alyafeai,  Zeerak\nTalat, Arun Raja, Benjamin Heinzerling, Chenglei Si,\nElizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee,\nAbheesht Sharma, Andrea Santilli, Antoine Chaffin,\nArnaud Stiegler, Debajyoti Datta, Eliza Szczechla,\nGunjan  Chhablani,   Han  Wang,   Harshit  Pandey,\nHendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo\nGao,  Lintang  Sutawika,  M  Saiful  Bari,  Maged  S.\nAl-shaibani, Matteo Manica, Nihal V. Nayak, Ryan\nTeehan, Samuel Albanie, Sheng Shen, Srulik Ben-\nDavid, Stephen H. Bach, Taewoon Kim, Tali Bers,\nThibault  Févry,  Trishala  Neeraj,  Urmish  Thakker,\nVikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing\nSun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam\nRoberts,  Hyung  Won  Chung,  Jaesung  Tae,  Jason\nPhang, Ofir Press, Conglong Li, Deepak Narayanan,\nHatim   Bourfoune,    Jared   Casper,    Jeff   Rasley,\nMax  Ryabinin,   Mayank  Mishra,   Minjia  Zhang,",
    "Mohammad Shoeybi, Myriam Peyrounette, Nicolas\nPatry, Nouamane Tazi, Omar Sanseviero, Patrick von\nPlaten, Pierre Cornette, Pierre Franccois Lavall’ee,\nRémi   Lacroix,   Samyam   Rajbhandari,   Sanchit\nGandhi,  Shaden  Smith,  Stéphane  Requena,  Suraj\nPatil,  Tim  Dettmers,  Ahmed  Baruwa,  Amanpreet\nSingh,  Anastasia Cheveleva,  Anne-Laure Ligozat,\nArjun  Subramonian,   Aur’elie  N’ev’eol,   Charles\nLovering, Daniel H Garrette, Deepak R. Tunuguntla,\nEhud   Reiter,    Ekaterina   Taktasheva,    Ekaterina\nVoloshina,   Eli  Bogdanov,   Genta  Indra  Winata,\nHailey Schoelkopf, Jan-Christoph Kalo, Jekaterina\nNovikova, Jessica Zosa Forde, Xiangru Tang, Jungo\nKasai, Ken Kawamura, Liam Hazan, Marine Carpuat,\nMiruna  Clinciu,   Najoung  Kim,   Newton  Cheng,\nOleg Serikov,  Omer Antverg,  Oskar van der Wal,\nRui Zhang, Ruochen Zhang, Sebastian Gehrmann,\nShachar  Mirkin,  S.  Osher  Pais,  Tatiana  Shavrina,\nThomas Scialom,  Tian Yun,  Tomasz Limisiewicz,\nVerena Rieser, Vitaly Protasov, Vladislav Mikhailov,\nYada  Pruksachatkun,  Yonatan  Belinkov,  Zachary\nBamberger, Zdenvek Kasner, Alice Rueda, Amanda\nPestana,   Amir   Feizpour,   Ammar   Khan,   Amy\nFaranak,   Ananda  Santa  Rosa  Santos,   Anthony\nHevia, Antigona Unldreaj, Arash Aghagol, Arezoo\nAbdollahi, Aycha Tammour, Azadeh HajiHosseini,\nBahareh   Behroozi,   Benjamin   Olusola   Ajibade,\nBharat  Kumar  Saxena,  Carlos  Muñoz  Ferrandis,\nDanish Contractor, David M. Lansky, Davis David,\nDouwe  Kiela,  Duong  Anh  Nguyen,  Edward  Tan,\nEmily Baylor, Ezinwanne Ozoani, Fatim T Mirza,\nFrankline Ononiwu, Habib Rezanejad, H.A. Jones,\nIndrani Bhattacharya, Irene Solaiman, Irina Sedenko,\nIsar  Nejadgholi,   Jan  Passmore,   Joshua  Seltzer,\nJulio   Bonis   Sanz,   Karen   Fort,   Lívia   Macedo\nDutra, Mairon Samagaio, Maraim Elbadri, Margot\nMieskes,   Marissa   Gerchick,   Martha   Akinlolu,\nMichael  McKenna,  Mike  Qiu,  M.  K.  K.  Ghauri,\nMykola Burynok, Nafis Abrar, Nazneen Rajani, Nour\nElkott, Nourhan Fahmy, Olanrewaju Samuel, Ran\nAn,  R.  P.  Kromann,  Ryan  Hao,  Samira  Alizadeh,\nSarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain\nViguier,   Thanh-Cong  Le,   Tobi  Oyebade,   Trieu\nNguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,\nAbhinav Ramesh Kashyap,  A. Palasciano,  Alison\nCallahan, Anima Shukla, Antonio Miranda-Escalada,\nAyush Kumar Singh, Benjamin Beilharz, Bo Wang,\nCaio Matheus Fonseca de Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on\nPerin’an,    Daniel   Molano,    Dian   Yu,    Enrique\nManjavacas,    Fabio   Barth,    Florian   Fuhrimann,\nGabriel  Altay,  Giyaseddin  Bayrak,  Gully  Burns,\nHelena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo\nKang, John Giorgi, Jonas Golde, Jose David Posada,\nKarthi  Sivaraman,  Lokesh  Bulchandani,  Lu  Liu,\nLuisa  Shinzato,  Madeleine  Hahn  de  Bykhovetz,\nMaiko   Takeuchi,   Marc   Pàmies,   María   Andrea\nCastillo,    Marianna   Nezhurina,    Mario   Sanger,\nMatthias   Samwald,    Michael   Cullan,    Michael\nWeinberg,    M   Wolf,    Mina   Mihaljcic,    Minna\nLiu,  Moritz  Freidank,  Myungsun  Kang,  Natasha\nSeelam, Nathan Dahlberg, Nicholas Michio Broad,\nNikolaus  Muellner,  Pascale  Fung,  Patricia  Haller,\nR.  Chandrasekhar,  R.  Eisenberg,  Robert  Martin,\nRodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel\nCahyawijaya, Samuele Garda, Shlok S Deshmukh,\nShubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee\nSang-aroonsiri,  Srishti  Kumar,  Stefan  Schweter,\nSushil  Pratap  Bharati,  T.  A.  Laud,  Th’eo  Gigant,\nTomoya  Kainuma,  Wojciech  Kusa,  Yanis  Labrak,\nYashasvi  Bajaj,  Y.  Venkatraman,  Yifan  Xu,  Ying\nXu,  Yun  chao  Xu,  Zhee  Xao  Tan,  Zhongli  Xie,\nZifan  Ye,  Mathilde  Bras,  Younes  Belkada,  and\nThomas  Wolf.  2022.   Bloom:  A  176b-parameter\nopen-access multilingual language model.ArXiv,\nabs/2211.05100.\nTimo Schick and Hinrich Schütze. 2021.  Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference.  InProceedings of the\n16th  Conference  of  the  European  Chapter  of  the\nAssociation  for  Computational  Linguistics:  Main\nVolume, pages 255–269.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017.  Proximal policy\noptimization algorithms.ArXiv, abs/1707.06347.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi  Tay,  Sebastian  Ruder,  Denny  Zhou,  Dipanjan\nDas, and Jason Wei. 2022.   Language models are\nmultilingual  chain-of-thought  reasoners.ArXiv,\nabs/2210.03057.\nSaleh    Soltan,Shankar    Ananthakrishnan,Jack\nFitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan,\nCharith  Peris,  Stephen  Rawls,  Andy  Rosenbaum,\nAnna  Rumshisky,   et  al.  2022.Alexatm  20b:\nFew-shot learning using a large-scale multilingual\nseq2seq model.arXiv preprint arXiv:2208.01448.\nAarohi  Srivastava,  Abhinav  Rastogi,  Abhishek  Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam  R  Brown,  Adam  Santoro,  Aditya  Gupta,\nAdrià  Garriga-Alonso,  et  al.  2022.Beyond  the\nimitation game:  Quantifying and extrapolating the\ncapabilities  of  language  models.arXiv  preprint\narXiv:2206.04615.\nWeiwei Sun, Hengyi Cai, Hongshen Chen, Pengjie Ren,\nZhumin Chen, Maarten de Rijke, and Zhaochun Ren.\n2023a. Answering ambiguous questions via iterative\nprompting.ArXiv, abs/2307.03897.\nXiaofei  Sun,  Linfeng  Dong,  Xiaoya  Li,  Zhen  Wan,\nShuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng,\nLingjuan  Lyu,  Fei  Wu,  et  al.  2023b.Pushing\nthe limits of chatgpt on nlp tasks.arXiv preprint\narXiv:2306.09719.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei\nGuo,  Tianwei  Zhang,  and  Guoyin  Wang.  2023c.\nText classification via large language models.arXiv\npreprint arXiv:2305.08377.\nMirac  Suzgun,   Nathan  Scales,   Nathanael  Scharli,\nSebastian Gehrmann, Yi Tay, Hyung Won Chung,",
    "Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin\nChi, Denny Zhou, and Jason Wei. 2022. Challenging\nbig-bench tasks and whether chain-of-thought can\nsolve them.ArXiv, abs/2210.09261.\nRohan  Taori,  Ishaan  Gulrajani,  Tianyi  Zhang,  Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand   Tatsunori   B   Hashimoto.   2023a.Alpaca:\nA  strong,  replicable  instruction-following  model.\nStanford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html,\n3(6):7.\nRohan  Taori,  Ishaan  Gulrajani,  Tianyi  Zhang,  Yann\nDubois,   Xuechen   Li,   Carlos   Guestrin,   Percy\nLiang,and   Tatsunori   B.   Hashimoto.   2023b.\nStanford  alpaca:   An  instruction-following  llama\nmodel.https://github.com/tatsu-lab/\nstanford_alpaca.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Steven Zheng, et al. 2022. Ul2:\nUnifying language learning paradigms.\nRomal  Thoppilan,   Daniel  De  Freitas,   Jamie  Hall,\nNoam  Shazeer,  Apoorv  Kulshreshtha,  Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022.   Lamda:  Language models for dialog\napplications.arXiv preprint arXiv:2201.08239.\nSun Tianxiang and Qiu Xipeng. 2023. Moss.Blog post\ntxsun1997.github.io/blogs/moss.html.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.ArXiv,\nabs/2302.13971.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste  Rozière,   Naman  Goyal,   Eric  Hambro,\nFaisal  Azhar,  et  al.  2023b.Llama:   Open  and\nefficient foundation language models.arXiv preprint\narXiv:2302.13971.\nSiddharth Varia, Shuai Wang, Kishaloy Halder, Robert\nVacareanu,  Miguel  Ballesteros,  Yassine  Benajiba,\nNeha   Ann   John,   Rishita   Anubhai,   Smaranda\nMuresan, and Dan Roth. 2022.  Instruction tuning\nfor few-shot aspect-based sentiment analysis.ArXiv,\nabs/2210.06629.\nZhen  Wan,  Fei  Cheng,  Zhuoyuan  Mao,  Qianying\nLiu, Haiyue Song, Jiwei Li, and Sadao Kurohashi.\n2023.Gpt-re:   In-context  learning  for  relation\nextraction  using  large  language  models.arXiv\npreprint arXiv:2305.02105.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou,  and Hongxia Yang. 2022a.   Ofa:  Unifying\narchitectures, tasks, and modalities through a simple\nsequence-to-sequence   learning   framework.In\nInternational  Conference  on  Machine  Learning,\npages 23318–23340. PMLR.\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,\nFei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.\n2023a. Gpt-ner: Named entity recognition via large\nlanguage models.arXiv preprint arXiv:2304.10428.\nXiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen,\nYuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao\nGui, Jihua Kang, J. Yang, Siyuan Li, and Chunsai\nDu.  2023b.Instructuie:   Multi-task  instruction\ntuning  for  unified  information  extraction.ArXiv,\nabs/2304.08085.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Huai hsin Chi, and Denny Zhou. 2022b.   Self-\nconsistency improves chain of thought reasoning in\nlanguage models.ArXiv, abs/2203.11171.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel,  Tushar  Khot,  Khyathi  Raghavi  Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hanna Hajishirzi. 2023c.  How far\ncan camels go?   exploring the state of instruction\ntuning on open resources.ArXiv, abs/2306.04751.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022c. Self-instruct: Aligning language\nmodel  with  self  generated  instructions.arXiv\npreprint arXiv:2212.10560.\nYizhongWang,SwaroopMishra,Pegah\nAlipoormolabashi,Yeganeh    Kordi,Amirreza\nMirzaei,Anjana    Arunkumar,Arjun    Ashok,\nArut  Selvan  Dhanasekaran,  Atharva  Naik,  David\nStap,   Eshaan   Pathak,   Giannis   Karamanolakis,\nHaizhi  Gary  Lai,  Ishan  Purohit,  Ishani  Mondal,\nJacob   Anderson,   Kirby   Kuznia,   Krima   Doshi,\nMaitreya Patel, Kuntal Kumar Pal, M. Moradshahi,\nMihir  Parmar,   Mirali  Purohit,   Neeraj  Varshney,\nPhani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh\nPuri,   Rushang   Karia,   Shailaja   Keyur   Sampat,\nSavan  Doshi,   Siddharth  Deepak  Mishra,   Sujan\nReddy, Sumanta Patro, Tanay Dixit, Xudong Shen,\nChitta  Baral,  Yejin  Choi,  Noah  A.  Smith,  Hanna\nHajishirzi,  and  Daniel  Khashabi.  2022d.Super-\nnaturalinstructions:  Generalization via declarative\ninstructions on 1600+ nlp tasks.  InConference on\nEmpirical Methods in Natural Language Processing.\nYizhongWang,SwaroopMishra,Pegah\nAlipoormolabashi,Yeganeh    Kordi,Amirreza\nMirzaei,Anjana    Arunkumar,Arjun    Ashok,\nArut  Selvan  Dhanasekaran,  Atharva  Naik,  David\nStap,   et   al.   2022e.Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+\nnlp tasks.arXiv preprint arXiv:2204.07705.\nYizhongWang,SwaroopMishra,Pegah\nAlipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana  Arunkumar,   Arjun  Ashok,   Arut  Selvan\nDhanasekaran,  Atharva  Naik,  David  Stap,  et  al.",
    "2022f. Super-naturalinstructions:generalization via\ndeclarative instructions on 1600+ tasks. InEMNLP.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma,  Ed  Huai  hsin  Chi,  F.  Xia,  Quoc  Le,  and\nDenny  Zhou.  2022.   Chain  of  thought  prompting\nelicits reasoning in large language models.ArXiv,\nabs/2201.11903.\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin  Zhang,  Shen  Huang,  Pengjun  Xie,  Jinan  Xu,\nYufeng Chen, Meishan Zhang, et al. 2023a.  Zero-\nshot information extraction via chatting with chatgpt.\narXiv preprint arXiv:2302.10205.\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and\nLingming Zhang. 2023b. Magicoder: Source code is\nall you need.arXiv preprint arXiv:2312.02120.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,\nMark  Riedl,  and  Yejin  Choi.  2021.Reframing\nhuman-ai   collaboration   for   generating   free-text\nexplanations.arXiv preprint arXiv:2112.08674.\nTianbao  Xie,   Chen  Henry  Wu,   Peng  Shi,   Ruiqi\nZhong, Torsten Scholak, Michihiro Yasunaga, Chien-\nSheng  Wu,  Ming  Zhong,  Pengcheng  Yin,  Sida  I.\nWang,  Victor  Zhong,  Bailin  Wang,  Chengzu  Li,\nConnor Boyle, Ansong Ni, Ziyu Yao, Dragomir R.\nRadev, Caiming Xiong, Lingpeng Kong, Rui Zhang,\nNoah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022.\nUnifiedskg:  Unifying and multi-tasking structured\nknowledge  grounding  with  text-to-text  language\nmodels.   InConference on Empirical  Methods in\nNatural Language Processing.\nCan  Xu,  Qingfeng  Sun,  Kai  Zheng,  Xiubo  Geng,\nPu  Zhao,   Jiazhan  Feng,   Chongyang  Tao,   and\nDaxin Jiang. 2023a.  Wizardlm: Empowering large\nlanguage models to follow complex instructions.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023b.   Baize:  An  open-source  chat  model  with\nparameter-efficient tuning on self-chat data.arXiv\npreprint arXiv:2304.01196.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023c.   Baize:  An  open-source  chat  model  with\nparameter-efficient tuning on self-chat data.ArXiv,\nabs/2304.01196.\nWeijia Xu,  Batool Haider,  and Saab Mansour. 2020.\nEnd-to-end slot alignment and recognition for cross-\nlingual nlu.arXiv preprint arXiv:2004.14353.\nZhiyang Xu,  Chao Feng,  Rulin Shao,  Trevor Ashby,\nYing Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu\nHuang. 2024.  Vision-flan:  Scaling human-labeled\ntasks  in  visual  instruction  tuning.arXiv  preprint\narXiv:2402.11690.\nZhiyang  Xu,   Ying  Shen,   and  Lifu  Huang.  2022.\nMultiinstruct:Improvingmulti-modalzero-\nshot   learning   via   instruction   tuning.ArXiv,\nabs/2212.10773.\nFuzhao Xue, Kabir Jain, Mahir Hitesh Shah, Zangwei\nZheng,    and   Yang   You.   2023.Instruction\nin   the   wild:A   user-based   instruction   dataset.\nhttps://github.com/XueFuzhao/InstructionWild.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\nHan, Qizhang Feng, Haoming Jiang, Bing Yin, and\nXia Hu. 2023a.   Harnessing the power of llms in\npractice:  A survey on chatgpt and beyond.arXiv\npreprint arXiv:2304.13712.\nKevin Yang, Nanyun Peng, Yuandong Tian, and Dan\nKlein. 2022a.  Re3:  Generating longer stories with\nrecursive reprompting and revision.arXiv preprint\narXiv:2210.06774.\nKexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong\nYang,  Mingfeng Xue,  Boxing Chen,  and Jun Xie.\n2022b.Tailor:A  prompt-based  approach  to\nattribute-based  controlled  text  generation.ArXiv,\nabs/2204.13362.\nZhengyuan  Yang,   Linjie  Li,   Kevin  Lin,   Jianfeng\nWang, Chung-Ching Lin, Zicheng Liu, and Lijuan\nWang.  2023b.    The  dawn  of  lmms:   Preliminary\nexplorations  with  gpt-4v  (ision).arXiv  preprint\narXiv:2309.17421, 9(1):1.\nShunyu  Yao,  Dian  Yu,  Jeffrey  Zhao,  Izhak  Shafran,\nThomas   L.   Griffiths,   Yuan   Cao,   and   Karthik\nNarasimhan.  2023.   Tree  of  thoughts:  Deliberate\nproblem solving with large language models.ArXiv,\nabs/2305.10601.\nZhenfei Yin,  Jiong Wang,  Jianjian Cao,  Zhelun Shi,\nDingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui\nHuang,  Zhiyong  Wang,  Wanli  Ouyang,  and  Jing\nShao.  2023.Lamm:    Language-assisted  multi-\nmodal  instruction-tuning  dataset,  framework,  and\nbenchmark.ArXiv, abs/2306.06687.\nZhaojian   Yu,   Xin   Zhang,   Ning   Shang,   Yangyu\nHuang, Can Xu, Yishujie Zhao, Wenxiang Hu, and\nQiufeng Yin. 2023.   Wavecoder:  Widespread and\nversatile  enhanced  instruction  tuning  with  refined\ndata generation.arXiv preprint arXiv:2312.14187.\nYuLan-Chat-Team.  2023.Yulan-chat:    An  open-\nsource bilingual chatbot.   https://github.com/RUC-\nGSAI/YuLan-Chat.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings  of  the  60th  Annual  Meeting  of  the\nAssociation for Computational Linguistics (Volume 2:\nShort Papers), ACL 2022, Dublin, Ireland, May 22-\n27, 2022, pages 1–9. Association for Computational\nLinguistics.\nGe Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi\nLi, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang,\nChenghua Lin, Wen-Fen Huang, and Jie Fu. 2023a.\nChinese open instruction generalist: A preliminary\nrelease.ArXiv, abs/2304.07987.",
    "Hang Zhang, Xin Li, and Lidong Bing. 2023b. Video-\nllama:  An instruction-tuned audio-visual language\nmodel  for  video  understanding.arXiv  preprint\narXiv:2306.02858.\nSusan  Zhang,  Stephen  Roller,  Naman  Goyal,  Mikel\nArtetxe,  Moya  Chen,  Shuohui  Chen,  Christopher\nDewan,  Mona  T.  Diab,  Xian  Li,  Xi  Victoria  Lin,\nTodor  Mihaylov,  Myle  Ott,  Sam  Shleifer,  Kurt\nShuster,  Daniel Simig,  Punit Singh Koura,  Anjali\nSridhar, Tianlu Wang, and Luke Zettlemoyer. 2022a.\nOpt: Open pre-trained transformer language models.\nArXiv, abs/2205.01068.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong\nLin, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023c.\nPmc-vqa: Visual instruction tuning for medical visual\nquestion answering.ArXiv, abs/2305.10415.\nYuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin\nHe, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao,\nJing Shao, and Ziwei Liu. 2022b. Bamboo: Building\nmega-scale vision dataset continually with human-\nmachine synergy.arXiv preprint arXiv:2203.07845.\nYue  Zhang,  Leyang  Cui,  Deng  Cai,  Xinting  Huang,\nTao Fang, and Wei Bi. 2023d. Multi-task instruction\ntuning of llama for specific scenarios: A preliminary\nstudy on writing assistance.ArXiv, abs/2305.13225.\nTony  Zhao,   Eric  Wallace,   Shi  Feng,   Dan  Klein,\nand  Sameer  Singh.  2021.Calibrate  before  use:\nImproving few-shot performance of language models.\nInInternational Conference on Machine Learning.\nWayne Xin Zhao,  Kun Zhou,  Junyi Li,  Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023.   A\nsurvey  of  large  language  models.arXiv  preprint\narXiv:2303.18223.\nWenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie,\nYejin Choi, and Yuntian Deng. 2024. wildchat: 570k\nchatgpt interaction logs in the wild.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang,  Zhanghao Wu,  Yonghao Zhuang,  Zi Lin,\nZhuohan  Li,  Dacheng  Li,  Eric  Xing,  et  al.  2024.\nJudging llm-as-a-judge with mt-bench and chatbot\narena.Advances in Neural Information Processing\nSystems, 36.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nL. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke\nZettlemoyer, and Omer Levy. 2023.  Lima: Less is\nmore for alignment.ArXiv, abs/2305.11206.\nBanghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu,\nand Jiantao Jiao. 2023a. Starling-7b: Improving llm\nhelpfulness & harmlessness with rlaif.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023b. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models.arXiv preprint arXiv:2304.10592.\nA    Datasets\nTable 7 gives an overview of our collected datasets.",
    "TypeDataset Name# of Instances# of LangConstructionOpen-source\nHuman-Crafted\nUnifiedQA (Khashabi et al., 2020)\n1\n750KEnhuman-craftedYes\nUnifiedSKG (Xie et al., 2022)\n3\n0.8MEnhuman-craftedYes\nNatural Instructions (Honovich et al., 2022)\n4\n193KEnhuman-craftedYes\nSuper-Natural Instructions (Wang et al., 2022f)\n5\n5M55 Langhuman-craftedYes\nP3 (Sanh et al., 2021)\n6\n12MEnhuman-craftedYes\nxP3 (Muennighoff et al., 2022)\n7\n81M46 Langhuman-craftedYes\nFlan 2021 (Longpre et al., 2023)\n8\n4.4MEnhuman-craftedYes\nCOIG (Zhang et al., 2023a)\n9\n---Yes\nInstructGPT (Ouyang et al., 2022)13KMultihuman-craftedNo\nDolly (Conover et al., 2023a)\n16\n15KEnhuman-craftedYes\nLIMA (Zhou et al., 2023)\n18\n1KEnhuman-craftedYes\nChatGPT (OpenAI, 2022)-Multihuman-craftedNo\nOpenAssistant (Köpf et al., 2023)\n20\n161,443Multihuman-craftedYes\nSynthetic Data\n(Distillation)\nOIG (LAION.ai, 2023)\n2\n43MEnChatGPT (No technique reports)Yes\nUnnatural Instructions (Honovich et al., 2022)\n10\n240KEnInstructGPT-GeneratedYes\nInstructWild (Xue et al., 2023)\n12\n104K-ChatGPT-GeneratedYes\nEvol-Instruct / WizardLM (Xu et al., 2023a)\n13\n52KEnChatGPT-generatedYes\nAlpaca (Taori et al., 2023a)\n14\n52KEnInstructGPT-generatedYes\nLogiCoT (Liu et al., 2023a)\n15\n-EnGPT-4-GeneratedYes\nGPT-4-LLM (Peng et al., 2023)\n17\n52KEn&ZhGPT-4-GeneratedYes\nVicuna (Chiang et al., 2023)70KEnReal User-ChatGPT ConversationsNo\nBaize v1 (Conover et al., 2023b)\n21\n111.5KEnChatGPT-GeneratedYes\nUltraChat (Ding et al., 2023a)\n22\n675KEn&ZhGPT 3/4-GeneratedYes\nGuanaco (JosephusCheung, 2021)\n19\n534,530MultiGPT (Unknown Version)-GeneratedYes\nOrca (Mukherjee et al., 2023)\n23\n1.5MEnGPT 3.5/4-GeneratedYes\nShareGPT\n24\n90KMultiReal User-ChatGPT ConversationsYes\nWildChat\n25\n150KMultiReal User-ChatGPT ConversationsYes\nWizardCoder (Luo et al., 2023)-CodeLLaMa 2-GeneratedNo\nMagicoder (Wei et al., 2023b)\n26\n75K/110KCodeGPT-3.5-GeneratedYes\nWaveCoder (Yu et al., 2023)-CodeGPT 4-GeneratedNo\nPhi-1 (Gunasekar et al., 2023)\n27\n6B TokensCode Q and AGPT-3.5-GeneratedYes\nPhi-1.5 (Li et al., 2023h)-Code Q and AGPT-3.5-GeneratedNo\nNectar (Zhu et al., 2023a)\n28\n183KEnGPT 4-GeneratedYes\nSynthetic Data\n(Self-Improvement)\nSelf-Instruct (Wang et al., 2022c)\n11\n52KEnInstructGPT-GeneratedYes\nInstruction Backtranslation (Li et al., 2023g)502KEnLLaMa-GeneratedNo\nSPIN (Chen et al., 2024b)\n29\n49.8KEnZephyr-GeneratedYes\n1\nhttps://github.com/allenai/unifiedqa\n2\nhttps://github.com/LAION-AI/Open-Instruction-Generalist\n3\nhttps://github.com/hkunlp/unifiedskg\n4\nhttps://github.com/allenai/natural-instructions-v1\n5\nhttps://github.com/allenai/natural-instructions\n6\nhttps://huggingface.co/datasets/bigscience/P3\n7\nhttps://github.com/bigscience-workshop/xmtf\n8\nhttps://github.com/google-research/FLAN\n9\nhttps://github.com/BAAI-Zlab/COIG\n10\nhttps://github.com/orhonovich/unnatural-instructions\n11\nhttps://github.com/yizhongw/self-instruct\n12\nhttps://github.com/XueFuzhao/InstructionWild\n13\nhttps://github.com/nlpxucan/evol-instruct\n14\nhttps://github.com/tatsu-lab/stanford_alpaca\n15\nhttps://github.com/csitfun/LogiCoT\n16\nhttps://huggingface.co/datasets/databricks/databricks-dolly-15k\n17\nhttps://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM\n18\nhttps://huggingface.co/datasets/GAIR/lima\n19\nhttps://huggingface.co/datasets/JosephusCheung/GuanacoDataset\n20\nhttps://github.com/LAION-AI/Open-Assistant\n21\nhttps://github.com/project-baize/baize-chatbot\n22\nhttps://github.com/thunlp/UltraChat#data\n23\nhttps://huggingface.co/datasets/Open-Orca/OpenOrca\n24\nhttps://huggingface.co/datasets/RyokoAI/ShareGPT52K\n25\nhttps://huggingface.co/datasets/allenai/WildChat\n26\nhttps://github.com/ise-uiuc/magicoder?tab=readme-ov-file#-dataset\n27\nhttps://huggingface.co/microsoft/phi-1\n28\nhttps://huggingface.co/datasets/berkeley-nest/Nectar\n29\nhttps://github.com/uclaml/SPIN?tab=readme-ov-file#Data\nTable 7: An overview of instruction tuning datasets."
  ]
}