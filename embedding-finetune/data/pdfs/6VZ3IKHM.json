{
  "key": "6VZ3IKHM",
  "url": "http://arxiv.org/pdf/2405.09798",
  "metadata": {
    "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
    "abstract": "  Large language models are well-known to be effective at few-shot in-context\nlearning (ICL). Recent advancements in multimodal foundation models have\nenabled unprecedentedly long context windows, presenting an opportunity to\nexplore their capability to perform ICL with many more demonstrating examples.\nIn this work, we evaluate the performance of multimodal foundation models\nscaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro\nacross 10 datasets spanning multiple domains (natural imagery, medical imagery,\nremote sensing, and molecular imagery) and tasks (multi-class, multi-label, and\nfine-grained classification). We observe that many-shot ICL, including up to\nalmost 2,000 multimodal demonstrating examples, leads to substantial\nimprovements compared to few-shot (&lt;100 examples) ICL across all of the\ndatasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly\nup to the maximum number of tested examples on many datasets. Given the high\ninference costs associated with the long prompts required for many-shot ICL, we\nalso explore the impact of batching multiple queries in a single API call. We\nshow that batching up to 50 queries can lead to performance improvements under\nzero-shot and many-shot ICL, with substantial gains in the zero-shot setting on\nmultiple datasets, while drastically reducing per-query cost and latency.\nFinally, we measure ICL data efficiency of the models, or the rate at which the\nmodels learn from more demonstrating examples. We find that while GPT-4o and\nGemini 1.5 Pro achieve similar zero-shot performance across the datasets,\nGemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most\ndatasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .\n",
    "published": "2024-05-16T04:02:43Z"
  },
  "text": [
    "Many-Shot In-Context Learning\nin Multimodal Foundation Models\nYixing Jiang\n∗\nJeremy Irvin\n∗\nJi Hun WangMuhammad Ahmed Chaudhry\nJonathan H. ChenAndrew Y. Ng\nStanford University\n{jiang6,jirvin16,jihunwang,ang}@cs.stanford.edu\n{mahmedch,jonc101}@stanford.edu\nAbstract\nLarge language models are well-known to be effective at few-shot in-context\nlearning (ICL). Recent advancements in multimodal foundation models have en-\nabled unprecedentedly long context windows, presenting an opportunity to explore\ntheir capability to perform ICL with many more demonstrating examples.   In\nthis work, we evaluate the performance of multimodal foundation models scaling\nfrom few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro\nacross 10 datasets spanning multiple domains (natural imagery, medical imagery,\nremote sensing, and molecular imagery) and tasks (multi-class, multi-label, and\nfine-grained classification).  We observe that many-shot ICL, including up to al-\nmost 2,000 multimodal demonstrating examples, leads to substantial improvements\ncompared to few-shot (<100 examples) ICL across all of the datasets.  Further,\nGemini 1.5 Pro performance continues to improve log-linearly up to the max-\nimum number of tested examples on many datasets.  Given the high inference\ncosts associated with the long prompts required for many-shot ICL, we also ex-\nplore the impact of batching multiple queries in a single API call. We show that\nbatching up to 50 queries can lead to performance improvements under zero-shot\nand many–shot ICL, with substantial gains in the zero-shot setting on multiple\ndatasets, while drastically reducing per-query cost and latency. Finally, we measure\nICL data efficiency of the models, or the rate at which the models learn from\nmore demonstrating examples.  We find that while GPT-4o and Gemini 1.5 Pro\nachieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits\nhigher ICL data efficiency than GPT-4o on most datasets.  Our results suggest\nthat many-shot ICL could enable users to efficiently adapt multimodal foundation\nmodels to new applications and domains.  Our codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL.\n1    Introduction\nLarge language models (LLMs) have been shown to substantially benefit from the inclusion of a few\ndemonstrating examples (shots) in the LLM context before the test query [1–3]. This phenomenon,\ncommonly referred to as in-context learning (ICL), enables LLMs to learn from few shots without\nany updates to model parameters, and therefore improves specialization to new tasks without any\nfurther model training. More recently, large multimodal models (LMMs) have also demonstrated the\ncapability of learning from in-context examples [4–6]. Han et al. [5] and Zhang et al. [6] both show\nthat few-shot multimodal ICL specifically helps to improve LMM performance on out-domain or\nout-of-distribution tasks.\n∗\nEqual contribution.\nPreprint. Under review.\narXiv:2405.09798v1  [cs.LG]  16 May 2024",
    "Figure  1:Many-shot  multimodal  in-context  learning  compared  to  zero-shot  and  few-shot\nmultimodal ICL.In zero-shot and few-shot settings, respectively, no demonstrating examples or\nonly a small number of demonstrating examples are provided in the context before the test query. In a\nmany-shot ICL setting, we include a large number of demonstrating examples in the prompt, whereas\nin batched many-shot ICL, we perform multiple queries at once using query references.\nWhile few-shot ICL has enabled promising performance improvements for both LLMs and LMMs,\nlimited model context windows have constrained research on the impact of increasing the number of\ndemonstrating examples on performance. This is especially true for LMMs as most use a large number\nof visual tokens to represent images. However, due to recent advancements enabling substantially\nlonger context windows – for example, 128,000 tokens for GPT-4o and up to one million tokens\nfor Gemini 1.5 Pro – it is now possible to explore the effect of drastically increasing the number of\ndemonstrating examples.\nTo investigate the capability of state-of-the-art multimodal foundation models to perform many-shot\nICL, we conduct a large suite of experiments benchmarking model performance on 10 datasets\nspanning several domains and image classification tasks after scaling up the number of demonstrating\nexamples by multiple orders of magnitude. Specifically, our contributions are as follows:\n1.We show that providing multimodal foundation models with many demonstrating examples\nleads to substantial performance improvements compared to providing only a few demon-\nstrating examples. We observe that the performance of Gemini 1.5 Pro generally improves\nlog-linearly as the number of demonstrating examples increases, whereas GPT-4o exhibits\nless stable improvements as the number of in-context examples increases.\n2.We measure the data efficiency of the models under ICL as the number of demonstrating\nexamples is increased, and find that Gemini 1.5 Pro exhibits higher ICL data efficiency than\nGPT-4o on most datasets.\n3.We demonstrate that batching multiple queries into a single request can achieve similar\nor better performance than single query requests in a many-shot setting, while enabling\nsubstantially lower per-example latency and much cheaper per-example inference cost.\n4.We find that batching multiple questions can lead to substantial performance improvements\nin a zero-shot setting.  We design experiments to explain this phenomenon, and find that\nthe improvements are due to a combination of domain calibration, class calibration, and\nself-generated demonstrating examples due to autoregressive decoding.\nOur codebase is publicly available athttps://github.com/stanfordmlgroup/ManyICL.\n2",
    "Figure 2:Gemini 1.5 Pro and GPT-4o performance from zero-shot to many-shot ICL.X-axis is\nin log scale. For Gemini 1.5 Pro, we observe log-linear improvement on 9 out of the 10 datasets and\nfor GPT-4o we observe improvement from more demonstrating examples on most datasets, albeit\nsubstantially less stable than Gemini 1.5 Pro.\n2    Related Work\nScaling ICL.The seminal work of Brown et al.[1]discovered performance improvements for LLMs\nfrom increasing the number of in-context examples, but the tested number of demonstrating examples\nwas low (10 to 100), likely due to the restrictive context size (2048 tokens for GPT3). Increasing\nthe number of in-context examples has only been explored recently by a few works [7–9]. Both Li\net al.[7]and Agarwal et al.[8]explore scaling in-context learning to more than 1,000 demonstrating\nexamples and find performance improvements across multiple tasks. However, their experiments are\nlimited to text-only benchmarks and do not compare performance across different models.\nMultimodal ICL.Due to the recent emergence of LMMs, research on multimodal ICL is still nascent.\nOne prior work developed a new model to leverage complex prompts composed of multimodal\ninputs in order to allow models to compare images [10], while other recent works explored the\ngeneralizability of GPT-4V and Gemini to multimodal out-domain and out-of-distribution tasks, and\nfound that ICL leads to performance benefits for both models across many tasks [6,5].  However,\nnone of these works have leveraged the new largely expanded context windows to investigate the\neffects of increasing the number of demonstrating examples.\nBatch Querying.Multiple prior works have explored batching queries (also commonly referred to\nas batch prompting) for more efficient and cheaper inference. Batch prompting was first introduced\nin Cheng et al.[11], leading to comparable or better performance than single prompting, while\nachieving substantially reduced inference token cost and latency. Lin et al.[12]observe performance\ndegradation with batched prompts in longer contexts, and propose a variety of techniques to mitigate\nthe performance loss. More recently, additional variations of batch prompting have been proposed,\nincluding grouping similar questions together [13], batching prompts of different tasks [14], and\nconcatenating multiple images into a single image collage [15]. We again note that batch prompting\nwith high numbers of demonstrating examples and high numbers of queries has only become feasible\ndue to larger context windows of recent models.\n3    Methods\nWe conduct several experiments to test the effect of increasing the number of demonstrating examples\non the performance of two state-of-the-art multimodal foundation models: GPT-4o and Gemini 1.5\nPro (Section 3.1). We benchmark their performance using standard performance metrics as well as\nan ICL data efficiency metric (Section 3.3) on 10 datasets spanning several vision domains and image\nclassification tasks (Section 3.2). We conduct ablation studies to test the impact of batching queries\n3",
    "on model performance and explain the substantial improvement in zero-shot settings (Section 4.2).\nWe refer to the many-shot in-context learning framework as many-shot ICL. Figure 1 provides an\nillustrative summary of many-shot ICL and batched many-shot ICL compared to zero-shot and\nfew-shot ICL.\n3.1    Models\nWe use three state-of-the-art multimodal foundation models with public API access, namely GPT-4o,\nGPT4(V)-Turbo [4], and Gemini 1.5 Pro [16]. Because GPT-4o performs substantially better than\nGPT4(V)-Turbo, we focus on the results of GPT-4o and Gemini 1.5 Pro in the main text, and include\nGPT4(V)-Turbo results in the Appendix.  We do not utilize Claude3-Opus in our experiments, as\nit only accepts up to 20 images in one request at the time of writing. The specific endpoint for for\nGPT-4o is “gpt-4o-2024-05-13”, for GPT-4(V)-Turbo is “gpt-4-turbo-2024-04-09”, and for Gemini\n1.5 Pro is “gemini-1.5-pro-preview-0409”. We use the API service provided by OpenAI for GPT-4o\nand GPT-4(V)-Turbo, and the API service provided by Google Cloud on Vertex AI for Gemini 1.5\nPro.  We set the temperature to zero for all models and a random seed for GPT-4(V)-Turbo and\nGPT-4o to obtain more deterministic responses. To prevent models from abstaining (which happens\nrarely), we rerun the query until an answer is provided.\n3.2    Datasets\nWe benchmark the model performance on 10 datasets spanning multiple domains (natural imagery,\nmedical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and\nfine-grained classification).  We choose to focus on image classification tasks as other tasks such\nas region captioning would require substantially more tokens thereby limiting the total number of\ndemonstrating examples, and most LMMs are not yet capable of accurately producing localizations\nrequired for other tasks like bounding boxes and segmentation masks [17,18]. Table 1 provides a\nsummary of the datasets used in this study.\nFor all datasets, we construct a set of demonstration (demo) examples from the original training and\nvalidation splits used for in-context learning and a test set from the original test split (if one exists)\nto evaluate the performance of the models.  We randomly sample the demo and test sets from the\noriginal dataset without replacement. For the multi-class and fine-grained classification datasets, we\nperform a class-stratified sampling, ensuring an equal number of examples per class in both the demo\nand test sets. For the multi-label classification dataset (CheXpert), we sample an equal number of\npositive and negative samples per class in both the demo and test sets. We note that, since the task is\nmulti-label, this sampling procedure does not result in an exactly equal number of examples per class.\nThe per-dataset sizes of the full demo and test sets are shown in Table 1, and we increase the number\nof demonstration examples up to the numbers shown in the table while ensuring class balance for the\nscaling experiments.\n3.3    Evaluation Metrics\nWe use standard metrics to evaluate model performance on each dataset. Specifically, we measure\nperformance using accuracy for all multi-class classification datasets as they are sampled to have a\nbalanced class distribution. For multi-label classification on CheXpert, we use the macro-averaged\nF1 metric. In the rare case of parsing errors, we consider the response as incorrect. To estimate the\nvariability around the evaluation metrics, we compute standard deviation using bootstrapping with\n1,000 bootstrap replicates.\nIn addition to standard performance metrics, we measure the data efficiency of each model. Specifi-\ncally, we compute a linear regression betweenlog\n10\n(N+ 1)(withNthe number of examples) and\nmodel performance, enforcing that the line passes through the zero-shot performance point.  This\nvalue approximates the amount of performance improvement from zero-shot expected from including\nan order of magnitude more demonstrating examples.\n4",
    "Table 1:Summary of benchmark datasets.We use 10 datasets spanning multiple domains (natural\nimagery, medical imagery, remote sensing, molecular imagery) and tasks (multi-class, multi-label,\nand fine-grained classification).\nDatasetTask and image type# ClassesDemo / test set sizeExample image\nHAM10000[19]\nSkin disease classification\non clinical photos\n7805 / 210\nFIVES [20]\nEye disease classification\non fundus images\n4400 / 120\nCheXpert [21]\nMulti-label  lung  disease\ndetection on chest X-rays\n5200 / 150\nCamelyon17 [22]\nTumordetectionon\npathology images\n22000 / 100\nTerraIncognita\n[23]\nAnimal  species  recogni-\ntion on camera images\n91035 / 270\nUCMerced[24]\nLand use classification on\nsatellite images\n211470 / 420\nEuroSAT [25]\nLand use / land cover clas-\nsification on satellite im-\nages\n101000 / 300\nOxford Pets [26]\nPet classification on cam-\nera images\n351750 / 700\nDTD [27]\nTexture  classification  on\nsynthetic images\n472350 / 940\nDrugOOD Assay\n[28]\nDrug  binding  prediction\non molecular images\n21600 / 200\n5",
    "Table 2:Many-shot ICL performance and efficiency comparison.We report the performance\nunder a zero-shot regime and performance at the optimal demo set size as well as the many-shot\nICL data efficiency of GPT-4o and Gemini 1.5 Pro.  We measure performance using accuracy on\nall datasets except CheXpert, for which we use macro-average F1.  We bold the highest ICL data\nefficiency between the two models on each dataset.\nDataset\nGPT-4oGemini 1.5 Pro\nZero-shotBestEfficiency  Zero-shotBestEfficiency\nHAM1000034.9353.59 (+18.66)5.9133.3356.46 (+23.13)6.94\nFIVES31.6737.50 (+5.83)0.3025.8355.00 (+29.17)7.56\nCheXpert28.4742.54 (+14.08)3.7022.1642.23 (+20.08)9.06\nCamelyon1777.0090.00 (+13.00)1.0071.0083.00 (+12.00)3.00\nTerraIncognita29.2659.26 (+30.00)20.5059.6366.67 (+7.04)3.50\nUCMerced90.9598.57 (+7.62)1.2091.1998.57 (+7.38)4.36\nEuroSAT55.3784.23 (+28.86)19.4036.2474.16 (+37.92)20.61\nOxford Pets83.1494.14 (+11.00)-3.7285.2997.43 (+12.14)4.26\nDTD39.2674.47 (+35.21)4.4869.8983.19 (+13.30)3.89\nDrugOOD Assay50.0055.00 (+5.00)2.0248.0055.50 (+7.50)2.03\n4    Results\nWe present many-shot ICL performance using batched queries in Section 4.1, investigate the impact\nof batching queries on performance in Section 4.2, and provide an analysis on cost and latency in\nSection 4.3. Results using GPT4(V)-Turbo are in Appendix C.\n4.1    Increasing number of demonstrating examples\nMain Results.Gemini 1.5 Pro exhibits consistent and substantial improvements as the number of\ndemonstrating examples increases across all datasets except for DrugOOD Assay (Figure 2). Gemini\n1.5 Pro shows particularly large improvements from many-shot ICL on HAM10000 (+23% accuracy\ncompared to zero-shot, +16% compared to 7 examples), FIVES (+29% compared to zero-shot, +27%\ncompared to 20 examples), and EuroSAT (+38% compared to zero-shot, +31% compared to 10\nexamples). Notably, for 5 out of the 10 datasets (FIVES, UCMerced, EuroSAT, Oxford Pets, and\nDTD), Gemini 1.5 Pro performance continues to improve up to the highest number of demonstrating\nexamples considered (~1,000 examples). On the other 5 datasets, the optimal performance occurs\nprior to the highest number of demo examples, with the maximum number of demo examples leading\nto similar or slightly worse performance than the optimal demo set size. On the other hand, Gemini\n1.5 Pro performance on DrugOOD Assay does not substantially benefit from many-shot ICL, with\nhigh variance in performance across demo sizes and the peak performance at 40 demo examples.\nSimilarly, GPT-4o shows substantial performance improvements on all datasets except FIVES and\nDrugOOD Assay using many-shot ICL, but the improvement is not consistent. For many datasets,\nperformance drops sharply at first and then improves significantly as the number of demonstrating\nexamples increases further, resulting in V-shaped scaling curves (Figure 2). We also note that we\nwere unable to increase the number of demo examples to the same level as considered for Gemini 1.5\nPro because GPT-4o has a shorter context window and is more prone to timeout errors with longer\ninputs. GPT-4o performance on DrugOOD Assay shows high variance, similar to Gemini 1.5 Pro,\nwith the peak performance observed at 50 demo examples.\nSensitivity to prompt selection.We also explore a different set of prompts to test the robustness of\nmany-shot ICL to differences in prompt wording on two datasets. While there is a small deviation in\nperformance between different prompts, the overall log-linear improvement trend is consistent across\nthe prompts. Details can be found in Appendix B.\nICL data efficiency.We find Gemini 1.5 Pro demonstrates higher ICL data efficiency than GPT-\n4o across all datasets except TerraIncognita and DTD (Table 2).  Gemini 1.5 Pro ICL efficiency\nis especially high on EuroSAT, with 20.61% improvement in accuracy for every 10x more demo\nexamples, and lowest on DrugOOD Assay (2.03), Camelyon17 (3.00), and TerraIncognita (3.50).\nGPT-4o ICL data efficiency is especially high on TerraIncognita (20.50%) and EuroSat (19.40).\n6",
    "Figure 3:Gemini 1.5 Pro performance under many-shot and zero-shot ICL when varying the\namount of queries included in every request.We show performance per batch size with the optimal\nnumber of demo examples (many-shot) and no demo examples (zero-shot). Thex-axis is in log scale.\nUnder the many-shot regime, batching queries leads to no substantial drop in performance compared\nto individual queries when we choose a suitable batch size. For zero-shot, including only one query\nis suboptimal for many datasets.\nGemini 1.5 Pro has a positive efficiency on all datasets and GPT-4o has a positive data efficiency on\n9 of the 10 datasets (excluding Oxford Pets). Importantly, both models benefit substantially from\nmany-shot ICL at the optimal demo set size, with an average improvement of +17% for both Gemini\n1.5 Pro and GPT-4o.\n4.2    Impact of batching queries\nAs including a large set of demo examples in the prompt leads to much longer sequence lengths and\ntherefore higher inference time and cost, we consider batching queries in a single prompt to reduce\nper-query cost, and examine the impact of different batch sizes on model performance. Due to its\nsuperior performance and free preview access, we use Gemini 1.5 Pro for these experiments.\nMain Results.We find minimal performance degradations, and sometimes performance improve-\nments, as we increase the number of queries included in each batch across under both zero-shot and\nmany-shot (at the optimal demo set size) regimes (Figure 3). Notably, using a single query each time\nwith many-shot ICL is suboptimal across many of the datasets. We find that the optimal batch size is\namong the three largest sizes on every dataset except CheXpert and EuroSAT, which both see optimal\nperformance with a single query at a time.\nWe additionally observe that including a single query at a time is suboptimal on most datasets in the\nzero-shot regime. Surprisingly, performance with the highest batch size is substantially higher across\nthree datasets under the zero-shot regime, with a consistent performance improvement as the batch\nsize is increased on both UCMerced and Terraincognita.\nZero-shot performance improvements from batching queries.We conduct several additional\nexperiments to investigate why batch querying can lead to large performance improvements under\nthe zero-shot regime on TerraIncognita and UCMerced. We hypothesize that this improvement may\nbe due to three potential benefits from ICL: (1) domain calibration, where the model benefits from\nseeing more images in the domain in order to adapt to it, (2) class calibration, where seeing images\nof different classes enables the model to better calibrate its outputs, and (3) self-ICL (shown to be\neffective in prior work [29]), where the model can learn from self-generated demonstrations due to\nautoregressive decoding. We design experiments to isolate the potential benefits from each of these\ntypes of ICL between asking a single query to batching 50 queries together.\nFirst, to measure potential improvement from domain calibration, we include 49 images from the\nsame class in the prompt without including any label. We find a 3.0% improvement on TerraIncognita\n7",
    "Figure 4:Ablation study to investigate why batching queries leads to performance improve-\nments when using Gemini 1.5 Pro in a zero-shot setting.The first bar shows performance when\nincluding a single query, the second adds 49 unlabeled images from a single class, the third adds 49\nunlabeled images in total from all classes, the fourth adds model responses to include self-generated\ndemonstrations, and the last includes 50 queries in one request.\nand 2.6% degradation on UCMerced, suggesting domain calibration is helpful for the former but not\nthe latter. Second, to capture performance gains from class calibration, we include a random sample\nof 49 images in the prompt, again without including the label. We see a further 3.5% improvement on\nTerraIncognita (6.5% improvement from a single query) and a 4.5% improvement from a single query\non UCMerced, suggesting including the context of class-balanced images is helpful even without\nlabels. Third, to capture additional performance improvements from the self-generated labels, we\nobtain predicted labels from the zero-shot model using a single query for each of the 49 randomly\nsampled images and add them to the prompt.  We observe further performance increase on both\ndatasets, with 5.5% on TerraIncognita and 2.7% on UCMerced. The final total accuracy is similar\nto asking the 50 questions each round, which suggests these three components mostly explain the\nreason for improved zero-shot performance under a larger query batch size.\n4.3    Cost and latency analysis\nMany-shot ICL incurs zero additional training cost, but per-query inference can be costly and slow\ndue to long input contexts. To quantitatively measure this, we compute the latency and cost associated\nwith the zero-shot and many-shot requests with and without batching when using Gemini 1.5 Pro on\nHAM10000 and TerraIncognita. We calculate the costs using the Gemini 1.5 Pro preview pricing\n($7 per 1 million input tokens and $21 per 1 million output tokens).  For fair comparison and to\nminimize data transfer artifacts, all requests are sent to the same location where the VM instance is\nheld (“us-central1”). We run the query three times under each setting and report the average.\nIn the zero-shot regime, we see substantial per-example latency reductions due to query batching,\nclose to a 10x reduction on HAM10000 and 2x on TerraIncognita (Table 3). The per-example cost\nis similar between the two as there is no additional context needed for including demonstrating\nexamples. In the many-shot regime, we observe substantial reductions in both per-example latency\nand cost on both datasets. Specifically, for HAM10000, we find a near 35x reduction in latency and\n10x reduction in cost, and 20x reduction in latency and 45x reduction in cost for TerraIncognita.\n5    Discussion\nIn this study, we evaluate many-shot ICL of state-of-the-art multimodal foundation models across 10\ndatasets and find consistent performance improvements across most of the datasets. Batching queries\nwith many-shot ICL further exhibits substantially reduced per-example latency and inference costs\nwithout compromising performance.\nOur findings suggest that these multimodal foundation models have the capability of performing\nICL with large numbers of demonstrating examples, which may have significant implications on\ntheir practical use. For example, it was previously impossible to adapt these large, private models to\n8",
    "Table 3:Inference latency and cost using Gemini 1.5 Pro with and without query batching.We\nuse 50 queries per batch. In the zero-shot setting, we can achieve lower per-example latency with\nbatching, but the per-example cost remains identical. In the many-shot setting, the per-example cost\nand per-example latency both drop substantially with query batching.\nDataset\nNo Query BatchingQuery Batching\nPer-batch\nlatency\nPer-\nexample\nlatency\nPer-\nexample\ncost\nPer-batch\nlatency\nPer-\nexample\nlatency\nPer-\nexample\ncost\nHAM10000 (zero-shot)2.2s2.2s$0.003811.4s0.23s$0.0038\nTerraIncognita (zero-shot)  2.0s2.0s$0.003751.6s1.0s$0.0038\nHAM10000 (350-shot)17.3s17.3s$0.842026.9s0.54s$0.0877\nTerraIncognita (810-shot)   34.9s34.9s$1.842085.9s1.7s$0.0406\nnew tasks and domains, but many-shot ICL would enable users to leverage demonstrating examples\nto adapt the models. One significant advantage of many-shot ICL is its ability to get quick results\neven on the same day of model release, and that’s why we can finish our evaluation using GPT-4o\nwithin days. Furthermore, fine-tuning open-source models is the standard practice when practitioners\nhave access to moderately sized datasets, but many-shot ICL may remove the need for fine-tuning,\nmaking it much easier to develop customized approaches. We note that it remains to be seen how\ntraditional fine-tuning of these models compares to many-shot ICL with foundation models in terms\nof absolute performance and data efficiency, so future work should explore this.  In addition, it is\nimportant to study general issues which plague those foundation models, such as hallucinations and\nbiases, under the context of many-shot ICL and batching queries. For example, it would be interesting\nto explore if carefully curated and large sets of demonstrating examples can reduce biases across\ndifferent sub-groups. We leave this to future work.\nOur study has limitations.  First, we only explore performance under many-shot ICL on image\nclassification tasks and with private foundation models. We believe these are the most practically\nrelevant and common multimodal settings, but it is worthwhile for future work to explore potential\nbenefits from many-shot ICL on other tasks and with upcoming open-source multimodal foundation\nmodels like LLaMA-3 [30].  Second, even after recent developments to increase context size, the\nsize prohibits many-shot ICL from being used on datasets with a large number (several hundred or\nmore) of classes. We anticipate that context window sizes will continue to increase in size over time\nwhich will mitigate this issue. Third, the datasets which were used to train these private models have\nnot been disclosed, so it is difficult to tell whether the models have been trained on the datasets we\nselected. We argue that zero-shot performance across the datasets is far from perfect which provides\nevidence that the datasets have not been used for training, but we cannot determine that with certainty.\n6    Conclusion\nIn summary, we show that multimodal foundation models are capable of many-shot ICL. We believe\nthat these results pave a promising path forward to improve the adaptability and accessibility of large\nmultimodal foundation models.\nReferences\n[1]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n[2]Archit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to\nfew-shot learning.arXiv preprint arXiv:2203.04291, 2022.\n9",
    "[3]Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni.  Generalizing from a few\nexamples: A survey on few-shot learning.ACM computing surveys (csur), 53(3):1–34, 2020.\n[4]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.  Gpt-4\ntechnical report.arXiv preprint arXiv:2303.08774, 2023.\n[5]\nZhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin Wu, Yilong Yin,\nSalman Khan, Lina Yao, Tongliang Liu, et al. How well does gpt-4v (ision) adapt to distribution\nshifts? a preliminary investigation.arXiv preprint arXiv:2312.07424, 2023.\n[6]\nXingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai\nGuan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large\nlanguage models.arXiv preprint arXiv:2402.06599, 2024.\n[7]Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng\nKong. In-context learning with many demonstration examples.arXiv preprint arXiv:2302.04931,\n2023.\n[8]Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand,\nZaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning.\narXiv preprint arXiv:2404.11018, 2024.\n[9]Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham\nNeubig. In-context learning with long-context models: An in-depth exploration.arXiv preprint\narXiv:2405.00200, 2024.\n[10]Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng\nWang, Wenjuan Han, and Baobao Chang.  Mmicl: Empowering vision-language model with\nmulti-modal in-context learning.arXiv preprint arXiv:2309.07915, 2023.\n[11]\nZhoujun Cheng, Jungo Kasai, and Tao Yu.  Batch prompting:  Efficient inference with large\nlanguage model apis.arXiv preprint arXiv:2301.08721, 2023.\n[12]Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish\nmore with less.arXiv preprint arXiv:2309.00384, 2023.\n[13]Jiayi Liu, Tinghan Yang, and Jennifer Neville.  Cliqueparcel: An approach for batching llm\nprompts that jointly optimizes efficiency and faithfulness.arXiv preprint arXiv:2402.14833,\n2024.\n[14]Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim.   Multi-task\ninference:  Can large language models follow multiple instructions at once?arXiv preprint\narXiv:2402.11597, 2024.\n[15]Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu.  Collage prompting: Budget-friendly\nvisual recognition with gpt-4v.arXiv preprint arXiv:2403.11468, 2024.\n[16]Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-\nbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.arXiv\npreprint arXiv:2403.05530, 2024.\n[17]Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and\nPhilip Torr.  Dettoolchain:  A new prompting paradigm to unleash detection ability of mllm.\narXiv preprint arXiv:2403.12488, 2024.\n[18]Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy.  Contextual object\ndetection with multimodal large language models.arXiv preprint arXiv:2305.18279, 2023.\n[19]Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection\nof multi-source dermatoscopic images of common pigmented skin lesions.Scientific data, 5(1):\n1–9, 2018.\n10",
    "[20]Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi\nWang, and Juan Ye.   Fives:  A fundus image dataset for artificial intelligence based vessel\nsegmentation.Scientific Data, 9(1):475, 2022.\n[21]Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute,\nHenrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large\nchest radiograph dataset with uncertainty labels and expert comparison. InProceedings of the\nAAAI conference on artificial intelligence, volume 33, pages 590–597, 2019.\n[22]Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol,\nMeyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong,\net al. From detection of individual metastases to classification of lymph node status at the patient\nlevel: the camelyon17 challenge.IEEE transactions on medical imaging, 38(2):550–560, 2018.\n[23]Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. InProceedings\nof the European conference on computer vision (ECCV), pages 456–473, 2018.\n[24]\nYi Yang and Shawn Newsam.  Bag-of-visual-words and spatial extensions for land-use clas-\nsification.  InProceedings of the 18th SIGSPATIAL international conference on advances in\ngeographic information systems, pages 270–279, 2010.\n[25]\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.  Eurosat:  A novel\ndataset and deep learning benchmark for land use and land cover classification.IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217–2226, 2019.\n[26]Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In2012\nIEEE conference on computer vision and pattern recognition, pages 3498–3505. IEEE, 2012.\n[27]Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\nDescribing textures in the wild. InProceedings of the IEEE conference on computer vision and\npattern recognition, pages 3606–3613, 2014.\n[28]Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong,\nLanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and\nbenchmark for ai-aided drug discovery–a focus on affinity prediction problems with noise\nannotations.arXiv preprint arXiv:2201.09637, 2022.\n[29]Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning\nwith self-generated demonstrations.arXiv preprint arXiv:2305.15035, 2023.\n[30]\nIntroducing  meta  llama  3:  The  most  capable  openly  available  llm  to  date.   URLhttps:\n//ai.meta.com/blog/meta-llama-3/.\n11",
    "A    Prompts used for ICL experiments\nA.1    Prompt used for image classification experiments\nprompt = \"\"\nfor demo in demo_examples:\nprompt += f\"\"\"<<IMG>>Given the image above, answer the following question-\nusing the specified format.\nQuestion: What is in the image above?\nChoices: {str(class_desp)}\nAnswer Choice: {demo.answer}\n\"\"\"\nprompt += f\"\"\"<<IMG>>Given the image above, answer the following question-\nusing the specified format.\nQuestion: What is in the image above?\nChoices: {str(class_desp)}\nPlease respond with the following format:\n---BEGIN FORMAT TEMPLATE---\nAnswer Choice: [Your Answer Choice Here]\nConfidence Score: [Your Numerical Prediction Confidence Score Here From 0 To 1]\n---END FORMAT TEMPLATE---\nDo not deviate from the above format. Repeat the format template for the answer.\"\"\"\nA.2    Prompts used for image classification experiments with batching\nprompt = \"\"\nfor demo in demo_examples:\nprompt += f\"\"\"<<IMG>>Given the image above, answer the following question-\nusing the specified format.\nQuestion: What is in the image above?\nChoices: {str(class_desp)}\nAnswer Choice: {demo[1]}\n\"\"\"\nfor idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()):\nprompt += f\"\"\"<<IMG>>Given the image above, answer the following question-\nusing the specified format.\nQuestion {qn_idx}: What is in the image above?\nChoices {qn_idx}: {str(class_desp)}\n\"\"\"\nfor i in range(start_idx, end_idx):\nqn_idx = i-start_idx+1\nprompt += f\"\"\"\nPlease respond with the following format for each question:\n---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---\nAnswer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}]\nConfidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here-\nFrom 0 To 1 for Question {qn_idx}]\n---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---\nDo not deviate from the above format. Repeat the format template for the answer.\"\"\"\n12",
    "A.3    Prompts used for batching ablation experiments\nA.3.1    Prefixing images\nprompt = \"\"\nfor demo in prefix_image_paths:\nprompt += f\"\"\"<<IMG>>\n\"\"\"\nprompt += \"Above are some images from the same dataset. \"\nqns_idx = []\nfor idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()):\nqn_idx = idx+1\nprompt += f\"\"\"<<IMG>> Given the image above, answer the following question-\nusing the specified format.\nQuestion {qn_idx}: What is in the image above?\nChoices {qn_idx}: {str(class_desp)}\n\"\"\"\nfor i in range(start_idx, end_idx):\nqn_idx = i-start_idx+1\nprompt += f\"\"\"\nPlease respond with the following format for each question:\n---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---\nAnswer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}]\nConfidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here-\nFrom 0 To 1 for Question {qn_idx}]\n---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---\nDo not deviate from the above format. Repeat the format template for the answer.\"\"\"\nB    Prompt selection\nWe utilize a different set of prompts to test the robustness of ManyICL to differences in prompt\nwording. We randomly sample two datasets (HAM10000 and EuroSAT) for this experiment due to\nbudget limit.\nB.1    Prompts used for prompt selection experiments\nNote that only the question section is shown here, and prompt 1 is used for all other image classifica-\ntion experiments.\nB.1.1    Prompt 1\n<<IMG>>Given the image above, answer the following question using the specified format.\nQuestion {qn_idx}: What is in the image above?\nChoices {qn_idx}: {str(class_desp)}\nB.1.2    Prompt 2\n<<IMG>>Given the image above, answer the following question using the specified format.\nQuestion {qn_idx}: Which class does this image belong to?\nChoices {qn_idx}: {str(class_desp)}\nB.1.3    Prompt 3\nQuestion {qn_idx}: <<IMG>>Classify the image above, choose from {str(class_desp)}\n13",
    "Figure 5:Sensitivity analysis of many-shot ICL.These plots show the change in task performance\non two datasets as the number of demonstrating examples increases, using three different prompts.\nFor all experiments on sensitivity analysis, the Gemini 1.5 Pro model is used. Thex-axis is in the\nlogarithmic scale, representing the number of demonstrating examples plus one.  The log-linear\nimprovement until the optimal performance is consistent across all prompts selected.\nB.2    Prompt selection results\nFigure 5 shows the sensitivity of performance to prompt selection on two datasets with three prompts.\nWhile there exists a small deviation in performance, but the overall log-linear improvement trend is\nconsistent.\nC    GPT4(V)-Turbo performance under many-shot ICL\nGPT4(V)-Turbo shows mixed results for many-shot ICL, with substantial performance improvements\non HAM1000, UCMerced, EuroSAT, and DTD, but minimal improvements or no improvement across\nthe other six datasets (Figure 6). However, we note that we were unable to increase the number of\ndemo examples to the same level as Gemini 1.5 Pro because GPT4(V)-Turbo has a shorter context\nwindow and is more prone to timeout errors when scaling.  Additionally, GPT4(V)-Turbo seems\nto generally underperform Gemini 1.5 Pro across the datasets excluding FIVES and EuroSAT for\nwhich it seems to mostly match the Gemini 1.5 Pro performance. GPT4(V)-Turbo performance on\nDrugOOD Assay shows high variance, resembling that of Gemini 1.5 Pro with the peak performance\nat 40 demo examples.\nD    Performance of many-shot ICL on medical QA tasks\nD.1    Prompt used for medical QA experiments (MedQA, MedMCQA)\nprompt = \"You are an expert in answering medical exam questions. \"\nfor demo in demo_examples:\nprompt += f\"\"\"Question: {demo.question}\nChoices: {demo.options}\nAnswer: {demo.answer}\n\"\"\"\nprompt += f\"\"\"Question: {actual.question}\nChoices: {actual.options}\n14",
    "Figure 6:GPT4(V)-Turbo and GPT-4o performance from zero-shot to many-shot ICL.X-axis is\nin log scale.\nFigure 7: Many-shot ICL performances of medical QA tasks.\nPlease respond with the following format:\n---BEGIN FORMAT TEMPLATE---\nAnswer: [Your Answer Choice Here]\nConfidence Score: [Your Numerical Prediction Confidence Score Here From 0 To 1]\n---END FORMAT TEMPLATE---\nDo not deviate from the above format. Repeat the format template for the answer.\"\"\"\nD.2    Results\nFigure 7 shows the results on medical QA tasks.\n15",
    "Acknowledgments and Disclosure of Funding\nWe thank Dr. Jeff Dean, Yuhui Zhang, Dr. Mutallip Anwar, Kefan Dong, Rishi Bommasani, Ravi B.\nSojitra, Chen Shani and Annie Chen for their feedback on the ideas and manuscript. Yixing Jiang\nis supported by National Science Scholarship (PhD). This work is also supported by Google cloud\ncredit. Dr. Jonathan Chen has received research funding support in part by NIH/National Institute of\nAllergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug Abuse Clinical\nTrials Network (UG1DA015815 - CTN-0136), Gordon and Betty Moore Foundation (Grant #12409),\nStanford Artificial Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence\n(AIMI-HAI) Partnership Grant, Google, Inc. Research collaboration Co-I to leverage EHR data to\npredict a range of clinical outcomes, American Heart Association - Strategically Focused Research\nNetwork - Diversity in Clinical Trials and NIH-NCATS-CTSA grant (UL1TR003142) for common\nresearch resources.\n16"
  ]
}