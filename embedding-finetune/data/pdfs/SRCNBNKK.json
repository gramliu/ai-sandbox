{
  "key": "SRCNBNKK",
  "url": "http://arxiv.org/pdf/2303.01469",
  "metadata": {
    "title": "Consistency Models",
    "abstract": "  Diffusion models have significantly advanced the fields of image, audio, and\nvideo generation, but they depend on an iterative sampling process that causes\nslow generation. To overcome this limitation, we propose consistency models, a\nnew family of models that generate high quality samples by directly mapping\nnoise to data. They support fast one-step generation by design, while still\nallowing multistep sampling to trade compute for sample quality. They also\nsupport zero-shot data editing, such as image inpainting, colorization, and\nsuper-resolution, without requiring explicit training on these tasks.\nConsistency models can be trained either by distilling pre-trained diffusion\nmodels, or as standalone generative models altogether. Through extensive\nexperiments, we demonstrate that they outperform existing distillation\ntechniques for diffusion models in one- and few-step sampling, achieving the\nnew state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for\none-step generation. When trained in isolation, consistency models become a new\nfamily of generative models that can outperform existing one-step,\nnon-adversarial generative models on standard benchmarks such as CIFAR-10,\nImageNet 64x64 and LSUN 256x256.\n",
    "published": "2023-03-02T18:30:16Z"
  },
  "text": [
    "Consistency Models\nYang Song\n1\nPrafulla Dhariwal\n1\nMark Chen\n1\nIlya Sutskever\n1\nAbstract\nDiffusion models have significantly advanced the\nfields of image, audio, and video generation, but\nthey depend on an iterative sampling process that\ncauses slow generation. To overcome this limita-\ntion, we proposeconsistency models, a new fam-\nily of models that generate high quality samples\nby directly mapping noise to data. They support\nfast one-step generation by design, while still al-\nlowing multistep sampling to trade compute for\nsample quality. They also support zero-shot data\nediting, such as image inpainting, colorization,\nand super-resolution, without requiring explicit\ntraining on these tasks. Consistency models can\nbe trained either by distilling pre-trained diffu-\nsion models, or as standalone generative models\naltogether.  Through extensive experiments, we\ndemonstrate that they outperform existing distilla-\ntion techniques for diffusion models in one- and\nfew-step sampling,  achieving the new state-of-\nthe-art FID of 3.55 on CIFAR-10 and 6.20 on\nImageNet64ˆ64for one-step generation. When\ntrained in isolation, consistency models become a\nnew family of generative models that can outper-\nform existing one-step, non-adversarial generative\nmodels on standard benchmarks such as CIFAR-\n10, ImageNet64ˆ64and LSUN256ˆ256.\n1. Introduction\nDiffusion models (Sohl-Dickstein et al., 2015; Song & Er-\nmon, 2019; 2020; Ho et al., 2020; Song et al., 2021), also\nknown as score-based generative models,  have achieved\nunprecedented success across multiple fields, including im-\nage generation (Dhariwal & Nichol, 2021; Nichol et al.,\n2021; Ramesh et al., 2022; Saharia et al., 2022; Rombach\net al., 2022), audio synthesis (Kong et al., 2020; Chen et al.,\n2021; Popov et al., 2021), and video generation (Ho et al.,\n1\nOpenAI, San Francisco, CA 94110, USA. Correspondence to:\nYang Song<songyang@openai.com>.\nProceedings of the40\nth\nInternational Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nFigure 1: Given a Probability Flow (PF) ODE that smoothly\nconverts data to noise, we learn to map any point (e.g.,x\nt\n,\nx\nt\n1\n, andx\nT\n) on the ODE trajectory to its origin (e.g.,x\n0\n)\nfor generative modeling.  Models of these mappings are\ncalled consistency models, as their outputs are trained to be\nconsistent for points on the same trajectory.\n2022b;a). A key feature of diffusion models is the iterative\nsampling process which progressively removes noise from\nrandom initial vectors.  This iterative process provides a\nflexible trade-off of compute and sample quality, as using\nextra compute for more iterations usually yields samples\nof better quality. It is also the crux of many zero-shot data\nediting capabilities of diffusion models, enabling them to\nsolve  challenging  inverse  problems  ranging  from  image\ninpainting,  colorization,  stroke-guided image editing,  to\nComputed Tomography and Magnetic Resonance Imaging\n(Song & Ermon, 2019; Song et al., 2021; 2022; 2023; Kawar\net al., 2021; 2022; Chung et al., 2023; Meng et al., 2021).\nHowever, compared to single-step generative models like\nGANs (Goodfellow et al., 2014), VAEs (Kingma & Welling,\n2014; Rezende et al., 2014), or normalizing flows (Dinh\net al., 2015; 2017; Kingma & Dhariwal, 2018), the iterative\ngeneration procedure of diffusion models typically requires\n10–2000 times more compute for sample generation (Song\n& Ermon, 2020; Ho et al., 2020; Song et al., 2021; Zhang\n& Chen, 2022; Lu et al., 2022), causing slow inference and\nlimited real-time applications.\nOur objective is to create generative models that facilitate ef-\nficient, single-step generation without sacrificing important\nadvantages of iterative sampling, such as trading compute\nfor sample quality when necessary, as well as performing\nzero-shot data editing tasks.  As illustrated in Fig. 1, we\nbuild on top of the probability flow (PF) ordinary differen-\ntial equation (ODE) in continuous-time diffusion models\n(Song et al., 2021), whose trajectories smoothly transition\n1\narXiv:2303.01469v2  [cs.LG]  31 May 2023",
    "Consistency Models\nthe data distribution into a tractable noise distribution. We\npropose to learn a model that maps any point at any time\nstep to the trajectory’s starting point.  A notable property\nof our model is self-consistency:points on the same tra-\njectory map to the same initial point. We therefore refer to\nsuch models asconsistency models. Consistency models\nallow us to generate data samples (initial points of ODE\ntrajectories,e.g.,x\n0\nin Fig. 1) by converting random noise\nvectors (endpoints of ODE trajectories,e.g.,x\nT\nin Fig. 1)\nwith only one network evaluation. Importantly, by chaining\nthe outputs of consistency models at multiple time steps,\nwe can improve sample quality and perform zero-shot data\nediting at the cost of more compute, similar to what iterative\nsampling enables for diffusion models.\nTo train a consistency model, we offer two methods based\non enforcing the self-consistency property. The first method\nrelies on using numerical ODE solvers and a pre-trained\ndiffusion model to generate pairs of adjacent points on a\nPF ODE trajectory. By minimizing the difference between\nmodel outputs for these pairs, we can effectively distill a\ndiffusion model into a consistency model, which allows gen-\nerating high-quality samples with one network evaluation.\nBy contrast, our second method eliminates the need for a\npre-trained diffusion model altogether, allowing us to train\na consistency model in isolation.  This approach situates\nconsistency models as an independent family of generative\nmodels.  Importantly, neither approach necessitates adver-\nsarial training, and they both place minor constraints on the\narchitecture, allowing the use of flexible neural networks\nfor parameterizing consistency models.\nWe demonstrate the efficacy of consistency models on sev-\neral image datasets, including CIFAR-10 (Krizhevsky et al.,\n2009), ImageNet64ˆ64(Deng et al., 2009), and LSUN\n256ˆ256(Yu et al., 2015). Empirically, we observe that\nas a distillation approach, consistency models outperform\nexisting diffusion distillation methods like progressive dis-\ntillation (Salimans & Ho, 2022) across a variety of datasets\nin few-step generation: On CIFAR-10, consistency models\nreach new state-of-the-art FIDs of 3.55 and 2.93 for one-step\nand two-step generation; on ImageNet64ˆ64, it achieves\nrecord-breaking FIDs of 6.20 and 4.70 with one and two net-\nwork evaluations respectively. When trained as standalone\ngenerative models, consistency models can match or surpass\nthe quality of one-step samples from progressive distillation,\ndespite having no access to pre-trained diffusion models.\nThey are also able to outperform many GANs, and exist-\ning non-adversarial, single-step generative models across\nmultiple datasets. Furthermore, we show that consistency\nmodels can be used to perform a wide range of zero-shot\ndata editing tasks, including image denoising, interpolation,\ninpainting, colorization, super-resolution, and stroke-guided\nimage editing (SDEdit, Meng et al. (2021)).\n2. Diffusion Models\nConsistency models are heavily inspired by the theory of\ncontinuous-time diffusion models (Song et al., 2021; Karras\net al., 2022).  Diffusion models generate data by progres-\nsively perturbing data to noise via Gaussian perturbations,\nthen creating samples from noise via sequential denoising\nsteps.  Letp\ndata\npxqdenote the data distribution.  Diffusion\nmodels start by diffusingp\ndata\npxqwith a stochastic differen-\ntial equation (SDE) (Song et al., 2021)\ndx\nt\n“μpx\nt\n,tqdt`σptqdw\nt\n,(1)\nwheretP r0,Ts,Tą0is a fixed constant,μp ̈, ̈qand\nσp ̈qare  the  drift  and  diffusion  coefficients  respectively,\nandtw\nt\nu\ntPr0,Ts\ndenotes  the  standard  Brownian  motion.\nWe denote the distribution ofx\nt\nasp\nt\npxqand as a result\np\n0\npxq ”p\ndata\npxq.  A remarkable property of this SDE is\nthe existence of an ordinary differential equation (ODE),\ndubbed  theProbability  Flow  (PF)  ODEby  Song  et  al.\n(2021),  whose solution trajectories sampled attare dis-\ntributed according top\nt\npxq:\ndx\nt\n“\n„\nμpx\nt\n,tq ́\n1\n2\nσptq\n2\n∇logp\nt\npx\nt\nq\nȷ\ndt.(2)\nHere∇logp\nt\npxqis thescore functionofp\nt\npxq; hence dif-\nfusion models are also known asscore-based generative\nmodels(Song & Ermon, 2019; 2020; Song et al., 2021).\nTypically, the SDE in Eq. (1) is designed such thatp\nT\npxq\nis  close  to  a  tractable  Gaussian  distributionπpxq.   We\nhereafter adopt the settings in Karras et al. (2022), where\nμpx,tq “0andσptq “\n?\n2t.   In  this  case,  we  have\np\nt\npxq “p\ndata\npxqbNp0,t\n2\nIq, wherebdenotes the convo-\nlution operation, andπpxq “Np0,T\n2\nIq. For sampling, we\nfirst train ascore models\nφ\npx,tq «∇logp\nt\npxqviascore\nmatching(Hyv\n ̈\narinen & Dayan, 2005; Vincent, 2011; Song\net al., 2019; Song & Ermon, 2019; Ho et al., 2020), then\nplug it into Eq. (2) to obtain an empirical estimate of the PF\nODE, which takes the form of\ndx\nt\ndt\n“  ́ts\nφ\npx\nt\n,tq.(3)\nWe call Eq. (3) theempirical PF ODE. Next, we sample\nˆ\nx\nT\n„π“Np0,T\n2\nIq\nto initialize the empirical PF ODE\nand solve it backwards in time with any numerical ODE\nsolver, such as Euler (Song et al., 2020; 2021) and Heun\nsolvers (Karras et al., 2022), to obtain the solution trajectory\nt\nˆ\nx\nt\nu\ntPr0,Ts\n.   The resulting\nˆ\nx\n0\ncan then  be viewed as an\napproximate sample from the data distributionp\ndata\npxq. To\navoid numerical instability, one typically stops the solver\natt“ε,  whereεis a fixed small positive number,  and\naccepts\nˆ\nx\nε\nas the approximate sample.  Following Karras\net al. (2022), we rescale image pixel values tor ́1,1s, and\nsetT“80,ε“0.002.\n2",
    "Consistency Models\nFigure 2: Consistency models are trained to map points on\nany trajectory of the PF ODE to the trajectory’s origin.\nDiffusion models are bottlenecked by their slow sampling\nspeed.  Clearly, using ODE solvers for sampling requires\niterative evaluations of the score models\nφ\npx,tq, which is\ncomputationally costly. Existing methods for fast sampling\ninclude faster numerical ODE solvers (Song et al., 2020;\nZhang & Chen, 2022; Lu et al., 2022; Dockhorn et al., 2022),\nand distillation techniques (Luhman & Luhman, 2021; Sali-\nmans & Ho, 2022; Meng et al., 2022; Zheng et al., 2022).\nHowever, ODE solvers still need more than 10 evaluation\nsteps to generate competitive samples.  Most distillation\nmethods like Luhman & Luhman (2021) and Zheng et al.\n(2022) rely on collecting a large dataset of samples from\nthe diffusion model prior to distillation, which itself is com-\nputationally expensive.  To our best knowledge, the only\ndistillation approach that does not suffer from this drawback\nis progressive distillation (PD, Salimans & Ho (2022)), with\nwhich we compare consistency models extensively in our\nexperiments.\n3. Consistency Models\nWe propose consistency models, a new type of models that\nsupport single-step generation at the core of its design, while\nstill allowing iterative generation for trade-offs between sam-\nple quality and compute, and zero-shot data editing. Consis-\ntency models can be trained in either the distillation mode or\nthe isolation mode. In the former case, consistency models\ndistill the knowledge of pre-trained diffusion models into a\nsingle-step sampler, significantly improving other distilla-\ntion approaches in sample quality, while allowing zero-shot\nimage editing applications.  In the latter case, consistency\nmodels are trained in isolation, with no dependence on pre-\ntrained diffusion models. This makes them an independent\nnew class of generative models.\nBelow we introduce the definition, parameterization, and\nsampling of consistency models, plus a brief discussion on\ntheir applications to zero-shot data editing.\nDefinitionGiven a solution trajectorytx\nt\nu\ntPrε,Ts\nof the\nPF ODE in Eq. (2), we define theconsistency functionas\nf:px\nt\n,tq ÞÑx\nε\n. A consistency function has the property\nofself-consistency: its outputs are consistent for arbitrary\npairs ofpx\nt\n,tqthat belong to the same PF ODE trajectory,\ni.e.,fpx\nt\n,tq “fpx\nt\n1\n,t\n1\nq\nfor allt,t\n1\nP rε,Ts. As illustrated\nin Fig. 2, the goal of aconsistency model, symbolized as\nf\nθ\n, is to estimate this consistency functionffrom data by\nlearning to enforce the self-consistency property (details\nin Sections 4 and 5). Note that a similar definition is used\nfor neural flows (Bilo\nˇ\ns et al., 2021) in the context of neural\nODEs (Chen et al., 2018). Compared to neural flows, how-\never, we do not enforce consistency models to be invertible.\nParameterizationFor any consistency functionfp ̈, ̈q, we\nhavefpx\nε\n,εq “x\nε\n,i.e.,fp ̈,εqis an identity function. We\ncall this constraint theboundary condition. All consistency\nmodels have to meet this boundary condition, as it plays a\ncrucial role in the successful training of consistency models.\nThis boundary condition is also the most confining archi-\ntectural constraint on consistency models. For consistency\nmodels based on deep neural networks,  we discuss two\nways to implement this boundary conditionalmost for free.\nSuppose we have a free-form deep neural networkF\nθ\npx,tq\nwhose output has the same dimensionality asx.  The first\nway is to simply parameterize the consistency model as\nf\nθ\npx,tq “\n#\nxt“ε\nF\nθ\npx,tqtP pε,Ts\n.(4)\nThe second method is to parameterize the consistency model\nusing skip connections, that is,\nf\nθ\npx,tq “c\nskip\nptqx`c\nout\nptqF\nθ\npx,tq,(5)\nwherec\nskip\nptqandc\nout\nptqare  differentiable  functions\nsuch  thatc\nskip\npεq “1,  andc\nout\npεq “0.This  way,\nthe  consistency  model  is  differentiable  att“εif\nF\nθ\npx,tq,c\nskip\nptq,c\nout\nptqare all differentiable, which is criti-\ncal for training continuous-time consistency models (Appen-\ndices B.1 and B.2). The parameterization in Eq. (5) bears\nstrong resemblance to many successful diffusion models\n(Karras et al., 2022; Balaji et al., 2022), making it easier to\nborrow powerful diffusion model architectures for construct-\ning consistency models.  We therefore follow the second\nparameterization in all experiments.\nSamplingWith a well-trained consistency modelf\nθ\np ̈, ̈q,\nwe can generate samples by sampling from the initial dis-\ntribution\nˆ\nx\nT\n„Np0,T\n2\nIqand then evaluating the consis-\ntency model for\nˆ\nx\nε\n“f\nθ\np\nˆ\nx\nT\n,Tq.  This involves only one\nforward pass through the consistency model and therefore\ngenerates samples in a single step.  Importantly, one can\nalso evaluate the consistency model multiple times by al-\nternating denoising and noise injection steps for improved\nsample quality. Summarized in Algorithm 1, thismultistep\nsampling procedure provides the flexibility to trade com-\npute for sample quality. It also has important applications\nin zero-shot data editing.  In practice, we find time points\n3",
    "Consistency Models\nAlgorithm 1Multistep Consistency Sampling\nInput:Consistency  modelf\nθ\np ̈, ̈q,  sequence  of  time\npointsτ\n1\nąτ\n2\ną  ̈ ̈ ̈ ąτ\nN ́1\n, initial noise\nˆ\nx\nT\nxÐf\nθ\np\nˆ\nx\nT\n,Tq\nforn“1toN ́1do\nSamplez„Np0,Iq\nˆ\nx\nτ\nn\nÐx`\na\nτ\n2\nn\n ́ε\n2\nz\nxÐf\nθ\np\nˆ\nx\nτ\nn\n,τ\nn\nq\nend for\nOutput:x\ntτ\n1\n,τ\n2\n, ̈ ̈ ̈,τ\nN ́1\nuin Algorithm 1 with a greedy algorithm,\nwhere the time points are pinpointed one at a time using\nternary search to optimize the FID of samples obtained from\nAlgorithm 1. This assumes that given prior time points, the\nFID is a unimodal function of the next time point. We find\nthis assumption to hold empirically in our experiments, and\nleave the exploration of better strategies as future work.\nZero-Shot Data EditingSimilar to diffusion models, con-\nsistency models enable various data editing and manipu-\nlation  applications  in  zero  shot;  they  do  not  require  ex-\nplicit training to perform these tasks. For example, consis-\ntency models define a one-to-one mapping from a Gaussian\nnoise vector to a data sample.  Similar to latent variable\nmodels like GANs, VAEs, and normalizing flows, consis-\ntency models can easily interpolate between samples by\ntraversing the latent space (Fig. 11). As consistency models\nare trained to recoverx\nε\nfrom any noisy inputx\nt\nwhere\ntP rε,Ts, they can perform denoising for various noise\nlevels (Fig. 12).  Moreover, the multistep generation pro-\ncedure in Algorithm 1 is useful for solving certain inverse\nproblems in zero shot by using an iterative replacement pro-\ncedure similar to that of diffusion models (Song & Ermon,\n2019; Song et al., 2021; Ho et al., 2022b).  This enables\nmany applications in the context of image editing, including\ninpainting (Fig. 10), colorization (Fig. 8), super-resolution\n(Fig. 6b) and stroke-guided image editing (Fig. 13) as in\nSDEdit (Meng et al., 2021).   In Section 6.3,  we empiri-\ncally demonstrate the power of consistency models on many\nzero-shot image editing tasks.\n4.Training Consistency Models via Distillation\nWe present our first method for training consistency mod-\nels based on distilling a pre-trained score models\nφ\npx,tq.\nOur discussion revolves around the empirical PF ODE in\nEq.  (3),  obtained  by  plugging  the  score  models\nφ\npx,tq\ninto the PF ODE. Consider discretizing the time horizon\nrε,TsintoN ́1sub-intervals, with boundariest\n1\n“εă\nt\n2\nă  ̈ ̈ ̈ ăt\nN\n“T\n.   In  practice,  we  follow  Karras\net al. (2022) to determine the boundaries with the formula\nt\ni\n“ pε\n1{ρ\n`\ni ́1\n{N ́1pT\n1{ρ\n ́ε\n1{ρ\nqq\nρ\n, whereρ“7. When\nNis sufficiently large, we can obtain an accurate estimate\nofx\nt\nn\nfromx\nt\nn`1\nby running one discretization step of a\nnumerical ODE solver. This estimate, which we denote as\nˆ\nx\nφ\nt\nn\n, is defined by\nˆ\nx\nφ\nt\nn\n:\n“x\nt\nn`1\n`pt\nn\n ́t\nn`1\nqΦpx\nt\nn`1\n,t\nn`1\n;φq,(6)\nwhereΦp ̈ ̈ ̈;φqrepresents the update function of a one-\nstep  ODE  solver  applied  to  the  empirical  PF  ODE.  For\nexample, when using the Euler solver, we haveΦpx,t;φq “\n ́ts\nφ\npx,tqwhich corresponds to the following update rule\nˆ\nx\nφ\nt\nn\n“x\nt\nn`1\n ́pt\nn\n ́t\nn`1\nqt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq.\nFor simplicity, we only consider one-step ODE solvers in\nthis work. It is straightforward to generalize our framework\nto multistep ODE solvers and we leave it as future work.\nDue to the connection between the PF ODE in Eq. (2) and\nthe SDE in Eq. (1) (see Section 2), one can sample along the\ndistribution of ODE trajectories by first samplingx„p\ndata\n,\nthen adding Gaussian noise tox. Specifically, given a data\npointx,  we  can  generate  a  pair  of  adjacent  data  points\np\nˆ\nx\nφ\nt\nn\n,x\nt\nn`1\nq\non the PF ODE trajectory efficiently by sam-\nplingxfrom the dataset, followed by samplingx\nt\nn`1\nfrom\nthe transition density of the SDENpx,t\n2\nn`1\nIq, and then\ncomputing\nˆ\nx\nφ\nt\nn\nusing one discretization step of the numeri-\ncal ODE solver according to Eq. (6). Afterwards, we train\nthe consistency model by minimizing its output differences\non the pairp\nˆ\nx\nφ\nt\nn\n,x\nt\nn`1\nq. This motivates our followingcon-\nsistency distillationloss for training consistency models.\nDefinition 1.The consistency distillation loss is defined as\nL\nN\nCD\npθ,θ\n ́\n;φq\n:\n“\nErλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqs,(7)\nwhere the expectation is taken with respect tox„p\ndata\n,n„\nUJ1,N ́1K, andx\nt\nn`1\n„Npx;t\n2\nn`1\nIq. HereUJ1,N ́1K\ndenotes the uniform distribution overt1,2, ̈ ̈ ̈,N ́1u,\nλp ̈q PR\n`\nis a positive weighting function,\nˆ\nx\nφ\nt\nn\nis given by\nEq.(6),θ\n ́\ndenotes a running average of the past values of\nθduring the course of optimization, anddp ̈, ̈qis a metric\nfunction that satisfies@x,y:dpx,yq ě0anddpx,yq “0\nif and only ifx“y.\nUnless otherwise stated,  we adopt the notations in Defi-\nnition 1 throughout this paper, and useEr ̈sto denote the\nexpectation over all random variables. In our experiments,\nwe consider the squaredℓ\n2\ndistancedpx,yq “ }x ́y}\n2\n2\n,ℓ\n1\ndistancedpx,yq “ }x ́y}\n1\n, and the Learned Perceptual\nImage Patch Similarity (LPIPS, Zhang et al. (2018)).  We\nfindλpt\nn\nq ”1performs well across all tasks and datasets.\nIn practice, we minimize the objective by stochastic gradient\ndescent on the model parametersθ, while updatingθ\n ́\nwith\nexponential moving average (EMA). That is, given a decay\n4",
    "Consistency Models\nAlgorithm 2Consistency Distillation (CD)\nInput:datasetD, initial model parameterθ, learning rate\nη, ODE solverΦp ̈, ̈;φq,dp ̈, ̈q,λp ̈q, andμ\nθ\n ́\nÐθ\nrepeat\nSamplex„Dandn„UJ1,N ́1K\nSamplex\nt\nn`1\n„Npx;t\n2\nn`1\nIq\nˆ\nx\nφ\nt\nn\nÐx\nt\nn`1\n`pt\nn\n ́t\nn`1\nqΦpx\nt\nn`1\n,t\nn`1\n;φq\nLpθ,θ\n ́\n;φq Ð\nλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\nθÐθ ́η∇\nθ\nLpθ,θ\n ́\n;φq\nθ\n ́\nÐstopgradpμθ\n ́\n`p1 ́μqθ)\nuntilconvergence\nrate0ďμă1, we perform the following update after each\noptimization step:\nθ\n ́\nÐstopgradpμθ\n ́\n`p1 ́μqθq.(8)\nThe  overall  training  procedure  is  summarized  in  Algo-\nrithm 2. In alignment with the convention in deep reinforce-\nment learning (Mnih et al., 2013; 2015; Lillicrap et al., 2015)\nand momentum based contrastive learning (Grill et al., 2020;\nHe et al., 2020), we refer tof\nθ\n ́\nas the “target network”,\nandf\nθ\nas the “online network”. We find that compared to\nsimply settingθ\n ́\n“θ, the EMA update and “stopgrad”\noperator in Eq. (8) can greatly stabilize the training process\nand improve the final performance of the consistency model.\nBelow we provide a theoretical justification for consistency\ndistillation based on asymptotic analysis.\nTheorem 1.Let∆t\n:\n“max\nnPJ1,N ́1K\nt|t\nn`1\n ́t\nn\n|u, and\nfp ̈, ̈;φqbe the consistency function of the empirical PF\nODE in Eq.(3). Assumef\nθ\nsatisfies the Lipschitz condition:\nthere existsLą0such that for alltP rε,Ts,x, andy,\nwe have∥f\nθ\npx,tq ́f\nθ\npy,tq∥\n2\nďL∥x ́y∥\n2\n.  Assume\nfurther that for allnPJ1,N ́1K, the ODE solver called\natt\nn`1\nhas local error uniformly bounded byOppt\nn`1\n ́\nt\nn\nq\np`1\nqwithpě1. Then, ifL\nN\nCD\npθ,θ;φq “0, we have\nsup\nn,x\n}f\nθ\npx,t\nn\nq ́fpx,t\nn\n;φq}\n2\n“Opp∆tq\np\nq.\nProof.\nThe proof is based on induction and parallels the\nclassic proof of global error bounds for numerical ODE\nsolvers (S\n ̈\nuli & Mayers, 2003). We provide the full proof in\nAppendix A.2.\nSinceθ\n ́\nis a running average of the history ofθ, we have\nθ\n ́\n“θwhen the optimization of Algorithm 2 converges.\nThat is, the target and online consistency models will eventu-\nally match each other. If the consistency model additionally\nachieves zero consistency distillation loss, then Theorem 1\nAlgorithm 3Consistency Training (CT)\nInput:datasetD, initial model parameterθ, learning rate\nη, step scheduleNp ̈q, EMA decay rate scheduleμp ̈q,\ndp ̈, ̈q, andλp ̈q\nθ\n ́\nÐθandkÐ0\nrepeat\nSamplex„D, andn„UJ1,Npkq ́1K\nSamplez„Np0,Iq\nLpθ,θ\n ́\nq Ð\nλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn\nz,t\nn\nqq\nθÐθ ́η∇\nθ\nLpθ,θ\n ́\nq\nθ\n ́\nÐstopgradpμpkqθ\n ́\n`p1 ́μpkqqθq\nkÐk`1\nuntilconvergence\nimplies that, under some regularity conditions, the estimated\nconsistency model can become arbitrarily accurate, as long\nas the step size of the ODE solver is sufficiently small. Im-\nportantly, our boundary conditionf\nθ\npx,εq ”xprecludes\nthe trivial solutionf\nθ\npx,tq ”0from arising in consistency\nmodel training.\nThe consistency distillation lossL\nN\nCD\npθ,θ\n ́\n;φq\ncan be ex-\ntended to hold for infinitely many time steps (NÑ 8) if\nθ\n ́\n“θorθ\n ́\n“stopgradpθq. The resulting continuous-\ntime loss functions do not require specifyingNnor the time\nstepstt\n1\n,t\n2\n, ̈ ̈ ̈,t\nN\nu. Nonetheless, they involve Jacobian-\nvector products and require forward-mode automatic dif-\nferentiation for efficient implementation, which may not\nbe well-supported in some deep learning frameworks. We\nprovide these continuous-time distillation loss functions in\nTheorems 3 to 5, and relegate details to Appendix B.1.\n5. Training Consistency Models in Isolation\nConsistency models can be trained without relying on any\npre-trained diffusion models.   This differs from existing\ndiffusion distillation techniques, making consistency models\na new independent family of generative models.\nRecall that  in consistency  distillation,  we  rely on  a pre-\ntrained score models\nφ\npx,tqto approximate the ground\ntruth score function∇logp\nt\npxq.  It turns out that we can\navoid this pre-trained score model altogether by leveraging\nthe following unbiased estimator (Lemma 1 in Appendix A):\n∇logp\nt\npx\nt\nq “  ́E\n„\nx\nt\n ́x\nt\n2\nˇ\nˇ\nˇ\nˇ\nx\nt\nȷ\n,\nwherex„p\ndata\nandx\nt\n„Npx;t\n2\nIq. That is, givenxand\nx\nt\n, we can estimate∇logp\nt\npx\nt\nqwith ́px\nt\n ́xq{t\n2\n.\nThis unbiased estimate suffices to replace the pre-trained\ndiffusion model in consistency distillation when using the\nEuler method as the ODE solver in the limit ofNÑ 8, as\n5",
    "Consistency Models\njustified by the following result.\nTheorem 2.Let∆t\n:\n“max\nnPJ1,N ́1K\nt|t\nn`1\n ́t\nn\n|u.  As-\nsumedandf\nθ\n ́\nare both twice continuously differentiable\nwith bounded second derivatives, the weighting function\nλp ̈qis  bounded,  andEr∥∇logp\nt\nn\npx\nt\nn\nq∥\n2\n2\ns ă 8.   As-\nsume further that we use the Euler ODE solver, and the\npre-trained  score  model  matches  the  ground  truth,i.e.,\n@tP rε,Ts:s\nφ\npx,tq ”∇logp\nt\npxq. Then,\nL\nN\nCD\npθ,θ\n ́\n;φq “L\nN\nCT\npθ,θ\n ́\nq`op∆tq,(9)\nwhere the expectation is taken with respect tox„p\ndata\n,n„\nUJ1,N ́1K, andx\nt\nn`1\n„Npx;t\n2\nn`1\nIq. The consistency\ntraining objective, denoted byL\nN\nCT\npθ,θ\n ́\nq, is defined as\nErλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn\nz,t\nn\nqqs\n,(10)\nwherez„Np0,Iq.  Moreover,L\nN\nCT\npθ,θ\n ́\nq ěOp∆tq\nif\ninf\nN\nL\nN\nCD\npθ,θ\n ́\n;φq ą0.\nProof.\nThe proof is based on Taylor series expansion and\nproperties of score functions (Lemma 1). A complete proof\nis provided in Appendix A.3.\nWe refer to Eq. (10) as theconsistency training(CT) loss.\nCrucially,Lpθ,θ\n ́\nqonly depends on the online network\nf\nθ\n, and the target networkf\nθ\n ́\n, while being completely\nagnostic to diffusion model parametersφ. The loss function\nLpθ,θ\n ́\nq ěOp∆tqdecreases  at  a  slower  rate  than  the\nremainderop∆tqand thus will dominate the loss in Eq. (9)\nasNÑ 8and∆tÑ0.\nFor improved practical performance, we propose to progres-\nsively increaseNduring training according to a schedule\nfunctionNp ̈q. The intuition (cf., Fig. 3d) is that the consis-\ntency training loss has less “variance” but more “bias” with\nrespect to the underlying consistency distillation loss (i.e.,\nthe left-hand side of Eq. (9)) whenNis small (i.e.,∆tis\nlarge), which facilitates faster convergence at the beginning\nof training. On the contrary, it has more “variance” but less\n“bias” whenNis large (i.e.,∆tis small), which is desirable\nwhen closer to the end of training.  For best performance,\nwe also find thatμshould change along withN, according\nto a schedule functionμp ̈q.  The full algorithm of consis-\ntency training is provided in Algorithm 3, and the schedule\nfunctions used in our experiments are given in Appendix C.\nSimilar to consistency distillation, the consistency training\nlossL\nN\nCT\npθ,θ\n ́\nq\ncan be extended to hold in continuous time\n(i.e.,NÑ 8) ifθ\n ́\n“stopgradpθq, as shown in Theo-\nrem 6. This continuous-time loss function does not require\nschedule functions forNorμ, but requires forward-mode\nautomatic differentiation for efficient implementation. Un-\nlike the discrete-time CT loss, there is no undesirable “bias”\nassociated with the continuous-time objective, as we effec-\ntively take∆tÑ0in Theorem 2. We relegate more details\nto Appendix B.2.\n6. Experiments\nWe employ consistency distillation and consistency train-\ning  to  learn  consistency  models  on  real  image  datasets,\nincluding CIFAR-10 (Krizhevsky et al., 2009), ImageNet\n64ˆ64(Deng et al., 2009), LSUN Bedroom256ˆ256,\nand LSUN Cat256ˆ256(Yu et al., 2015).  Results are\ncompared according to Fr\n ́\nechet Inception Distance (FID,\nHeusel et al. (2017), lower is better), Inception Score (IS,\nSalimans et al. (2016), higher is better), Precision (Prec.,\nKynk\n ̈\na\n ̈\nanniemi et al. (2019), higher is better), and Recall\n(Rec., Kynk\n ̈\na\n ̈\nanniemi et al. (2019), higher is better). Addi-\ntional experimental details are provided in Appendix C.\n6.1. Training Consistency Models\nWe perform a series of experiments on CIFAR-10 to under-\nstand the effect of various hyperparameters on the perfor-\nmance of consistency models trained by consistency distil-\nlation (CD) and consistency training (CT). We first focus on\nthe effect of the metric functiondp ̈, ̈q, the ODE solver, and\nthe number of discretization stepsNin CD, then investigate\nthe effect of the schedule functionsNp ̈qandμp ̈qin CT.\nTo set up our experiments for CD, we consider the squared\nℓ\n2\ndistancedpx,yq “ }x ́y}\n2\n2\n,ℓ\n1\ndistancedpx,yq “\n}x ́y}\n1\n, and the Learned Perceptual Image Patch Simi-\nlarity (LPIPS, Zhang et al. (2018)) as the metric function.\nFor the ODE solver, we compare Euler’s forward method\nand Heun’s second order method as detailed in Karras et al.\n(2022). For the number of discretization stepsN, we com-\npareNP t9,12,18,36,50,60,80,120u.  All consistency\nmodels trained by CD in our experiments are initialized with\nthe corresponding pre-trained diffusion models, whereas\nmodels trained by CT are randomly initialized.\nAs visualized in Fig. 3a, the optimal metric for CD is LPIPS,\nwhich outperforms bothℓ\n1\nandℓ\n2\nby a large margin over\nall training iterations.  This is expected as the outputs of\nconsistency models are images on CIFAR-10, and LPIPS is\nspecifically designed for measuring the similarity between\nnatural images. Next, we investigate which ODE solver and\nwhich discretization stepNwork the best for CD. As shown\nin Figs. 3b and 3c, Heun ODE solver andN“18are the\nbest choices.  Both are in line with the recommendation\nof Karras et al. (2022) despite the fact that we are train-\ning consistency models, not diffusion models.  Moreover,\nFig. 3b shows that with the sameN, Heun’s second order\nsolver uniformly outperforms Euler’s first order solver. This\ncorroborates with Theorem 1, which states that the optimal\nconsistency models trained by higher order ODE solvers\nhave smaller estimation errors with the sameN. The results\nof Fig. 3c also indicate that onceNis sufficiently large, the\nperformance of CD becomes insensitive toN. Given these\ninsights, we hereafter use LPIPS and Heun ODE solver for\nCD unless otherwise stated.  ForNin CD, we follow the\n6",
    "Consistency Models\n(a) Metric functions in CD.(b) Solvers andNin CD.(c)Nwith Heun solver in CD.(d) AdaptiveNandμin CT.\nFigure 3: Various factors that affect consistency distillation (CD) and consistency training (CT) on CIFAR-10. The best\nconfiguration for CD is LPIPS, Heun ODE solver, andN“18. Our adaptive schedule functions forNandμmake CT\nconverge significantly faster than fixing them to be constants during the course of optimization.\n(a) CIFAR-10(b) ImageNet64ˆ64(c) Bedroom256ˆ256(d) Cat256ˆ256\nFigure 4:  Multistep image generation with consistency distillation (CD). CD outperforms progressive distillation (PD)\nacross all datasets and sampling steps. The only exception is single-step generation on Bedroom256ˆ256.\nsuggestions in Karras et al. (2022) on CIFAR-10 and Im-\nageNet64ˆ64.  We tuneNseparately on other datasets\n(details in Appendix C).\nDue to the strong connection between CD and CT, we adopt\nLPIPS for our CT experiments throughout this paper. Unlike\nCD, there is no need for using Heun’s second order solver\nin CT as the loss function does not rely on any particular\nnumerical ODE solver. As demonstrated in Fig. 3d, the con-\nvergence of CT is highly sensitive toN—smallerNleads\nto faster convergence but worse samples, whereas larger\nNleads to slower convergence but better samples upon\nconvergence. This matches our analysis in Section 5, and\nmotivates our practical choice of progressively growingN\nandμfor CT to balance the trade-off between convergence\nspeed and sample quality.  As shown in Fig. 3d, adaptive\nschedules ofNandμsignificantly improve the convergence\nspeed and sample quality of CT. In our experiments, we\ntune the schedulesNp ̈qandμp ̈qseparately for images of\ndifferent resolutions, with more details in Appendix C.\n6.2. Few-Step Image Generation\nDistillationIn current literature, the most directly compara-\nble approach to our consistency distillation (CD) is progres-\nsive distillation (PD, Salimans & Ho (2022)); both are thus\nfar the only distillation approaches thatdo not construct\nsynthetic data before distillation. In stark contrast, other dis-\ntillation techniques, such as knowledge distillation (Luhman\n& Luhman, 2021) and DFNO (Zheng et al., 2022), have to\nprepare a large synthetic dataset by generating numerous\nsamples from the diffusion model with expensive numerical\nODE/SDE solvers. We perform comprehensive comparison\nfor PD and CD on CIFAR-10, ImageNet64ˆ64, and LSUN\n256ˆ256, with all results reported in Fig. 4. All methods\ndistill from an EDM (Karras et al., 2022) model that we pre-\ntrained in-house. We note that across all sampling iterations,\nusing the LPIPS metric uniformly improves PD compared\nto the squaredℓ\n2\ndistance in the original paper of Salimans\n& Ho (2022).  Both PD and CD improve as we take more\nsampling steps.  We find that CD uniformly outperforms\nPD across all datasets, sampling steps, and metric functions\nconsidered, except for single-step generation on Bedroom\n256ˆ256, where CD withℓ\n2\nslightly underperforms PD\nwithℓ\n2\n. As shown in Table 1, CD even outperforms distilla-\ntion approaches that require synthetic dataset construction,\nsuch as Knowledge Distillation (Luhman & Luhman, 2021)\nand DFNO (Zheng et al., 2022).\nDirect GenerationIn Tables 1 and 2,  we compare the\nsample quality of consistency training (CT) with other gen-\nerative models using one-step and two-step generation. We\nalso include PD and CD results for reference. Both tables re-\nport PD results obtained from theℓ\n2\nmetric function, as this\nis the default setting used in the original paper of Salimans\n7",
    "Consistency Models\nTable 1: Sample quality on CIFAR-10.\n ̊\nMethods that require\nsynthetic data construction for distillation.\nMETHODNFE (Ó)FID (Ó)IS (Ò)\nDiffusion + Samplers\nDDIM (Song et al., 2020)504.67\nDDIM (Song et al., 2020)206.84\nDDIM (Song et al., 2020)108.23\nDPM-solver-2 (Lu et al., 2022)105.94\nDPM-solver-fast (Lu et al., 2022)104.70\n3-DEIS (Zhang & Chen, 2022)104.17\nDiffusion + Distillation\nKnowledge Distillation\n ̊\n(Luhman & Luhman, 2021)19.36\nDFNO\n ̊\n(Zheng et al., 2022)14.12\n1-Rectified Flow (+distill)\n ̊\n(Liu et al., 2022)16.189.08\n2-Rectified Flow (+distill)\n ̊\n(Liu et al., 2022)14.859.01\n3-Rectified Flow (+distill)\n ̊\n(Liu et al., 2022)15.218.79\nPD (Salimans & Ho, 2022)18.348.69\nCD13.559.48\nPD (Salimans & Ho, 2022)25.589.05\nCD22.939.75\nDirect Generation\nBigGAN (Brock et al., 2019)114.79.22\nDiffusion GAN (Xiao et al., 2022)114.68.93\nAutoGAN (Gong et al., 2019)112.48.55\nE2GAN (Tian et al., 2020)111.38.51\nViTGAN (Lee et al., 2021)16.669.30\nTransGAN (Jiang et al., 2021)19.269.05\nStyleGAN2-ADA (Karras et al., 2020)12.929.83\nStyleGAN-XL (Sauer et al., 2022)11.85\nScore SDE (Song et al., 2021)20002.209.89\nDDPM (Ho et al., 2020)10003.179.46\nLSGM (Vahdat et al., 2021)1472.10\nPFGM (Xu et al., 2022)1102.359.68\nEDM (Karras et al., 2022)352.049.84\n1-Rectified Flow (Liu et al., 2022)13781.13\nGlow (Kingma & Dhariwal, 2018)148.93.92\nResidual Flow (Chen et al., 2019)146.4\nGLFlow (Xiao et al., 2019)144.6\nDenseFlow (Grci\n ́\nc et al., 2021)134.9\nDC-VAE (Parmar et al., 2021)117.98.20\nCT18.708.49\nCT25.838.85\nTable 2: Sample quality on ImageNet64ˆ64, and LSUN\nBedroom & Cat256ˆ256.\n:\nDistillation techniques.\nMETHODNFE (Ó)FID (Ó)Prec. (Ò)Rec. (Ò)\nImageNet64ˆ64\nPD\n:\n(Salimans & Ho, 2022)115.390.590.62\nDFNO\n:\n(Zheng et al., 2022)18.35\nCD\n:\n16.200.680.63\nPD\n:\n(Salimans & Ho, 2022)28.950.630.65\nCD\n:\n24.700.690.64\nADM (Dhariwal & Nichol, 2021)2502.070.740.63\nEDM (Karras et al., 2022)792.440.710.67\nBigGAN-deep (Brock et al., 2019)14.060.790.48\nCT113.00.710.47\nCT211.10.690.56\nLSUN Bedroom256ˆ256\nPD\n:\n(Salimans & Ho, 2022)116.920.470.27\nPD\n:\n(Salimans & Ho, 2022)28.470.560.39\nCD\n:\n17.800.660.34\nCD\n:\n25.220.680.39\nDDPM (Ho et al., 2020)10004.890.600.45\nADM (Dhariwal & Nichol, 2021)10001.900.660.51\nEDM (Karras et al., 2022)793.570.660.45\nPGGAN (Karras et al., 2018)18.34\nPG-SWGAN (Wu et al., 2019)18.0\nTDPM (GAN) (Zheng et al., 2023)15.24\nStyleGAN2 (Karras et al., 2020)12.350.590.48\nCT116.00.600.17\nCT27.850.680.33\nLSUN Cat256ˆ256\nPD\n:\n(Salimans & Ho, 2022)129.60.510.25\nPD\n:\n(Salimans & Ho, 2022)215.50.590.36\nCD\n:\n111.00.650.36\nCD\n:\n28.840.660.40\nDDPM (Ho et al., 2020)100017.10.530.48\nADM (Dhariwal & Nichol, 2021)10005.570.630.52\nEDM (Karras et al., 2022)796.690.700.43\nPGGAN (Karras et al., 2018)137.5\nStyleGAN2 (Karras et al., 2020)17.250.580.43\nCT120.70.560.23\nCT211.70.630.36\nFigure 5: Samples generated by EDM (top), CT + single-step generation (middle), and CT + 2-step generation (Bottom). All\ncorresponding images are generated from the same initial noise.\n8",
    "Consistency Models\n(a)Left: The gray-scale image.Middle: Colorized images.Right: The ground-truth image.\n(b)Left: The downsampled image (32ˆ32).Middle: Full resolution images (256ˆ256).Right: The ground-truth image (256ˆ256).\n(c)Left: A stroke input provided by users.Right: Stroke-guided image generation.\nFigure 6: Zero-shot image editing with a consistency model trained by consistency distillation on LSUN Bedroom256ˆ256.\n& Ho (2022). For fair comparison, we ensure PD and CD\ndistill the same EDM models. In Tables 1 and 2, we observe\nthat CT outperforms existing single-step, non-adversarial\ngenerative models,i.e., VAEs and normalizing flows, by a\nsignificant margin on CIFAR-10.  Moreover,CT achieves\ncomparable quality to one-step samples from PD without\nrelying on distillation. In Fig. 5, we provide EDM samples\n(top), single-step CT samples (middle), and two-step CT\nsamples (bottom). In Appendix E, we show additional sam-\nples for both CD and CT in Figs. 14 to 21. Importantly,all\nsamples obtained from the same initial noise vector share\nsignificant structural similarity, even though CT and EDM\nmodels are trained independently from one another.  This\nindicates that CT is less likely to suffer from mode collapse,\nas EDMs do not.\n6.3. Zero-Shot Image Editing\nSimilar to diffusion models, consistency models allow zero-\nshot image editing by modifying the multistep sampling\nprocess in Algorithm 1.   We demonstrate this capability\nwith a consistency model trained on the LSUN bedroom\ndataset using consistency distillation. In Fig. 6a, we show\nsuch a consistency model can colorize gray-scale bedroom\nimages at test time, even though it has never been trained\non colorization tasks.  In Fig. 6b, we show the same con-\nsistency model can generate high-resolution images from\nlow-resolution inputs.  In Fig. 6c, we additionally demon-\nstrate that it can generate images based on stroke inputs cre-\nated by humans, as in SDEdit for diffusion models (Meng\net al., 2021).   Again,  this editing capability is zero-shot,\nas  the  model  has  not  been  trained  on  stroke  inputs.   In\nAppendix  D,  we  additionally  demonstrate  the  zero-shot\ncapability of consistency models on inpainting (Fig. 10),\ninterpolation (Fig. 11) and denoising (Fig. 12), with more\nexamples on colorization (Fig. 8), super-resolution (Fig. 9)\nand stroke-guided image generation (Fig. 13).\n7. Conclusion\nWe have introduced consistency models, a type of generative\nmodels that are specifically designed to support one-step\nand few-step generation. We have empirically demonstrated\nthat our consistency distillation method outshines the exist-\ning distillation techniques for diffusion models on multiple\nimage benchmarks and small sampling iterations. Further-\nmore, as a standalone generative model, consistency models\ngenerate better samples than existing single-step genera-\ntion models except for GANs. Similar to diffusion models,\nthey also allow zero-shot image editing applications such as\ninpainting, colorization, super-resolution, denoising, inter-\npolation, and stroke-guided image generation.\nIn addition, consistency models share striking similarities\nwith techniques employed in other fields, including deep\nQ-learning (Mnih et al., 2015) and momentum-based con-\ntrastive learning (Grill et al., 2020; He et al., 2020).  This\noffers exciting prospects for cross-pollination of ideas and\nmethods among these diverse fields.\nAcknowledgements\nWe thank Alex Nichol for reviewing the manuscript and\nproviding valuable feedback, Chenlin Meng for providing\nstroke inputs needed in our stroke-guided image generation\nexperiments, and the OpenAI Algorithms team.\n9",
    "Consistency Models\nReferences\nBalaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis,\nK., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Kar-\nras, T., and Liu, M.-Y.  ediff-i: Text-to-image diffusion\nmodels with ensemble of expert denoisers.arXiv preprint\narXiv:2211.01324, 2022.\nBilo\nˇ\ns, M., Sommer, J., Rangapuram, S. S., Januschowski, T.,\nand G\n ̈\nunnemann, S. Neural flows: Efficient alternative to\nneural odes.Advances in Neural Information Processing\nSystems, 34:21325–21337, 2021.\nBrock,  A.,  Donahue,  J.,  and Simonyan,  K.   Large scale\nGAN training for high fidelity natural image synthesis. In\nInternational Conference on Learning Representations,\n2019.  URLhttps://openreview.net/forum?\nid=B1xsqj09Fm.\nChen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and\nChan, W. Wavegrad: Estimating gradients for waveform\ngeneration.   InInternational Conference on Learning\nRepresentations (ICLR), 2021.\nChen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,\nD. K.  Neural Ordinary Differential Equations.  InAd-\nvances  in  neural  information  processing  systems,  pp.\n6571–6583, 2018.\nChen, R. T., Behrmann, J., Duvenaud, D. K., and Jacobsen,\nJ.-H. Residual flows for invertible generative modeling.\nInAdvances in Neural Information Processing Systems,\npp. 9916–9926, 2019.\nChung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye,\nJ. C.  Diffusion posterior sampling for general noisy in-\nverse problems. InInternational Conference on Learning\nRepresentations, 2023. URLhttps://openreview.\nnet/forum?id=OnD9zGAGT0k.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248–255. Ieee, 2009.\nDhariwal, P. and Nichol, A.  Diffusion models beat gans\non image synthesis.Advances in Neural Information\nProcessing Systems (NeurIPS), 2021.\nDinh, L., Krueger, D., and Bengio, Y.  NICE: Non-linear\nindependent components estimation.International Con-\nference  in  Learning  Representations  Workshop  Track,\n2015.\nDinh, L., Sohl-Dickstein, J., and Bengio, S.  Density es-\ntimation using real NVP.  In5th International Confer-\nence on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings.\nOpenReview.net, 2017. URLhttps://openreview.\nnet/forum?id=HkpbnH9lx.\nDockhorn, T., Vahdat, A., and Kreis, K.  Genie:  Higher-\norder  denoising  diffusion  solvers.arXiv  preprint\narXiv:2210.05475, 2022.\nGong, X., Chang, S., Jiang, Y., and Wang, Z.   Autogan:\nNeural architecture search for generative adversarial net-\nworks.  InProceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 3224–3234, 2019.\nGoodfellow,  I.,  Pouget-Abadie,  J.,  Mirza,  M.,  Xu,  B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY.  Generative adversarial nets.  InAdvances in neural\ninformation processing systems, pp. 2672–2680, 2014.\nGrci\n ́\nc, M., Grubi\nˇ\nsi\n ́\nc, I., and\nˇ\nSegvi\n ́\nc, S. Densely connected\nnormalizing flows.Advances in Neural Information Pro-\ncessing Systems, 34:23968–23982, 2021.\nGrill, J.-B., Strub, F., Altch\n ́\ne, F., Tallec, C., Richemond, P.,\nBuchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z.,\nGheshlaghi Azar, M., et al. Bootstrap your own latent-a\nnew approach to self-supervised learning.Advances in\nneural information processing systems, 33:21271–21284,\n2020.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.   Mo-\nmentum contrast for unsupervised visual representation\nlearning. InProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738,\n2020.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. GANs trained by a two time-scale update\nrule converge to a local Nash equilibrium. InAdvances in\nNeural Information Processing Systems, pp. 6626–6637,\n2017.\nHo, J., Jain, A., and Abbeel, P. Denoising Diffusion Proba-\nbilistic Models.Advances in Neural Information Process-\ning Systems, 33, 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\net al.  Imagen video:  High definition video generation\nwith diffusion models.arXiv preprint arXiv:2210.02303,\n2022a.\nHo, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi,\nM., and Fleet, D. J.  Video diffusion models.  InICLR\nWorkshop on Deep Generative Models for Highly Struc-\ntured  Data,  2022b.    URLhttps://openreview.\nnet/forum?id=BBelR2NdDZ5.\nHyv\n ̈\narinen, A. and Dayan, P. Estimation of non-normalized\nstatistical models by score matching.Journal of Machine\nLearning Research (JMLR), 6(4), 2005.\n10",
    "Consistency Models\nJiang, Y., Chang, S., and Wang, Z.  Transgan:  Two pure\ntransformers can make one strong gan, and that can scale\nup.Advances in Neural Information Processing Systems,\n34:14745–14758, 2021.\nKarras, T., Aila, T., Laine, S., and Lehtinen, J.  Progres-\nsive growing of GANs for improved quality,  stability,\nand variation. InInternational Conference on Learning\nRepresentations, 2018. URLhttps://openreview.\nnet/forum?id=Hk99zCeAb.\nKarras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J.,\nand Aila, T. Analyzing and improving the image quality\nof stylegan. 2020.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models. In\nProc. NeurIPS, 2022.\nKawar,  B.,  Vaksman,  G.,  and Elad,  M.   Snips:  Solving\nnoisy inverse problems stochastically.arXiv preprint\narXiv:2105.14951, 2021.\nKawar, B., Elad, M., Ermon, S., and Song, J.  Denoising\ndiffusion restoration models. InAdvances in Neural In-\nformation Processing Systems, 2022.\nKingma, D. P. and Dhariwal, P.   Glow:  Generative flow\nwith invertible 1x1 convolutions.   In Bengio,  S.,  Wal-\nlach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,\nand Garnett, R. (eds.),Advances in Neural Information\nProcessing Systems 31, pp. 10215–10224. 2018.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes.  InInternational Conference on Learning Repre-\nsentations, 2014.\nKong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro,\nB.  DiffWave:  A Versatile Diffusion Model for Audio\nSynthesis.arXiv preprint arXiv:2009.09761, 2020.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nKynk\n ̈\na\n ̈\nanniemi, T., Karras, T., Laine, S., Lehtinen, J., and\nAila, T. Improved precision and recall metric for assess-\ning generative models.Advances in Neural Information\nProcessing Systems, 32, 2019.\nLee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., and Liu,\nC. Vitgan: Training gans with vision transformers.arXiv\npreprint arXiv:2107.04589, 2021.\nLillicrap,  T. P.,  Hunt,  J. J.,  Pritzel,  A.,  Heess,  N.,  Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D.  Continuous\ncontrol with deep reinforcement learning.arXiv preprint\narXiv:1509.02971, 2015.\nLiu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and\nHan, J. On the variance of the adaptive learning rate and\nbeyond.arXiv preprint arXiv:1908.03265, 2019.\nLiu,  X.,  Gong,  C.,  and  Liu,  Q.   Flow  straight  and  fast:\nLearning to generate and transfer data with rectified flow.\narXiv preprint arXiv:2209.03003, 2022.\nLu,  C.,  Zhou,  Y.,  Bao,  F.,  Chen,  J.,  Li,  C.,  and  Zhu,  J.\nDpm-solver: A fast ode solver for diffusion probabilis-\ntic model sampling in around 10 steps.arXiv preprint\narXiv:2206.00927, 2022.\nLuhman,  E.  and  Luhman,  T.   Knowledge  distillation  in\niterative generative models for improved sampling speed.\narXiv preprint arXiv:2101.02388, 2021.\nMeng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,\nS.  Sdedit:  Image synthesis and editing with stochastic\ndifferential equations.arXiv preprint arXiv:2108.01073,\n2021.\nMeng, C., Gao, R., Kingma, D. P., Ermon, S., Ho, J., and\nSalimans, T. On distillation of guided diffusion models.\narXiv preprint arXiv:2210.03142, 2022.\nMnih,   V.,   Kavukcuoglu,   K.,   Silver,   D.,   Graves,   A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning.arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al.  Human-level control\nthrough deep reinforcement learning.nature, 518(7540):\n529–533, 2015.\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,\nP.,  McGrew,  B.,  Sutskever,  I.,  and  Chen,  M.   Glide:\nTowards  photorealistic  image  generation  and  editing\nwith  text-guided  diffusion  models.arXiv  preprint\narXiv:2112.10741, 2021.\nParmar, G., Li, D., Lee, K., and Tu, Z. Dual contradistinctive\ngenerative autoencoder. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 823–832, 2021.\nPopov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudi-\nnov, M.  Grad-TTS: A diffusion probabilistic model for\ntext-to-speech.arXiv preprint arXiv:2105.06337, 2021.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents.arXiv preprint arXiv:2204.06125, 2022.\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochastic\nbackpropagation and approximate inference in deep gen-\nerative models. InProceedings of the 31st International\nConference on Machine Learning, pp. 1278–1286, 2014.\n11",
    "Consistency Models\nRombach,  R.,  Blattmann,  A.,  Lorenz,  D.,  Esser,  P.,  and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. InProceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n10684–10695, 2022.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S.,\nLopes, R. G., et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding.arXiv preprint\narXiv:2205.11487, 2022.\nSalimans,  T. and Ho,  J.   Progressive distillation for fast\nsampling of diffusion models.  InInternational Confer-\nence on Learning Representations, 2022. URLhttps:\n//openreview.net/forum?id=TIdIXIpzhoI.\nSalimans,  T.,  Goodfellow,  I.,  Zaremba,  W.,  Cheung,  V.,\nRadford, A., and Chen, X. Improved techniques for train-\ning gans. InAdvances in neural information processing\nsystems, pp. 2234–2242, 2016.\nSauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling\nstylegan to large diverse datasets.  InACM SIGGRAPH\n2022 conference proceedings, pp. 1–10, 2022.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep Unsupervised Learning Using Nonequi-\nlibrium Thermodynamics.  InInternational Conference\non Machine Learning, pp. 2256–2265, 2015.\nSong, J., Meng, C., and Ermon, S.   Denoising diffusion\nimplicit models.arXiv preprint arXiv:2010.02502, 2020.\nSong,   J.,   Vahdat,   A.,   Mardani,   M.,   and   Kautz,   J.\nPseudoinverse-guided diffusion models for inverse prob-\nlems. InInternational Conference on Learning Represen-\ntations,  2023.   URLhttps://openreview.net/\nforum?id=9_gsMA8MRKQ.\nSong, Y. and Ermon, S. Generative Modeling by Estimating\nGradients of the Data Distribution. InAdvances in Neural\nInformation Processing Systems, pp. 11918–11930, 2019.\nSong, Y. and Ermon, S. Improved Techniques for Training\nScore-Based Generative Models.Advances in Neural\nInformation Processing Systems, 33, 2020.\nSong,  Y.,  Garg,  S.,  Shi,  J.,  and Ermon,  S.   Sliced score\nmatching: A scalable approach to density and score esti-\nmation. InProceedings of the Thirty-Fifth Conference on\nUncertainty in Artificial Intelligence, UAI 2019, Tel Aviv,\nIsrael, July 22-25, 2019, pp. 204, 2019.\nSong,  Y.,  Sohl-Dickstein,  J.,  Kingma,  D. P.,  Kumar,  A.,\nErmon, S., and Poole, B.  Score-based generative mod-\neling  through  stochastic  differential  equations.   InIn-\nternational  Conference  on  Learning  Representations,\n2021.  URLhttps://openreview.net/forum?\nid=PxTIG12RRHS.\nSong, Y., Shen, L., Xing, L., and Ermon, S. Solving inverse\nproblems in medical imaging with score-based genera-\ntive models.  InInternational Conference on Learning\nRepresentations, 2022. URLhttps://openreview.\nnet/forum?id=vaRCHVj0uGI.\nS\n ̈\nuli, E. and Mayers, D. F.An introduction to numerical\nanalysis. Cambridge university press, 2003.\nTian, Y., Wang, Q., Huang, Z., Li, W., Dai, D., Yang, M.,\nWang, J., and Fink, O.  Off-policy reinforcement learn-\ning for efficient and effective gan architecture search. In\nComputer Vision–ECCV 2020:  16th European Confer-\nence, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart VII 16, pp. 175–192. Springer, 2020.\nVahdat, A., Kreis, K., and Kautz, J. Score-based generative\nmodeling in latent space.Advances in Neural Information\nProcessing Systems, 34:11287–11302, 2021.\nVincent, P.  A Connection Between Score Matching and\nDenoising Autoencoders.Neural Computation, 23(7):\n1661–1674, 2011.\nWu, J., Huang, Z., Acharya, D., Li, W., Thoma, J., Paudel,\nD.  P.,  and  Gool,  L.  V.   Sliced  wasserstein  generative\nmodels.  InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 3713–\n3722, 2019.\nXiao, Z., Yan, Q., and Amit, Y. Generative latent flow.arXiv\npreprint arXiv:1905.10485, 2019.\nXiao, Z., Kreis, K., and Vahdat, A. Tackling the generative\nlearning trilemma with denoising diffusion GANs.  In\nInternational Conference on Learning Representations,\n2022.  URLhttps://openreview.net/forum?\nid=JprM0p-q0Co.\nXu, Y., Liu, Z., Tegmark, M., and Jaakkola, T. S.   Pois-\nson flow generative models. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.),Advances in Neural\nInformation Processing Systems, 2022.   URLhttps:\n//openreview.net/forum?id=voV_TRqcWh.\nYu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and\nXiao, J. Lsun: Construction of a large-scale image dataset\nusing  deep  learning  with  humans  in  the  loop.arXiv\npreprint arXiv:1506.03365, 2015.\nZhang,  Q.  and  Chen,  Y.Fast  sampling  of  diffusion\nmodels  with  exponential  integrator.arXiv  preprint\narXiv:2204.13902, 2022.\n12",
    "Consistency Models\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,\nO. The unreasonable effectiveness of deep features as a\nperceptual metric. InCVPR, 2018.\nZheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K., and\nAnandkumar,  A.    Fast  sampling  of  diffusion  models\nvia operator learning.arXiv preprint arXiv:2211.13449,\n2022.\nZheng, H., He, P., Chen, W., and Zhou, M. Truncated diffu-\nsion probabilistic models and diffusion-based adversarial\nauto-encoders.   InThe Eleventh International Confer-\nence on Learning Representations, 2023. URLhttps:\n//openreview.net/forum?id=HDxgaKk956l.\n13",
    "Consistency Models\nContents\n1    Introduction1\n2    Diffusion Models2\n3    Consistency Models3\n4    Training Consistency Models via Distillation4\n5    Training Consistency Models in Isolation5\n6    Experiments6\n6.1Training Consistency Models  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .6\n6.2Few-Step Image Generation    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .7\n6.3Zero-Shot Image Editing   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .9\n7    Conclusion9\nAppendices15\nAppendix A   Proofs15\nA.1   Notations   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .15\nA.2   Consistency Distillation .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .15\nA.3   Consistency Training   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .16\nAppendix B   Continuous-Time Extensions18\nB.1    Consistency Distillation in Continuous Time .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .18\nB.2    Consistency Training in Continuous Time   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .22\nB.3    Experimental Verifications   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .24\nAppendix C   Additional Experimental Details25\nModel Architectures   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .25\nParameterization for Consistency Models   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .25\nSchedule Functions for Consistency Training   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .26\nTraining Details .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .26\nAppendix D   Additional Results on Zero-Shot Image Editing26\nInpainting  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .27\nColorization    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .27\nSuper-resolution   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .28\n14",
    "Consistency Models\nStroke-guided image generation   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .28\nDenoising  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .28\nInterpolation   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .28\nAppendix E   Additional Samples from Consistency Models28\nAppendices\nA. Proofs\nA.1. Notations\nWe usef\nθ\npx,tqto denote a consistency model parameterized byθ, andfpx,t;φqthe consistency function of the empirical\nPF ODE in Eq. (3). Hereφsymbolizes its dependency on the pre-trained score models\nφ\npx,tq. For the consistency function\nof the PF ODE in Eq. (2), we denote it asfpx,tq.  Given a multi-variate functionhpx,yq, we letB\n1\nhpx,yqdenote the\nJacobian ofhoverx, and analogouslyB\n2\nhpx,yqdenote the Jacobian ofhovery. Unless otherwise stated,xis supposed to\nbe a random variable sampled from the data distributionp\ndata\npxq,nis sampled uniformly at random fromJ1,N ́1K, and\nx\nt\nn\nis sampled fromNpx;t\n2\nn\nIq. HereJ1,N ́1Krepresents the set of integerst1,2, ̈ ̈ ̈,N ́1u. Furthermore, recall that\nwe define\nˆ\nx\nφ\nt\nn\n:\n“x\nt\nn`1\n`pt\nn\n ́t\nn`1\nqΦpx\nt\nn`1\n,t\nn`1\n;φq,\nwhereΦp ̈ ̈ ̈;φqdenotes the update function of a one-step ODE solver for the empirical PF ODE defined by the score\nmodels\nφ\npx,tq. By default,Er ̈sdenotes the expectation over all relevant random variables in the expression.\nA.2. Consistency Distillation\nTheorem 1.Let∆t\n:\n“max\nnPJ1,N ́1K\nt|t\nn`1\n ́t\nn\n|u, andfp ̈, ̈;φqbe the consistency function of the empirical PF ODE\nin Eq.(3). Assumef\nθ\nsatisfies the Lipschitz condition: there existsLą0such that for alltP rε,Ts,x, andy, we have\n∥f\nθ\npx,tq ́f\nθ\npy,tq∥\n2\nďL∥x ́y∥\n2\n. Assume further that for allnPJ1,N ́1K, the ODE solver called att\nn`1\nhas local\nerror uniformly bounded byOppt\nn`1\n ́t\nn\nq\np`1\nqwithpě1. Then, ifL\nN\nCD\npθ,θ;φq “0, we have\nsup\nn,x\n}f\nθ\npx,t\nn\nq ́fpx,t\nn\n;φq}\n2\n“Opp∆tq\np\nq.\nProof.FromL\nN\nCD\npθ,θ;φq “0, we have\nL\nN\nCD\npθ,θ;φq “Erλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqs “0.(11)\nAccording to the definition, we havep\nt\nn\npx\nt\nn\nq “p\ndata\npxqbNp0,t\n2\nn\nIqwheret\nn\něεą0. It follows thatp\nt\nn\npx\nt\nn\nq ą0for\neveryx\nt\nn\nand1ďnďN. Therefore, Eq. (11) entails\nλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq ”0.(12)\nBecauseλp ̈q ą0anddpx,yq “0ôx“y, this further implies that\nf\nθ\npx\nt\nn`1\n,t\nn`1\nq ”f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq.(13)\nNow lete\nn\nrepresent the error vector att\nn\n, which is defined as\ne\nn\n:\n“f\nθ\npx\nt\nn\n,t\nn\nq ́fpx\nt\nn\n,t\nn\n;φq.\nWe can easily derive the following recursion relation\ne\nn`1\n“f\nθ\npx\nt\nn`1\n,t\nn`1\nq ́fpx\nt\nn`1\n,t\nn`1\n;φq\n15",
    "Consistency Models\npiq\n“f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq ́fpx\nt\nn\n,t\nn\n;φq\n“f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq ́f\nθ\npx\nt\nn\n,t\nn\nq`f\nθ\npx\nt\nn\n,t\nn\nq ́fpx\nt\nn\n,t\nn\n;φq\n“f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq ́f\nθ\npx\nt\nn\n,t\nn\nq`e\nn\n,(14)\nwhere (i) is due to Eq. (13) andfpx\nt\nn`1\n,t\nn`1\n;φq “fpx\nt\nn\n,t\nn\n;φq. Becausef\nθ\np ̈,t\nn\nqhas Lipschitz constantL, we have\n∥e\nn`1\n∥\n2\nď∥e\nn\n∥\n2\n`L\n\r\n\r\n\r\nˆ\nx\nφ\nt\nn\n ́x\nt\nn\n\r\n\r\n\r\n2\npiq\n“∥e\nn\n∥\n2\n`L ̈Oppt\nn`1\n ́t\nn\nq\np`1\nq\n“∥e\nn\n∥\n2\n`Oppt\nn`1\n ́t\nn\nq\np`1\nq,\nwhere (i) holds because the ODE solver has local error bounded byOppt\nn`1\n ́t\nn\nq\np`1\nq. In addition, we observe thate\n1\n“0,\nbecause\ne\n1\n“f\nθ\npx\nt\n1\n,t\n1\nq ́fpx\nt\n1\n,t\n1\n;φq\npiq\n“x\nt\n1\n ́fpx\nt\n1\n,t\n1\n;φq\npiiq\n“x\nt\n1\n ́x\nt\n1\n“0.\nHere (i) is true because the consistency model is parameterized such thatfpx\nt\n1\n,t\n1\n;φq “x\nt\n1\nand (ii) is entailed by the\ndefinition offp ̈, ̈;φq. This allows us to perform induction on the recursion formula Eq. (14) to obtain\n∥e\nn\n∥\n2\nď∥e\n1\n∥\n2\n`\nn ́1\nÿ\nk“1\nOppt\nk`1\n ́t\nk\nq\np`1\nq\n“\nn ́1\nÿ\nk“1\nOppt\nk`1\n ́t\nk\nq\np`1\nq\n“\nn ́1\nÿ\nk“1\npt\nk`1\n ́t\nk\nqOppt\nk`1\n ́t\nk\nq\np\nq\nď\nn ́1\nÿ\nk“1\npt\nk`1\n ́t\nk\nqOpp∆tq\np\nq\n“Opp∆tq\np\nq\nn ́1\nÿ\nk“1\npt\nk`1\n ́t\nk\nq\n“Opp∆tq\np\nqpt\nn\n ́t\n1\nq\nďOpp∆tq\np\nqpT ́εq\n“Opp∆tq\np\nq,\nwhich completes the proof.\nA.3. Consistency Training\nThe following lemma provides an unbiased estimator for the score function, which is crucial to our proof for Theorem 2.\nLemma 1.Letx„p\ndata\npxq,x\nt\n„Npx;t\n2\nIq, andp\nt\npx\nt\nq “p\ndata\npxqbNp0,t\n2\nIq. We have∇logp\nt\npxq “  ́Er\nx\nt\n ́x\nt\n2\n|x\nt\ns.\nProof.According to the definition ofp\nt\npx\nt\nq, we have∇logp\nt\npx\nt\nq “∇\nx\nt\nlog\nş\np\ndata\npxqppx\nt\n|xqdx, whereppx\nt\n|xq “\nNpx\nt\n;x,t\n2\nIq. This expression can be further simplified to yield\n∇logp\nt\npx\nt\nq “\nş\np\ndata\npxq∇\nx\nt\nppx\nt\n|xqdx\nş\np\ndata\npxqppx\nt\n|xqdx\n16",
    "Consistency Models\n“\nş\np\ndata\npxqppx\nt\n|xq∇\nx\nt\nlogppx\nt\n|xqdx\nş\np\ndata\npxqppx\nt\n|xqdx\n“\nş\np\ndata\npxqppx\nt\n|xq∇\nx\nt\nlogppx\nt\n|xqdx\np\nt\npx\nt\nq\n“\nż\np\ndata\npxqppx\nt\n|xq\np\nt\npx\nt\nq\n∇\nx\nt\nlogppx\nt\n|xqdx\npiq\n“\nż\nppx|x\nt\nq∇\nx\nt\nlogppx\nt\n|xqdx\n“Er∇\nx\nt\nlogppx\nt\n|xq |x\nt\ns\n“  ́E\n„\nx\nt\n ́x\nt\n2\n|x\nt\nȷ\n,\nwhere (i) is due to Bayes’ rule.\nTheorem 2.Let∆t\n:\n“max\nnPJ1,N ́1K\nt|t\nn`1\n ́t\nn\n|u. Assumedandf\nθ\n ́\nare both twice continuously differentiable with\nbounded second derivatives, the weighting functionλp ̈qis bounded, andEr∥∇logp\nt\nn\npx\nt\nn\nq∥\n2\n2\ns ă 8. Assume further that\nwe use the Euler ODE solver, and the pre-trained score model matches the ground truth,i.e.,@tP rε,Ts:s\nφ\npx,tq ”\n∇logp\nt\npxq. Then,\nL\nN\nCD\npθ,θ\n ́\n;φq “L\nN\nCT\npθ,θ\n ́\nq`op∆tq,\nwhere the expectation is taken with respect tox„p\ndata\n,n„UJ1,N ́1K, andx\nt\nn`1\n„Npx;t\n2\nn`1\nIq\n. The consistency\ntraining objective, denoted byL\nN\nCT\npθ,θ\n ́\nq, is defined as\nErλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn\nz,t\nn\nqqs,\nwherez„Np0,Iq. Moreover,L\nN\nCT\npθ,θ\n ́\nq ěOp∆tqifinf\nN\nL\nN\nCD\npθ,θ\n ́\n;φq ą0.\nProof.With Taylor expansion, we have\nL\nN\nCD\npθ,θ\n ́\n;φq “Erλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqs\n“Erλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n`pt\nn`1\n ́t\nn\nqt\nn`1\n∇logp\nt\nn`1\npx\nt\nn`1\nq,t\nn\nqqs\n“Erλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq`B\n1\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn`1\n ́t\nn\nqt\nn`1\n∇logp\nt\nn`1\npx\nt\nn`1\nq\n`B\n2\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nq`op|t\nn`1\n ́t\nn\n|qqs\n“Etλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqq`λpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqr\nB\n1\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn`1\n ́t\nn\nqt\nn`1\n∇logp\nt\nn`1\npx\nt\nn`1\nq`B\n2\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nq`op|t\nn`1\n ́t\nn\n|qsu\n“Erλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqs\n`Etλpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqrB\n1\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn`1\n ́t\nn\nqt\nn`1\n∇logp\nt\nn`1\npx\nt\nn`1\nqsu\n`Etλpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqrB\n2\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nqsu`Erop|t\nn`1\n ́t\nn\n|qs.\n(15)\nThen, we apply Lemma 1 to Eq. (15) and use Taylor expansion in the reverse direction to obtain\nL\nN\nCD\npθ,θ\n ́\n;φq\n“Erλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqs\n`E\n\"\nλpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqq\n„\nB\n1\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nqt\nn`1\nE\n„\nx\nt\nn`1\n ́x\nt\n2\nn`1\nˇ\nˇ\nˇ\nx\nt\nn`1\nȷȷ*\n`Etλpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqrB\n2\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nqsu`Erop|t\nn`1\n ́t\nn\n|qs\npiq\n“Erλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqs\n`E\n\"\nλpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqq\n„\nB\n1\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nqt\nn`1\nˆ\nx\nt\nn`1\n ́x\nt\n2\nn`1\n ̇ȷ*\n17",
    "Consistency Models\n`Etλpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqrB\n2\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nqsu`Erop|t\nn`1\n ́t\nn\n|qs\n“E\n„\nλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqq\n`λpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqq\n„\nB\n1\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nqt\nn`1\nˆ\nx\nt\nn`1\n ́x\nt\n2\nn`1\n ̇ȷ\n`λpt\nn\nqB\n2\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqqrB\n2\nf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nqpt\nn\n ́t\nn`1\nqs`op|t\nn`1\n ́t\nn\n|q\nȷ\n`Erop|t\nn`1\n ́t\nn\n|qs\n“E\n„\nλpt\nn\nqd\nˆ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\nˆ\nx\nt\nn`1\n`pt\nn\n ́t\nn`1\nqt\nn`1\nx\nt\nn`1\n ́x\nt\n2\nn`1\n,t\nn\n ̇ ̇ȷ\n`Erop|t\nn`1\n ́t\nn\n|qs\n“E\n„\nλpt\nn\nqd\nˆ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\nˆ\nx\nt\nn`1\n`pt\nn\n ́t\nn`1\nq\nx\nt\nn`1\n ́x\nt\nn`1\n,t\nn\n ̇ ̇ȷ\n`Erop|t\nn`1\n ́t\nn\n|qs\n“Erλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn`1\nz`pt\nn\n ́t\nn`1\nqz,t\nn\nqqs`Erop|t\nn`1\n ́t\nn\n|qs\n“Erλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn\nz,t\nn\nqqs`Erop|t\nn`1\n ́t\nn\n|qs\n“Erλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn\nz,t\nn\nqqs`Erop∆tqs\n“Erλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn\nz,t\nn\nqqs`op∆tq\n“L\nN\nCT\npθ,θ\n ́\nq`op∆tq,(16)\nwhere  (i)  is  due  to  the  law  of  total  expectation,  andz\n:\n“\nx\nt\nn`1\n ́x\nt\nn`1\n„Np0,Iq.   This  impliesL\nN\nCD\npθ,θ\n ́\n;φq “\nL\nN\nCT\npθ,θ\n ́\nq `op∆tqand thus completes the proof for Eq. (9).   Moreover,  we haveL\nN\nCT\npθ,θ\n ́\nq ěOp∆tq\nwhenever\ninf\nN\nL\nN\nCD\npθ,θ\n ́\n;φq ą0.   Otherwise,L\nN\nCT\npθ,θ\n ́\nq ăOp∆tqand thuslim\n∆tÑ0\nL\nN\nCD\npθ,θ\n ́\n;φq “0,  which is a clear\ncontradiction toinf\nN\nL\nN\nCD\npθ,θ\n ́\n;φq ą0.\nRemark 1.When the conditionL\nN\nCT\npθ,θ\n ́\nq ěOp∆tqis not satisfied, such as in the case whereθ\n ́\n“stopgradpθq, the\nvalidity ofL\nN\nCT\npθ,θ\n ́\nqas a training objective for consistency models can still be justified by referencing the result provided\nin Theorem 6.\nB. Continuous-Time Extensions\nThe consistency distillation and consistency training objectives can be generalized to hold for infinite time steps (NÑ 8)\nunder suitable conditions.\nB.1. Consistency Distillation in Continuous Time\nDepending on whetherθ\n ́\n“θorθ\n ́\n“stopgradpθq(same as settingμ“0), there are two possible continuous-time\nextensions for the consistency distillation objectiveL\nN\nCD\npθ,θ\n ́\n;φq\n. Given a twice continuously differentiable metric function\ndpx,yq, we defineGpxqas a matrix, whosepi,jq-th entry is given by\nrGpxqs\nij\n:\n“\nB\n2\ndpx,yq\nBy\ni\nBy\nj\nˇ\nˇ\nˇ\nˇ\ny“x\n.\nSimilarly, we defineHpxqas\nrHpxqs\nij\n:\n“\nB\n2\ndpy,xq\nBy\ni\nBy\nj\nˇ\nˇ\nˇ\nˇ\ny“x\n.\nThe matricesGandHplay a crucial role in forming continuous-time objectives for consistency distillation. Additionally,\nwe denote the Jacobian off\nθ\npx,tqwith respect toxas\nBf\nθ\npx,tq\nBx\n.\nWhenθ\n ́\n“θ(with no stopgrad operator), we have the following theoretical result.\nTheorem 3.Lett\nn\n“τp\nn ́1\nN ́1\nq, wherenPJ1,NK, andτp ̈qis a strictly monotonic function withτp0q “εandτp1q “T.\nAssumeτis continuously differentiable inr0,1s,dis three times continuously differentiable with bounded third derivatives,\n18",
    "Consistency Models\nandf\nθ\nis twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting\nfunctionλp ̈qis bounded, andsup\nx,tPrε,Ts\n∥s\nφ\npx,tq∥\n2\nă 8. Then with the Euler solver in consistency distillation, we have\nlim\nNÑ8\npN ́1q\n2\nL\nN\nCD\npθ,θ;φq “L\n8\nCD\npθ,θ;φq,(17)\nwhereL\n8\nCD\npθ,θ;φqis defined as\n1\n2\nE\n«\nλptq\nrpτ\n ́1\nq\n1\nptqs\n2\nˆ\nBf\nθ\npx\nt\n,tq\nBt\n ́t\nBf\nθ\npx\nt\n,tq\nBx\nt\ns\nφ\npx\nt\n,tq\n ̇\nT\nGpf\nθ\npx\nt\n,tqq\nˆ\nBf\nθ\npx\nt\n,tq\nBt\n ́t\nBf\nθ\npx\nt\n,tq\nBx\nt\ns\nφ\npx\nt\n,tq\n ̇\nff\n.(18)\nHere the expectation above is taken overx„p\ndata\n,u„Ur0,1s,t“τpuq, andx\nt\n„Npx,t\n2\nIq.\nProof.Let∆u“\n1\nN ́1\nandu\nn\n“\nn ́1\nN ́1\n. First, we can derive the following equation with Taylor expansion:\nf\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq ́f\nθ\npx\nt\nn`1\n,t\nn`1\nq “f\nθ\npx\nt\nn`1\n`t\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nqτ\n1\npu\nn\nq∆u,t\nn\nq ́f\nθ\npx\nt\nn`1\n,t\nn`1\nq\n“t\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nqτ\n1\npu\nn\nq∆u ́\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\nτ\n1\npu\nn\nq∆u`Opp∆uq\n2\nq,(19)\nNote thatτ\n1\npu\nn\nq “\n1\nτ\n ́1\npt\nn`1\nq\n. Then, we apply Taylor expansion to the consistency distillation loss, which gives\npN ́1q\n2\nL\nN\nCD\npθ,θ;φq “\n1\np∆uq\n2\nL\nN\nCD\npθ,θ;φq “\n1\np∆uq\n2\nErλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqs\npiq\n“\n1\n2p∆uq\n2\nˆ\nEtλpt\nn\nqτ\n1\npu\nn\nq\n2\nrf\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq ́f\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nGpf\nθ\npx\nt\nn`1\n,t\nn`1\nqq\n ̈rf\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq ́f\nθ\npx\nt\nn`1\n,t\nn`1\nqsu`ErOp|∆u|\n3\nqs\n ̇\npiiq\n“\n1\n2\nE\n„\nλpt\nn\nqτ\n1\npu\nn\nq\n2\nˆ\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\n ́t\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq\n ̇\nT\nGpf\nθ\npx\nt\nn`1\n,t\nn`1\nqq\n ̈\nˆ\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\n ́t\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq\n ̇ȷ\n`ErOp|∆u|qs\n“\n1\n2\nE\n„\nλpt\nn\nq\nrpτ\n ́1\nq\n1\npt\nn\nqs\n2\nˆ\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\n ́t\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq\n ̇\nT\nGpf\nθ\npx\nt\nn`1\n,t\nn`1\nqq\n ̈\nˆ\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\n ́t\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq\n ̇ȷ\n`ErOp|∆u|qs\n(20)\nwhere we obtain (i) by expandingdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq, ̈q\nto second order and observingdpx,xq ”0and∇\ny\ndpx,yq|\ny“x\n”0.\nWe obtain (ii) using Eq. (19). By taking the limit for both sides of Eq. (20) as∆uÑ0or equivalentlyNÑ 8, we arrive at\nEq. (17), which completes the proof.\nRemark 2.Although Theorem 3 assumes the Euler ODE solver for technical simplicity, we believe an analogous result can\nbe derived for more general solvers, since all ODE solvers should perform similarly asNÑ 8. We leave a more general\nversion of Theorem 3 as future work.\nRemark 3.Theorem 3 implies that consistency models can be trained by minimizingL\n8\nCD\npθ,θ;φq\n. In particular, when\ndpx,yq “∥x ́y∥\n2\n2\n, we have\nL\n8\nCD\npθ,θ;φq “E\n«\nλptq\nrpτ\n ́1\nq\n1\nptqs\n2\n\r\n\r\n\r\n\r\nBf\nθ\npx\nt\n,tq\nBt\n ́t\nBf\nθ\npx\nt\n,tq\nBx\nt\ns\nφ\npx\nt\n,tq\n\r\n\r\n\r\n\r\n2\n2\nff\n.(21)\nHowever, this continuous-time objective requires computing Jacobian-vector products as a subroutine to evaluate the loss\nfunction, which can be slow and laborious to implement in deep learning frameworks that do not support forward-mode\nautomatic differentiation.\n19",
    "Consistency Models\nRemark 4.Iff\nθ\npx,tqmatches the ground truth consistency function for the empirical PF ODE ofs\nφ\npx,tq, then\nBf\nθ\npx,tq\nBt\n ́t\nBf\nθ\npx,tq\nBx\ns\nφ\npx,tq ”0\nand thereforeL\n8\nCD\npθ,θ;φq “0. This can be proved by noting thatf\nθ\npx\nt\n,tq ”x\nε\nfor alltP rε,Ts, and then taking the\ntime-derivative of this identity:\nf\nθ\npx\nt\n,tq ”x\nε\nðñ\nBf\nθ\npx\nt\n,tq\nBx\nt\ndx\nt\ndt\n`\nBf\nθ\npx\nt\n,tq\nBt\n”0\nðñ\nBf\nθ\npx\nt\n,tq\nBx\nt\nr ́ts\nφ\npx\nt\n,tqs`\nBf\nθ\npx\nt\n,tq\nBt\n”0\nðñ\nBf\nθ\npx\nt\n,tq\nBt\n ́t\nBf\nθ\npx\nt\n,tq\nBx\nt\ns\nφ\npx\nt\n,tq ”0.\nThe above observation provides another motivation forL\n8\nCD\npθ,θ;φq, as it is minimized if and only if the consistency model\nmatches the ground truth consistency function.\nFor some metric functions, such as theℓ\n1\nnorm, the HessianGpxqis zero so Theorem 3 is vacuous. Below we show that a\nnon-vacuous statement holds for theℓ\n1\nnorm with just a small modification of the proof for Theorem 3.\nTheorem 4.Lett\nn\n“τp\nn ́1\nN ́1\nq\n, wherenPJ1,NK, andτp ̈qis a strictly monotonic function withτp0q “εandτp1q “T.\nAssumeτis continuously differentiable inr0,1s, andf\nθ\nis twice continuously differentiable with bounded first and second\nderivatives. Assume further that the weighting functionλp ̈qis bounded, andsup\nx,tPrε,Ts\n∥s\nφ\npx,tq∥\n2\nă 8. Suppose we use\nthe Euler ODE solver, and setdpx,yq “∥x ́y∥\n1\nin consistency distillation. Then we have\nlim\nNÑ8\npN ́1qL\nN\nCD\npθ,θ;φq “L\n8\nCD,ℓ\n1\npθ,θ;φq,(22)\nwhere\nL\n8\nCD,ℓ\n1\npθ,θ;φq\n:\n“E\n„\nλptq\npτ\n ́1\nq\n1\nptq\n\r\n\r\n\r\n\r\nt\nBf\nθ\npx\nt\n,tq\nBx\nt\ns\nφ\npx\nt\n,tq ́\nBf\nθ\npx\nt\n,tq\nBt\n\r\n\r\n\r\n\r\n1\nȷ\nwhere the expectation above is taken overx„p\ndata\n,u„Ur0,1s,t“τpuq, andx\nt\n„Npx,t\n2\nIq.\nProof.Let∆u“\n1\nN ́1\nandu\nn\n“\nn ́1\nN ́1\n. We have\npN ́1qL\nN\nCD\npθ,θ;φq “\n1\n∆u\nL\nN\nCD\npθ,θ;φq “\n1\n∆u\nErλpt\nn\nq}f\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq}\n1\ns\npiq\n“\n1\n∆u\nE\n„\nλpt\nn\nq\n\r\n\r\n\r\n\r\nt\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nqτ\n1\npu\nn\nq ́\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\nτ\n1\npu\nn\nq`Opp∆uq\n2\nq\n\r\n\r\n\r\n\r\n1\nȷ\n“E\n„\nλpt\nn\nqτ\n1\npu\nn\nq\n\r\n\r\n\r\n\r\nt\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq ́\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\n`Op∆uq\n\r\n\r\n\r\n\r\n1\nȷ\n“E\n„\nλpt\nn\nq\npτ\n ́1\nq\n1\npt\nn\nq\n\r\n\r\n\r\n\r\nt\nn`1\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq ́\nBf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\n`Op∆uq\n\r\n\r\n\r\n\r\n1\nȷ\n(23)\nwhere (i) is obtained by plugging Eq. (19) into the previous equation. Taking the limit for both sides of Eq. (23) as∆uÑ0\nor equivalentlyNÑ 8leads to Eq. (22), which completes the proof.\nRemark 5.According to Theorem 4, consistency models can be trained by minimizingL\n8\nCD,ℓ\n1\npθ,θ;φq. Moreover, the same\nreasoning in Remark 4 can be applied to show thatL\n8\nCD,ℓ\n1\npθ,θ;φq “0if and only iff\nθ\npx\nt\n,tq “x\nε\nfor allx\nt\nPR\nd\nand\ntP rε,Ts.\nIn the second case whereθ\n ́\n“stopgradpθq, we can derive a so-called “pseudo-objective” whose gradient matches the\ngradient ofL\nN\nCD\npθ,θ\n ́\n;φqin the limit ofNÑ 8. Minimizing this pseudo-objective with gradient descent gives another\nway to train consistency models via distillation. This pseudo-objective is provided by the theorem below.\n20",
    "Consistency Models\nTheorem 5.Lett\nn\n“τp\nn ́1\nN ́1\nq\n, wherenPJ1,NK, andτp ̈qis a strictly monotonic function withτp0q “εandτp1q “T.\nAssumeτis continuously differentiable inr0,1s,dis three times continuously differentiable with bounded third derivatives,\nandf\nθ\nis twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting\nfunctionλp ̈qis bounded,sup\nx,tPrε,Ts\n∥s\nφ\npx,tq∥\n2\nă 8, andsup\nx,tPrε,Ts\n∥∇\nθ\nf\nθ\npx,tq∥\n2\nă 8. Suppose we use the Euler\nODE solver, andθ\n ́\n“stopgradpθqin consistency distillation. Then,\nlim\nNÑ8\npN ́1q∇\nθ\nL\nN\nCD\npθ,θ\n ́\n;φq “∇\nθ\nL\n8\nCD\npθ,θ\n ́\n;φq,(24)\nwhere\nL\n8\nCD\npθ,θ\n ́\n;φq\n:\n“E\n„\nλptq\npτ\n ́1\nq\n1\nptq\nf\nθ\npx\nt\n,tq\nT\nHpf\nθ\n ́\npx\nt\n,tqq\nˆ\nBf\nθ\n ́\npx\nt\n,tq\nBt\n ́t\nBf\nθ\n ́\npx\nt\n,tq\nBx\nt\ns\nφ\npx\nt\n,tq\n ̇ȷ\n.(25)\nHere the expectation above is taken overx„p\ndata\n,u„Ur0,1s,t“τpuq, andx\nt\n„Npx,t\n2\nIq.\nProof.We denote∆u“\n1\nN ́1\nandu\nn\n“\nn ́1\nN ́1\n. First, we leverage Taylor series expansion to obtain\npN ́1qL\nN\nCD\npθ,θ\n ́\n;φq “\n1\n∆u\nL\nN\nCD\npθ,θ\n ́\n;φq “\n1\n∆u\nErλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqs\npiq\n“\n1\n2∆u\nˆ\nEtλpt\nn\nqrf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\n ̈rf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqsu`ErOp|∆u|\n3\nqs\n ̇\n“\n1\n2∆u\nEtλpt\nn\nqrf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqrf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqsu`ErOp|∆u|\n2\nqs\n(26)\nwhere (i) is derived by expandingdp ̈,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\nto second order and leveragingdpx,xq ”0and∇\ny\ndpy,xq|\ny“x\n”0.\nNext, we compute the gradient of Eq. (26) with respect toθand simplify the result to obtain\npN ́1q∇\nθ\nL\nN\nCD\npθ,θ\n ́\n;φq “\n1\n∆u\n∇\nθ\nL\nN\nCD\npθ,θ\n ́\n;φq\n“\n1\n2∆u\n∇\nθ\nEtλpt\nn\nqrf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqrf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqsu`ErOp|∆u|\n2\nqs\npiq\n“\n1\n∆u\nEtλpt\nn\nqr∇\nθ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqrf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqsu`ErOp|∆u|\n2\nqs\npiiq\n“\n1\n∆u\nE\n\"\nλpt\nn\nqr∇\nθ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\n„\nt\nn`1\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nqτ\n1\npu\nn\nq∆u\n ́\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\nτ\n1\npu\nn\nq∆u\nȷ*\n`ErOp|∆u|qs\n“E\n\"\nλpt\nn\nqr∇\nθ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\n„\nt\nn`1\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nqτ\n1\npu\nn\nq\n ́\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\nτ\n1\npu\nn\nq\nȷ*\n`ErOp|∆u|qs\n“∇\nθ\nE\n\"\nλpt\nn\nqrf\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\n„\nt\nn`1\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nqτ\n1\npu\nn\nq\n ́\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\nτ\n1\npu\nn\nq\nȷ*\n`ErOp|∆u|qs\n(27)\n“∇\nθ\nE\n\"\nλpt\nn\nq\npτ\n ́1\nq\n1\npt\nn\nq\nrf\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\n„\nt\nn`1\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBx\nt\nn`1\ns\nφ\npx\nt\nn`1\n,t\nn`1\nq\n ́\nBf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq\nBt\nn`1\nȷ*\n`ErOp|∆u|qs\n21",
    "Consistency Models\nHere (i) results from the chain rule, and (ii) follows from Eq. (19) andf\nθ\npx,tq ”f\nθ\n ́\npx,tq, sinceθ\n ́\n“stopgradpθq.\nTaking the limit for both sides of Eq. (28) as∆uÑ0(orNÑ 8) yields Eq. (24), which completes the proof.\nRemark 6.Whendpx,yq “∥x ́y∥\n2\n2\n, the pseudo-objectiveL\n8\nCD\npθ,θ\n ́\n;φqcan be simplified to\nL\n8\nCD\npθ,θ\n ́\n;φq “2E\n„\nλptq\npτ\n ́1\nq\n1\nptq\nf\nθ\npx\nt\n,tq\nT\nˆ\nBf\nθ\n ́\npx\nt\n,tq\nBt\n ́t\nBf\nθ\n ́\npx\nt\n,tq\nBx\nt\ns\nφ\npx\nt\n,tq\n ̇ȷ\n.(28)\nRemark 7.The objectiveL\n8\nCD\npθ,θ\n ́\n;φqdefined in Theorem 5 is only meaningful in terms of its gradient—one cannot\nmeasure the progress of training by tracking the value ofL\n8\nCD\npθ,θ\n ́\n;φq, but can still apply gradient descent to this objective\nto distill consistency models from pre-trained diffusion models. Because this objective is not a typical loss function, we refer\nto it as the “pseudo-objective” for consistency distillation.\nRemark  8.Following  the  same  reasoning  in  Remark  4,   we  can  easily  derive  thatL\n8\nCD\npθ,θ\n ́\n;φq “0and\n∇\nθ\nL\n8\nCD\npθ,θ\n ́\n;φq “0iff\nθ\npx,tqmatches the ground truth consistency function for the empirical PF ODE that in-\nvolvess\nφ\npx,tq. However, the converse does not hold true in general. This distinguishesL\n8\nCD\npθ,θ\n ́\n;φqfromL\n8\nCD\npθ,θ;φq,\nthe latter of which is a true loss function.\nB.2. Consistency Training in Continuous Time\nA remarkable observation is that the pseudo-objective in Theorem 5 can be estimated without any pre-trained diffusion\nmodels, which enables direct consistency training of consistency models. More precisely, we have the following result.\nTheorem 6.Lett\nn\n“τp\nn ́1\nN ́1\nq, wherenPJ1,NK, andτp ̈qis a strictly monotonic function withτp0q “εandτp1q “T.\nAssumeτis continuously differentiable inr0,1s,dis three times continuously differentiable with bounded third derivatives,\nandf\nθ\nis twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting\nfunctionλp ̈qis bounded,Er∥∇logp\nt\nn\npx\nt\nn\nq∥\n2\n2\ns ă 8,sup\nx,tPrε,Ts\n∥∇\nθ\nf\nθ\npx,tq∥\n2\nă 8, andφrepresents diffusion model\nparameters that satisfys\nφ\npx,tq ”∇logp\nt\npxq. Then ifθ\n ́\n“stopgradpθq, we have\nlim\nNÑ8\npN ́1q∇\nθ\nL\nN\nCD\npθ,θ\n ́\n;φq “lim\nNÑ8\npN ́1q∇\nθ\nL\nN\nCT\npθ,θ\n ́\nq “∇\nθ\nL\n8\nCT\npθ,θ\n ́\nq,(29)\nwhereL\nN\nCD\nuses the Euler ODE solver, and\nL\n8\nCT\npθ,θ\n ́\nq\n:\n“E\n„\nλptq\npτ\n ́1\nq\n1\nptq\nf\nθ\npx\nt\n,tq\nT\nHpf\nθ\n ́\npx\nt\n,tqq\nˆ\nBf\nθ\n ́\npx\nt\n,tq\nBt\n`\nBf\nθ\n ́\npx\nt\n,tq\nBx\nt\n ̈\nx\nt\n ́x\nt\n ̇ȷ\n.(30)\nHere the expectation above is taken overx„p\ndata\n,u„Ur0,1s,t“τpuq, andx\nt\n„Npx,t\n2\nIq.\nProof.The proof mostly follows that of Theorem 5. First, we leverage Taylor series expansion to obtain\npN ́1qL\nN\nCT\npθ,θ\n ́\nq “\n1\n∆u\nL\nN\nCT\npθ,θ\n ́\nq “\n1\n∆u\nErλpt\nn\nqdpf\nθ\npx`t\nn`1\nz,t\nn`1\nq,f\nθ\n ́\npx`t\nn\nz,t\nn\nqqs\npiq\n“\n1\n2∆u\nˆ\nEtλpt\nn\nqrf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n ̈rf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqsu`ErOp|∆u|\n3\nqs\n ̇\n“\n1\n2∆u\nEtλpt\nn\nqrf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n ̈rf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqsu`ErOp|∆u|\n2\nqs\n(31)\nwherez„Np0,Iq, (i) is derived by first expandingdp ̈,f\nθ\n ́\npx`t\nn\nz,t\nn\nqqto second order, and then noting thatdpx,xq ”0\nand∇\ny\ndpy,xq|\ny“x\n”0. Next, we compute the gradient of Eq. (31) with respect toθand simplify the result to obtain\npN ́1q∇\nθ\nL\nN\nCT\npθ,θ\n ́\nq “\n1\n∆u\n∇\nθ\nL\nN\nCT\npθ,θ\n ́\nq\n“\n1\n2∆u\n∇\nθ\nEtλpt\nn\nqrf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n ̈rf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqsu`ErOp|∆u|\n2\nqs\n22",
    "Consistency Models\npiq\n“\n1\n∆u\nEtλpt\nn\nqr∇\nθ\nf\nθ\npx`t\nn`1\nz,t\nn`1\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n ̈rf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqsu`ErOp|∆u|\n2\nqs\n(32)\npiiq\n“\n1\n∆u\nE\n\"\nλpt\nn\nqr∇\nθ\nf\nθ\npx`t\nn`1\nz,t\nn`1\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n„\nτ\n1\npu\nn\nq∆uB\n1\nf\nθ\n ́\npx`t\nn\nz,t\nn\nqz\n`B\n2\nf\nθ\n ́\npx`t\nn\nz,t\nn\nqτ\n1\npu\nn\nq∆u\nȷ*\n`ErOp|∆u|qs\n“E\n\"\nλpt\nn\nqτ\n1\npu\nn\nqr∇\nθ\nf\nθ\npx`t\nn`1\nz,t\nn`1\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n„\nB\n1\nf\nθ\n ́\npx`t\nn\nz,t\nn\nqz\n`B\n2\nf\nθ\n ́\npx`t\nn\nz,t\nn\nq\nȷ*\n`ErOp|∆u|qs\n“∇\nθ\nE\n\"\nλpt\nn\nqτ\n1\npu\nn\nqrf\nθ\npx`t\nn`1\nz,t\nn`1\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n„\nB\n1\nf\nθ\n ́\npx`t\nn\nz,t\nn\nqz\n`B\n2\nf\nθ\n ́\npx`t\nn\nz,t\nn\nq\nȷ*\n`ErOp|∆u|qs\n“∇\nθ\nE\n\"\nλpt\nn\nqτ\n1\npu\nn\nqrf\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nHpf\nθ\n ́\npx\nt\nn\n,t\nn\nqq\n„\nB\n1\nf\nθ\n ́\npx\nt\nn\n,t\nn\nq\nx\nt\nn\n ́x\nt\nn\n`B\n2\nf\nθ\n ́\npx\nt\nn\n,t\nn\nq\nȷ*\n`ErOp|∆u|qs\n“∇\nθ\nE\n\"\nλpt\nn\nq\npτ\n ́1\nq\n1\npt\nn\nq\nrf\nθ\npx\nt\nn`1\n,t\nn`1\nqs\nT\nHpf\nθ\n ́\npx\nt\nn\n,t\nn\nqq\n„\nB\n1\nf\nθ\n ́\npx\nt\nn\n,t\nn\nq\nx\nt\nn\n ́x\nt\nn\n`B\n2\nf\nθ\n ́\npx\nt\nn\n,t\nn\nq\nȷ*\n`ErOp|∆u|qs\n(33)\nHere (i) results from the chain rule, and (ii) follows from Taylor expansion. Taking the limit for both sides of Eq. (33) as\n∆uÑ0orNÑ 8yields the second equality in Eq. (29).\nNow we prove the first equality. Applying Taylor expansion again, we obtain\npN ́1q∇\nθ\nL\nN\nCD\npθ,θ\n ́\n;φq “\n1\n∆u\n∇\nθ\nL\nN\nCD\npθ,θ\n ́\n;φq “\n1\n∆u\n∇\nθ\nErλpt\nn\nqdpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqs\n“\n1\n∆u\nErλpt\nn\nq∇\nθ\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqs\n“\n1\n∆u\nErλpt\nn\nq∇\nθ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nT\nB\n1\ndpf\nθ\npx\nt\nn`1\n,t\nn`1\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqs\n“\n1\n∆u\nE\n\"\nλpt\nn\nq∇\nθ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nT\n„\nB\n1\ndpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nq,f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq\n`Hpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqpf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqq`Op|∆u|\n2\nq\nȷ*\n“\n1\n∆u\nEtλpt\nn\nq∇\nθ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nT\nrHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqpf\nθ\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqs`Op|∆u|\n2\nqu\n“\n1\n∆u\nEtλpt\nn\nq∇\nθ\nf\nθ\npx\nt\nn`1\n,t\nn`1\nq\nT\nrHpf\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqpf\nθ\n ́\npx\nt\nn`1\n,t\nn`1\nq ́f\nθ\n ́\np\nˆ\nx\nφ\nt\nn\n,t\nn\nqqs`Op|∆u|\n2\nqu\npiq\n“\n1\n∆u\nEtλpt\nn\nqr∇\nθ\nf\nθ\npx`t\nn`1\nz,t\nn`1\nqs\nT\nHpf\nθ\n ́\npx`t\nn\nz,t\nn\nqq\n ̈rf\nθ\npx`t\nn`1\nz,t\nn`1\nq ́f\nθ\n ́\npx`t\nn\nz,t\nn\nqsu`ErOp|∆u|\n2\nqs\nwhere (i) holds becausex\nt\nn`1\n“x`t\nn`1\nzand\nˆ\nx\nφ\nt\nn\n“x\nt\nn`1\n ́pt\nn\n ́t\nn`1\nqt\nn`1\n ́px\nt\nn`1\n ́xq\nt\n2\nn`1\n“x\nt\nn`1\n`pt\nn\n ́t\nn`1\nqz“\nx`t\nn\nz\n. Because (i) matches Eq. (32), we can use the same reasoning procedure from Eq. (32) to Eq. (33) to conclude\nlim\nNÑ8\npN ́1q∇\nθ\nL\nN\nCD\npθ,θ\n ́\n;φq “lim\nNÑ8\npN ́1q∇\nθ\nL\nN\nCT\npθ,θ\n ́\nq, completing the proof.\nRemark 9.Note thatL\n8\nCT\npθ,θ\n ́\nq\ndoes not depend on the diffusion model parameterφand hence can be optimized without\nany pre-trained diffusion models.\n23",
    "Consistency Models\n(a) Consistency Distillation(b) Consistency Training\nFigure 7: Comparing discrete consistency distillation/training algorithms with continuous counterparts.\nRemark 10.Whendpx,yq “∥x ́y∥\n2\n2\n, the continuous-time consistency training objective becomes\nL\n8\nCT\npθ,θ\n ́\nq “2E\n„\nλptq\npτ\n ́1\nq\n1\nptq\nf\nθ\npx\nt\n,tq\nT\nˆ\nBf\nθ\n ́\npx\nt\n,tq\nBt\n`\nBf\nθ\n ́\npx\nt\n,tq\nBx\nt\n ̈\nx\nt\n ́x\nt\n ̇ȷ\n.(34)\nRemark 11.Similar toL\n8\nCD\npθ,θ\n ́\n;φqin Theorem 5,L\n8\nCT\npθ,θ\n ́\nqis a pseudo-objective; one cannot track training by\nmonitoring the value ofL\n8\nCT\npθ,θ\n ́\nq\n,  but can still apply gradient descent on this loss function to train a consistency\nmodelf\nθ\npx,tqdirectly  from  data.   Moreover,  the  same  observation  in  Remark  8  holds  true:L\n8\nCT\npθ,θ\n ́\nq “0\nand\n∇\nθ\nL\n8\nCT\npθ,θ\n ́\nq “0iff\nθ\npx,tqmatches the ground truth consistency function for the PF ODE.\nB.3. Experimental Verifications\nTo experimentally verify the efficacy of our continuous-time CD and CT objectives, we train consistency models with a\nvariety of loss functions on CIFAR-10. All results are provided in Fig. 7. We setλptq “ pτ\n ́1\nq\n1\nptqfor all continuous-time\nexperiments. Other hyperparameters are the same as in Table 3. We occasionally modify some hyperparameters for improved\nperformance. For distillation, we compare the following objectives:\n•  CDpℓ\n2\nq: Consistency distillationL\nN\nCD\nwithN“18and theℓ\n2\nmetric.\n•  CDpℓ\n1\nq: Consistency distillationL\nN\nCD\nwithN“18and theℓ\n1\nmetric. We set the learning rate to 2e-4.\n•  CD (LPIPS): Consistency distillationL\nN\nCD\nwithN“18and the LPIPS metric.\n•\nCD\n8\npℓ\n2\nq\n: Consistency distillationL\n8\nCD\nin Theorem 3 with theℓ\n2\nmetric. We set the learning rate to 1e-3 and dropout\nto 0.13.\n•CD\n8\npℓ\n1\nq: Consistency distillationL\n8\nCD\nin Theorem 4 with theℓ\n1\nmetric. We set the learning rate to 1e-3 and dropout\nto 0.3.\n•  CD\n8\n(stopgrad,ℓ\n2\n): Consistency distillationL\n8\nCD\nin Theorem 5 with theℓ\n2\nmetric. We set the learning rate to 5e-6.\n•\nCD\n8\n(stopgrad, LPIPS): Consistency distillationL\n8\nCD\nin Theorem 5 with the LPIPS metric. We set the learning rate to\n5e-6.\nWe did not investigate using the LPIPS metric in Theorem 3 because minimizing the resulting objective would require\nback-propagating through second order derivatives of the VGG network used in LPIPS, which is computationally expensive\nand prone to numerical instability. As revealed by Fig. 7a, the stopgrad version of continuous-time distillation (Theorem 5)\nworks better than the non-stopgrad version (Theorem 3) for both the LPIPS andℓ\n2\nmetrics, and the LPIPS metric works\nthe best for all distillation approaches. Additionally, discrete-time consistency distillation outperforms continuous-time\n24",
    "Consistency Models\nTable 3: Hyperparameters used for training CD and CT models\nHyperparameterCIFAR-10ImageNet64ˆ64LSUN256ˆ256\nCDCTCDCTCDCT\nLearning rate4e-44e-48e-68e-61e-51e-5\nBatch size5125122048204820482048\nμ00.950.95\nμ\n0\n0.90.950.95\ns\n0\n222\ns\n1\n150200150\nN184040\nODE solverHeunHeunHeun\nEMA decay rate0.99990.99990.9999430.9999430.9999430.999943\nTraining iterations800k800k600k800k600k1000k\nMixed-Precision (FP16)NoNoYesYesYesYes\nDropout probability0.00.00.00.00.00.0\nNumber of GPUs8864646464\nconsistency distillation, possibly due to the larger variance in continuous-time objectives, and the fact that one can use\neffective higher-order ODE solvers in discrete-time objectives.\nFor consistency training (CT), we find it important to initialize consistency models from a pre-trained EDM model in order\nto stabilize training when using continuous-time objectives. We hypothesize that this is caused by the large variance in our\ncontinuous-time loss functions. For fair comparison, we thus initialize all consistency models from the same pre-trained\nEDM model on CIFAR-10 for both discrete-time and continuous-time CT, even though the former works well with random\ninitialization. We leave variance reduction techniques for continuous-time CT to future research.\nWe empirically compare the following objectives:\n•CT (LPIPS): Consistency trainingL\nN\nCT\nwithN“120and the LPIPS metric. We set the learning rate to 4e-4, and the\nEMA decay rate for the target network to 0.99. We do not use the schedule functions forNandμhere because they\ncause slower learning when the consistency model is initialized from a pre-trained EDM model.\n•  CT\n8\npℓ\n2\nq: Consistency trainingL\n8\nCT\nwith theℓ\n2\nmetric. We set the learning rate to 5e-6.\n•  CT\n8\n(LPIPS): Consistency trainingL\n8\nCT\nwith the LPIPS metric. We set the learning rate to 5e-6.\nAs shown in Fig. 7b, the LPIPS metric leads to improved performance for continuous-time CT. We also find that continuous-\ntime CT outperforms discrete-time CT with the same LPIPS metric. This is likely due to the bias in discrete-time CT, as\n∆tą0in Theorem 2 for discrete-time objectives, whereas continuous-time CT has no bias since it implicitly drives∆tto0.\nC. Additional Experimental Details\nModel ArchitecturesWe follow Song et al. (2021); Dhariwal & Nichol (2021) for model architectures. Specifically, we\nuse the NCSN++ architecture in Song et al. (2021) for all CIFAR-10 experiments, and take the corresponding network\narchitectures from Dhariwal & Nichol (2021) when performing experiments on ImageNet64ˆ64, LSUN Bedroom\n256ˆ256and LSUN Cat256ˆ256.\nParameterization for Consistency ModelsWe use the same architectures for consistency models as those used for\nEDMs. The only difference is we slightly modify the skip connections in EDM to ensure the boundary condition holds for\nconsistency models. Recall that in Section 3 we propose to parameterize a consistency model in the following form:\nf\nθ\npx,tq “c\nskip\nptqx`c\nout\nptqF\nθ\npx,tq.\nIn EDM (Karras et al., 2022), authors choose\nc\nskip\nptq “\nσ\n2\ndata\nt\n2\n`σ\n2\ndata\n,  c\nout\nptq “\nσ\ndata\nt\na\nσ\n2\ndata\n`t\n2\n,\n25",
    "Consistency Models\nwhereσ\ndata\n“0.5. However, this choice ofc\nskip\nandc\nout\ndoes not satisfy the boundary condition when the smallest time\ninstantε‰0. To remedy this issue, we modify them to\nc\nskip\nptq “\nσ\n2\ndata\npt ́εq\n2\n`σ\n2\ndata\n,  c\nout\nptq “\nσ\ndata\npt ́εq\na\nσ\n2\ndata\n`t\n2\n,\nwhich clearly satisfiesc\nskip\npεq “1andc\nout\npεq “0.\nSchedule Functions for Consistency TrainingAs discussed in Section 5, consistency generation requires specifying\nschedule functionsNp ̈qandμp ̈qfor best performance. Throughout our experiments, we use schedule functions that take\nthe form below:\nNpkq “\nS\nc\nk\nK\npps\n1\n`1q\n2\n ́s\n2\n0\nq`s\n2\n0\n ́1\nW\n`1\nμpkq “exp\nˆ\ns\n0\nlogμ\n0\nNpkq\n ̇\n,\nwhereKdenotes the total number of training iterations,s\n0\ndenotes the initial discretization steps,s\n1\nąs\n0\ndenotes the target\ndiscretization steps at the end of training, andμ\n0\ną0denotes the EMA decay rate at the beginning of model training.\nTraining DetailsIn both consistency distillation and progressive distillation, we distill EDMs (Karras et al., 2022). We\ntrained these EDMs ourselves according to the specifications given in Karras et al. (2022). The original EDM paper did\nnot provide hyperparameters for the LSUN Bedroom256ˆ256and Cat256ˆ256datasets, so we mostly used the same\nhyperparameters as those for the ImageNet64ˆ64dataset. The difference is that we trained for 600k and 300k iterations\nfor the LSUN Bedroom and Cat datasets respectively, and reduced the batch size from 4096 to 2048.\nWe used the same EMA decay rate for LSUN256ˆ256datasets as for the ImageNet64ˆ64dataset.  For progressive\ndistillation, we used the same training settings as those described in Salimans & Ho (2022) for CIFAR-10 and ImageNet\n64ˆ64. Although the original paper did not test on LSUN256ˆ256datasets, we used the same settings for ImageNet\n64ˆ64and found them to work well.\nIn all distillation experiments, we initialized the consistency model with pre-trained EDM weights. For consistency training,\nwe initialized the model randomly, just as we did for training the EDMs.  We trained all consistency models with the\nRectified Adam optimizer (Liu et al., 2019), with no learning rate decay or warm-up, and no weight decay. We also applied\nEMA to the weights of the online consistency models in both consistency distillation and consistency training, as well as\nto the weights of the training online consistency models according to Karras et al. (2022). For LSUN256ˆ256datasets,\nwe chose the EMA decay rate to be the same as that for ImageNet64ˆ64, except for consistency distillation on LSUN\nBedroom256ˆ256, where we found that using zero EMA worked better.\nWhen using the LPIPS metric on CIFAR-10 and ImageNet64ˆ64, we rescale images to resolution224ˆ224with bilinear\nupsampling before feeding them to the LPIPS network. For LSUN256ˆ256, we evaluated LPIPS without rescaling inputs.\nIn addition, we performed horizontal flips for data augmentation for all models and on all datasets. We trained all models on\na cluster of Nvidia A100 GPUs. Additional hyperparameters for consistency training and distillation are listed in Table 3.\nD. Additional Results on Zero-Shot Image Editing\nWith consistency models, we can perform a variety of zero-shot image editing tasks. As an example, we present additional\nresults on colorization (Fig. 8), super-resolution (Fig. 9), inpainting (Fig. 10), interpolation (Fig. 11), denoising (Fig. 12),\nand stroke-guided image generation (SDEdit, Meng et al. (2021), Fig. 13). The consistency model used here is trained via\nconsistency distillation on the LSUN Bedroom256ˆ256.\nAll these image editing tasks, except for image interpolation and denoising, can be performed via a small modification to the\nmultistep sampling algorithm in Algorithm 1. The resulting pseudocode is provided in Algorithm 4. Hereyis a reference\nimage that guides sample generation,Ωis a binary mask,dcomputes element-wise products, andAis an invertible linear\ntransformation that maps images into a latent space where the conditional information inyis infused into the iterative\n26",
    "Consistency Models\nAlgorithm 4Zero-Shot Image Editing\n1:Input:Consistency modelf\nθ\np ̈, ̈q, sequence of time pointst\n1\nąt\n2\ną  ̈ ̈ ̈ ąt\nN\n, reference imagey, invertible linear\ntransformationA, and binary image maskΩ\n2:yÐA\n ́1\nrpAyqdp1 ́Ωq`0dΩs\n3:Samplex„Npy,t\n2\n1\nIq\n4:xÐf\nθ\npx,t\n1\nq\n5:xÐA\n ́1\nrpAyqdp1 ́Ωq`pAxqdΩs\n6:forn“2toNdo\n7:Samplex„Npx,pt\n2\nn\n ́ε\n2\nqIq\n8:xÐf\nθ\npx,t\nn\nq\n9:xÐA\n ́1\nrpAyqdp1 ́Ωq`pAxqdΩs\n10:end for\n11:Output:x\ngeneration procedure by masking withΩ. Unless otherwise stated, we choose\nt\ni\n“\nˆ\nT\n1{ρ\n`\ni ́1\nN ́1\npε\n1{ρ\n ́T\n1{ρ\nq\n ̇\nρ\nin our experiments, whereN“40for LSUN Bedroom256ˆ256.\nBelow we describe how to perform each task using Algorithm 4.\nInpaintingWhen using Algorithm 4 for inpainting, we letybe an image where missing pixels are masked out,Ωbe a\nbinary mask where 1 indicates the missing pixels, andAbe the identity transformation.\nColorizationThe algorithm for image colorization is similar, as colorization becomes a special case of inpainting once we\ntransform data into a decoupled space. Specifically, letyPR\nhˆwˆ3\nbe a gray-scale image that we aim to colorize, where\nall channels ofyare assumed to be the same,i.e.,yr:,:,0s “yr:,:,1s “yr:,:,2sin NumPy notation. In our experiments,\neach channel of this gray scale image is obtained from a colorful image by averaging the RGB channels with\n0.2989R`0.5870G`0.1140B.\nWe defineΩP t0,1u\nhˆwˆ3\nto be a binary mask such that\nΩri,j,ks “\n#\n1,   k“1or2\n0,   k“0\n.\nLetQPR\n3ˆ3\nbe an orthogonal matrix whose first column is proportional to the vectorp0.2989,0.5870,0.1140q.  This\northogonal matrix can be obtained easily via QR decomposition, and we use the following in our experiments\nQ“\n ̈\n ̋\n0.4471 ́0.82040.3563\n0.87800.47850\n0.1705 ́0.3129 ́0.9343\n ̨\n‚\n.\nWe then define the linear transformationA:xPR\nhˆwˆ3\nÞÑyPR\nhˆwˆ3\n, where\nyri,j,ks “\n2\nÿ\nl“0\nxri,j,lsQrl,ks.\nBecauseQis orthogonal, the inversionA\n ́1\n:yPR\nhˆw\nÞÑxPR\nhˆwˆ3\nis easy to compute, where\nxri,j,ks “\n2\nÿ\nl“0\nyri,j,lsQrk,ls.\nWithAandΩdefined as above, we can now use Algorithm 4 for image colorization.\n27",
    "Consistency Models\nSuper-resolutionWith a similar strategy, we employ Algorithm 4 for image super-resolution. For simplicity, we assume\nthat the down-sampled image is obtained by averaging non-overlapping patches of sizepˆp. Suppose the shape of full\nresolution images ishˆwˆ3.  LetyPR\nhˆwˆ3\ndenote a low-resolution image naively up-sampled to full resolution,\nwhere pixels in each non-overlapping patch share the same value. Additionally, letΩP t0,1u\nh{pˆw{pˆp\n2\nˆ3\nbe a binary\nmask such that\nΩri,j,k,ls “\n#\n1,   kě1\n0,   k“0\n.\nSimilar  to  image  colorization,  super-resolution  requires  an  orthogonal  matrixQPR\np\n2\nˆp\n2\nwhose  first  column  is\np\n1\n{p,\n1\n{p, ̈ ̈ ̈,\n1\n{pq.  This orthogonal matrix can be obtained with QR decomposition.  To perform super-resolution, we\ndefine the linear transformationA:xPR\nhˆwˆ3\nÞÑyPR\nh{pˆw{pˆp\n2\nˆ3\n, where\nyri,j,k,ls “\np\n2\n ́1\nÿ\nm“0\nxriˆp`pm ́mmodpq{p,jˆp`mmodp,lsQrm,ks.\nThe inverse transformationA\n ́1\n:yPR\nh{pˆw{pˆp\n2\nˆ3\nÞÑxPR\nhˆwˆ3\nis easy to derive, with\nxri,j,k,ls “\np\n2\n ́1\nÿ\nm“0\nyriˆp`pm ́mmodpq{p,jˆp`mmodp,lsQrk,ms.\nAbove definitions ofAandΩallow us to use Algorithm 4 for image super-resolution.\nStroke-guided image generationWe can also use Algorithm 4 for stroke-guided image generation as introduced in\nSDEdit (Meng et al., 2021). Specifically, we letyPR\nhˆwˆ3\nbe a stroke painting. We setA“I, and defineΩPR\nhˆwˆ3\nas a matrix of ones. In our experiments, we sett\n1\n“5.38andt\n2\n“2.24, withN“2.\nDenoisingIt is possible to denoise images perturbed with various scales of Gaussian noise using a single consistency\nmodel. Suppose the input imagexis perturbed withNp0;σ\n2\nIq. As long asσP rε,Ts, we can evaluatef\nθ\npx,σqto produce\nthe denoised image.\nInterpolationWe can interpolate between two images generated by consistency models. Suppose the first samplex\n1\nis\nproduced by noise vectorz\n1\n, and the second samplex\n2\nis produced by noise vectorz\n2\n. In other words,x\n1\n“f\nθ\npz\n1\n,Tqand\nx\n2\n“f\nθ\npz\n2\n,Tq. To interpolate betweenx\n1\nandx\n2\n, we first use spherical linear interpolation to get\nz“\nsinrp1 ́αqψs\nsinpψq\nz\n1\n`\nsinpαψq\nsinpψq\nz\n2\n,\nwhereαP r0,1sandψ“arccosp\nz\nT\n1\nz\n2\n∥z\n1\n∥\n2\n∥z\n2\n∥\n2\nq, then evaluatef\nθ\npz,Tqto produce the interpolated image.\nE. Additional Samples from Consistency Models\nWe provide additional samples from consistency distillation (CD) and consistency training (CT) on CIFAR-10 (Figs. 14\nand 18), ImageNet64ˆ64(Figs. 15 and 19), LSUN Bedroom256ˆ256(Figs. 16 and 20) and LSUN Cat256ˆ256\n(Figs. 17 and 21).\n28",
    "Consistency Models\nFigure 8: Gray-scale images (left), colorized images by a consistency model (middle), and ground truth (right).\n29",
    "Consistency Models\nFigure 9: Downsampled images of resolution32ˆ32(left), full resolution (256ˆ256) images generated by a consistency\nmodel (middle), and ground truth images of resolution256ˆ256(right).\n30",
    "Consistency Models\nFigure 10: Masked images (left), imputed images by a consistency model (middle), and ground truth (right).\n31",
    "Consistency Models\nFigure 11: Interpolating between leftmost and rightmost images with spherical linear interpolation. All samples are generated\nby a consistency model trained on LSUN Bedroom256ˆ256.\n32",
    "Consistency Models\nFigure 12: Single-step denoising with a consistency model. The leftmost images are ground truth. For every two rows, the\ntop row shows noisy images with different noise levels, while the bottom row gives denoised images.\n33",
    "Consistency Models\nFigure 13: SDEdit with a consistency model. The leftmost images are stroke painting inputs. Images on the right side are\nthe results of stroke-guided image generation (SDEdit).\n34",
    "Consistency Models\n(a) EDM (FID=2.04)\n(b) CD with single-step generation (FID=3.55)\n(c) CD with two-step generation (FID=2.93)\nFigure 14: Uncurated samples from CIFAR-1032ˆ32. All corresponding samples use the same initial noise.\n35",
    "Consistency Models\n(a) EDM (FID=2.44)\n(b) CD with single-step generation (FID=6.20)\n(c) CD with two-step generation (FID=4.70)\nFigure 15: Uncurated samples from ImageNet64ˆ64. All corresponding samples use the same initial noise.\n36",
    "Consistency Models\n(a) EDM (FID=3.57)\n(b) CD with single-step generation (FID=7.80)\n(c) CD with two-step generation (FID=5.22)\nFigure 16: Uncurated samples from LSUN Bedroom256ˆ256. All corresponding samples use the same initial noise.\n37",
    "Consistency Models\n(a) EDM (FID=6.69)\n(b) CD with single-step generation (FID=10.99)\n(c) CD with two-step generation (FID=8.84)\nFigure 17: Uncurated samples from LSUN Cat256ˆ256. All corresponding samples use the same initial noise.\n38",
    "Consistency Models\n(a) EDM (FID=2.04)\n(b) CT with single-step generation (FID=8.73)\n(c) CT with two-step generation (FID=5.83)\nFigure 18: Uncurated samples from CIFAR-1032ˆ32. All corresponding samples use the same initial noise.\n39",
    "Consistency Models\n(a) EDM (FID=2.44)\n(b) CT with single-step generation (FID=12.96)\n(c) CT with two-step generation (FID=11.12)\nFigure 19: Uncurated samples from ImageNet64ˆ64. All corresponding samples use the same initial noise.\n40",
    "Consistency Models\n(a) EDM (FID=3.57)\n(b) CT with single-step generation (FID=16.00)\n(c) CT with two-step generation (FID=7.80)\nFigure 20: Uncurated samples from LSUN Bedroom256ˆ256. All corresponding samples use the same initial noise.\n41",
    "Consistency Models\n(a) EDM (FID=6.69)\n(b) CT with single-step generation (FID=20.70)\n(c) CT with two-step generation (FID=11.76)\nFigure 21: Uncurated samples from LSUN Cat256ˆ256. All corresponding samples use the same initial noise.\n42"
  ]
}