{
  "key": "TZ6EGBTD",
  "url": "http://arxiv.org/pdf/2305.13048",
  "metadata": {
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "abstract": "  Transformers have revolutionized almost all natural language processing (NLP)\ntasks but suffer from memory and computational complexity that scales\nquadratically with sequence length. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and computational requirements but\nstruggle to match the same performance as Transformers due to limitations in\nparallelization and scalability. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that combines the efficient\nparallelizable training of transformers with the efficient inference of RNNs.\n  Our approach leverages a linear attention mechanism and allows us to\nformulate the model as either a Transformer or an RNN, thus parallelizing\ncomputations during training and maintains constant computational and memory\ncomplexity during inference. We scale our models as large as 14 billion\nparameters, by far the largest dense RNN ever trained, and find RWKV performs\non par with similarly sized Transformers, suggesting future work can leverage\nthis architecture to create more efficient models. This work presents a\nsignificant step towards reconciling trade-offs between computational\nefficiency and model performance in sequence processing tasks.\n",
    "published": "2023-05-22T13:57:41Z"
  },
  "text": [
    "RWKV: Reinventing RNNs for the Transformer Era\nBo Peng\n1,2∗\nEric Alcaide\n2,3,4∗\nQuentin Anthony\n2,5∗\nAlon Albalak\n2,6\nSamuel Arcadinho\n2,7\nStella Biderman\n2,8\nHuanqi Cao\n9\nXin Cheng\n10\nMichael Chung\n11\nXingjian Du\n1\nMatteo Grella\n12\nKranthi Kiran GV\n2,13\nXuzheng He\n2\nHaowen Hou\n14\nJiaju Lin\n1\nPrzemysław Kazienko\n15\nJan Koco\n ́\nn\n15\nJiaming Kong\n16\nBartłomiej Koptyra\n15\nHayden Lau\n2\nKrishna Sri Ipsit Mantri\n17\nFerdinand Mom\n18,19\nAtsushi Saito\n2,20\nGuangyu Song\n21\nXiangru Tang\n22\nBolun Wang\n23\nJohan S. Wind\n24\nStanisław Wo ́zniak\n15\nRuichong Zhang\n9\nZhenyuan Zhang\n2\nQihang Zhao\n25,26\nPeng Zhou\n23\nQinghua Zhou\n5\nJian Zhu\n27\nRui-Jie Zhu\n28,29\n1\nGenerative AI Commons\n2\nEleutherAI\n3\nU. of Barcelona\n4\nCharm Therapeutics\n5\nOhio State U.\n6\nU. of C., Santa Barbara\n7\nZendesk\n8\nBooz Allen Hamilton\n9\nTsinghua University\n10\nPeking University\n11\nStoryteller.io\n12\nCrisis24\n13\nNew York U.\n14\nNational U. of Singapore\n15\nWroclaw U. of Science and Technology\n16\nDatabaker Technology\n17\nPurdue U.\n18\nCriteo AI Lab\n19\nEpita\n20\nNextremer\n21\nMoves\n22\nYale U.\n23\nRuoxinTech\n24\nU. of Oslo\n25\nU. of Science and Technology of China\n26\nKuaishou Technology\n27\nU. of British Columbia\n28\nU. of C., Santa Cruz\n29\nU. of Electronic Science and Technology of China\nAbstract\nTransformers  have  revolutionized  almost  all\nnatural language processing (NLP) tasks but\nsuffer from memory and computational com-\nplexity that scales quadratically with sequence\nlength. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and\ncomputational  requirements  but  struggle  to\nmatch the same performance as Transformers\ndue to limitations in parallelization and scala-\nbility. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that\ncombines the efficient parallelizable training\nof transformers with the efficient inference of\nRNNs.\nOur approach leverages a linear attention mech-\nanism and allows us to formulate the model as\neither a Transformer or an RNN, thus paralleliz-\ning computations during training and maintains\nconstant computational and memory complex-\nity during inference.  We scale our models as\nlarge as 14 billion parameters, by far the largest\ndense RNN ever trained, and find RWKV per-\nforms on par with similarly sized Transformers,\nsuggesting future work can leverage this archi-\ntecture to create more efficient models.  This\nwork presents a significant step towards rec-\nonciling trade-offs between computational ef-\nficiency and model performance in sequence\nprocessing tasks.\n1\n1    Introduction\nDeep learning has greatly advanced artificial intelli-\ngence, impacting a range of scientific and industrial\nuses. These often involve complex sequential data\n∗\nEqual first authorship. Others listed alphabetically.\n1\nCode at: https://github.com/BlinkDL/RWKV-LM\nprocessing tasks such as natural language under-\nstanding, conversational AI, time-series analysis,\nand indirectly sequential formats like images and\ngraphs (Brown et al., 2020; Ismail Fawaz et al.,\n2019; Wu et al., 2020; Albalak et al., 2022). Pre-\ndominant among these techniques include RNNs\nand Transformers (Vaswani et al., 2017), each with\nspecific benefits and drawbacks. RNNs require less\nmemory, particularly for handling long sequences.\nHowever, they suffer from the vanishing gradient\nproblem and non-parallelizability in the time di-\nmension during training, limiting their scalability\n(Hochreiter, 1998; Le and Zuidema, 2016).\n10\n2\n10\n3\nCompute (exaFLOP)\n40\n45\n50\n55\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\nFigure 1: Average performance of RWKV models com-\npared to transformers across twelve NLP tasks.   For\nfurther details, see section 5.\nTransformers emerged as a powerful alternative,\nadept at managing local and long-range dependen-\ncies and supporting parallelized training (Tay et al.,\n2022). Models such as GPT-3 (Brown et al., 2020),\nChatGPT  (OpenAI,  2022;  Koco\n ́\nn  et  al.,  2023),\narXiv:2305.13048v2  [cs.CL]  11 Dec 2023",
    "ModelTimeSpace\nTransformerO(T\n2\nd)O(T\n2\n+T d)\nReformerO(TlogT d)O(TlogT+T d)\nPerformerO(T d\n2\nlogd)O(T dlogd+d\n2\nlogd)\nLinear TransformersO(T d\n2\n)O(T d+d\n2\n)\nAFT-fullO(T\n2\nd)O(T d)\nAFT-localO(T sd)O(T d)\nMEGAO(cT d)O(cd)\nRWKV (ours)O(Td)O(d)\nTable 1: Inference complexity comparison with different\nTransformers.   HereTdenotes  the  sequence  length,\ndthe feature dimension,cis MEGA’s chunk size of\nquadratic attention, andsis the size of a local window\nfor AFT.\nLLaMA  (Touvron  et  al.,  2023),  and  Chinchilla\n(Hoffmann et al., 2022) showcase the potential of\nTransformers in NLP. However, the self-attention\nmechanism’s quadratic complexity makes it compu-\ntationally and memory intensive for tasks involving\nlong sequences and constrained resources.  This\nhas stimulated research to enhance Transformers’\nscalability, sometimes sacrificing some of their ef-\nfectiveness (Wang et al., 2020; Zaheer et al., 2020;\nDao et al., 2022a).\nTo tackle these challenges, we introduce the Re-\nceptance  Weighted  Key  Value  (RWKV)  model,\ncombining the strengths of RNNs and Transform-\ners while circumventing key drawbacks.  RWKV\nalleviates memory bottleneck and quadratic scaling\nassociated with Transformers (Katharopoulos et al.,\n2020) with efficient linear scaling, while maintain-\ning the expressive properties of the Transformer,\nsuch as parallelized training and robust scalability.\nRWKV reformulates the attention mechanism with\na variant of linear attention, replacing traditional\ndot-product token interaction with more effective\nchannel-directed attention.  This implementation,\nwithout approximation, offers the lowest computa-\ntional and memory complexity; see Table 1.\nThe motivation behind RWKV is to balance com-\nputational efficiency with expressive capacity in\nneural networks. It offers a solution for handling\nlarge-scale models with billions of parameters, ex-\nhibiting competitive performance at a reduced com-\nputational cost.  Experiments suggest RWKV ad-\ndresses scaling and deployment challenges in AI,\nespecially for sequential data processing, pointing\ntowards more sustainable and efficient AI models.\nOur contributions in this paper are as follows:\n•\nThe introduction of RWKV, a novel architec-\nture combining RNNs and Transformer advan-\ntages while mitigating their limitations.\n•Detailed experiments demonstrating RWKV’s\nperformance  and  efficiency  on  benchmark\ndatasets for large-scale models.\n•\nThe release of pretrained models, from 169\nmillion to 14 billion parameters, trained on\nthe Pile (Gao et al., 2020; Biderman et al.,\n2022).\n2\n2    Background\nHere we briefly review the fundamentals of RNNs\nand Transformers.\n2.1    Recurrent Neural Networks (RNNs)\nPopular RNN architectures such as LSTM (Hochre-\niter  and  Schmidhuber,  1997)  and  GRU  (Chung\net al., 2014) are characterized by the following for-\nmulation (shown for LSTM, others can be reasoned\nsimilarly):\nf\nt\n=σ\ng\n(W\nf\nx\nt\n+U\nf\nh\nt−1\n+b\nf\n),(1)\ni\nt\n=σ\ng\n(W\ni\nx\nt\n+U\ni\nh\nt−1\n+b\ni\n),(2)\no\nt\n=σ\ng\n(W\no\nx\nt\n+U\no\nh\nt−1\n+b\no\n),(3)\n ̃c\nt\n=σ\nc\n(W\nc\nx\nt\n+U\nc\nh\nt−1\n+b\nc\n),(4)\nc\nt\n=f\nt\n⊙c\nt−1\n+i\nt\n⊙ ̃c\nt\n,(5)\nh\nt\n=o\nt\n⊙σ\nh\n(c\nt\n).(6)\nAlthough RNNs can be factored into two linear\nblocks (WandU) and an RNN-specific block (1)–\n(6), as noted by Bradbury et al. (2017), the data de-\npendency relying on previous time steps prohibits\nparallelizing these typical RNNs.\n2.2    Transformers and AFT\nIntroduced by Vaswani et al. (2017), Transformers\nare a class of neural networks that have become\nthe dominant architecture for several NLP tasks.\nInstead of operating on sequences step-by-step like\nRNNs, Transformers rely on attention mechanisms\nto capture relationships between all input and all\noutput tokens:\nAttn(Q,K,V) = softmax(QK\n⊤\n)V,(7)\nwhere the multi-headness and scaling factor\n1\n√\nd\nk\nis\nomitted for convenience. The coreQK\n⊤\nmultipli-\ncation is an ensemble of pairwise attention scores\n2\nhttps://huggingface.co/RWKV",
    "between each token in a sequence, which can be\ndecomposed as vector operations:\nAttn(Q,K,V)\nt\n=\nP\nT\ni=1\ne\nq\n⊤\nt\nk\ni\n⊙v\ni\nP\nT\ni=1\ne\nq\n⊤\nt\nk\ni\n.(8)\nAFT (Zhai et al., 2021), alternately formulates\nAttn\n+\n(W,K,V)\nt\n=\nP\nt\ni=1\ne\nw\nt,i\n+k\ni\n⊙v\ni\nP\nt\ni=1\ne\nw\nt,i\n+k\ni\n,(9)\nwhere{w\nt,i\n}∈R\nT×T\nis the learned pair-wise po-\nsition biases, and eachw\nt,i\nis a scalar.\nInspired  by  AFT,  RWKV  takes  a  similar  ap-\nproach.  However, for simplicity, it modifies the\ninteraction weights so that it can be transformed\ninto an RNN. Eachw\nt,i\nin RWKV is a channel-\nwise time decay vector multiplied by the relative\nposition and traced backward from current time as\nit decays:\nw\nt,i\n=−(t−i)w,(10)\nwherew∈(R\n≥0\n)\nd\n, withdthe number of chan-\nnels.  We requirewto be non-negative to ensure\nthate\nw\nt,i\n≤1\nand the per-channel weights decay\nbackwards in time.\n3    RWKV\nThe RWKV model architecture is defined by four\nfundamental elements that are intrinsic to the time-\nmixing and channel-mixing blocks:\n•R: TheReceptancevector acts as the receiver\nof past information.\n•W: TheWeightsignifies the positional\nweight  decay  vector,  a  trainable  parameter\nwithin the model.\n•K\n: TheKeyvector performs a role analogous\ntoKin traditional attention mechanisms.\n•V: TheValuevector functions similarly toV\nin conventional attention processes.\nThese  core  elements  interact  multiplicatively  at\neach timestep, as depicted in Figure 2.\n3.1    Architecture\nThe RWKV model is composed of stacked residual\nblocks. Each block consists of a time-mixing and\na channel-mixing sub-block, embodying recurrent\nstructures to leverage past information.\nThis model uses a unique attention-like score\nupdate process, which includes a time-dependent\nTime \nMixing\nLayerNorm\nLayerNorm\n?\nChannel \nMixing\n?\n?'\nOut\nWKV\nVKR\n?\nLayerNorm\nR'K'\nV'\nRWKV-LM \nHead\nInput \nEmbedding\nLayerNorm\nLayerNorm\nOutput Probabilities\nLayerNorm\nLayerNorm\nChannel Mixing\nTime Mixing\nOut\nSoftmax\nFigure 2: Elements within an RWKV block (left) and\nthe complete RWKV residual block, equipped with a\nfinal head for language modeling (right).\nsoftmax operation improving numerical stability\nand mitigating vanishing gradients (for rigorous\nproof,  see Appendix H). It ensures that the gra-\ndient is propagated along the most relevant path.\nAdditionally, layer normalization (Ba et al., 2016)\nincorporated within the architecture aids in stabi-\nlizing  the  gradients,  effectively  addressing  both\nvanishing and exploding gradient issues.   These\ndesign elements not only enhance the training dy-\nnamics of deep neural networks but also facilitate\nthe stacking of multiple layers, leading to superior\nperformance over conventional RNN models by\ncapturing complex patterns across different levels\nof abstraction (see also Appendix I).\n3.1.1    Token Shift\nIn this architecture,  all linear projection vectors\n(R,K,Vin time-mixing, andR\n′\n,K\n′\nin channel-\nmixing) involved in computations are produced by\nlinear interpolation between current and previous\ntimestep inputs, facilitating a token shift.\nThe  vectors  for  time-mixing  computation  are\nlinear  projections  of  linear  combinations  of  the\ncurrent and previous inputs of the block:\nr\nt\n=W\nr\n·(μ\nr\n⊙x\nt\n+ (1−μ\nr\n)⊙x\nt−1\n),(11)\nk\nt\n=W\nk\n·(μ\nk\n⊙x\nt\n+ (1−μ\nk\n)⊙x\nt−1\n),(12)\nv\nt\n=W\nv\n·(μ\nv\n⊙x\nt\n+ (1−μ\nv\n)⊙x\nt−1\n),(13)",
    "Token\nshift\nToken\nshift\nStatesStates\nLayer Norm\nMy\nLayer Norm\nname\nLayer Norm\nTime Mix\nLM Head\nLayer Norm\nis\nLayer Norm\nBob\nLayer Norm\nChannel Mix\nTime Mix\nLM Head\nLayer Norm\nname\nLayer Norm\nis\nLayer Norm\nChannel Mix\nTime Mix\nLM Head\nToken\nshift\nToken\nshift\nChannel Mix\nFigure 3: RWKV architecture for language modeling.\nas are the channel-mixing inputs:\nr\n′\nt\n=W\n′\nr\n·(μ\n′\nr\n⊙x\nt\n+ (1−μ\n′\nr\n)⊙x\nt−1\n),(14)\nk\n′\nt\n=W\n′\nk\n·(μ\n′\nk\n⊙x\nt\n+ (1−μ\n′\nk\n)⊙x\nt−1\n).(15)\nThe token shift is implemented as a simple off-\nset in the temporal dimension at each block us-\ning the PyTorch (Paszke et al., 2019) library as\nnn.ZeroPad2d((0,0,1,-1)).\n3.1.2    WKV Operator\nThe  computation  of  theWKVoperator  in  our\nmodel parallels the method used in Attention Free\nTransformer (AFT) (Zhai et al., 2021).  However,\nunlike  AFT  whereWis  a  pairwise  matrix,  our\nmodel treatsWas a channel-wise vector that is\nmodified by relative position.  In our model, this\nrecurrent behavior is defined by the time-dependent\nupdate of theWKVvectors, formalized in the fol-\nlowing equation:\nwkv\nt\n=\nP\nt−1\ni=1\ne\n−(t−1−i)w+k\ni\n⊙v\ni\n+e\nu+k\nt\n⊙v\nt\nP\nt−1\ni=1\ne\n−(t−1−i)w+k\ni\n+e\nu+k\nt\n.\n(16)\nTo circumvent any potential degradation ofW,\nwe introduce a vectorUthat separately attends to\nthe current token. More information about this can\nbe found in Appendix I.\n3.1.3    Output Gating\nOutput gating is implemented in both time-mixing\nand channel-mixing blocks using the sigmoid of\nthe receptance,σ(r). The output vectoro\nt\npost the\nWKVoperator is given by:\no\nt\n=W\no\n·(σ(r\nt\n)⊙wkv\nt\n).(17)\nIn the channel-mixing block, a similar operation\nis performed:\no\n′\nt\n=σ(r\n′\nt\n)⊙(W\n′\nv\n·max(k\n′\nt\n,0)\n2\n),(18)\nwhere we adopt the squared ReLU activation func-\ntion (So et al., 2021).\n3.2    Transformer-like Training\nRWKV can be efficiently parallelized using a tech-\nnique calledtime-parallel mode,  reminiscent of\nTransformers. The time complexity of processing\na batch of sequences in a single layer isO(BTd\n2\n),\nprimarily consisting of matrix multiplicationsW\nλ\n,\nwhereλ∈{r,k,v,o}(assumingBsequences,T\nmaximum tokens, anddchannels). In contrast, up-\ndating attention scoreswkv\nt\ninvolves a serial scan\n(see Appendix D for more detail) and has complex-\nityO(BTd).\nThe matrix multiplications can be parallelized\nsimilarly toW\nλ\n, whereλ∈ {Q,K,V,O}in con-\nventional Transformers. The element-wiseWKV\ncomputation is time-dependent but can be readily\nparallelized along the other two dimensions (Lei\net al., 2018)\n3\n.\n3.3    RNN-like Inference\nRecurrent networks commonly utilize the output\nat statetas input at statet+ 1.   This usage is\nalso observed in the autoregressive decoding infer-\nence of language models, where each token must\nbe computed before being passed to the next step.\nRWKV takes advantage of this RNN-like structure,\nknown astime-sequential mode.  In this context,\nRWKV can be conveniently formulated recursively\nfor decoding during inference, as demonstrated in\nAppendix D.\n3.4    Additional Optimizations\nCustom KernelsTo address inefficiencies in the\nWKVcomputation arising from the sequential na-\nture of the task when using standard deep learning\nframeworks, we have developed a custom CUDA\n3\nFor extremely long sequences, more sophisticated meth-\nods such as Martin and Cundy (2017) that parallelize over\nsequence length could be used.",
    "kernel. This kernel enables the execution of a sin-\ngle compute kernel on training accelerators, while\nall other parts of the model, such as matrix multi-\nplications and point-wise operations, are already\ninherently parallelizable and efficient.\nSmall Init EmbeddingDuring the initial stage\nof training a transformer model (Vaswani et al.,\n2017), we observe that the embedding matrix un-\ndergoes slow changes, presenting a challenge for\nthe model to move away from its initial noisy em-\nbedding state.  To address this issue, we propose\nan approach that involves initializing the embed-\nding matrix with small values and subsequently\napplying an additional LayerNorm operation. This\naccelerates and stabilizes the training process, al-\nlowing for the training of deep architectures with\npost-LN  components.   The  effectiveness  of  this\napproach is demonstrated in Figure 9, illustrating\nimproved convergence by enabling the model to\nquickly transition away from the initially small em-\nbedding. This is achieved through small changes\noccurring in a single step, which subsequently lead\nto substantial alterations in directions and further\nnotable changes after the LayerNorm operation.\nCustom  InitializationBuilding  on  principles\nfrom previous works (He et al., 2016; Jumper et al.,\n2021), we adopt an initialization strategy where\nparameters are set to values resembling an identity\nmapping while breaking symmetry to establish a\nclear information flow. The majority of weights are\ninitialized to zero, and linear layers do not employ\nbiases. Detailed formulas are given in Appendix E.\nWe observe that the choice of initialization plays a\ncrucial role in both the speed and quality of conver-\ngence (refer to Appendix F for further details).\n3.5    Implementation\nRWKV is implemented using the PyTorch Deep\nLearning Library (Paszke et al., 2019).  We inte-\ngrate additional optimization strategies inspired by\nDeepSpeed (Rasley et al., 2020) into the system,\nimproving its efficiency and scalability.\nThe model begins with an embedding layer, as\ndetailed  in  Section  3.4.   Following  this  are  sev-\neral identical residual blocks arranged sequentially.\nThese are depicted in Figures 2 and 3 and adheres\nto the principles outlined in Section 3.1.1.  After\nthe last block, a simple output projection head, con-\nsisting  of  a  LayerNorm  (Ba  et  al.,  2016)  and  a\nlinear projection, is employed for logits generation\nfor next-token prediction and computation of the\ncross-entropy loss during training.\n4    Trained Models and Computing Costs\nTo demonstrate the scalability of RWKV, we train\nsix models ranging from 169 million to 14 billion\nparameters as shown in Table 2.  All models are\ntrained for one epoch (330 billion tokens) on the\nPile (Gao et al., 2020; Biderman et al., 2022).\nNameLayersModel DimensionParametersFLOP per token\n169 M127681.693×10\n8\n2.613×10\n8\n430 M2410244.304×10\n8\n7.573×10\n8\n1.5 B2420481.515×10\n9\n2.823×10\n9\n3 B3225602.985×10\n9\n5.710×10\n9\n7 B3240967.393×10\n9\n1.437×10\n10\n14 B4051201.415×10\n10\n2.778×10\n10\nTable 2: RWKV model architectures and FLOP counts.\nFurther details of these hyperparameters are elaborated\nupon in Appendix G.\nThe  number  of  parameters  for  each  model  is\ncomputed  using  the  formula:# parameters=\n2V D+ 13D\n2\nL+D(11L+ 4)whereV= 50277\nis the vocabulary size,Drepresents the Model Di-\nmension andLcorresponds to the number of lay-\ners. FLOPs is for a forward pass for one token. It\nwas calculated as2(2V D+ 13D\n2\nL), which is the\ntwice (add and multiply) the number of parameters\nin linear layers.  The backwards pass FLOPs can\nbe approximated as twice that of the forward pass,\ngiving a total of6(2V D+ 13D\n2\nL)FLOP per to-\nken.  Notably, this matches the standard formula\nfor FLOP calculations in transformers Kaplan et al.\n(2020): FLOP= 6·[# tokens]·[# parameters].\n4.1    Additional Training Details\nFor training, we use the standard Adam optimizer\nwithout weight decay, use bfloat16 precision, and\ntrain with a context length of 1024 tokens.  Fur-\nther details on hyperparameters are in Appendix G.\nDiverting from standard practice for transformers,\nwe apply exponential decay to our learning rate.\nWe also incorporate the auxiliary loss introduced\nby PaLM (Chowdhery et al., 2022), supplement-\ning the standard cross-entropy loss function. This\nauxiliary loss encourages the softmax normalizer\nto approximate zero closely.  As for the learning\nrate schedule,  it remains constant for the initial\niterations, and subsequently decays exponentially.",
    "4.2    Scaling Laws\nScaling laws (Kaplan et al., 2020; Henighan et al.,\n2020; Hoffmann et al., 2022; Muennighoff et al.,\n2023) in language models refer to the mathemati-\ncal relationships that describe how the performance\nof a language model changes with respect to var-\nious factors. These factors can include the model\nsize (N), dataset size (D), or the optimally allo-\ncated compute budget (C\nmin\n).   Scaling laws are\nimportant for two primary reasons: they allow us\nto make predictions and plans regarding the costs\nand performance of large models before they are\ntrained via interpolation and extrapolation (Black\net al., 2022; Le Scao et al., 2022) and the contexts\nin which they fail provides rich feedback on im-\nportant areas for future research (Wei et al., 2022a;\nBiderman et al., 2023a).\nPrevious work on scaling laws for RNNs has\nclaimed that LSTMs do not strictly follow the same\nlog-log linear scaling that transformers do (Kaplan\net al., 2020).   We train 45 RWKV models for a\nvariety of pairs (dataset, parameters) and find that\nRWKVdoesfollow the same general form of the\nscaling law that is well established for transformers.\nFigure 4 shows our results for loss as a function of\ncompute, with the linear fit to the Pareto optimal\npoints holding anr\n2\nvalue of0.994.  Even when\nwe  extrapolate  our  curve  an  additional  order  of\nmagnitude (blue), we find an extremely good fit\nwith anr\n2\nof0.875.\n10\n1\n10\n0\n10\n1\n10\n2\n10\n3\nCompute (exaFLOP)\n2 × 10\n0\n3 × 10\n0\nLoss\nNon-Optimal\nOptimal\nTest\nTrend Line\nFigure 4: Scaling laws curves for RWKV models\n5    Evaluations\nHaving  demonstrated  the  scalability  of  RWKV\nmodels in the previous section, we now turn our\nattention to their competitiveness with traditional\ntransformers. We focus on two questions:\nCompetitivenessIs RWKV competitive against\nquadratic transformer architectures with the same\namount of compute?\nLong   ContextDoes   increasing   the   context\nlength of RWKV yield better language modeling\nloss when RWKV models are trained for context\nlengths  that  most  open-sourced  quadratic  trans-\nformerscannotefficiently process?\n5.1    NLP Evaluations\nTo demonstrate that RWKV is competitive with\ntraditional transformers at NLP tasks, we compare\nwith similarly sized models trained for a similar\nnumber of tokens (Pythia (Biderman et al., 2023b),\nOPT (Zhang et al., 2022) and BLOOM (Scao et al.,\n2022)).  All RWKV models were trained for one\nepoch on the Pile (330B tokens), which is close but\nnot identical to the amount of tokens the Pythia,\nOPT, and BLOOM models were trained for. Con-\nsequently,  we compare our models on aFLOP-\nmatched basis.  We avoid comparing with model\ntrained  in  the  Chinchilla-optimal  regime  (Hoff-\nmann et al., 2022) or the overtrained regime (Tou-\nvron et al., 2023) to ensure the most equitable com-\nparison.\nWe report results on ARC (both Easy and Chal-\nlenge)  (Clark et al., 2018), BoolQ  (Clark et al.,\n2019), COPA  (Roemmele et al., 2018), HeadQA\n(Vilares and Gómez-Rodríguez, 2019), HellaSwag\n(Zellers et al., 2019), LAMBADA (Paperno et al.,\n2016),  OpenBookQA   (Mihaylov  et  al.,  2018),\nPIQA (Bisk et al., 2020), ReCoRD  (Zhang et al.,\n2018), SciQ  (Johannes Welbl Nelson F. Liu, 2017),\nand Winogrande  (Zellers et al., 2020).  Figure 1\nshows the average results across all benchmarks.\nSome individual benchmarks are shown in Fig 5,\nwith the rest in Appendix J.\nAdditionally, we carried out comparative studies\non RWKV and ChatGPT / GPT-4, see Appendix\nL. They revealed that RWKV is very sensitive to\nprompt engineering.  When the prompts were ad-\njusted (re-ordered) from the ones used for GPT to\nmore suitable for RWKV, the performance (F1) in-\ncreased even from 44.2% to 74.8%.  For sarcasm\ndetection, RWKV outperformed ChatGPT, but was\nstill slightly worse than the SOTA solution.\n5.2    Extended Context Finetuning\nUnlike  transformers,  RNNs  do  not  have  a  pre-\ndefined sequences length when they are created.\nHowever in order to efficient make use of compute",
    "10\n2\n10\n3\nCompute (exaFLOP)\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(a) ARC (Challenge)\n10\n2\n10\n3\nCompute (exaFLOP)\n30\n35\n40\n45\n50\nAccuracy\n(b) HellaSwag\n10\n2\n10\n3\nCompute (exaFLOP)\n20\n30\n40\n50\n60\n70\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(c) LAMBADA (OpenAI)\n10\n2\n10\n3\nCompute (exaFLOP)\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nAccuracy\n(d) OpenBookQA\n10\n2\n10\n3\nCompute (exaFLOP)\n50\n55\n60\n65\n70\n75\n80\n85\n90\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(e) ReCoRD\n10\n2\n10\n3\nCompute (exaFLOP)\n50\n52\n54\n56\n58\n60\n62\n64\n66\nAccuracy\n(f) Winogrande\nFigure 5: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. Additional\nplots can be found in Appendix J.\nwe nevertheless need to preprocess the training data\ninto contexts of the same length. We find that we\nare able to teach the model how to efficiently handle\nsubstantially larger batch sizes by finetuning with\nprogressively increasing sequence length. Specif-\nically, we first double the sequence length from\n1024 to 2048 and finetune for 10B tokens from the\noriginal pretraining corpus, then we double again\nto 4096 for 100B tokens from the same corpus, and\nfinally double to 8192 tokens for another 100B to-\nkens from the same corpus. In Fig. 6 we show that\nincreasing context length leads to lower test loss\non the Pile, an indication that RWKV can make\neffective use of long contextual information.\n2\n1\n2\n3\n2\n5\n2\n7\n2\n9\n2\n11\nContext Length\n2\n1\n2\n2\nPile test loss\n7B 8k\n14B 8k\nFigure 6: RWKV shows decreasing mean test loss as a\nfunction of context length on the Pile (Gao et al., 2020)\n5.3    Long Context Benchmarks\nAdditionally,  we evaluate our model’s ability to\nhandle very long sequences by comparing to state-\nof-the-art  long  sequence  models  on  the  Long-\nRange Arena (LRA) benchmark (Tay et al., 2021).\nLRA is designed to assess the performance of mod-\nels in handling lengthy context situations.  It in-\ncludes a collection of tasks with sequences rang-\ning from 1,000 to 16,000 tokens, covering various\ntypes of data like text, natural language, synthetic\nimages, and mathematical expressions. We apply\nRWKV on the LRA benchmark and the results are\nin Appendix J.2. The results show that RWKV per-\nforms second only to the S4 model in five datasets.\n6    Inference Experiments\nWe benchmark inference requirements according to\nsize and family. Specifically, we evaluate text gen-\neration speed and memory requirements on typical\ncompute platforms including CPU (x86) and GPU\n(NVIDIA A100 80 GB). For all of our inference\nexperiments we use float32 precision and the Hug-\ngingFace Transformers (Wolf et al., 2020). We in-\nclude all model parameters in the parameter count,\nincluding both embedding and non-embedding lay-\ners. Performance under different quantization se-\ntups is left to further work.  See Appendix K for",
    "more results.\nFigure 7: Cumulative time on text generation for LLMs.\nUnlike transformers, RWKV exhibits linear scaling.\n7    Future Work\nThere are several promising directions for future\nwork on the RWKV architecture.   Work can be\ndone to increase model expressivity by enhancing\nthe time-decay formulations and exploring initial\nmodel states while maintaining efficiency.\nThe  RWKV  computational  efficiency  can  be\nfurther improved by applying a parallel scan in\nthewkv\nt\nstep to reduce the computational cost to\nO(Blog(T)d).\nThe mechanisms used in RWKV can be applied\nto  encoder-decoder  architectures,  potentially  re-\nplacing the cross-attention mechanism. This could\nbe applicable in seq2seq or multimodal settings,\nthereby enhancing efficiency during both training\nand inference.\nRWKV’s state (orcontext) can be leveraged for\ninterpretability, predictability in sequence data, and\nsafety.  Manipulating the hidden state could also\nguide behavior and allow greater customizability\nthrough prompt tuning.\nThe RWKV architecture is not perfect, and can\nbe improved via many aspects, such as modifying\nthe formulae or implementing larger internal states.\nLarger states can enhance the model’s memory to\nprevious context and improve performance over\nvarious tasks.\n8    Conclusions\nWe introduced RWKV, a new approach to RNN\nmodels exploiting the potential of time-based mix-\ning components.  RWKV introduces several key\nstrategies that allow it to capture locality and long-\nrange dependencies while addressing limitations of\ncurrent architectures by: (1) replacing the quadratic\nQK attention with a scalar formulation at linear\ncost, (2) reformulating recurrence and sequential\ninductive biases to enable efficient training paral-\nlelization and efficient inference, and (3) enhancing\ntraining dynamics using custom initializations.\nWe benchmark the proposed architecture in a\nwide variety of NLP tasks and show comparable\nperformance to SoTA with reduced cost.  Further\nexperiments on expressivity, interpretability, and\nscaling showcase the model capabilities and draw\nparallels in behavior between RWKV and other\nLLMs.\nRWKV opens a new route for scalable and ef-\nficient  architectures  to  model  complex  relation-\nships in sequential data. While many alternatives\nto Transformers have been proposed with similar\nclaims, ours is the first to back up those claims with\npretrained models with tens of billions of parame-\nters.\n9    Limitations\nWhile  our  proposed  RWKV  model  has  demon-\nstrated  promising  results  regarding  training  and\nmemory efficiency during inference, some limita-\ntions should be acknowledged and addressed in\nfuture work.\nFirst, the linear attention of RWKV leads to sig-\nnificant efficiency gains but still, it may also limit\nthe model’s performance on tasks that require re-\ncalling minutiae information over very long con-\ntexts.   This  is  due  to  the  funneling  of  informa-\ntion through a single vector representation over\nmany time steps, compared with the full informa-\ntion maintained by the quadratic attention of stan-\ndard Transformers.  In other words, the model’s\nrecurrent architecture inherently limits its ability to\n“look back” at previous tokens, as opposed to tra-\nditional self-attention mechanisms. While learned\ntime decay helps prevent the loss of information,\nit is mechanistically limited compared to full self-\nattention.\nAnother limitation of this work is the increased\nimportance of prompt engineering in comparison to\nstandard Transformer models. The linear attention\nmechanism used in RWKV limits the information\nfrom the prompt that will be carried over to the\nmodel’s continuation.   As a result,  carefully de-\nsigned prompts may be even more crucial for the\nmodel to perform well on tasks.\nThe above RWKV property was confirmed by\nstudies on prompt engineering presented in Ap-\npendix L. By changing the order of the information",
    "pieces,  we were even able to almost double the\nRWKV performance for some tasks.\n10    Ethics Statement\nIn this paper, we present a novel architecture for se-\nquential data processing and prove its effectiveness\nby building a series of LLMs trained on publicly re-\nleased pretraining data (Gao et al., 2020; Biderman\net al., 2022) and later fine-tuned on publicly avail-\nable instructions (Taori et al., 2023; Chaudhary,\n2023; Cheung, 2023; Anand et al., 2023; Anony-\nmous, 2023; Yang, 2023; Ji et al., 2023a,b).\nAs  a  novel  architecture  for  sequential  data,\nRWKV  has  the  potential  to  improve  sequence-\nbased models across different applications ranging\nfrom natural language processing to biomedical\ndata processing or climate modelling.  Since the\ntraining code is released open source, RWKV con-\ntributes to the democratization of AI, levels the\nplaying field, and empowers members of the Open\nSource community to inspect, study, and finetune\nRWKV in particular tasks. Moreover, it contributes\nto advancing the understanding of LLMs capabili-\nties and limitations. A significant amount of work\nhas been devoted to increasing the efficiency of\nRWKV training so as to minimize its cost and pro-\nmote accessibility.\nAs LLMs trained on public data, RWKV’s lower\ninference cost compared to Transformer alterna-\ntives makes it more suitable for deployment in con-\nsumer and edge hardware, which is a step towards\nthe democratization and distribution of LLMs to\nthe general public, creating better privacy and own-\nership incentives.  It also lowers the resource bar-\nrier to Chat assistants and text generation for small\nand/or underrepresented communities. PreTrained\nmodel weights for different sizes ranging from 0.1B\nto 14B parameters trained on multiple languages\nare released to increase ease of adoption and allow\nfor the study of emergent phenomena.\nOn the other hand,  with lower resource barri-\ners, the spreading of AI-generated text might be-\ncome more prevalent. Current RWKV LLMs may\nexhibit  and/or  reproduce  biases  and  potentially\nharmful content present in the data used for train-\ning. Nonetheless, mitigation and finetuning strate-\ngies discussed for other, large Transformer models\nshould be applicable to RWKV as well.\nAcknowledgements\nWe thank StabilityAI for the compute used to train\nour models and for technical support in develop-\nment of RWKV. We also thank the members of the\nRWKV and EleutherAI Discord servers for their\nhelp and work on further extending the applicabil-\nity of RWKV to different domains.\nReferences\nMohammad Mahmudul Alam, Edward Raff, Stella Bi-\nderman, Tim Oates, and James Holt. 2023. Recasting\nself-attention with holographic reduced representa-\ntions.arXiv preprint arXiv:2305.19534.\nAlon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor\nPryor,  Luke  Yoffe,  Deepak  Ramachandran,  Lise\nGetoor, Jay Pujara, and William Yang Wang. 2022.\nFETA: A benchmark for few-sample task transfer\nin  open-domain  dialogue.   InProceedings  of  the\n2022  Conference  on  Empirical  Methods  in  Natu-\nral Language Processing, pages 10936–10953, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nYuvanesh  Anand,  Zach  Nussbaum,  Brandon  Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with large\nscale  data distillation  from  gpt-3.5-turbo.https:\n//github.com/nomic-ai/gpt4all.\nAnonymous. 2023. Sharegpt_vicuna_unfiltered.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification.  InFindings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644–1650, Online. Association for Computational\nLinguistics.\nIz  Beltagy,  Matthew  E.  Peters,  and  Arman  Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nStella   Biderman,   Kieran   Bicheno,   and   Leo   Gao.\n2022.Datasheet  for  the  pile.arXiv  preprint\narXiv:2201.07311.\nStella  Biderman,   USVSN  Sai  Prashanth,   Lintang\nSutawika,  Hailey  Schoelkopf,  Quentin  Anthony,\nShivanshu Purohit, and Edward Raf. 2023a.  Emer-\ngent and predictable memorization in large language\nmodels.arXiv preprint arXiv:2304.11158.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023b. Pythia: A suite\nfor analyzing large language models across training\nand scaling.arXiv preprint arXiv:2304.01373.",
    "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020.  Piqa: Reasoning about\nphysical commonsense in natural language.  InThirty-\nFourth AAAI Conference on Artificial Intelligence.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nURL: https://doi. org/10.5281/zenodo, 5297715.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022.   Gpt-neox-20b:  An open-source autoregres-\nsive language model. InProceedings of BigScience\nEpisode\\# 5–Workshop on Challenges & Perspec-\ntives in Creating Large Language Models, pages 95–\n136.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017.  Quasi-recurrent neural net-\nworks. InICLR.\nTom  Brown,  Benjamin  Mann,  Nick  Ryder,  Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020.  Language models are few-shot\nlearners.Advances in neural information processing\nsystems, 33:1877–1901.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev.\n2023. Scaling transformer to 1m tokens and beyond\nwith rmt.\nAydar  Bulatov,  Yury  Kuratov,  and  Mikhail  Burtsev.\n2022.   Recurrent memory transformer.Advances\nin Neural Information Processing Systems, 35:11079–\n11091.\nSahil Chaudhary. 2023.  Code alpaca:  An instruction-\nfollowing llama model for code generation.https:\n//github.com/sahil280114/codealpaca.\nJoseph Cheung. 2023. Guanacodataset.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2020. Rethinking attention with per-\nformers.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten  Bosma,  Gaurav  Mishra,  Adam  Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian  Gehrmann,  Parker  Schuh,  Kensen  Shi,\nSasha  Tsvyashchenko,  Joshua  Maynez,  Abhishek\nRao,  Parker  Barnes,  Yi  Tay,  Noam  Shazeer,  Vin-\nodkumar  Prabhakaran,  Emily  Reif,  Nan  Du,  Ben\nHutchinson,  Reiner Pope,  James Bradbury,  Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke,  Anselm Levskaya,  Sanjay Ghemawat,\nSunipa Dev,  Henryk Michalewski,  Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon  Child,  Oleksandr  Polozov,  Katherine  Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.CoRR, abs/2204.02311.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014.  Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning.  InNIPS 2014 Deep Learning and Representa-\ntion Learning Workshop.\nChristopher  Clark,  Kenton  Lee,  Ming-Wei  Chang,\nTom  Kwiatkowski,  Michael  Collins,  and  Kristina\nToutanova. 2019.  Boolq:  Exploring the surprising\ndifficulty of natural yes/no questions.arXiv preprint\narXiv:1905.10044.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018.  Think you have solved question an-\nswering?  try arc, the ai2 reasoning challenge.   In\narXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Ja-\ncob Hilton,  Reiichiro Nakano,  Christopher Hesse,\nand  John  Schulman.  2021.    Training  verifiers  to\nsolve  math  word  problems.InarXiv,  volume\nabs/2110.14168.\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and\nChristopher  Re.  2022a.   Flashattention:  Fast  and\nmemory-efficient exact attention with IO-awareness.\nInAdvances in Neural Information Processing Sys-\ntems.\nTri  Dao,  Daniel  Y  Fu,  Khaled  K  Saab,  Armin  W\nThomas,  Atri  Rudra,  and  Christopher  Ré.  2022b.\nHungry  hungry  hippos:   Towards  language  mod-\neling  with  state  space  models.arXiv  preprint\narXiv:2212.14052.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan S. Cowen, Gaurav Nemade, and Sujith Ravi.\n2020.  Goemotions: A dataset of fine-grained emo-\ntions. InProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020,  Online,  July  5-10,  2020,  pages  4040–4054.\nAssociation for Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile:  An 800gb dataset of diverse text for lan-\nguage modeling.arXiv preprint arXiv:2101.00027.\nAlbert Gu, Karan Goel, and Christopher Ré. 2021. Effi-\nciently modeling long sequences with structured state\nspaces.arXiv preprint arXiv:2111.00396.",
    "Albert Gu, Karan Goel, and Christopher Ré. 2022. Effi-\nciently modeling long sequences with structured state\nspaces. InThe International Conference on Learning\nRepresentations (ICLR).\nMandy Guo, Joshua Ainslie, David C Uthus, Santiago\nOntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei\nYang.  2022.    Longt5:   Efficient  text-to-text  trans-\nformer for long sequences. InFindings of the Associ-\nation for Computational Linguistics: NAACL 2022,\npages 724–736.\nAnkit Gupta, Albert Gu, and Jonathan Berant. 2022. Di-\nagonal state spaces are as effective as structured state\nspaces.Advances in Neural Information Processing\nSystems, 35:22982–22994.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016.  Identity mappings in deep residual net-\nworks.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher  Hesse,  Jacob  Jackson,  Heewoo  Jun,\nTom B Brown, Prafulla Dhariwal, Scott Gray, et al.\n2020.   Scaling  laws  for  autoregressive  generative\nmodeling.arXiv preprint arXiv:2010.14701.\nSepp Hochreiter. 1998. The vanishing gradient problem\nduring learning recurrent neural nets and problem so-\nlutions.International Journal of Uncertainty, Fuzzi-\nness and Knowledge-Based Systems, 6(02):107–116.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory.Neural Computation, 9(8):1735–\n1780.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya,  Trevor Cai,  Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl,  Aidan Clark,  Tom Hennigan,  Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc,  Aurelia  Guy,  Simon  Osindero,  Karen  Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nHassan Ismail Fawaz, Germain Forestier, Jonathan We-\nber, Lhassane Idoumghar, and Pierre-Alain Muller.\n2019.  Deep learning for time series classification:\na review.Data mining and knowledge discovery,\n33(4):917–963.\nAndrew  Jaegle,  Felix  Gimeno,  Andy  Brock,  Oriol\nVinyals, Andrew Zisserman, and Joao Carreira. 2021.\nPerceiver:  General perception with iterative atten-\ntion. InInternational conference on machine learn-\ning, pages 4651–4664. PMLR.\nHanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee,\nand Jangwoo Kim. 2019.  Mnnfast: A fast and scal-\nable system architecture for memory-augmented neu-\nral networks.   InProceedings of the 46th Interna-\ntional Symposium on Computer Architecture, pages\n250–263.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Baochang Ma, and Xiangang Li. 2023a. Belle:\nBe everyone’s large language model engine.https:\n//github.com/LianjiaTech/BELLE.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu,  Lei Zhang,  Baochang Ma,  and Xiangang Li.\n2023b.    Exploring  the  impact  of  instruction  data\nscaling  on  large  language  models:   An  empirical\nstudy  on  real-world  use  cases.arXiv  preprint\narXiv:2303.14742.\nMatt  Gardner  Johannes  Welbl  Nelson  F.  Liu.  2017.\nCrowdsourcing  multiple  choice  science  questions.\nInDOI:10.18653/v1/W17-4413.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim\nGreen, Michael Figurnov, Olaf Ronneberger, Kathryn\nTunyasuvunakool, Russ Bates, Augustin Žídek, Anna\nPotapenko,  and et al. 2021.   Highly accurate pro-\ntein  structure  prediction  with  alphafold.Nature,\n596(7873):583–589.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling  laws  for  neural  language  models.arXiv\npreprint arXiv:2001.08361.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020.  Transformers are\nrnns:  Fast autoregressive transformers with linear\nattention.  InInternational Conference on Machine\nLearning, pages 5156–5165. PMLR.\nNikita   Kitaev,   L.   Kaiser,   and   Anselm   Levskaya.\n2020.  Reformer:  The efficient transformer.ArXiv,\nabs/2001.04451.\nJan Koco\n ́\nn, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz,  Anna Koco\n ́\nn,  Bartłomiej Koptyra,  Wik-\ntoria Mieleszczenko-Kowszewicz, Piotr Miłkowski,\nMarcin Oleksy, Maciej Piasecki, Łukasz Radli\n ́\nnski,\nKonrad  Wojtasik,  Stanisław  Wo ́zniak,  and  Prze-\nmysław Kazienko. 2023. Chatgpt: Jack of all trades,\nmaster of none.Information Fusion, page 101861.\nJan  Koco\n ́\nn,   Piotr  Miłkowski,   and  Monika  Za\n ́\nsko-\nZieli\n ́\nnska. 2019.  Multi-level sentiment analysis of\npolemo 2.0: Extended corpus of multi-domain con-\nsumer reviews.  InProceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 980–991.\nPhong Le and Willem Zuidema. 2016.   Quantifying\nthe vanishing gradient and long distance dependency\nproblem in recursive neural networks and recursive\nlstms. InProceedings of the 1st Workshop on Repre-\nsentation Learning for NLP, pages 87–93.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile\nSaulnier, Stas Bekman, M Saiful Bari, Stella Bider-\nman, Hady Elsahar, Jason Phang, Ofir Press, et al.",
    "2022. What language model to train if you have one\nmillion gpu hours?   InProceedings of BigScience\nEpisode #5–Workshop on Challenges & Perspectives\nin Creating Large Language Models.\nTao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav\nArtzi. 2018. Simple recurrent units for highly paral-\nlelizable recurrence. InProceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing,  pages  4470–4481,  Brussels,  Belgium.\nAssociation for Computational Linguistics.\nHanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le.\n2021. Pay attention to mlps.\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,\nJonathan May, Hao Ma, and Luke Zettlemoyer. 2021.\nLuna:  Linear  unified  nested  attention.Advances\nin Neural Information Processing Systems, 34:2441–\n2453.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,\nLiangke Gui, Graham Neubig, Jonathan May, and\nLuke Zettlemoyer. 2023.   Mega:  Moving average\nequipped gated attention. InICLR.\nEric Martin and Chris Cundy. 2017. Parallelizing linear\nrecurrent neural nets over sequence length.ArXiv,\nabs/1709.04057.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022.  Locating and editing factual asso-\nciations in GPT.Advances in Neural Information\nProcessing Systems, 36.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. InEMNLP.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak,\nTeven Le Scao, Aleksandra Piktus, Nouamane Tazi,\nSampo  Pyysalo,  Thomas  Wolf,  and  Colin  Raffel.\n2023.   Scaling  data-constrained  language  models.\narXiv preprint arXiv:2305.16264.\nOpenAI. 2022. Introducing chatgpt.https://openai.\ncom/blog/chatgpt.\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan\nFernando,  Caglar Gulcehre,  Razvan Pascanu,  and\nSoham  De.  2023.Resurrecting  recurrent  neu-\nral  networks  for  long  sequences.arXiv  preprint\narXiv:2303.06349.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernandez. 2016.  The LAMBADA dataset:  Word\nprediction requiring a broad discourse context.  In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nAdam  Paszke,  Sam  Gross,  Francisco  Massa,  Adam\nLerer,  James  Bradbury,  Gregory  Chanan,  Trevor\nKilleen,  Zeming  Lin,  Natalia  Gimelshein,  Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019.  Pytorch:  An\nimperative style, high-performance deep learning li-\nbrary.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y\nFu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-\nfano Ermon, and Christopher Ré. 2023.  Hyena hierar-\nchy: Towards larger convolutional language models.\narXiv preprint arXiv:2302.10866.\nIlan Price, Jordan Gifford-Moore, Jory Flemming, Saul\nMusker,  Maayan  Roichman,  Guillaume  Sylvain,\nNithum Thain, Lucas Dixon, and Jeffrey Sorensen.\n2020.  Six attributes of unhealthy conversations.  In\nProceedings of the Fourth Workshop on Online Abuse\nand Harms, pages 114–124, Online. Association for\nComputational Linguistics.\nMarkus  N.  Rabe  and  Charles  Staats.  2022.Self-\nattention does not needo(n\n2\n)memory.\nJeff Rasley,  Samyam Rajbhandari,  Olatunji Ruwase,\nand Yuxiong He. 2020.   Deepspeed:  System opti-\nmizations enable training deep learning models with\nover 100 billion parameters.  InProceedings of the\n26th  ACM  SIGKDD  International  Conference  on\nKnowledge  Discovery  &  Data  Mining,  KDD  ’20,\npage 3505–3506, New York, NY, USA. Association\nfor Computing Machinery.\nMelissa Roemmele, Cosmin Adrian Bejan, , and An-\ndrew S. Gordon. 2018.  Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. InAAAI.\nTeven  Le  Scao,  Angela  Fan,  Christopher  Akiki,  El-\nlie  Pavlick,  Suzana  Ili\n ́\nc,  Daniel  Hesslow,  Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias  Gallé,  et  al.  2022.Bloom:   A  176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nRamsha Siddiqui. 2019.  SARCASMANIA: Sarcasm\nExposed!http://www.kaggle.com/rmsharks4/\nsarcasmania-dataset.[Online;   accessed  02-\nFebruary-2023].\nDavid R. So, Wojciech Manke, Hanxiao Liu, Zihang\nDai, Noam Shazeer, and Quoc V. Le. 2021. Primer:\nSearching  for  efficient  transformers  for  language\nmodeling.CoRR, abs/2109.08668.\nRohan  Taori,  Ishaan  Gulrajani,  Tianyi  Zhang,  Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn  instruction-following  llama  model.https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020.  Synthesizer: Re-\nthinking self-attention in transformer models.",
    "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri,  Philip Pham,  Jinfeng Rao,  Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021.  Long\nrange arena : A benchmark for efficient transformers.\nInInternational Conference on Learning Representa-\ntions.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022.  Efficient transformers: A survey.ACM\nComputing Surveys, 55(6):1–28.\nIlya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov,\nLucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-\nsica Yung, Andreas Steiner, Daniel Keysers, Jakob\nUszkoreit,  Mario  Lucic,  and  Alexey  Dosovitskiy.\n2021.   Mlp-mixer:  An all-mlp architecture for vi-\nsion.CoRR, abs/2105.01601.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit,  Llion  Jones,  Aidan  N  Gomez,  Łukasz\nKaiser, and Illia Polosukhin. 2017.  Attention is all\nyou need.  InAdvances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nDavid  Vilares  and  Carlos  Gómez-Rodríguez.  2019.\nHead-qa: A healthcare dataset for complex reasoning.\nInACL.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019.  Superglue: A stickier\nbenchmark for general-purpose language understand-\ning  systems.   InAdvances  in  Neural  Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018.  GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. InProceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020.  Linformer: Self-attention with\nlinear complexity.\nJason  Wei,  Yi  Tay,  Rishi  Bommasani,  Colin  Raf-\nfel,  Barret  Zoph,  Sebastian  Borgeaud,  Dani  Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler,  Ed Chi,  Tatsunori Hashimoto,  Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022a.\nEmergent abilities of large language models.ArXiv,\nabs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models.arXiv preprint arXiv:2201.11903.\nThomas  Wolf,  Lysandre  Debut,  Victor  Sanh,  Julien\nChaumond, Clement Delangue, Anthony Moi, Perric\nCistac, Clara Ma, Yacine Jernite, Julien Plu, Can-\nwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame,  Quentin  Lhoest,  and  Alexander  M.  Rush.\n2020.  Transformers:  State-of-the-Art Natural Lan-\nguage  Processing.    pages  38–45.  Association  for\nComputational Linguistics.\nZonghan  Wu,  Shirui  Pan,  Fengwen  Chen,  Guodong\nLong, Chengqi Zhang, and S Yu Philip. 2020. A com-\nprehensive survey on graph neural networks.IEEE\ntransactions on neural networks and learning sys-\ntems, 32(1):4–24.\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.\nEx machina: Personal attacks seen at scale. InPro-\nceedings  of  the  26th  International  Conference  on\nWorld Wide Web, WWW 2017, Perth, Australia, April\n3-7, 2017, pages 1391–1399. ACM.\nJianxin Yang. 2023.   Firefly.https://github.com/\nyangjianxin1/Firefly.\nManzil  Zaheer,  Guru  Guruganesh,  Kumar  Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang,  et al. 2020.   Big bird:  Transformers for\nlonger sequences.Advances in Neural Information\nProcessing Systems, 33.\nRowan  Zellers,  Ari  Holtzman,  Yonatan  Bisk,  Ali\nFarhadi, and Yejin Choi. 2019.  Hellaswag:  Can a\nmachine really finish your sentence?  InACL.\nRowan  Zellers,  Ari  Holtzman,  Yonatan  Bisk,  Ali\nFarhadi,  and Yejin Choi. 2020.   Winogrande:  An\nadversarial winograd schema challenge at scale. In\nACL.\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen\nHuang,  Hanlin  Goh,  Ruixiang  Zhang,  and  Josh\nSusskind. 2021. An attention free transformer.\nSheng Zhang,  Xiaodong Liu,  Jingjing Liu,  Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord:  Bridging the gap between human and ma-\nchine  commonsense  reading  comprehension.In\narXiv:1810.12885.\nSusan  Zhang,  Stephen  Roller,  Naman  Goyal,  Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.",
    "A    Author Contributions\nAll authors contributed to the drafting of this paper. Eric Alcaide and Quentin Anthony organized the\npaper and its experiments and were involved in all phases of the development process.\nModel Design and DevelopmentBo Peng (lead), Matteo Grella, Xuzheng He, Haowen Hou, Jiaming\nKong, Johan S. Wind\nModel TrainingBo Peng\nScaling Laws AnalysisStella Biderman, Bo Peng\nBenchmark EvaluationsStella Biderman (lead), Kranthi Kiran GV, Krishna Sri Ipsit Mantri, Atsushi\nSaito, Qihang Zhao, Peng Zhou, Rui-Jie Zhuåç\nLong Context ExperimentsXingjian Du,  Rui-Jie Zhu,  Bolun Wang,  Ruichong Zhang,  Jian Zhu,\nRui-Jie Zhu\nInference Speed ExperimentsSamuel Arcadinho, Przemysław Kazienko, Qinghua Zhou\nInformation  Flow  ExperimentsHuanqi  Cao,  Michael  Chung,  Matteo  Grella,  Ferdinand  Mom,\nZhenyuan Zhang\nChat ExperimentsJan Koco\n ́\nn (lead), Przemysław Kazienko, Bartłomiej Koptyra, Hayden Lau, Xiangru\nTang, Stanisław Wo ́zniak, Zhenyuan Zhang\nEthics and Broader ImpactsStella Biderman, Guangyu Song",
    "B    Author Contributions\nBo PengOriginal RWKV idea, original code, performance optimizations, original experiments, and\ntrained RWKV models from 0.1B to 14B.\nEric AlcaideManuscript (initial draft sections 1, C; sections 3, 7 and 8; revision and proofreading; final\nversion ). Figures (2, 3, 3, 8). Experiments section 6. Appendices E, K. Contributions to Appendix M.\nQuentin AnthonyManuscript (organization, initial draft sections 1, C, 2; revision and proofreading;\nfinal version).\nAlon AlbalakManuscript (abstract and sections 1, 9, and 7; proofreading and revision).\nSamuel ArcadinhoContributions to Figures 7, 13, and 14. Contributions to Appendix K.\nStella BidermanPerformed the scaling laws analysis and evaluated competitor models on benchmark\ntasks.\nHuanqi CaoManuscript (contributions to 3.2 and 3.3; proofreading and revision).  Experiments for\nAppendix I.\nXin ChengManuscript (proofreading and revision). Contributions to Appendix M, J.\nMichael ChungManuscript (contributions to section I; proofreading and revision).\nXingjian DuEvaluation on Long Range Arena Benchmark (TBD until 5.31).\nMatteo GrellaManuscript (sections H, I, 8; contributions to sections 1, 7 and 9; proofreading and\nrevision). Contributions to Appendix D.\nKranthi Kiran GVManuscript (sections C and 5; contributions to section 2; revision and proofreading).\nTables K and K. Appendix 4.\nXuzheng HeManuscript (contributions to section 2;  proofreading and revision).  Contributions to\nFigure8. Appendix I. Contributions to appendix H.\nHaowen HouFigure 9. Appendix F.\nJiaju LinRWKV on LRA benchmarking\nPrzemysław KazienkoManuscript (proofreading and revision).  Contributions to Section 6, 9, and\nAppendix L.\nJan KoconManuscript (Section 1; proofreading and revision). Contributions to Appendix L.\nJiaming KongManuscript (revision and proofreading). Appendix H.\nBartłomiej KoptyraManuscript (revision and proofreading) Contributions to Appendix L.\nHayden LauManuscript (contributions to section 1 and 9; proofreading and revision). Contributions to\nAppendix M.\nKrishna Sri Ipsit MantriFigure 12\nFerdinand MomManuscript (contributions to section 1, C, 3.3, I; proofreading and revision). Contribu-\ntions to Appendix D.\nAtsushi SaitoManuscript (sections 2 and 5; contributions to section C). Contributions to Appendix J\nGuangyu SongManuscript (rewrote section 3; final version). Initial draft Ethics Statement).\nXiangru TangManuscript (sections C and 2; contributions to abstract; revision and proofreading).\nContributions to Appendix M.",
    "Bolun WangContributions to Tables 1.\nJohan S. WindRWKV performance optimizations (CUDA), Contributions to Appendix 4.\nStanisław Wo ́zniakContributions to Appendix L.\nRuichong ZhangManuscript (proofreading and revision); Contributions to Figure 6 and Appendix M.\nZhenyuan ZhangManuscript (revision and proofreading). Figure 3. Experiments Appendix I. Contri-\nbutions to Appendices D and M.\nQihang ZhaoManuscript (proofreading and revision). Contributions to Table 5.\nPeng ZhouContributions to Tables 1 and Table 5.\nQinghua ZhouManuscript (Proofreading and revision of section 3; Add missing citations in 3.3).\nRevision of Figures 2 and 12.\nJian ZhuManuscript (section C; proofreading and revision). Figures 3 and 6.\nRui-Jie ZhuTables 1 and 5. Experiments for table 5.\nC    Additional Related Work\nRecently, a number of techniques have been proposed to address the limitations of transformers.\nOptimizing Attention MechanismMany transformer variants (“x-formers”) have been introduced to\nreduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020;\nKitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al.,\n2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other\nefficient methods (Katharopoulos et al., 2020; Jaegle et al., 2021).\nSome recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022; Jang et al.,\n2019) share similarities with RWKV’s chunked computation scheme. Despite being memory-efficient,\ntheir time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV\nachieves better space and time complexity during inference by formulating a linear attention as an RNN.\nAttention Free ModelsAnother line of research replaces the attention mechanism with other modules to\nscale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021; Liu et al., 2021) propose replacing\nattention by Multi-Layer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer\n(AFT) (Zhai et al., 2021) and HrrFormer (Alam et al., 2023) replaces dot-product self-attention with a\ncomputationally efficient alternative. None of these models have been successfully scaled to the point\nwhere drawing comparisons with transformer-based large language models makes sense.\nThere has also been substantial research into state space models (SSM) (Gu et al., 2021) and its variants\n(Dao et al., 2022b; Gupta et al., 2022; Poli et al., 2023). In contrast to the preceding models, SSM and\nits successors have shown substantial progress towards efficient scaling. Simultaneously with this work,\nPoli et al. (2023) train SSM-based models with 125 million and 355 million parameters and show that the\nperformance is on-par with a transformer that uses a mix of local and global attention (Black et al., 2021).\nAdvances in RNNsInspired by the success of transformers, RNN-style (Hochreiter and Schmidhuber,\n1997; Chung et al., 2014) recursive components have also been modified to increase context length, such\nas the Recurrent Memory Transformer (Bulatov et al., 2022, 2023) and Linear Recurrent Units (Orvieto\net al., 2023). Most similar to our work, the Quasi-Recurrent neural network (QRNN) (Bradbury et al.,\n2017) uses both convolutional layers and recurrent pooling functions across timesteps and channels.\nWhile QRNN utilizes convolutional filters with fixed sizes, RWKV employs a time-mixing module as\nan attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN,\nRWKV includes a parametrized channel-mixing module that is parallelizable.",
    "D    Time-Mixing Block as an RNN Cell\nAs stated in 3.3, the RWKV time-mixing block can be formulated as an RNN, as theWKVcomputation\ncan be written in such a recursive form:\na\n0\n,b\n0\n= 0,(19)\nwkv\nt\n=\na\nt−1\n+e\nu+k\nt\n⊙v\nt\nb\nt−1\n+e\nu+k\nt\n,(20)\na\nt\n=e\n−w\n⊙a\nt−1\n+e\nk\nt\n⊙v\nt\n,(21)\nb\nt\n=e\n−w\n⊙b\nt−1\n+e\nk\nt\n.(22)\n12\ne\n3\nFigure 8:  RWKV time-mixing block formulated as an\nRNN cell.  Color codes:  yellow (μ) denotes the token\nshift, red (1) denotes the denominator, blue (2) denotes the\nnumerator, and pink (3) denotes the fraction computations\nin 16.hdenotes the numerator-denominator tuple.\nThe dataflow of the RNN-like time-mixing is\nshown in Fig.  8, where the hidden stateshis\nthe numerator-denominator tuple(a,b). To avoid\noverflow in calculatinge\nk\nt\n, a numerical trick is\nused in the official implementation.   Noticing\nthata\n1\n=e\nk\n1\n⊙v\n1\nandb\n1\n=e\nk\n1\n, we seta\n′\n1\n=\nv\n1\n,b\n′\n1\n= 1,p\n1\n=k\n1\n, wherep\nt\nstores the shared\nexponents ofa\nt\nandb\nt\n. Now the above recursion\ncan be converted into a numerical safe version,\nfor each time stept >1:\nq:= max(p\nt−1\n,u+k\nt\n),(23)\nwkv\nt\n=\ne\np\nt−1\n−q\n⊙a\n′\nt−1\n+e\nu+k\nt\n−q\n⊙v\nt\ne\np\nt−1\n−q\n⊙b\n′\nt−1\n+e\nu+k\nt\n−q\n.(24)\nThe update toa\n′\nt\n,b\n′\nt\n, and their shared exponent is\nalso carried out in a similar fashion:\nq\n′\n:= max(p\nt−1\n−w,k\nt\n),(25)\na\n′\nt\n=e\np\nt−1\n−w−q\n′\n⊙a\n′\nt−1\n+e\nk\nt\n−q\n′\n⊙v\nt\n,(26)\nb\n′\nt\n=e\np\nt−1\n−w−q\n′\n⊙b\n′\nt−1\n+e\nk\nt\n−q\n′\n,(27)\np\nt\n=q\n′\n.(28)\nThe RWKV model has an internal state that stores some previous information. In each layer, the internal\nstate consists five parts, each of which is a vector withDnumbers, whereDis the model dimension. The\nfive parts are:\n•  The current input of the Time-mix blockx\nt\n;\n•  The current input of the Channel-mix blocky\nt\n;\n•  The numerator of theWKVvaluea\n′\nt\n, as defined in equation (26);\n•  The denominator of theWKVvalueb\n′\nt\n, as defined in equation (27);\n•\nAn auxiliary statep\nt\nin(28), which is used forWKVcomputation to maintain numerical precision.\nWhich yields a total size of5DLparameters. It is worth noting that in an algebraic context with infinite\nprecision, the helper statep\nt\ncan be ignored, and theWKVnumerator and denominator can be computed\ndirectly using equations (21) and (22), reducing the size of the internal state to4DL.\nE    Parameter initializations\nWe describe the specific parameter initializations below and motivate the design choices.  Parameters\nbelonging to residual blocks are often adjusted by layer depth and total number of layers. Let#denote\nthe vocabulary size,sdenote the embedding dimension,ddenote the hidden size (we used= 4s),Lthe\nnumber of layers,lthe layer index (from 0 toL−1), we use the following initializations:",
    "•  Embeddings are initialized toU(±1×10\n−4\n) as explained in 3.4\n•\nFor the time-mixing blocks (11, 12, 13), initializations areμ\nk\ni\n= (\ni\ns\n)\n1−\nl\nL\n,μ\nv\ni\n= (\ni\ns\n)\n1−\nl\nL\n+\n0.3l\nL−1\nand\nμ\nr\ni\n=\n1\n2\n·(\ni\ns\n)\n1−\nl\nL\n•  For the channel-mixing blocks (14, 15),μ\nk\ni\nandμ\nr\ni\nare initialized to(\ni\ns\n)\n1−\nl\nL\n•w\ni\n(16), also known as “time decay”, is initialized to−5 + 8·(\ni\nd−1\n)\n0.7+\n1.3l\nL−1\n. Intuitively, it is the\ndiscount factor applied to previous tokens over time.\n•u\ni\n(16), also known as “bonus”, is set to0.5·(((i+ 1) mod 3)−1) + log 0.3.  It is the special\nweighting applied to the current token in equation 16.   The alternating zigzag pattern initially\ncreates subtle variations in the tensor elements, which are intended to help the model treat different\ndimensions of the embedding distinctively.\n•W\no\n(17) (time-mixing) andW\nv\n(channel-mixing) are initialized toN(0,\nq\nd\ns\n= 2)\n•\nAll otherW\nr\n,W\nk\n,W\nv\nweights are initialized to 0 so the model can start learning from the beginning\nwithout noisy signals.\n•  All LayerNorm weights start from 1 and biases from 0.\nF    Small Init Embedding\nThis section presents the experimental validation of small initialization embedding. The experimental\nsetup is as follows. In the baseline configuration, the parameters are initialized using a normal distribution\nwith a mean of 0.0 and a standard deviation of 0.02, which is a commonly used initialization method\nin models like BERT and GPT. On the other hand, in the small initialization of the embedding (small\ninit emb) experiment, the parameters are initialized using a uniform distribution with a range of 1e-4,\nwhich is slightly different from RWKV where a normal distribution with a standard deviation of 1e-4\nis used.  However, this difference is negligible and does not affect our conclusions.  The experiments\nwere conducted with a batch size of 400. As depicted in Figure 9, the loss curve for the small init emb\nexhibits a faster rate of decrease and convergence compared to the traditional initialization using a normal\ndistribution.\n01000020000300004000050000\nStep\n4\n5\n6\n7\n8\n9\n10\n11\nLoss\nBaseline\nSmall Init Emb\nFigure 9: Effect of small initialization embedding.\nG    Hyperparameters\nTo train the models mentioned, we useε= (0.9,0.99)without weight decay for the Adam optimizer, and\nswitch batch size dynamically between 128 or 256 sequences, each of 1024 tokens. We further organize",
    "Model169M430M1.5B3B7B14B\nInit LR0.00060.00040.00030.000150.000150.0001\nWarmup Mini-Epochs361411443451465544\nEnd LR0.000010.000010.000010.000010.000010.000007\nTable 3: Hyperparameters for our learning rate (LR) schedule of the pretrained models.\nthe training into multiple mini-epochs, each of 40320 samples, to guide our learning rate schedule. The\ntraining process takes 8043 mini-epochs to make one pass over the Pile. The initial warming up mini-\nepochs have a constant learning rate of “Init LR”. After the warming up mini-epochs, the learning rate\nexponentially decays until in the last mini-epoch, in which the model finishes training on the entire Pile,\nthe learning rate arrives at the “End LR”. The related hyperparameters are shown in Table 3.\nH    Gradient Stability in RWKV\nIn this section, we present a mathematical description of the gradient stability property in RWKV, focusing\nspecifically on the time-mixing block. By gradient stability we mean that if the inputsx\nt\nare bounded and\nthe model parameters are fixed, then the gradients with respect toW\nk\nandW\nv\nare uniformly bounded for\nallT(thus not exploding). Consequently, we can control the amount eachx\nt\ncontributes to the gradient at\nTin a naturally decaying fashion by the weight decay mechanismw(thus not vanishing unless desired).\nFirst, we make the simplification that there are no token shifts, this will not affect the final conclusion.\nIn this scenario,wkv\nT\ncan be written as\nwkv\nT\n=\nP\nT\nt=1\nK\ne\nt\n⊙v\nt\nP\nT\nt=1\nK\ne\nt\n= E(v\nt\n) =\nS(v\nt\n)\nS(1)\n,(29)\nwhere\nv\nt\n=W\nv\nx\nt\n,\n∂(v\nt\n)\ni\n∂(W\nv\n)\ni,j\n= (x\nt\n)\nj\n,\nK\ne\nt\n=e\nW\nk\nx\nt\n+w\nT,t\n,\n∂(K\ne\nt\n)\ni\n∂(W\nk\n)\ni,j\n= (x\nt\n)\nj\n(K\ne\nt\n)\ni\n,\nandS(·)andE(·)are shorthand for denoting sums and averages over weightsK\ne\nt\n.\nThe loss function at positionTcan be written as\nL\nT\n=l(f(wkv\nT\n),y\nT\n).(30)\nBecausewkv\nT\nrelates to(W\nk\n)\ni,j\nand(W\nv\n)\ni,j\nonly through thei-th channel(wkv\nT\n)\ni\n, we have\n∂L\nT\n∂(W\nv\n)\ni,j\n=\n∂L\nT\n∂(wkv\nT\n)\ni\n∂(wkv\nT\n)\ni\n∂(W\nv\n)\ni,j\n.(31)\nThe first part of the above equation contains trivial operations like output layers, and other layers of\ntime-mixing, which can be proven inductively. The second part of the above equation can be bounded as\n\f\n\f\n\f\n\f\n∂(wkv\nT\n)\ni\n∂(W\nv\n)\ni,j\n\f\n\f\n\f\n\f\n=\n\f\n\f\n\f\n\f\n∂E\ni\n[(v\nt\n)\ni\n]\n∂(W\nv\n)\ni,j\n\f\n\f\n\f\n\f\n=|E\ni\n[(x\nt\n)\nj\n]|≤max\nt\n|(x\nt\n)\nj\n|,(32)",
    "which is irrelevant toT. Similarly,\n∂(wkv\nT\n)\ni\n∂(W\nk\n)\ni,j\n=∂\nS\ni\n[(v\nt\n)\ni\n]\nS\ni\n(1)\n/∂(W\nk\n)\ni,j\n=\nS\ni\n[(x\nt\n)\nj\n(v\nt\n)\ni\n]\nS\ni\n(1)\n−\nS\ni\n[(x\nt\n)\nj\n]S\ni\n[(v\nt\n)\ni\n]\nS\ni\n(1)\n2\n= E\ni\n[(x\nt\n)\nj\n(v\nt\n)\ni\n]−E\ni\n[(x\nt\n)\nj\n]E\ni\n[(v\nt\n)\ni\n]\n= cov\ni\n((x\nt\n)\nj\n,(v\nt\n)\ni\n)(33)\ncan also be bounded. Note thatwkv’s softmax operation contains at least two non-zero terms (uandw),\nso the above “covariance” will not degenerate into 0.\nI    Model Behavior Visualization\n0100200300400500600700800\nChannel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime Decay\nTime decay (sorted along channel axis)\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\nFigure 10: Model behavior visualizations of RWKV.\nThe right plot illustrates the time decays (e\n−w\n)\nin each layer of the RWKV-169M model, sorted\nalong the channel axis. Notably, several decays\nin the last layers are very close or equal to one,\nimplying that certain information is preserved\nand propagated throughout the model’s temporal\ncontext. Meanwhile, many decays in the initial\nlayer are close to zero, which corresponds to lo-\ncal operations inwkv(16), likely to be associated\nwith tasks such as text parsing or lexical analy-\nsis.  (Note that the local operations inwkvare\ndue to the extra parameteru, whene\n−w\nis de-\ngenerated into 0.) These patterns of time decays\nare partly learned, but also come from parameter\ninitialization as it speeds up training.\nThe plot below shows the information retrieval\nand propagation path in the RWKV-430M model.  The experiment follows thecausal tracemethod\nintroduced by Meng et al. (2022), where we\n1.  Run the model once, and record all states and activation of each layer during the computation;\n2.  Corrupt the input embeddings of the subject using noise (“The Eiffel Tower” in this example);\n3.\nRestore the states and activation of a certain layer at a certain token during the computation, and\nrecord the log-probability of the model outputting the correct answer (“Paris”).\nUnlike transformers, RWKV relies on the recursive propagation of information in the time dimension.\nIn this case, the fact that the Eiffel Tower is located in Paris is retrieved in layer 4 just after the model\nsees “The Eiffel”. It is then passed down to the subsequent layers. In layer 20, mostly, the information is\npropagated through time until reaching where it is needed. Finally, at the token “of”, it is passed down to\nthe last layer for outputting the answer.\nJ    Additional Evaluations\nJ.1    Further details on NLP tasks\nWe evaluate on the following tasks:\nARC  (Clark et al., 2018)A dataset designed for multiple-choice question answering, encompassing\nscience exam questions ranging from third grade to ninth grade. It has Easy and Challenge subsets that\nwe report results on separately.\nBoolQ  (Clark et al., 2019)A binary yes/no question answering benchmark.",
    "16111621\nLayer\nThe\n E\niff\nel\n Tower\n is\n located\n in\n the\n city\n of\nInformation propagation path\n7\n6\n5\n4\n3\n2\n1\nLog-probability of \"Paris\"\nFigure 11: Model behavior visualizations of the RWKV model.\nCOPA  (Roemmele et al., 2018)A dataset to evaluate achievement in open-domain commonsense\ncausal reasoning.\nHeadQA  (Vilares and Gómez-Rodríguez, 2019)A benchmark consisting of graduate-level questions\nencompassing various fields such as medicine, nursing, biology, chemistry, psychology, and pharmacology.\nHellaSwag(Zellers et al., 2019) A novel benchmark for commonsense Natural Language Inference\n(NLI) which is build by adversarial filtering against transformer models.\nLAMBADA (Paperno et al., 2016)A benchmark dataset that evaluates the model’s contextual reasoning\nand language comprehension abilities by presenting context-target pairs, where the objective is to predict\nthe most probable target token. We follow standard practice and use the untokenized version created by\nOpenAI (Brown et al., 2020).\nOpenBookQA  (Mihaylov et al., 2018)A QA dataset to evaluate human comprehension of a subject by\nincorporating open book facts, scientific knowledge, and perceptual common sense, drawing inspiration\nfrom open book exams.\nPIQA (Bisk et al., 2020)A benchmark for the task of physical common sense reasoning, which consists\nof a binary choice task that can be better understood as a set of two pairs, namely (Goal, Solution).\nReCoRD   (Zhang  et  al.,  2018)A  benchmark  for  evaluating  commonsense  reasoning  in  reading\ncomprehension by generating queries from CNN/Daily Mail news articles and requiring text span answers\nfrom corresponding summarizing passages.\nSciQ  (Johannes Welbl Nelson F. Liu, 2017)A multiple-choice QA dataset which was created using\nan innovative approach to gather well-crafted multiple-choice questions that are focused on a specific\ndomain.\nWinogrande  (Zellers et al., 2020)A dataset designed to evaluate the acquisition of common sense\nreasoning by neural language models, aiming to determine whether we are accurately assessing the true\ncapabilities of machine common sense.",
    "10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(a) ARC (Challenge)\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(b) ARC (Easy)\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(c) BoolQ\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(d) COPA\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(e) HeadQA\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(f) HellaSwag\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(g) LAMBADA (OpenAI)\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(h) OpenBookQA\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(i) PiQA\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(j) ReCoRD\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(k) SciQ\n10\n2\n10\n3\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(l) Winogrande\nFigure 12: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks.",
    "J.2    Evaluation on Long Range Arena\nThe Long-Range Arena (LRA) benchmark (Tay et al., 2021) is designed to assess the performance of\nmodels in handling lengthy context situations. It includes a collection of tasks with sequences ranging\nfrom 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images,\nand mathematical expressions.  We apply RWKV on the LRA benchmark and the report results are in\nTable 4. Other models’ performances are directly cited from Gu et al. (2022); Alam et al. (2023).\nTable 4: Evaluation on Long Range Arena. Other models reported in the literature (Gu et al., 2022; Alam et al.,\n2023).Boldedvalues are the best.\nMODELLISTOPSTEXTRETRIEVALIMAGEPATHFINDERPATH-XAVG\nTransformer36.3764.2757.4642.4471.40✗53.66\nReformer37.2756.1053.4038.0768.50✗50.56\nBigBird36.0564.0259.2940.8374.87✗54.17\nLinear Trans.16.1365.9053.0942.3475.30✗50.46\nPerformer18.0165.4053.8242.7777.05✗51.18\nFNet35.3365.1159.6138.6777.80✗54.42\nNyströmformer37.1565.5279.5641.5870.94✗57.46\nLuna-25637.2564.5779.2947.3877.72✗59.37\nHrrformer39.9865.3876.1550.4572.17✗60.83\nS459.6086.8290.9088.6594.2096.3586.09\nRWKV55.8886.0488.3470.5358.42✗72.07\nThe results show that RWKV performs second only to the S4 model in five datasets. While RWKV\nsubstantially underpreforms S4 on Image, Pathfinder, and Path-X, on the problems related to natural\nlanguage and computer code processing RWKV performs on par with S4 or nearly so.\nJ.3    Enwik8 Perplexity\nWe also evaluate our model in terms of perplexity on the Enwik8 dataset.  Baseline comparisons are\nmade with Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020) (the best performing dense\nversion), Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020).L,d,\nandTdenote the number of blocks (network depth), dimension of features, and sequence length, re-\nspectively.  Both Linear Transformer and Performer are implemented with customized CUDA kernels\n(github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch.\n1\nNo weight\ndecay nor dropout was used.\n2\nTrained with AdamW and weight decay set to 0.1, dropout of 0.1, batch\nsize of 16, and initial learning rate of 6e-4.\nMethodLdTTrain bpcTest bpcTime ComplexitySpace Complexity\nTransformer1251210240.9771.137O(T\n2\nd)O(T\n2\n+T d)\nTransformer2425610241.0391.130O(T\n2\nd)O(T\n2\n+T d)\nReformer1251210241.0401.195O(TlogT d)O(TlogT+T d)\nSynthesizer1251210240.9941.298O(T\n2\nd)O(T\n2\n+T d)\nLinear Transformer1251210240.9811.207O(T d\n2\n)O(T d+d\n2\n)\nPerformer1251210241.0021.199O(T d\n2\nlogd)O(T dlogd+d\n2\nlogd)\nAFT-simple1251210241.0461.209O(T d)O(T d)\nRWKV-RNN\n1\n651210240.720-O(Td)O(d)\nRWKV-RNN\n2\n1251210241.0101.178O(Td)O(d)\nTable 5: Enwik8 results, measured in bits per character (bpc).\nK    Inference results\nFigures 13 and 14 illustrate, respectively, the results on time (s) and memory (RAM, VRAM) requirements\nfor LLM inference infloat32precision. We benchmark the following model families and sizes:",
    "•RWKV: 169m, 430m, 1.4b, 3b, 7b, 14b\n•Bloom(Scao et al., 2022): 560m, 1b, 3b\n•OPT(Zhang et al., 2022): 125m, 350m, 1.3b, 2.7b, 6.7b, 13b\n•GPT-Neo(Black et al., 2021): 125m, 1.3b, 2.7b\n•Pythia(Biderman et al., 2023b): 160m, 410m, 1.4b, 2.8b, 6.7b, 12b\nFigure 13: Text generation inference memory (CPU RAM, GPU VRAM) for LLMs. Model parameters are not\naccounted.\nFigure 14: Text generation inference time for LLMs.",
    "Task NameMeasureChatGPTGPT-4RWKV-GPTRWKV-adaptedSOTA\nRTEF1 Macro88.191.344.274.892.1\nWNLIAccuracy81.791.647.949.397.9\nGoEmotionsF1 Macro25.623.17.97.952.8\nPolEmo2F1 Macro44.141.038.240.976.4\nTable 6: ChatGPT, GPT-4 and RWKV-4-Raven-14B reasoning performance comparison in RTE (Wang et al., 2019),\nWNLI (Wang et al., 2018), GoEmotions (Demszky et al., 2020), and PolEmo2 (Koco\n ́\nn et al., 2019) benchmarks.\nRWKV GPT prompts were primarily used for ChatGPT in (Koco\n ́\nn et al., 2023). SOTA is provided as a supplementary\nreference.\nTask NameMeasureChatGPTRWKV-adaptedSOTA\nAggressionF1 Macro69.1056.6674.45\nMathQAAccuracy71.405.4383.20\nSarcasmF1 Macro49.8850.9653.57\nTweetSentF1 Macro63.3252.5072.07\nUnhealthyF1 Macro45.2143.3050.96\nTable 7: ChatGPT and RWKV-4-Raven-14B performance comparison in Aggresion (Wulczyn et al., 2017), Sarcasm\n(Siddiqui, 2019), Unhealthy (Price et al., 2020), MathQA (Cobbe et al., 2021), and TweetSent (Barbieri et al., 2020)\nbenchmarks. SOTA is provided as a supplementary reference.\nL    Importance of prompt construction and comparison to GPT models\nInspired by Koco\n ́\nn et al. (2023), we compared the zero-shot performance of the RWKV-4-Raven-14B with\nChatGPT (access in February 2023) and GPT-4 using several known NLP tasks, i.e., recognizing textual\nentailment (RTE), Winograd Natural Language Inference (WNLI), and recognizing emotions elicited\nin readers (GoEmotions and PolEmo2). Each model got the same prompts manually chosen to receive\nproper responses from the ChatGPT model. As shown in Tab. 6, RWKV performs significantly worse\nthan ChatGPT and GPT-4 in several specific tasks. We suspect that this disparity is likely caused by the\nchoice of prompts used to generate the answers since the prompts are written in natural language and do\nnot take into account that RWKV, as an RNN, is unable to look back inside an instruction.\nWhen the instruction style was adapted (re-ordered) to respect that RNNs are not capable of \"retro-\nspective processing\", the quality may significantly change, e.g., for RTE (Wang et al., 2019) F1 Macro\nincreased from 44.2% to 74.8%. We hypothesize that RWKV models are more sensitive to the position of\nthe components in the context, as RNN-based architectures cannot look back and readjust the weight of\nprevious information. For better performance, the desired information should be placedafterthe main\nquestion.\nAn example ChatGPT prompt for recognizing textual entailment (RTE)\nHaving premise <here is a premise> judge if the following hypothesis <here is a hypothesis> is\nlogically connected with the premise. Answer \"entailment\" if yes, or \"not_entailment\" if no.\nA re-ordered RWKV prompt for RTE taking into account the nature of the RNN\nCan you tell me if the hypothesis is entailment or is not entailment to the premise?\npremise: <here is a premise>\nhypothesis: <here is a hypothesis>\nWhile separating the instruction from the input is relatively easy to do, some other aspects of prompt\nengineering are harder to quantify. For that purpose, we also tested the approach of stating the input after",
    "the question on multiple other tasks, i.e., aggression and sarcasm detection, classification of unhealthy\n(offensive) texts, mathematical Q&A, and sentiment analysis, see Tab. 7. The results suggest that better\nprompts might reduce the disparity between models. Raven achieves comparable results to ChatGPT on\nunhealthy conversation detection and even surpasses it on the sarcasm detection dataset. While such an\napproach to prompting looks necessary, it is not enough in itself to replace the capability of having free\naccess to the whole context. Therefore, prompt engineering seems to be significantly more important for\nthe RNN models rather than for standard transformers. It is entirely possible that good prompts to RNN\nmodels do not mean additional restrictions, but should simply be constructed using completely different\nguidelines.  The authors of the aforementioned paper (Koco\n ́\nn et al., 2023)\n4\nperform chain-of-thought\nto improve results on the MathQA dataset. Even including this approach, the Raven model achieved a\nvery low accuracy of 5.43%. Without it, the model performed even worse, performing only very basic\nand simple calculations and achieving 4.13% accuracy.  Raven struggled with questions that required\nintermediate results. It is likely that the order of information presented in the math questions inside the\ndataset poses a challenge for the RWKV model. It is yet to be seen if prompt engineering can address this\nissue. This further emphasizes the importance of the order of information the model receives.\nTemplate used to prompt the Raven model in MathQA with chain-of-thought\nWrite the reasoning and highlight the answer to the question at the end in the format: \"Answer: \".\nThe question is: <here is a question>\nTemplate used to prompt the Raven model in MathQA without chain-of-thought\nWrite the answer to the math question in the format: \"Answer: \".\nThe question is: <here is a question>\nM    Cases\nIn this part, we present a few instances of outputs produced by RWKV model using a Chat interface\n56\n.\nRWKV-4-Raven-14B\nTell me about ravens.\nRavens are large, black birds with a distinctive white head and neck. They are found in most parts\nof the world, including North America, Europe, Asia, and Australia. Ravens are known for their\nintelligence and problem-solving abilities. They are also considered to be symbols of death and\ntransformation in many cultures. Ravens are often associated with the afterlife or death because\nthey have been known to eat carrion or even other birds. In some cultures, ravens are also believed\nto be messengers of the gods or guardians of treasure.\n4\nThis is in line with the idea discussed in (Wei et al., 2022b)\n5\nttps://github.com/BlinkDL/ChatRWKV\n6\nhttps://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio",
    "RWKV-4-Raven-14B\nExplain the concept of machine learning to a 10-year-old.\nMachine learning is like a super-smart computer that can learn on its own. It can recognize patterns\nand make decisions based on what it has learned. Just like how you might remember something\nfrom the past, the computer can remember what it’s learned and use it to make predictions or\ndecisions in the future. It’s like having a smart friend who always knows what to do!\nRWKV-4-Raven-14B\nWrite a short poem describing nature’s beauty in springtime.\nNature’s beauty in springtime is rare,\nA symphony of colors, a gentle surprise.\nThe flowers bloom, the birds sing and sing,\nAs the sun shines down, warming our skin.\nThe trees sway in the gentle breeze,\nAs we take in their beauty with ease.\nThe sky is blue, the clouds are white,\nA sight to behold, it’s truly divine.",
    "RWKV-4-Raven-14B\nBelow is the numerical stable formula for updating the state variable in RWKV:\na\n∗\nt\n=e\np−q\na\nt−1\n+e\nu+k\nt\n−q\nv\nt\nb\n∗\nt\n=e\np−q\nb\nt−1\n+e\nu+k\nt\n−q\nwkv\nt\n=\na\n∗\nt\nb\n∗\nt\nPlease  write  out  the  corresponding  pytorch  code  to  do  so,  where  botha\nt\nandb\nt\nare\ntorch.Tensor([n])\nHere is the PyTorch code to update the state variable in RWKV using the given formula:\ni m p o r tt o r c h\nd e fu p d a t e _ s t a t e ( a ,   b ,   u ,   v ,   p ,   q ) :\na _ t   =   t o r c h . e x p ( p   −   q )\n*\na   +   t o r c h . e x p ( u   +   k   −   q )\n*\nv\nb _ t   =   t o r c h . e x p ( p   −   q )\n*\nb   +   t o r c h . e x p ( u   +   k   −   q )\nr e t u r na _ t  ,    b _ t\na ,   b   =   t o r c h . r a n d n ( 2 ,   3 )\nu   =   t o r c h . r a n d n ( 2 )\nv   =   t o r c h . r a n d n ( 2 )\nwkv   =   t o r c h . r a n d n ( 2 )\na _ t  ,    b _ t   =   u p d a t e _ s t a t e ( a ,   b ,   u ,   v ,   wkv ,    t o r c h . r a n d n ( 2 ) )\np r i n t( \" a _ t : \" ,   a _ t )\np r i n t( \" b _ t : \" ,   b _ t )\nThis code defines a function update_state that takes in the state variables a and b, the control\nvariables u and v, and the parameters wkv and torch.randn.  It then uses the PyTorch torch.exp\nfunction to compute the exponential of the control variables and the log of the normalization factor.\nFinally, it uses these values to update the state.\nRWKV-4-Raven-14B\nExplain the following metaphor: Life is like cats.\nThe metaphor \"Life is like cats\" suggests that life can be unpredictable and full of unexpected\ntwists and turns, just like a cat’s behavior. It implies that we cannot fully control or predict the\noutcome of our lives, just as we cannot control the movements of a cat. The metaphor also implies\nthat life is complex and multifaceted, with many different variables and factors at play. Just as a\ncat can have multiple personalities or moods, life can have many twists and turns that are difficult\nto predict or understand. Overall, the metaphor suggests that life is complex and unpredictable,\nmuch like a cat’s behavior.",
    "RWKV-4-Raven-14B\nTest the convergence of\nP\n∞\ni=0\n2\n−i\n.\nThe given series is a geometric series with the first terma\n0\n= 2and common ratior=\n1\n2\n. We can\nfind the sum of this series using the formula for a geometric series:\n∞\nX\ni=0\na\ni\n=\na\n0\n1−r\n=\n1\n2\n1−\n1\n2\n=\n1\n1−\n1\n2\n=\n2\n1\n= 2\nSo, the sum of the given series is2.\nRWKV-4-Raven-14B\nI would like to cook some Chinese food at home. Do you have any suggestions on the meal I\ncould choose?\nYes, of course!  If you are not very experienced in cooking Chinese food, you could start with\nmaking noodles or rice, then make a dish using one of those.  I also think making stir-fry with\nsome meat is an excellent idea.\nI want to make something more special. What about red braised pork belly?\nYes, red braised pork belly is also very popular and tasty.\nTell me the steps to cook.\nYou will need a few ingredients such as pork belly, star anise, sugar, ginger, scallions and wine.\nPut all of these in a pot, cook on medium heat and simmer until the meat is tender.  After that,\nremove the meat from the pot and reserve the sauce. Slice the meat and add some ginger slices\ninto it, then boil until it’s almost done.\nWhat do I need to be careful with the most during the cooking process?\nTo cook pork belly properly, it is very important to monitor the heat of the stove and be patient\nwith the cooking process. Overcooking it can cause the pork to be tough, dry and difficult to chew.\nYou can add a few ingredients like soy sauce or black beans to give the pork more flavor.\nTell me more about the proper heat and timing.\nIf the temperature of the water is too high, it can damage the collagen and connective tissues,\nresulting  in  tough  pork  belly.   It  is  essential  to  control  the  temperature  carefully,  especially\nwhen braising the pork.  Additionally, keep the water simmering but not boiling to maintain a\nlow simmer, as high heat will dry out the pork belly and result in tough and unpalatable meat.\nLastly, monitor the heat during cooking and make sure the heat is medium, as this is what is most\nappropriate for slow cooking pork belly.\nI see. Thank you for your advice.\nYou’re welcome!  It was a pleasure chatting with you.  Let me know if you need any further\nassistance in the future.",
    "RWKV-4-Raven-14B\nIf all apples are fruits and all fruits are sweet, are all apples sweet?  Please explain your\nanswer in detail.\nYes, all apples are fruits, and all fruits are sweet. Therefore, it is safe to assume that all apples are\nsweet.\nIf birds can fly. Penguins are birds. Can penguins fly? Please explain your answer in detail.\nNo, penguins cannot fly.  Penguins are birds, but they are not capable of flight like other birds.\nThey have wings and can glide for short distances, but they cannot take off or fly long distances\nlike other birds.\nThe day before two days after the day before tomorrow is Saturday. What day is it today?\nPlease explain your answer in detail.\nToday is Friday. Two days after tomorrow is Sunday. Tomorrow is Saturday.\nIf five cats can catch five mice in five minutes, how long will it take one cat to catch one\nmouse? Please explain your answer in detail.\nIf five cats can catch five mice in five minutes, it means that each cat can catch one mouse in one\nminute. To find out how long it will take one cat to catch one mouse, we can use the concept of\nproportionality.\nLet’s assume that it takes x minutes for a cat to catch a mouse. Then, the number of mice caught\nby a cat in one minute is equal to the number of cats multiplied by the number of mice caught by\neach cat: x = 5 * 1 = 5\nSo, it will take 5 minutes for one cat to catch one mouse.\nRWKV-4-Raven-14B\nSummarize the following paragraph.\n<The Abstract of this paper goes here>\nThe paper presents a novel architecture, Receptance Weighted Key Value (RWKV), that combines\nthe efficient parallelizable training of Transformers with the efficient inference of RNNs.  The\nmodel is able to scale to tens of billions of parameters and exhibits linear computational complexity\nduring training and inference, making it a promising alternative to Transformers for sequence\nprocessing tasks.\nPlease further summarize it in one sentence.\nThe researchers have developed a new architecture that combines the efficient parallelizable\ntraining of Transformers with the efficient inference of RNNs, which allows for linear scaling in\nmemory and computational requirements."
  ]
}