{
  "key": "4J6FHRHV",
  "url": "http://arxiv.org/pdf/2309.05519",
  "metadata": {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "abstract": "  While recently Multimodal Large Language Models (MM-LLMs) have made exciting\nstrides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities.\nAs we humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in\narbitrary combinations of text, images, videos, and audio. By leveraging the\nexisting well-trained highly-performing encoders and decoders, NExT-GPT is\ntuned with only a small amount of parameter (1%) of certain projection layers,\nwhich not only benefits low-cost training and also facilitates convenient\nexpansion to more potential modalities. Moreover, we introduce a\nmodality-switching instruction tuning (MosIT) and manually curate a\nhigh-quality dataset for MosIT, based on which NExT-GPT is empowered with\ncomplex cross-modal semantic understanding and content generation. Overall, our\nresearch showcases the promising possibility of building an AI agent capable of\nmodeling universal modalities, paving the way for more human-like AI research\nin the community. Project page: https://next-gpt.github.io/\n",
    "published": "2023-09-11T15:02:25Z"
  },
  "text": [
    "NExT-GPT: Any-to-Any Multimodal LLM\nShengqiong Wu   Hao Fei\n∗\nLeigang Qu   Wei Ji   Tat-Seng Chua\nNExT++, School of Computing, National University of Singapore\nProject:https://next-gpt.github.io/\nLLM\nAudio \nDiffusion\nVideo \nDiffusion\nLLM-based Semantic \nUnderstanding\nInstruction-following \nAlignment\nImage \nDiffusion \nText\nImage\nAudio\nVideo\nImage Output \nProjection\nAudio Output \nProjection\nVideo Output \nProjection\nImage Input \nProjection\nImage \nEncoder\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\n...\nMore modalities\n...\nMultimodal Input \nEncoding \nMultimodal Output \nGeneration\nLLM-centric \nAlignment\nFigure 1: By connecting LLM with multimodal adaptors and diffusion decoders, NExT-GPT achieves\nuniversal multimodal understanding and any-to-any modality input and output.\nAbstract\nWhile  recently  Multimodal  Large  Language  Models  (MM-LLMs)  have  made\nexciting strides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities. As\nwe humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary\ncombinations  of  text,  images,  videos,  and  audio.   By  leveraging  the  existing\nwell-trained highly-performing encoders and decoders, NExT-GPT is tuned with\nonly a small amount of parameter (1%) of certain projection layers, which not\nonly benefits low-cost training and also facilitates convenient expansion to more\npotential modalities.  Moreover, we introduce a modality-switching instruction\ntuning (MosIT) and manually curate a high-quality dataset for MosIT, based on\nwhich NExT-GPT is empowered with complex cross-modal semantic understanding\nand content generation. Overall, our research showcases the promising possibility\nof building a unified AI agent capable of modeling universal modalities, paving the\nway for more human-like AI research in the community.\n∗\nHao Fei is the corresponding author:haofei37@nus.edu.sg\nPreprint, work in progress.\narXiv:2309.05519v2  [cs.AI]  13 Sep 2023",
    "1  Introduction\nRecently, the topic of Artificial Intelligence Generated Content (AIGC) has witnessed unprecedented\nadvancements with certain technologies, such as ChatGPT for text generation [59] and diffusion\nmodels for visual generation [21]. Among these, the rise of Large Language Models (LLMs) has been\nparticularly remarkable, e.g., Flan-T5 [13], Vicuna [12], LLaMA [80] and Alpaca [79], showcasing\ntheir formidable human-level language reasoning and decision-making capabilities, shining a light on\nthe path of Artificial General Intelligence (AGI). Our world is inherently multimodal, and humans\nperceive the world with different sensory organs for varied modal information, such as language,\nimages, videos, and sounds, which often complement and synergize with each other.  With such\nintuition, the purely text-based LLMs have recently been endowed with other modal understanding\nand perception capabilities of visual, video, audio, etc.\nA notable approach involves employing adapters that align pre-trained encoders in other modalities\nto textual LLMs. This endeavor has led to the rapid development of multimodal LLMs (MM-LLMs),\nsuch as BLIP-2 [43], Flamingo [1], MiniGPT-4 [109], Video-LLaMA [103], LLaVA [52], PandaGPT\n[77], SpeechGPT [102].  Nevertheless, most of these efforts pay the attention to the multimodal\ncontent understanding at the input side, lacking the ability to output content in multiple modalities\nmore than texts. We emphasize that real human cognition and communication indispensably require\nseamless transitions between any modalities of information. This makes the exploration of any-to-any\nMM-LLMs critical to achieving real AGI, i.e., accepting inputs in any modality and delivering\nresponses in the appropriate form of any modality.\nCertain efforts have been made to mimic the human-like any-to-any modality conversion. Lately, CoDi\n[78] has made strides in implementing the capability of simultaneously processing and generating\narbitrary combinations of modalities, while it lacks the reasoning and decision-making prowess of\nLLMs as its core, and also is limited to the simple paired content generation.  On the other hand,\nsome efforts, e.g., Visual-ChatGPT [88] and HuggingGPT [72] have sought to combine LLMs with\nexternal tools to achieve approximately the ‘any-to-any’ multimodal understanding and generation.\nUnfortunately, these systems suffer from critical challenges due to the complete pipeline architecture.\nFirst, the information transfer between different modules is entirely based on discrete texts produced\nby the LLM, where the cascade process inevitably introduces noise and propagates errors.  More\ncritically, the entire system only leverages existing pre-trained tools for inference only. Due to the\nlack of overall end-to-end training in error propagation, the capabilities of content understanding\nand multimodal generation can be very limited, especially in interpreting intricate and implicit user\ninstructions. In a nutshell, there is a compelling need for constructing an end-to-end MM-LLM of\narbitrary modalities.\nIn pursuit of this goal, we presentNExT-GPT, an any-to-any MM-LLM designed to seamlessly\nhandle input and output in any combination of four modalities: text, images, videos, and audio. As\ndepicted in Figure 1, NExT-GPT comprises three tiers.First, we leverage established encoders to\nencode inputs in various modalities, where these representations are projected into language-like\nrepresentations comprehensible to the LLM through a projection layer.Second, we harness an\nexisting open-sourced LLM as the core to process input information for semantic understanding and\nreasoning.  The LLM not only directly generates text tokens but also produces unique “modality\nsignal” tokens that serve as instructions to dictate the decoding layers whether & what modal content\nto output correspondingly.Third, the produced multimodal signals with specific instructions, after\nprojection, route to different encoders and finally generate content in corresponding modalities.\nAs NExT-GPT encompasses encoding and generation of various modalities, training the system\nfrom scratch would entail substantial costs. Instead, we take advantage of the existing pre-trained\nhigh-performance encoders and decoders, such as Q-Former [43], ImageBind [25] and the state-\nof-the-art latent diffusion models [68,69,8,2,51,33].  By loading the off-the-shelf parameters,\nwe not only avoid cold-start training but also facilitate the potential growth of more modalities.\nFor the feature alignment across the three tiers,  we consider fine-tuning locally only the input\nprojection and output projection layers, with an encoding-side LLM-centric alignment and decoding-\nside instruction-following alignment, where the minimal computational overhead ensures higher\nefficiency.  Furthermore, to empower our any-to-any MM-LLM with human-level capabilities in\ncomplex cross-modal generation and reasoning, we introduce amodality-switching instruction tuning\n(termedMosit), equipping the system with sophisticated cross-modal semantic understanding and\ncontent generation.   To combat the absence of such cross-modal instruction tuning data in the\ncommunity, we manually collect and annotate aMositdataset consisting of 5,000 samples of high\n2",
    "quality. Employing the LoRA technique [32], we fine-tune the overall NExT-GPT system onMosIT\ndata, updating the projection layers and certain LLM parameters.\nOverall, this work showcases the promising possibility of developing a more human-like MM-LLM\nagent capable of modeling universal modalities. The contributions of this project are as follows:\n•We for the first time present an end-to-end general-purpose any-to-any MM-LLM, NExT-\nGPT, capable of semantic understanding and reasoning and generation of free input and\noutput combinations of text, images, videos, and audio.\n•We introduce lightweight alignment learning techniques, the LLM-centric alignment at\nthe encoding side, and the instruction-following alignment at the decoding side, efficiently\nrequiring minimal parameter adjustments (only 1% params) for effective semantic alignment.\n•We annotate a high-quality modality-switching instruction tuning dataset covering intricate\ninstructions across various modal combinations of text, images, videos, and audio, aiding\nMM-LLM with human-like cross-modal content understanding and instruction reasoning.\n2  Related Work\nCross-modal Understanding and GenerationOur world is replete with multimodal information,\nwherein we continuously engage in the intricate task of comprehending and producing cross-modal\ncontent. The AI community correspondingly emerges varied forms of cross-modal learning tasks,\nsuch as Image/Video Captioning [99,16,56,56,27,49], Image/Video Question Answering [94,\n90,48,98,3], Text-to-Image/Video/Speech Synthesis [74,30,84,23,17,51,33], Image-to-Video\nSynthesis [18,37] and more, all of which have experienced rapid advancements in past decades.\nResearchers have proposed highly effective multimodal encoders,  with the aim of constructing\nunified representations encompassing various modalities. Meanwhile, owing to the distinct feature\nspaces of different modalities, it is essential to undertake modality alignment learning. Moreover, to\ngenerate high-quality content, a multitude of strong-performing methods have been proposed, such\nas Transformer [82,101,17,24], GANs [53,7,93,110], VAEs [81,67], Flow models [73,6] and the\ncurrent state-of-the-art diffusion models [31,64,57,22,68]. Especially, the diffusion-based methods\nhave recently delivered remarkable performance in a plethora of cross-modal generation tasks, such as\nDALL-E [66], Stable Diffusion [68]. While all previous efforts of cross-modal learning are limited to\nthe comprehension of multimodal inputs only, CoDi [78] lately presents groundbreaking development.\nLeveraging the power of diffusion models, CoDi possesses the ability to generate any combination\nof output modalities, including language, images, videos, or audio, from any combination of input\nmodalities in parallel. Regrettably, CoDi might still fall short of achieving human-like deep reasoning\nof input content, with only parallel cross-modal feeding&generation.\nMultimodal Large Language ModelsLLMs have already made profound impacts and revolutions\non the entire AI community and beyond. The most notable LLMs, i.e., OpenAI’s ChatGPT [59] and\nGPT4 [60], with alignment techniques such as instruction tuning [61,47,104,52] and reinforcement\nlearning from human feedback (RLHF) [75], have demonstrated remarkable language understanding\nand reasoning abilities. And a series of open-source LLMs, e.g., Flan-T5 [13], Vicuna [12], LLaMA\n[80] and Alpaca [79], have greatly spurred advancement and made contributions to the community\n[109,100]. Afterward, significant efforts have been made to construct LLMs dealing with multimodal\ninputs and tasks, leading to the development of MM-LLMs.\nOn the one hand, most of the researchers build fundamental MM-LLMs by aligning the well-trained\nencoders of various modalities to the textual feature space of LLMs, so as to let LLMs perceive other\nmodal inputs [35,109,76,40]. For example, Flamingo [1] uses a cross-attention layer to connect a\nfrozen image encoder to the LLMs. BLIP-2 [43] employs a Q-Former to translate the input image\nqueries to the LLMs. LLaVA [52] employs a simple projection scheme to connect image features into\nthe word embedding space. There are also various similar practices for building MM-LLMs that are\nable to understand videos (e.g., Video-Chat [44] and Video-LLaMA [103]), audios (e.g., SpeechGPT\n[102]), etc.  Profoundly, PandaGPT [77] achieves a comprehensive understanding of six different\nmodalities simultaneously by integrating the multimodal encoder, i.e., ImageBind [25].\nNevertheless, these MM-LLMs all are subject to the limitation of only perceiving multimodal data,\nwithout generating content in arbitrary modalities.  To achieve LLMs with both multimodal input\nand output, some thus explore employing LLMs as decision-makers, and utilizing existing off-the-\nshelf multimodal encoders and decoders as tools to execute multimodal input and output, such\nas Visual-ChatGPT [88],  HuggingGPT [72],  and AudioGPT [34].   As aforementioned,  passing\n3",
    "LLM\nImage Input \nProjection\nImage \nEncoder\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\nAbsolutely! There are so \nmany  activities  to  enjoy \nby  the  sea,  like  beach \nvolleyball.\n But I'm really interested \nin trying out surfing. I \nthink it's super cool. It \nwould be even better if I \ncould create a vlog to \nshowcase my progress.\nLLM\nImage Input \nProjection\nImage \nEncoder\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\n Vlog content can be \nquite diverse. Here's a \nreference \nImg. Sig. Tok. Rep.\nVid. Sig. Tok. Rep.\nLLM\nImage Input \nProjection\nImage \nEncoder\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\nFor the cover, it should \ndefinitely feature you \nsurfing. As for the \nmusic, I recommend \nsomething lively, like \nthis?\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\nAbsolutely! There are so \nmany  activities  to  enjoy \nby  the  sea,  like  beach \nvolleyball.\nAbsolutely! There are so \nmany  activities  to  enjoy \nby  the  sea,  like  beach \nvolleyball.\n But I'm really interested \nin trying out surfing. I \nthink it's super cool. It \nwould be even better if I \ncould create a vlog to \nshowcase my progress.\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\n Vlog content can be \nquite diverse. Here's a \nreference \nCreating a vlog requires a cover and \nmusic. What do you think would be \na good style?\nImg. Sig. Tok. Rep.\nAud. Sig. Tok. Rep.\nLLM\nImage Input \nProjection\nImage \nEncoder\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\nOf course! To begin learning surfing, \nyou should first find a good surfing \ninstructor to learn basic skills and \nsafety knowledge. Additionally ...\nAbsolutely! There are so \nmany  activities  to  enjoy \nby  the  sea,  like  beach \nvolleyball.\n But I'm really interested \nin trying out surfing. I \nthink it's super cool. It \nwould be even better if I \ncould create a vlog to \nshowcase my progress.\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\n Vlog content can be \nquite diverse. Here's a \nreference \nCreating a vlog requires a cover and \nmusic. What do you think would be \na good style?\nFor the cover, it should \ndefinitely feature you \nsurfing. As for the \nmusic, I recommend \nsomething lively, like \nthis?\nCan you provide me with some  \nlearning tips? I can't wait to start \nlearning.\nFigure 2: NExT-GPT inference process. Grey colors denote the deactivation of the modules.\nmessages between modules with pure texts (i.e., LLM textual instruction) under the discrete pipeline\nscheme will inevitably introduce noises. Also lacking comprehensive tuning across the whole system\nsignificantly limits the efficacy of semantics understanding. Our work takes the mutual benefits of\nboth the above two types, i.e., learning an any-to-any MM-LLM in an end-to-end manner.\n3  Overall Architecture\nFigure 1 presents the schematic overview of the framework. NExT-GPT consists of three main tiers:\nthe encoding stage, the LLM understanding and reasoning stage, and the decoding stage.\nMultimodal Encoding StageFirst, we leverage existing well-established models to encode inputs\nof various modalities.  There are a set of alternatives of encoders for different modalities, e.g., Q-\nFormer [43], ViT [19], CLIP [65]. Here we take advantage of the ImageBind [25], which is a unified\nhigh-performance encoder across six modalities. With ImageBind, we are spared from managing\n4",
    "EncoderInput ProjectionLLMOutput ProjectionDiffusion\nNameParam   NameParamNameParamNameParamNameParam\nText————————\nImageVicuna [12]7BTransformer31MSD [68]1.3B\nAudio(LoRA33M)Transformer31MAudioLDM [51]975M\nVideo\nImageBind [25]1.2BLinear4M\nTransformer32MZeroscope [8]1.8B\nTable 1: Summary of system configuration. Only 1% parameters need updating.\nmany numbers of heterogeneous modal encoders. Then, via the linear projection layer, different input\nrepresentations are mapped into language-like representations that are comprehensible to the LLM.\nLLM Understanding and Reasoning StageAn LLM is used as the core agent of NExT-GPT.\nTechnically, we employ the Vicuna\n2\n[12], which is the open-source text-based LLM that is widely\nused in the existing MM-LLMs [77,103].  LLM takes as input the representations from different\nmodalities and carries out semantic understanding and reasoning over the inputs. It outputs 1) the\ntextual responses directly, and 2) signal tokens of each modality that serve as instructions to dictate\nthe decoding layers whether to generate multimodal contents, and what content to produce if yes.\nMultimodal Generation StageReceiving the multimodal signals with specific instructions from\nLLM (if any), the Transformer-based output projection layers map the signal token representations\ninto the ones that are understandable to following multimodal decoders. Technically, we employ the\ncurrent off-the-shelf latent conditioned diffusion models of different modal generations, i.e., Stable\nDiffusion (SD)\n3\nfor image synthesis [68], Zeroscope\n4\nfor video synthesis [8], and AudioLDM\n5\nfor\naudio synthesis [51]. The signal representations are fed into the condition encoders of the conditioned\ndiffusion models for content generation.\nIn Table 1 we summarize the overall system configurations.   It is noteworthy that in the entire\nsystem,  only  the  input  and  output  projection  layers  of  lower-scale  parameters  (compared  with\nthe overall huge capacity framework) are required to be updated during the following learning,\nwith  all  the  rest  encoders  and  decoders  frozen.   That  is,  131M(=4+33+31+31+32)  /  [131M  +\n12.275B(=1.2+7+1.3+1.8+0.975)],  only1%parameters are to be updated.   This is also one of\nthe key advantages of our MM-LLM.\nIn Figure 2 we further illustrate the inference procedure of NExT-GPT. Given certain user inputs of\nany combination of modalities, the corresponding modal encoders, and projectors transform them\ninto feature representations and pass them to LLM\n6\n. Then, LLM decides what content to generate,\ni.e., textual tokens, and modality signal tokens. If LLM identifies a certain modality content (except\nlanguage) to be produced, a special type of token [40] will be output indicating the activation of\nthat modality; otherwise, no special token output means deactivation of that modality. Technically,\nwe design the ‘<IMG\ni\n>’ (i= 0,···,4) as image signal tokens; ‘<AUD\ni\n>’ (i= 0,···,8) as audio\nsignal tokens; and ‘<VID\ni\n>’ (i= 0,···,24) as video signal tokens. After LLM, the text responses\nare output to the user; while the representations of the signal tokens of certain activated modalities\nare passed to the corresponding diffusion decoders for content generation.\n4  Lightweight Multimodal Alignment Learning\nTo bridge the gap between the feature space of different modalities, and ensure fluent semantics\nunderstanding of different inputs, it is essential to perform alignment learning for NExT-GPT. Since\nwe design the loosely-coupled system with mainly three tiers,  we only need to update the two\nprojection layers at the encoding side and decoding side.\n4.1  Encoding-side LLM-centric Multimodal Alignment\nFollowing the common practice of existing MM-LLMs, we consider aligning different inputting\nmultimodal features with the text feature space, the representations that are understandable to the core\n2\nhttps://huggingface.co/lmsys/vicuna-7b-delta-v0, 7B, version 0\n3\nhttps://huggingface.co/runwayml/stable-diffusion-v1-5, version1.5.\n4\nhttps://huggingface.co/cerspense/zeroscope_v2_576w, versionzeroscope_v2_576w.\n5\nhttps://audioldm.github.io/, versionaudioldm-l-full.\n6\nExcept the text inputs, which will be directly fed into LLM.\n5",
    "Image\nLLM\nAudio\nVideo\nImage \nCaption\nAudio \nCaption\nVideo \nCaption\nError Back-prop.\nAudio \nEncoder\nVideo \nEncoder\nImage Input \nProjection\nImage \nEncoder\nAudio Input \nProjection\nVideo Input \nProjection\nLLM\nText Encoder \n(in Aud. Diff.)\nText Encoder \n(in Vid. Diff.)\nText Encoder \n(in Img. Diff.) \nImage Output \nProjection\nAudio Output \nProjection\nVideo Output \nProjection\nImage \nCaption\nAudio \nCaption\nVideo \nCaption\nImage Signal Token\nText Response\nAudio signal token\nText Response\nVideo signal token\nText Response\n`\nMin. Eucli. Dist.\nVid. Rep.\nAligned Vid. Rep.\nLLM Output Rep.\n...\n...\n(a) Encoding-side LLM-centric Alignment\n(b) Decoding-side Instruction-following Alignment\nAligned Aud. Rep.\nAligned Img. Rep.\nAud. Rep.\nImg. Rep.\n...\n...\nFigure 3: Illustration of the lightweight multimodal alignment learning of encoding and decoding.\nLLM. This is thus intuitively named the LLM-centric multimodal alignment learning. To accomplish\nthe alignment, we prepare the ‘X-caption’ pair (‘X’ stands for image, audio, or video) data from\nexisting corpus and benchmarks. We enforce LLM to produce the caption of each input modality\nagainst the gold caption. Figure 3(a) illustrates the learning process.\n4.2  Decoding-side Instruction-following Alignment\nOn the decoding end, we have integrated pre-trained conditional diffusion models from external\nresources.   Our  main  purpose  is  to  align  the  diffusion  models  with  LLM’s  output  instructions.\nHowever, performing a full-scale alignment process between each diffusion model and the LLM\nwould entail a significant computational burden.  Alternatively, we here explore a more efficient\napproach, decoding-side instruction-following alignment, as depicted in Figure 3(b). Specifically,\nsince diffusion models of various modalities are conditioned solely on textual token inputs.  This\nconditioning diverges significantly from the modal signal tokens from LLM in our system, which\nleads to a gap in the diffusion models’ accurate interpretation of the instructions from LLM. Thus, we\nconsider minimizing the distance between the LLM’s modal signal token representations (after each\nTransformer-based project layer) and the conditional text representations of the diffusion models.\nSince only the textual condition encoders are used (with the diffusion backbone frozen), the learning\nis merely based on the purely captioning texts, i.e., without any visual or audio resources. This also\nensures highly lightweight training.\n5  Modality-switching Instruction Tuning\n5.1  Instruction Tuning\nDespite aligning both the encoding and decoding ends with LLM, there remains a gap towards\nthe goal of enabling the overall system to faithfully follow and understand users’ instructions and\ngenerate desired multimodal outputs.  To address this, further instruction tuning (IT) [97,77,52]\nis deemed necessary to enhance the capabilities and controllability of LLM. IT involves additional\ntraining of overall MM-LLMs using ‘(INPUT, OUTPUT)’ pairs, where ‘INPUT’ represents the user’s\ninstruction, and ‘OUTPUT’ signifies the desired model output that conforms to the given instruction.\nTechnically, we leverage LoRA [32] to enable a small subset of parameters within NExT-GPT to be\nupdated concurrently with two layers of projection during the IT phase. As illustrated in Figure 4,\nwhen an IT dialogue sample is fed into the system, the LLM reconstructs and generates the textual\n6",
    "Text\nLLM\nLoRA\nImage Input \nProjection\nImage \nEncoder\nText Encoder \n(in Img. Diff.) \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nText Encoder \n(in Aud. Diff.) \nText Encoder \n(in Vid. Diff.) \nAudio Output \nProjection\nVideo Output \nProjection\nText\n+\nText\nText\n+\nText\nText\n+\nText\nText\n+\nText\nText\nText\nText\nText\nText\nText\nText\n<IMG\nk\n> ... \n<AUD\nn\n>...\n<VID\nm\n> ... \n<IMG\nk\n>... \n...\nImg. Sig. \nTok. Rep.\nAud. Sig. \nTok. Rep.\nVid. Sig. \nTok. Rep.\n...\nMin. Eucli. Dist.\nImage Caption\n1\nAudio Caption\nVideo Caption\nImage Caption\n2\nInput Instructions\nCross \nEncropy\n...\n...\n...\nLLM OutputGold Annotation\nText\nText\nText\nText\nText\nText\nText\nText\n<IMG\nk\n> ... \n<AUD\nn\n>...\n<VID\nm\n> ... \n<IMG\nk\n>... \nGold Annotation\nFigure 4: Illustration of modality-switching instruction tuning.\ncontent of input (and represents the multimodal content with the multimodal signal tokens).  The\noptimization is imposed based on gold annotations and LLM’s outputs. In addition to the LLM tuning,\nwe also fine-tune the decoding end of NExT-GPT. We align the modal signal token representation\nencoded by the output projection with the gold multimodal caption representation encoded by the\ndiffusion condition encoder. Thereby, the comprehensive tuning process brings closer to the goal of\nfaithful and effective interaction with users.\n5.2  Instruction Dataset\nFor the IT of NExT-GPT, we consider the following datasets.\n‘Text+X’ — ‘Text’ DataThe commonly used datasets for MM-LLM IT entail inputs of both\ntexts and multimodal contents (i.e., ‘X’ could be the image, video, audio, or others), and the outputs\nare textual responses from LLM. There are well-established data of this type, e.g., LLaVA [52],\nminiGPT-4 [109], VideoChat [44], where we directly employ them for our tuning purpose.\n‘Text’ — ‘Text+X’ DataSignificantly unlike existing MM-LLMs, in our any-to-any scenario, the\ntarget not only includes the generations of texts, but also the multimodal contents, i.e., ‘Text+X’.\nThus, we construct the ‘Text’ — ‘Text+X’ data, i.e., text-to-multimodal (namely T2M) data. Based\non the rich volume of ‘X-caption’ pairs from the existing corpus and benchmarks [71,50,5,38], with\nsome templates, we borrow GPT-4 to produce varied textual instructions to wrap the captions, and\nresult in the data.\nMosITDataCrafting high-quality instructions that comprehensively cover the desired target be-\nhaviors is non-trivial.  We notice that the above IT datasets fail to meet the requirements for our\nany-to-any MM-LLM scenario. Firstly, during a human-machine interaction, users and LLM involve\ndiverse and dynamically changing modalities in their inputs and outputs.  Additionally, we allow\nmulti-turn conversations in the process, and thus processing and understanding of complex user\nintentions is required. However, the above two types of data lack variable modalities, and also are\nrelatively short in dialogues, failing to mimic real-world scenarios adequately.\nTo facilitate the development of any-to-any MM-LLM, we propose a novel Modality-switching\nInstruction  Tuning  (MosIT).MosITnot  only  supports  complex  cross-modal  understanding  and\nreasoning but also enables sophisticated multimodal content generation. In conjunction withMosIT,\nwe manually and meticulously construct a high-quality dataset. TheMosITdata encompasses a wide\nrange of multimodal inputs and outputs, offering the necessary complexity and variability to facilitate\nthe training of MM-LLMs that can handle diverse user interactions and deliver desired responses\naccurately. Specifically, we design some template dialogue examples between a ‘Human’ role and a\n‘Machine’ role, based on which we prompt the GPT-4 to generate more conversations under various\nscenarios with more than 100 topics or keywords.  The interactions are required to be diversified,\ne.g., including both straightforward and implicit requirements by the ‘Human’, and execution of\nperception, reasoning, suggestion, planning, etc., by the ‘Machine’.  And the interactive content\nshould be logically connected and semantically inherent and complex, with in-depth reasoning details\nin each response by the ‘Machine’. Each conversation should include 3-7 turns (i.e., QA pairs), where\nthe ‘Human’-‘Machine’ interactions should involve multiple modalities at either the input or output\nside, and switch the modalities alternately. Whenever containing multimodal contents (e.g., image,\naudio, and video) in the conversations, we look for the best-matched contents from the external\nresources, including the retrieval systems, e.g., Youtube\n7\n, and even AIGC tools, e.g., Stable-XL [63],\n7\nhttps://www.youtube.com/\n7",
    "Dataset\nData Source\nIn\n→\nOut Modality\nApproach   Multi-turn Reason  #Img/Vid/Aud  #Dialog Turn.  #Instance\n▶\nExisting data\nMiniGPT-4 [109]\nCC [10], CC3M [71]\nT+I\n→\nT\nAuto\n✗\n134M/-/-\n1\n5K\nStableLLaVA [47]\nSD [68]\nT+I\n→\nT\nAuto+Manu.\n✗\n126K/-/-\n1\n126K\nLLaVA [104]\nCOCO [50]\nT+I\n→\nT\nAuto\n✓\n81K/-/-\n2.29\n150K\nSVIT [106]\nMS-COCO [50], VG [41]\nT+I\n→\nT\nAuto\n✓\n108K/-/-\n5\n3.2M\nLLaVAR [104]\nCOCO [50], CC3M [71], LAION [70]\nT+I\n→\nT\nLLaVA+Auto\n✓\n20K/-/-\n2.27\n174K\nVideoChat [44]\nWebVid [5]\nT+V\n→\nT\nAuto\n✓\n-/8K/-\n1.82\n11K\nVideo-ChatGPT [54]\nActivityNet [28]\nT+V\n→\nT\nInherit\n✗\n-/100K/-\n1\n100K\nVideo-LLaMA [103]\nMiniGPT-4, LLaVA, VideoChat\nT+I/V\n→\nT\nAuto\n✓\n81K/8K/-\n2.22\n171K\nInstructBLIP [15]\nMultiple\nT+I/V\n→\nT\nAuto\n✗\n-\n-\n∼\n1.6M\nMIMIC-IT [42]\nMultiple\nT+I/V\n→\nT\nAuto\n✗\n8.1M/502K/-\n1\n2.8M\nPandaGPT [77]\nMiniGPT-4, LLaVA\nT+I\n→\nT\nInherit\n✓\n81K/-/-\n2.29\n160K\nMGVLID [107]\nMultiple\nT+I+B\n→\nT\nAuto+Manu.\n✗\n108K/-/-\n-\n108K\nM\n3\nIT [45]\nMultiple\nT+I/V/B\n→\nT\nAuto+Manu.\n✗\n-/-/-\n1\n2.4M\nLAMM [97]\nMultiple\nT+I+PC\n→\nT\nAuto+Manu.\n✓\n91K/-/-\n3.27\n196k\nBuboGPT [108]\nClotho [20], VGGSS [11]\nT+A/(I+A)\n→\nT\nAuto\n✗\n5k/-/9K\n-\n9K\nmPLUG-DocOwl [96]\nMultiple\nT+I/Tab/Web\n→\nT\nInherit\n✗\n-\n-\n-\n▶\nIn this work\nT2M\nWebvid [5], CC3M [71], AudioCap [38]\nT\n→\nT+I/A/V\nAuto\n✗\n4.9K/4.9K/4.9K\n1\n14.7K\nMosIT\nYoutube, Google, Flickr, Midjourney, etc.\nT+I+A+V\n→\nT+I+A+V\nAuto+Manu.\n✓\n4K/4K/4K\n4.8\n5K\nTable 2: Summary and comparison of existing datasets for multimodal instruction tuning. T: text, I: image, V: video, A: audio, B: bounding box, PC: point cloud, Tab:\ntable, Web: web page.\n8",
    "MethodFID (↓)\nCogVideo [17]27.10\nGLIDE [58]12.24\nCoDi [78]11.26\nSD [68]11.21\nNExT-GPT11.28\nTable 3:   Text-to-image\ngeneration    results    on\nCOCO-caption data [50].\nMethodFD (↓)  IS (↑)\nDiffSound [95]47.684.01\nAudioLDM-S [51]29.486.90\nAudioLDM-L [51]23.318.13\nCoDi [78]22.90   8.77\nNExT-GPT23.588.35\nTable 4:   Text-to-audio genera-\ntion results on AudioCaps [38].\nMethodFID (↓)  CLIPSIM (↑)\nCogVideo [30]23.590.2631\nMakeVideo [74]13.170.3049\nLatent-VDM [68]14.250.2756\nLatent-Shift [2]15.230.2773\nCoDi [78]—0.2890\nNExT-GPT13.040.3085\nTable 5:  Text-to-video generation re-\nsults (zero-shot) on MSR-VTT [92].\nMethodB@4  METEOR  CIDEr\nOscar [46]36.5830.4124.12\nBLIP-2 [43]43.7—145.8\nOFA [86]44.932.5154.9\nCoDi [78]40.231.0149.9\nNExT-GPT44.332.9156.7\nTable  6:Image-to-text  genera-\ntion  (image  captioning)  results  on\nCOCO-caption data [50].\nMethodSPIDEr  CIDEr\nAudioCaps [38]0.3690.593\nBART [26]0.4650.753\nAL-MixGen [39]0.4660.755\nCoDi [78]0.4800.789\nNExT-GPT0.521   0.802\nTable 7:  Audio-to-text genera-\ntion (audio captioning) results\non AudioCaps [38].\nMethodB@4  METEOR\nORG-TRL [105]    43.628.8\nGIT [85]54.833.1\nmPLUG-2 [91]57.834.9\nCoDi [78]52.132.5\nNExT-GPT58.438.5\nTable 8:  Video-to-text genera-\ntion (video captioning) results\non MSR-VTT [92].\nMethod\nObjectBackground\nCLIP (↑)FID (↓)CLIP (↑)FID (↓)\nPTP [29]30.339.5831.5513.92\nBLDM [4]29.956.1430.3820.44\nDiffEdit [14]29.303.7826.921.74\nPFB-Diff [36]30.815.9332.2513.77\nNExT-GPT29.316.5227.2915.20\nTable 9:   Text+image-to-image genera-\ntion (text-conditioned image editing) re-\nsults on COCO data [50].\nMethodMCD (↓)\nCampNet [87]0.380\nMakeAudio [33]0.375\nAudioLDM-L [51]    0.349\nNExT-GPT0.302\nTable  10:Text+audio-\nto-audio generation (text-\nconditioned speech edit-\ning)  results  on  VCTK\ndata [83].\nMethodCLIP-T (↑) CLIP-I (↑)\nCogVideo [30]0.23910.9064\nTuneVideo [89]0.27580.9240\nSDEdit [55]0.27750.8731\nPix2Video [9]0.28910.9767\nNExT-GPT0.26830.9645\nTable 11:  Text+video-to-video\ngeneration(text-conditioned\nvideo editing) results on DAVIS\ndata [62].\nMidjourney\n8\n. After human inspections and filtering of inappropriate instances, we obtain a total of\n5K dialogues in high quality. In Table 2 we compare the existing multimodal IT datasets with our\nMosITdata.\n6  Experiments\n6.1  Any-to-any Multimodal Generation\nWe try to quantify the generation quality of NExT-GPT on certain benchmark datasets under some\ncommon settings, such as text-to-X generation, X-to-text generation, and Text-conditioned modality\nediting. We mimic the task by taking only one turn of interaction between the user and the model.\n•‘Text’ — ‘X’ Generationrepresents the most frequent tasks of text-conditioned modal synthesis.\nTable 3, 4 and 5 present the comparisons between ours and some state-of-the-art systems. Overall\nNExT-GPT shows nice performance on par with the values from the best-performing baselines.\n•‘X’ — ‘Text’ Generationrepresents the tasks of modal captioning.  Table 6, 7 and 8 show\nthe results on different tasks.  Overall, we find that NExT-GPT can mostly achieve much better\nperformance on the X-to-text generation than the CoDi baseline, owing to the direct generation of\ntexts from LLM, which is inherently expertized by the LLM.\n•\n‘Text+X’ — ‘X’ Generationrepresents a task category of text-conditioned modal editing. Table\n9, 10 and 11 show the performances on different tasks. Compared with the above two types of tasks,\nNExT-GPT could be not that superior for the text-conditioned modal editing tasks. Yet, it still shows\ncompetitive performance.\n8\nhttps://www.midjourney.com/\n9",
    "T    I+A\nT    T+V\nT+I    VT+I    A\nT+V    V\nT+V    I+A\nT+A    I\nT+A    I+A\nT+A    V\nT+A+I    V\nT+A+I    I\nT+A+I+V\nI\n0\n2\n4\n6\n8\n10\nPerformance\nFigure 5: Comparative performance of NExT-GPT on various complex cross-modal conversions.\n•\nHuman Evaluation on Complex Any-to-any QAWe also carry out evaluation on some more\nscenarios where there are complicated cross-modal interactions between inputs and outputs.  We\nmainly compare the model performance for the settings with different modality conversions. As no\nstandard benchmark can be leveraged, here we adopt human evaluation. We ask several evaluators to\nscore the performance of NExT-GPT on a scale from 1 to 10. Figure 5 shows the comparisons. We\nfind NExT-GPT is more competent in producing images, compared with the generations on videos\nand audio.  Also generating mixed combinations of multimodal content is slightly inferior to the\ngeneration of single-modal content, due to the complexity of the latter.\n6.2  Example Demonstrations\nTo demonstrate the effectiveness and potential of our proposed NExT-GPT in developing human-like\nconversational agents, here we further offer compelling examples that vividly illustrate the system’s\nexceptional capacity to comprehend and reason contents across various modalities in any combination.\nFigure 6, 7, 8, 9, 10 and 11 show the examples from NExT-GPT. Go to the project page for more\nexamples and access the dynamic video and audio contents.\n7  Conclusion\nIn this work, we present an end-to-end general-purpose any-to-any multimodal Large Language Model\n(MM-LLM). By connecting an LLM with multimodal adaptors and different diffusion decoders,\nNExT-GPT is capable of perceiving inputs and generating outputs in any combination of text, images,\nvideos, and audio. Harnessing the existing well-trained highly-performing encoders and decoders,\ntraining NExT-GPT only entails a few number of parameters (1%) of certain projection layers,\nwhich not only benefits low costs but also facilitates convenient expansion to future more potential\nmodalities. To enable our NExT-GPT with complex cross-modal semantic understanding and content\ngeneration, we introduce a modality-switching instruction tuning (MosIT), and manually curated a\nhigh-quality dataset for MosIT. Overall, our research showcases the potential of any-to-any MM-\nLLMs in bridging the gap between various modalities and paving the way for more human-like AI\nsystems in the future.\nLimitation and Future workAs future work, there are at least following four avenues to explore.\ni) Modalities & Tasks Expansion: Due to resource limitations, currently, our system supports input\nand output in four modalities: language, images, videos, and audio. Next, we plan to extend this to\naccommodate even more modalities (e.g., web page, 3D vision, heat map, tables&figures) and tasks\n(e.g., object detection, segmentation, grounding and tracking), broadening the system’s applicability\nsuch that it becomes more universal.\nii) LLM Variants: Currently, we have implemented the 7B Vicuna version of the LLM. Our next\nplans involve incorporating various LLM types and sizes, allowing practitioners to choose the most\nsuitable one for their specific requirements.\n10",
    "iii) Multimodal Generation Strategies:  While our system excels in generating content across\nmodalities, the quality of generative outputs can sometimes be limited by the capabilities of the\ndiffusion model.  It is very promising to explore the integration of retrieval-based approaches to\ncomplement the generative process, potentially improving the overall system’s performance.\niv) MosIT Dataset Expansion:  Currently, our IT dataset has room for expansion.  We intend to\nsignificantly increase the amount of annotated data, ensuring a more comprehensive and diverse set\nof instructions to further enhance the MM-LLMs’ ability to understand and follow user prompts\neffectively.\nFigure 6: Example of Text+Image→Text+Audio.\n11",
    "Figure 7: Example of Text→Text+Image+Video+Audio.\n12",
    "Figure 8: Example of Text+Image→Text+Image+Video+Audio.\n13",
    "Figure 9: Example of Text+Video→Text+Image.\n14",
    "Figure 10: Example of Text+Audio→Text+Image+Video.\n15",
    "Figure 11: Example of Text+Video→Text+Audio.\n16",
    "References\n[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud,\nAndy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karén Simonyan. Flamingo: a visual language model for few-shot learning. In\nProceedings of the NeurIPS, 2022.\n[2]Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift:\nLatent diffusion with temporal shift for efficient text-to-video generation.CoRR, abs/2304.08477, 2023.\n[3]Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\nZhang.  Bottom-up and top-down attention for image captioning and visual question answering.  In\nProceedings of the CVPR, pages 6077–6086, 2018.\n[4]Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion.ACM Trans. Graph., 42(4):\n149:1–149:11, 2023.\n[5]Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. InProceedings of the ICCV, pages 1708–1718, 2021.\n[6]Mohammad Bashiri, Edgar Y. Walker, Konstantin-Klemens Lurz, Akshay Jagadish, Taliah Muhammad,\nZhiwei Ding, Zhuokun Ding, Andreas S. Tolias, and Fabian H. Sinz. A flow-based latent state generative\nmodel of neural population responses to natural images. InProceedings of the NeurIPS, pages 15801–\n15815, 2021.\n[7]Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural\nimage synthesis. InProceedings of the ICLR, 2019.\n[8]\nCerspense. Zeroscope: Diffusion-based text-to-video synthesis. 2023. URLhttps://huggingface.\nco/cerspense.\n[9]\nDuygu Ceylan, Chun-Hao Paul Huang, and Niloy J. Mitra.   Pix2video:  Video editing using image\ndiffusion.CoRR, abs/2303.12688, 2023.\n[10]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts.   InProceedings of the CVPR, pages\n3558–3568, 2021.\n[11]Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman.\nLocalizing visual sounds the hard way. InProceedings of the CVPR, pages 16867–16876, 2021.\n[12]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90 2023.\n[13]\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y.\nZhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models,\n2022.\n[14]Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord.  Diffedit: Diffusion-based\nsemantic image editing with mask guidance. InProceedings of the ICLR, 2023.\n[15]Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models\nwith instruction tuning.CoRR, abs/2305.06500, 2023.\n[16]Roberto Dessì, Michele Bevilacqua, Eleonora Gualdoni, Nathanaël Carraz Rakotonirina, Francesca Fran-\nzon, and Marco Baroni. Cross-domain image captioning with discriminative finetuning. InProceedings\nof the CVPR, pages 6935–6944, 2023.\n[17]Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. In\nProceedings of the NeurIPS, pages 19822–19835, 2021.\n17",
    "[18]Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos G. Derpanis,\nand Björn Ommer. Stochastic image-to-video synthesis using cinns. InProceedings of the CVPR, pages\n3742–3753, 2021.\n[19]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby.  An image is worth 16x16 words:  Transformers for image recognition at scale.  In\nProceedings of the ICLR, 2021.\n[20]Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In\nProceedings of the ICASSP, pages 736–740, 2020.\n[21]Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang.\nFrido: Feature pyramid diffusion for complex scene image synthesis.CoRR, abs/2208.13753, 2022.\n[22]Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu,\nXin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional\ntext-to-image synthesis.CoRR, abs/2212.05032, 2022.\n[23]\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.\nCoRR, abs/2208.01618, 2022.\n[24]\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi\nParikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. InProceedings\nof the ECCV, pages 102–118, 2022.\n[25]Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. Imagebind: One embedding space to bind them all.CoRR, abs/2305.05665, 2023.\n[26]\nFélix Gontier, Romain Serizel, and Christophe Cerisara.  Automated audio captioning by fine-tuning\nBART with audioset tags. InProceedings of the DCASE, pages 170–174, 2021.\n[27]Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo, and Longyin Wen. Text with knowledge\ngraph augmented transformer for video captioning. InProceedings of the CVPR, pages 18941–18951,\n2023.\n[28]Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.  Activitynet:  A\nlarge-scale video benchmark for human activity understanding.  InProceedings of the CVPR, pages\n961–970, 2015.\n[29]Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-\nprompt image editing with cross-attention control. InProceedings of the ICLR, 2023.\n[30]Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining\nfor text-to-video generation via transformers.CoRR, abs/2205.15868, 2022.\n[31]Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and\nmultinomial diffusion: Towards non-autoregressive language models.CoRR, 2021.\n[32]  Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. InProceedings of the ICLR, 2022.\n[33]Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu,\nXiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion\nmodels. InProceedings of the ICML, pages 13916–13932, 2023.\n[34]Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Under-\nstanding and generating speech, music, sound, and talking head.CoRR, abs/2304.12995, 2023.\n[35]Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\nVishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei.  Language is not all you need: Aligning\nperception with language models.CoRR, abs/2302.14045, 2023.\n[36]\nWenjing Huang, Shikui Tu, and Lei Xu. Pfb-diff: Progressive feature blending diffusion for text-driven\nimage editing.CoRR, abs/2306.16894, 2023.\n18",
    "[37]Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose:\nFashion image-to-video synthesis via stable diffusion.CoRR, abs/2304.06025, 2023.\n[38]Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.   Audiocaps:  Generating\ncaptions for audios in the wild. InProceedings of the NAACL, pages 119–132, 2019.\n[39]Eungbeom Kim, Jinhee Kim, Yoori Oh, Kyungsu Kim, Minju Park, Jaeheon Sim, Jinwoo Lee, and Kyogu\nLee.  Improving audio-language learning with mixgen and multi-level test-time augmentation.CoRR,\nabs/2210.17143, 2022.\n[40]Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.  Generating images with multimodal language\nmodels.CoRR, abs/2305.17216, 2023.\n[41]Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome:\nConnecting language and vision using crowdsourced dense image annotations.International Journal of\nComputer Vision, 123(1):32–73, 2017.\n[42]Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei\nLiu. MIMIC-IT: multi-modal in-context instruction tuning.CoRR, abs/2306.05425, 2023.\n[43]Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large language models. InProceedings of the ICML, pages\n19730–19742, 2023.\n[44]Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding.CoRR, abs/2305.06355, 2023.\n[45]Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu.  M\n3\nit:  A large-scale dataset towards multi-modal\nmultilingual instruction tuning.CoRR, abs/2306.04387, 2023.\n[46]Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. InProceedings of the ECCV, pages 121–137, 2020.\n[47]Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and\nYunchao Wei.  Stablellava:  Enhanced visual instruction tuning with synthesized image-dialogue data.\nCoRR, abs/2308.10253, 2023.\n[48]Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. Invariant grounding for video question\nanswering. InProceedings of the CVPR, pages 2918–2927, 2022.\n[49]Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan\nWang. Swinbert: End-to-end transformers with sparse attention for video captioning. InProceedings of\nthe CVPR, pages 17928–17937, 2022.\n[50]Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, Tomás Pajdla,\nBernt Schiele, and Tinne Tuytelaars, editors,Proceedings of the ECCV, pages 740–755, 2014.\n[51]Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and Mark D.\nPlumbley. Audioldm: Text-to-audio generation with latent diffusion models. InProceedings of the ICML,\npages 21450–21474, 2023.\n[52]\nHaotian  Liu,  Chunyuan  Li,  Qingyang  Wu,  and  Yong  Jae  Lee.   Visual  instruction  tuning.CoRR,\nabs/2304.08485, 2023.\n[53]Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image generation\nvia self-conditioned gans. InProceedings of the CVPR, pages 14274–14283, 2020.\n[54]\nMuhammad Maaz, Hanoona Abdul Rasheed, Salman H. Khan, and Fahad Shahbaz Khan. Video-chatgpt:\nTowards detailed video understanding via large vision and language models.CoRR, abs/2306.05424,\n2023.\n[55]\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations. InProceedings of the\nICLR, 2022.\n19",
    "[56]Victor Siemen Janusz Milewski, Marie-Francine Moens, and Iacer Calixto. Are scene graphs good enough\nto improve image captioning?  InProceedings of the AACL, pages 504–515, 2020.\n[57]Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.arXiv\npreprint arXiv:2302.08453, 2023.\n[58]Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing\nwith text-guided diffusion models. InProceedings of the ICML, pages 16784–16804, 2022.\n[59]  OpenAI. Introducing chatgpt. 2022.\n[60]  OpenAI. Gpt-4 technical report. 2022.\n[61]\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\nTraining language models to follow instructions with human feedback. InProceedings of the NeurIPS,\n2022.\n[62]Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus H. Gross, and Alexander\nSorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In\nProceedings of the CVPR, pages 724–732, 2016.\n[63]Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\nand Robin Rombach.  SDXL: improving latent diffusion models for high-resolution image synthesis.\nCoRR, abs/2307.01952, 2023.\n[64]Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-Seng Chua. Layoutllm-t2i: Eliciting layout\nguidance from LLM for text-to-image generation.CoRR, abs/2308.05095, 2023.\n[65]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision.   InProceedings of the ICML, pages\n8748–8763, 2021.\n[66]Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. InProceedings of the ICML, pages 8821–8831, 2021.\n[67]Ali Razavi,  Aäron van den Oord,  and Oriol Vinyals.   Generating diverse high-fidelity images with\nVQ-VAE-2. InProceedings of the NeurIPS, pages 14837–14847, 2019.\n[68]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. InProceedings of the CVPR, pages 10674–10685, 2022.\n[69]Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation.CoRR, abs/2208.12242,\n2022.\n[70]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes,  Aarush Katta,  Clayton Mullis,  Mitchell Wortsman,  Patrick Schramowski,  Srivatsa\nKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an\nopen large-scale dataset for training next generation image-text models. InProceedings of the NeurIPS,\n2022.\n[71]Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.  Conceptual captions:  A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. InProceedings of the ACL, pages\n2556–2565, 2018.\n[72]Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving AI tasks with chatgpt and its friends in huggingface.CoRR, abs/2303.17580, 2023.\n[73]Hisaichi Shibata, Shouhei Hanaoka, Yang Cao, Masatoshi Yoshikawa, Tomomi Takenaga, Yukihiro\nNomura, Naoto Hayashi, and Osamu Abe. Local differential privacy image generation using flow-based\ndeep generative models.CoRR, abs/2212.10688, 2022.\n[74]Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video\ngeneration without text-video data.CoRR, abs/2209.14792, 2022.\n20",
    "[75]Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. InProceedings of\nthe NeurIPS, 2020.\n[76]Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel\nCollier. Language models can see: Plugging visual controls in text generation.CoRR, abs/2205.02655,\n2022.\n[77]Yixuan Su,  Tian Lan,  Huayang Li,  Jialu Xu,  Yan Wang,  and Deng Cai.   Pandagpt:  One model to\ninstruction-follow them all.CoRR, abs/2305.16355, 2023.\n[78]Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via\ncomposable diffusion.CoRR, abs/2305.11846, 2023.\n[79]Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto.   Stanford alpaca:  An instruction-following llama model.   2023.   URL\nhttps://github.com/tatsu-lab/stanford_alpaca.\n[80]Hugo  Touvron,  Thibaut  Lavril,  Gautier  Izacard,  Xavier  Martinet,  Marie-Anne  Lachaux,  Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.CoRR,\nabs/2302.13971, 2023.\n[81]\nArash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. InProceedings of the\nNeurIPS, 2020.\n[82]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. InProceedings of the NeurIPS, pages 5998–6008,\n2017.\n[83]\nChristophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Cstr vctk corpus: English multi-speaker\ncorpus for cstr voice cloning toolkit.CSTR, 6:15, 2017.\n[84]  Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. P+: extended textual conditioning\nin text-to-image generation.CoRR, abs/2303.09522, 2023.\n[85]Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. GIT: A generative image-to-text transformer for vision and language.Trans. Mach. Learn.\nRes., 2022, 2022.\n[86]\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-\nto-sequence learning framework. InProceedings of the ICML, volume 162, 2022.\n[87]Tao Wang, Jiangyan Yi, Ruibo Fu, Jianhua Tao, and Zhengqi Wen.  Campnet:  Context-aware mask\nprediction for end-to-end text-based speech editing.IEEE ACM Trans. Audio Speech Lang. Process., 30:\n2241–2254, 2022.\n[88]\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.  Visual\nchatgpt: Talking, drawing and editing with visual foundation models.CoRR, abs/2303.04671, 2023.\n[89]\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video\ngeneration.CoRR, abs/2212.11565, 2022.\n[90]Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua.  Video as conditional\ngraph hierarchy for multi-granular question answering. InProceedings of the AAAI, pages 2804–2812,\n2022.\n[91]Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian,\nWei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jingren Zhou. mplug-2: A modularized\nmulti-modal foundation model across text, image and video. InProceedings of the ICML, pages 38728–\n38748, 2023.\n[92]\nJun Xu, Tao Mei, Ting Yao, and Yong Rui.  MSR-VTT: A large video description dataset for bridging\nvideo and language. InProceedings of the CVPR, pages 5288–5296, 2016.\n[93]Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\nAttngan:  Fine-grained text to image generation with attentional generative adversarial networks.  In\nProceedings of the CVPR, pages 1316–1324, 2018.\n21",
    "[94]Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.  Just ask:  Learning to\nanswer questions from millions of narrated videos. InProceedings of the ICCV, pages 1666–1677, 2021.\n[95]Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound:\nDiscrete diffusion model for text-to-sound generation.IEEE ACM Trans. Audio Speech Lang. Process.,\n31:1720–1733, 2023.\n[96]Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu,\nChenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal\nlarge language model for document understanding.CoRR, abs/2307.02499, 2023.\n[97]Zhenfei Yin,  Jiong Wang,  Jianjian Cao,  Zhelun Shi,  Dingning Liu,  Mukai Li,  Lu Sheng,  Lei Bai,\nXiaoshui Huang, Zhiyong Wang, Jing Shao, and Wanli Ouyang. LAMM: language-assisted multi-modal\ninstruction-tuning dataset, framework, and benchmark.CoRR, abs/2306.06687, 2023.\n[98]Bowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, and Yongbin Li.  Unified language representation for\nquestion answering over text, tables, and images.CoRR, abs/2306.16762, 2023.\n[99]Zequn Zeng,  Hao Zhang,  Ruiying Lu,  Dongsheng Wang,  Bo Chen,  and Zhengjue Wang.   Conzic:\nControllable zero-shot image captioning by sampling-based polishing.  InProceedings of the CVPR,\npages 23465–23476, 2023.\n[100]\nAo Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual prompt\ngenerator across llms.CoRR, abs/2305.01278, 2023.\n[101]Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining\nGuo. Styleswin: Transformer-based GAN for high-resolution image generation. InProceedings of the\nCVPR, pages 11294–11304, 2022.\n[102]\nDong  Zhang,  Shimin  Li,  Xin  Zhang,  Jun  Zhan,  Pengyu  Wang,  Yaqian  Zhou,  and  Xipeng  Qiu.\nSpeechgpt:  Empowering  large  language  models  with  intrinsic  cross-modal  conversational  abilities.\nCoRR, abs/2305.11000, 2023.\n[103]\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\nfor video understanding.CoRR, abs/2306.02858, 2023.\n[104]Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar:\nEnhanced visual instruction tuning for text-rich image understanding.CoRR, abs/2306.17107, 2023.\n[105]  Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun Zha. Object\nrelational graph with teacher-recommended learning for video captioning. InProceedings of the CVPR,\npages 13275–13285, 2020.\n[106]\nBo Zhao, Boya Wu, and Tiejun Huang. SVIT: scaling up visual instruction tuning.CoRR, abs/2307.04087,\n2023.\n[107]Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng,\nRunpei Dong, Chunrui Han, and Xiangyu Zhang. Chatspot: Bootstrapping multimodal llms via precise\nreferring instruction tuning.CoRR, abs/2307.09474, 2023.\n[108]\nYang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling\nvisual grounding in multi-modal llms.CoRR, abs/2307.08581, 2023.\n[109]Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.   Minigpt-4:  Enhancing\nvision-language understanding with advanced large language models.CoRR, abs/2304.10592, 2023.\n[110]\nMinfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: dynamic memory generative adversarial\nnetworks for text-to-image synthesis. InProceedings of the CVPR, pages 5802–5810, 2019.\n22"
  ]
}