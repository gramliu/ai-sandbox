{
  "key": "JPCB79EP",
  "url": "http://arxiv.org/pdf/2311.08105",
  "metadata": {
    "title": "DiLoCo: Distributed Low-Communication Training of Language Models",
    "abstract": "  Large language models (LLM) have become a critical component in many\napplications of machine learning. However, standard approaches to training LLM\nrequire a large number of tightly interconnected accelerators, with devices\nexchanging gradients and other intermediate states at each optimization step.\nWhile it is difficult to build and maintain a single computing cluster hosting\nmany accelerators, it might be easier to find several computing clusters each\nhosting a smaller number of devices. In this work, we propose a distributed\noptimization algorithm, Distributed Low-Communication (DiLoCo), that enables\ntraining of language models on islands of devices that are poorly connected.\nThe approach is a variant of federated averaging, where the number of inner\nsteps is large, the inner optimizer is AdamW, and the outer optimizer is\nNesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8\nworkers performs as well as fully synchronous optimization while communicating\n500 times less. DiLoCo exhibits great robustness to the data distribution of\neach worker. It is also robust to resources becoming unavailable over time, and\nvice versa, it can seamlessly leverage resources that become available during\ntraining.\n",
    "published": "2023-11-14T12:05:45Z"
  },
  "text": [
    "DiLoCo: Distributed Low-Communication\nTraining of Language Models\nArthur Douillard\n1\n, Qixuan Feng\n1\n, Andrei A. Rusu\n1\n, Rachita Chhaparia\n1\n, Yani Donchev\n1\n, Adhiguna Kuncoro\n1\n,\nMarc’Aurelio Ranzato\n1\n, Arthur Szlam\n1\nand Jiajun Shen\n1\n1\nGoogle DeepMind\nLarge language models (LLM) have become a critical component in many applications of machine\nlearning. However, standard approaches to training LLM require a large number of tightly interconnected\naccelerators, with devices exchanging gradients and other intermediate states at each optimization\nstep. While it is difficult to build and maintain a single computing cluster hosting many accelerators, it\nmight be easier to find several computing clusters each hosting a smaller number of devices. In this\nwork, we propose a distributed optimization algorithm, Distributed Low-Communication (DiLoCo), that\nenables training of language models on islands of devices that are poorly connected. The approach\nis a variant of federated averaging, where the number of inner steps is large, the inner optimizer is\nAdamW, and the outer optimizer is Nesterov momentum. On the widely used C4 dataset, we show that\nDiLoCo on 8 workers performs as well as fully synchronous optimization while communicating 500\ntimes less. DiLoCo exhibits great robustness to the data distribution of each worker. It is also robust\nto resources becoming unavailable over time, and vice versa, it can seamlessly leverage resources that\nbecome available during training.\nKeywords: large-scale, language modeling, distributed learning\n1. Introduction\nLanguage models have shown remarkable ability\nto generalize to new tasks, and are at the heart\nof a multitude of new applications of machine\nlearning. Because performance has scaled with\nmodel size, practitioners train increasingly larger\nmodels on increasingly large data. Nevertheless,\nat a high level, the basic training approach re-\nmains standard mini-batch back-propagation of\nthe error.\nAt modern scale, training via standard back-\npropagation poses unprecedented engineering\nand infrastructure challenges. To start, several\nthousands of devices need to be powered and be\nplaced at the same physical location; and inter-\nconnected with high-bandwidth cables to mini-\nmize latency. Careful software engineering is re-\nquired to orchestrate the passage of gradients, pa-\nrameters and intermediate states between these\ndevices at each optimization step, keeping all\ndevices fully utilized. Furthermore, the more de-\nvices that are used for each synchronous train-\ning step, the more chances there are that one of\nthem fails, risking halting training, or introduc-\ning subtle numerical issues. Moreover, the cur-\nrent paradigm poorly leverages heterogeneous de-\nvices, that might have different speed and topol-\nogy. In the simplest terms, it is difficult to co-\nlocate and tightly synchronize a large number of\naccelerators.\nIn this work, we take inspiration from litera-\nture on Federated Learning, to address the above\nmentioned difficulties. In Federated Learning,\nthere are푘workers, each operating on their own\nisland of devices, each consuming a certain par-\ntition of the data, and each updating a model\nreplica. Such workers perform some amount of\nwork locally, and then exchange gradients every\n퐻steps to get their model replica back in sync.\nWe propose a variant of the popular Feder-\nated Averaging (FedAvg) algorithm (McMahan\net al., 2017), or a particular instantiation with\na momentum-based optimizer as in the FedOpt\nalgorithm (Reddi et al., 2021), whereby the num-\nber of inner steps is large, the inner optimizer is\nreplaced with AdamW, and the outer optimizer\nCorresponding author(s): douillard@google.com\n©2023 Google DeepMind. All rights reserved\narXiv:2311.08105v2  [cs.LG]  2 Dec 2023",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nwith Nesterov Momentum for best performance.\nThis combination enables us to address the short-\ncomings mentioned above, because a) while each\nworker requires co-located devices their number\nis roughly푘times smaller than the total, b) work-\ners need not to communicate at each and every\nsingle step but only every퐻steps which can be in\nthe order of hundreds or even thousands, and c)\nwhile devices within an island need to be homoge-\nneous, different islands can operate with different\ndevice types. We dub this approach Distributed\nLow-Communication (DiLoCo) training.\nIn a large-batch training setting with over-\ntraining, our empirical validation on the C4\ndataset (Raffel et al., 2020) demonstrates that\nDiLoCo can achieve even better performance\n(as measured in perplexity) than a fully syn-\nchronous model, while communicating 500 times\nless. DiLoCo is capable of effectively utilizing sev-\neral islands of devices at training time, despite a\nlow bandwidth connectivity among these islands.\nFinally, at inference time the resulting model has\nthe same size and speed as a model trained in\nfully synchronous mode.\nOur experiments show that DiLoCo is robust\nagainst different data distributions used by lo-\ncal workers and frequency of global parameter\nupdates. Finally, DiLoCo exhibits robustness to\nisland failure, and nicely leverage islands when-\never these become available.\n2. DiLoCo\nWe assume that we have a base model architec-\nture (e.g., a transformer) with parameters휃. We\ndenote a training dataset asD={(x,y), ...}with\nxandybeing respectively the input data and tar-\nget. In language modeling (Vaswani et al., 2017),\nthe input is a sequence of tokens and the target\nis the input sequence shifted by one. When the\ndataset is split across multiple shards, we denote\nthe푖-th shard withD\n푖\n.\nDiLoCo training proceeds as outlined in Algo-\nrithm 1 (Reddi et al., 2021), and illustrated in\nFigure 1. First, we start from an initial model\nwith parameters휃\n(0)\n, which can be initialized at\nrandom or using a pretrained model (see subsec-\ntion 3.1). We also have푘workers each capable\nof training a model replica and푘shards of data,\none for each worker.\nThere are two optimization processes. There\nis anouteroptimization (line 1, 12, and 14 in\nAlgorithm 1), which consists of푇outer steps. At\neach outer step푡, gradients from each worker are\ngathered, averaged and sent to an outer optimizer\n(OuterOpt) to update the shared copy of the\nparameters. Afterwards, this shared copy of the\nparameters is re-dispatched to each local worker\n(line 3).\nWithin each phase, each worker (line 3) per-\nformsindependently and in parallelits own inner\noptimization (lines 4 to 9) for퐻steps using an\ninner optimizer, denoted byInnerOpt.  Each\nworker samples data from its own shard (line 5),\nand updates its own local copy of the parameters\n(line 8). Note that the inner optimization consists\nof퐻≫1steps; for instance, several hundred\nsteps. Therefore, communication across workers\nis minimal.\nOnce all workers have completed their inner\noptimization step, the cumulative gradients of\neach worker are averaged (line 12), and the re-\nsultingouter gradientis used to update the shared\ncopy of the parameters (line 14), which is then\nused as starting point for the next round of in-\nner optimizations. This is the only time when\ncommunication among workers is required, and\nit happens once every퐻inner optimization steps.\nIn total, a worker trains for푁=푇×퐻inner steps.\nIn our work,  we use asinneroptimizer\n(InnerOpt) AdamW (Kingma and Ba, 2014;\nLoshchilov and Hutter, 2019), which is the most\nwidely used optimizer for transformer language\nmodels. As for theouteroptimizer (OuterOpt)\nwe use Nesterov momentum (Sutskever et al.,\n2013) because it gave the best convergence em-\npirically (see Figure 6). WhenOuterOptis SGD,\nthen the outer optimizer is equivalent to classical\nFederated Averaging (McMahan et al., 2017). If\nthe total number of outer optimization steps푇is\nfurther set to 1, then DiLoCo reduces to “soup-\ning” (Wortsman et al., 2021). Finally, if the num-\nber of inner optimization steps퐻is set to 1 and\nInnerOptis SGD, DiLoCo is equivalent to large-\n2",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nConfidential — Google DeepMind\n5\nt = 1\nReplicas Training \nfor H inner steps\nOuter Optimization\nPretrained Model\nt = 2\nt = 3\nReplicas Training \nfor H inner steps\nOuter Optimization\n...\n TPUv4є\n V100σ\n  TPUv5Ϧ\n  A100υ\n TPUv4є\n V100Ϣ\n  TPUv5Ϧ\n  V100υ\nFigure 1|DiLoCo: First, a pretrained model휃\n(0)\nis replicated푘times (in this illustration푘=4) and\neach worker휃\n(1)\n푖\ntrains a model replica on its own shard of data for퐻steps independently and in\nparallel. Afterwards, workers average their outer gradients and an outer optimizer updates the global\ncopy of the parameters휃\n(1)\n. This will then be re-dispatched to the workers. The process repeats푇\ntimes (in this illustration only the first two iterations are displayed). Each replica can be trained in\ndifferent locations of the world, with different accelerators.\nbatch training with data-parallelism.\nOverall, DiLoCo can be interpreted as a data\nparallelism method that requires very little com-\nmunication, and therefore, it can scale to workers\nthat are poorly connected, e.g., workers placed in\nvery distant geographic regions. Workers could of\ncourse use standard data and model parallelism\nfor their inner optimization.\nAlgorithm 1DiLoCo Algorithm\nRequire:Initial model휃\n(0)\nRequire:푘workers\nRequire:Data shards{D\n1\n, . . . ,D\n푘\n}\nRequire:OptimizersInnerOptandOuterOpt\n1:forouter step푡=1. . .푇do\n2:forworker푖=1. . . 푘do\n3:휃\n(푡)\n푖\n←휃\n(푡−1)\n4:forinner stepℎ=1. . . 퐻do\n5:푥∼D\n푖\n6:L ←푓(푥, 휃\n(푡)\n푖\n)\n7:⊲Inner optimization:\n8:휃\n(푡)\n푖\n←InnerOpt(휃\n(푡)\n푖\n,∇\nL\n)\n9:end for\n10:end for\n11:⊲Averaging outer gradients:\n12:Δ\n(푡)\n←\n1\n푘\nÍ\n푘\n푖=1\n(휃\n(푡−1)\n−휃\n(푡)\n푖\n)\n13:⊲Outer optimization:\n14:휃\n(푡)\n←OuterOpt(휃\n(푡−1)\n,Δ\n(푡)\n)\n15:end for\nHyperparameter60M  150M  400M\nNumber of layers31212\nHidden dim896    896    1536\nNumber of heads\n161612\nK/V size6464128\nVocab size32,000\nTable 1|Model Configurationfor the three eval-\nuated sizes. All are based on the transformer\narchitecture, chinchilla-style (Hoffmann et al.,\n2022).\n3. Experiments\nIn this section we report the main experiments\nvalidating DiLoCo. We consider a language mod-\neling task on the C4 dataset, a dataset derived\nfrom Common Crawl (Raffel et al., 2020). We\nreport perplexity on the validation set against\nnumber of steps used at training time, which is\na good proxy for wall clock time since communi-\ncation across workers is rather infrequent. The\ntotal number of steps is set to88,000. We consider\nthree model sizes, all decoder-only transformers\nadapted from the Chinchilla architecture (Hoff-\nmann et al., 2022). Their respective configuration\nis described in Table 1. We perform experiments\nboth in the i.i.d. and non-i.i.d. settings, meaning\nwhen the data distribution of the shardsD\n푖\nis the\n3",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nFigure 2|Main result: After pretraining a 150M baseline for24,000training steps on C4, we compare\nnetworks finetuned for an additional64,000steps (teal using the same batch size, and purple using8\ntimes bigger batch size), and a transformer model trained from scratch (red). DiLoCo(blue) using8\nworkers yields lower perplexity, even compared to the baseline using8times bigger batch size, while\nbeing8times faster in wall-clock time and communicating500times less.\nModelCommunication  Time  Compute & Data  Perplexity\nBaseline01×1×16.23\nBaseline,8×batch size with data parallelism\n8×푁1×8×15.30\nBaseline,8×batch size with microbatching\n08×8×15.30\nBaseline,8×updates08×8×14.72\nDiLoCo\n8×\n푁\n/퐻1×8×15.02\nTable 2|Trade-offs of various training algorithms: We compare four baselinesvsDiLoCo across\ntheir communication cost, time spent, and compute & data used. For the same time and amount of\ncompute, we can compare the second baseline and DiLoCo. The former communicates gradients at\neach time step (푁total steps), while DiLoCo communicates퐻=500times less (and is amenable to\ndistributed training) while also reaching better generalization performance. Note that푇=\n푁\n/퐻(see\nAlgorithm 1).\nsame for all푖and when these are different like in\nheterogeneous federated learning. Since the lat-\nter is a more challenging use case, we use this set-\nting by default except when indicated otherwise.\nSimilarly, by default all training experiments start\nfrom a transformer language model pretrained\nfor24,000steps on the same training set, refer\nto subsection 3.1 for further details.\nIn our experiments we have searched over the\nhyper-parameters of the outer optimizer (e.g.\nlearning rate, momentum, etc.). We use a se-\nquence length of1,024tokens and a batch of size\n512but otherwise we left unchanged the inner\noptimization and model architecture. We list all\nthe hyper-parameters in the appendix (Table 5).\nIn Figure 2, we show the performance through\ntime of DiLoCo (in blue with푘=8replicas in the\nnon-i.i.d. data setting) when each worker per-\nforms푇=128times퐻=500inner steps (64,000\nsteps in total). In this experiment, DiLoCo starts\nfrom a model휃\n(0)\npretrained for24,000steps.\nThere are four baselines. The first baseline is\na model trained from scratch for88,000steps\n(in red), the second starts from a model pre-\ntrained for24,000steps and performs an addi-\ntional64,000steps (in teal). The third baseline\nstarts from the same pre-trained model, but dur-\n4",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nFigure 3|Impact of number of pretraining\nstepsin a non-i.i.d. setting. DiLoCo can be initial-\nized from a pretrained model휃\n(0)\n, or even from\nscratch with minimal (-0.1 PPL) degradation of\nmodel quality. The vertical dashed lines indicate\nthe transition between pretraining and DiLoCo\ntraining.\ning finetuning uses an8×bigger batch size (in\npurple). The fourth baseline is running the stan-\ndard batch size for8×the number of updates.\nWe compare in Table 2 all baselines with respect\nto the communication cost, time spent training,\nand the amount of compute & data used.  In-\ncreasing the batch size can be done in two man-\nners: with data parallelism (second row) at the\ncost of increased communication, or with micro-\nbatching (third row) at the cost of longer training\ntime. DiLoCo (last row) doesn’t increase train-\ning time, communicates퐻=500×less than the\nsecond baseline (and is thus amenable to dis-\ntributed training across compute islands), while\nalso reaching better generalization. Increasing\nby8×the number of updates improves perplex-\nity over our method, but at the cost of being8×\nslower.\n3.1. Ablations\nWe perform extensive ablations of DiLoCo to bet-\nter understand its capabilities and stress-test its\nlimits.\nNumber of Pretraining StepsFor all experi-\nments here we perform88,000training steps. A\nsubset of those steps are done during the pre-\ntraining stage, and the remainder with DiLoCo.\nFigure    4|Varying   the   commu-\nnication    frequencyevery퐻=\n{50,100,250,500,1000,2000}steps  in  a\nnon-i.i.d setting.\nIn Figure 3, we study the impact of the number of\npretraining steps on the final generalization per-\nformance in a non-i.i.d. data regime. Specifically,\nwe compare no pretraining (in teal), pretrain-\ning of 12k (in purple), 24k (in red), and 48k (in\norange) steps. We highlight the pretrain’s end-\ning and DiLoCo’s beginning with vertical dashed\nlines.  Note that as we keep the total amount\nof steps (wall-clock time) fixed, few or no pre-\ntraining steps will result in more compute spent\noverall.\nIn general, we observe that starting DiLoCo\nbefore 24k steps achieves a similar final PPL,\ndemonstrating the robustness of the approach.\nInterestingly, performance is not degraded even\nwhen starting from a randomly initialized net-\nwork. This result contradicts the findings of prior\nwork onpost local-SGD(Lin et al., 2020) and its\nlarge-scale study on a vision classification task\n(Ortiz et al., 2021).\nThe attentive reader may also note spikes in\nperplexity after the vertical dashed lines: a warm-\nup of the inner learning rate is the culprit. Despite\nthe transient spike, such warm up is ultimately\nbeneficial, as previously noted also in thecontin-\nual pretrainingsetting by Gupta et al. (2023).\nCommunication frequencyIn order to scale\nup distributed training across a set of poorly con-\nnected machines, the frequency of communica-\ntion needs to be reduced. Doing a single commu-\n5",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nnication at the training’s end (Wortsman et al.,\n2022a) is sub-optimal. Most works instead con-\nsider communicating every퐻≤20steps Ortiz\net al. (2021), which is too frequent for many dis-\ntirbuted learning applications.\nIn Figure 4, we vary the communication fre-\nquency for a 150M transformer, in the non-i.i.d.\ndata regime, from퐻=50steps (in teal) to\n퐻=2000steps (in green). In general, we observe\nthat communicating more frequently improves\ngeneralization performance. However, commu-\nnicating more frequently than퐻=500steps\nleads to diminishing returns. Moreover, the per-\nformance degradation is very mild up to퐻=1000\nsteps. For instance, when퐻=1000the perplex-\nity increases by only2.9%relative to퐻=50,\ndespite communicating20×less. Based on these\nconsiderations, for all remaining experiments we\nchoose퐻=500as this strikes a good trade-off\nbetween generalization performance and commu-\nnication cost.\ni.i.d. vs non-i.i.d. data regimesAccording\nto Gao et al. (2022), the distribution of the data\nacross replicas can have a significant impact on\ngeneralization. In this ablation study we assess\nthe effect that different data distributions have\non the convergence of DiLoCo.\nSimilarly Gururangan et al. (2023), we create\nthe non-i.i.d. setting by clustering with푘-Means\nthe entire training set using a pretrained model’s\nlast layer features. The i.i.d. setting is a ran-\ndom partitioning of the data. We showcase in\nFigure 5 the performance of DiLoCo with푘=8\nworkers/shards in a non-i.i.d. setting (in blue)\nand i.i.d setting (in red). Despite the latter con-\nverging faster early on in training, the final gener-\nalization performance of the two settings is com-\nparable. Intuitively, we would expect the non-i.i.d.\nsetting to yield worse performance because each\nworker might produce very different outer gradi-\nents, but DiLoCo exhibits very strong robustness.\nThe reason why this might be happening is further\ninvestigated in the appendix (subsection 6.2).\nNumber of replicasWe now investigate the im-\npact of the number of replicas/clusters in Table 3,\nFigure 5|i.i.d.vsnon-i.i.d.  data regimes:\nDiLoCo converges faster in the i.i.d. setting but\ntowards the end both data regimes attain simi-\nlar generalization, highlighting the robustness of\nDiLoCo.\nNumber of replicasi.i.dnon-i.i.d\n116.23\n415.23    15.18\n8\n15.08    15.02\n16\n15.02    14.91\n64\n14.95    14.96\nTable 3|Impact of the number of repli-\ncas/clusterson the evaluation perplexity for a\nfixed amount of inner steps per replica. With\nmore replicas, the model consumes more data\nand uses more compute overall, although this\nrequires very infrequent communication (once ev-\nery 500 inner steps).\nassuming there are as many workers as there\nare shards of data. The results in Table 3 show\nthat increasing the number of replicas improves\ngeneralization performance, but with diminish-\ning returns when there are more than 8 workers.\nThis finding applies to both i.i.d. and non-i.i.d.\nsettings. Unlike what is reported in prior work\nin the vision domain on ImageNet (Ortiz et al.,\n2021), we do not observe significant performance\ndegradation by increasing the number of replicas.\nModel sizeIn Table 4 we vary the model size.\nWe train models of size 60, 150 and 400 million\nparameters. We consider the usual setting where\ndata distribution is non i.i.d. and all workers start\nfrom a model (of the same size) pretrained for\n6",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nModel SizeRelative (%)  Absolute (PPL)\n60M4.33%1.01\n150M7.45%1.21\n400M7.49%1.01\nTable 4|Varying the model size: For each model\nsize, we report the improvement of DiLoCo over\nthe baseline (using a single worker). DiLoCo uses\n8 workers and non-i.i.d. shards.\n24,000steps. Hyper-parameters were tuned on\nthe 150M model, which may be sub-optimal for\nthe other model sizes. We observe a monotonic\nimprovement of performance as the model size\nincreases. We surmise that (1) in an overtrained\nsetting with large amount of steps, larger mod-\nels are more efficient at fitting the same amount\nof data, and (2) as the linear connectivity litera-\nture (Ilharco et al., 2022) suggests, larger models\nare less subject to interference when averaging\ntheir parameters.\nOuter OptimizersWe experimented with var-\nious outer optimizers (see L14 of Algorithm 1).\nFor each, we tuned their momentum if any, and\ntheir outer learning rate. We found that using\nas outer optimizer SGD (equivalent to FedAvg\n(McMahan et al., 2017)) or Adam (eq. to FedOpt\n(Reddi et al., 2021)) performed poorly, as shown\nin Figure 6. Adam was particularly unstable with\na high second order momemtum norm. We alle-\nviated the issue by increasing the휖factor to0.1.\nWe found Nesterov optimizer (Sutskever et al.,\n2013) (see FedMom in (Huo et al., 2020)) to per-\nform the best. In particular, the setting with outer\nlearning rate equal to0.7and outer momentum\nequal to0.9is very robust, and it is adopted for\nall our experiments throughout. We hypothesize\nthat the Nesterov’s gradient correction is partic-\nularly helpful with the outer gradient that span\nhundred of training steps.\nWe also considered decaying the outer learning\nrate with a cosine scheduling but it resulted in\nsimilar performance. Since we decay the inner\nlearning rate, the outer gradient norm gets natu-\nrally smaller over the course of training, removing\nthe need to further decay the outer learning rate.\nFigure 6|Outer Optimizers: Comparison of dif-\nferent outer optimizers.\nAdaptive compute poolThe total amount of\ncompute any given user has, is rarely constant\nover time. For instance, a preemptible machine,\nthat is regularly killed, is a cheaper alternative to\na dedicated server. Similarly, university’s comput-\ning clusters often usekarmasystems to balance\ncompute among all users, but this means that re-\nsources available to each user vary over time. Fi-\nnally, a collaborative system like Petals (Borzunov\net al., 2022) or Diskin et al. (2021) where individ-\nual users provide their own devices to the shared\ncompute pool is subject to extreme pool resizing\ndepending on how many people participate at\nany given time.\nIn this study, we then explore the performance\nof DiLoCo when the amount of compute varies\nthroughout training. In our case, the amount\nof compute is varied by changing the number\nof replicas used in an i.i.d.  setting.  In Fig-\nure 7, we show the validation perplexity through\ntime when using different schedules of compute\nallocation.Constant local(in green) and\nConstant Distributed(in blue) use a con-\nstant amount of replicas: respectively 1 (base-\nline) and 8 (standard DiLoCo setting).Doubling\nCompute(in teal) andHalving Compute(in\npurple) use respectively 4 and 8 replicas during\nthe first half of the training, and then 8 and 4.\nRamping Up(in red) andRamping Down(in or-\nange) ramps up (respectively ramps down) the\ncompute from 1 to 8 (resp. from 8 to 1).\nWe observe that the factor determining the\nultimate generalization ability of the model is\nthe total amount of compute given to DiLoCo,\n7",
    "DiLoCo: Distributed Low-Communication Training of Language Models\n(a) Number of replicas per training steps.(b) Perplexity across training steps.\nFigure 7|Adaptive compute: We vary the number of replicas (i.e., the amount of compute) across\ntime. Models generalize equally well for the same total amount of compute, regardless of how this is\nmade available over time.\nbut this is robust to how the budget is spread\nover time.  For instance,Doubling Compute\nandHalving Computeuse as much compute\nin total and achieve similar performance. Sim-\nilarly,Ramping UpandRamping Downobtain\nsimilar performance despite the different budget-\ning schedule, and their generalization is worse\nthan other baselines using more total compute.\nIn conclusion, models quality is affected by the\ntotal amount of compute, but not as much by how\nsuch computed is allocated over time.\nAsynchronous CommunicationIn DiLoCo all\nworkers communicate their outer-gradients after\n퐻inner optimization steps. In practice, it might\nhappen that a worker gets rebooted or that the\nnetwork might lose packets. In these cases, com-\nmunication is not feasible.\nIn Figure 8, we simulate such inability to com-\nmunicate by randomly dropping outer gradients\nwith probability equal to 0% (in teal), 10% (in\npurple), 30% (in red), to 50% (in orange). When\nan outer gradient is dropped, the local worker\ncontinues training for the following퐻steps start-\ning from its own parameters휃\n(푡)\n푖\n(as opposed to\nthe shared parameters휃\n(푡)\n).\nIn both i.i.d. and non-i.i.d. data settings, a\nhigher probability of being dropped results in\nmore unstable learning with transient spikes in\nperplexity. However, even in the extreme non-i.i.d\nsetting where each replica has 50% probability\nof dropping communication, the degradation of\nperplexity relative to perfect communication is\nonly2.1%. Consequently, with robustness to com-\nmunication failure, the need of a synchronization\nbarrier is less critical and thus training can be\naccelerated without having to wait all replicas.\nAccelerating a single workerRecently works\nhave shown that linear connectivity can alsoaccel-\neratethe convergence of a non-distributed model.\nFor instance, the Lookahead optimizer (Zhang\net al., 2019) proposes to take an outer step of\nouter SGD using a single replica, which is equiv-\nalent at interpolating between the starting and\nend points of a phase in the parameters space.\nFigure 9 shows that DiLoCo applied to asingle\nreplica/cluster (푘=1but퐻≫1) can improve\nboth convergence speed and final generalization\nperformance at null communication cost. Specifi-\ncally, every퐻=500inner steps, we compute the\nonly outer gradient as described in Algorithm 1,\nand then (locally) update the parameters using\nthe outer optimizer. This experiment further cor-\nroborates the robustness and wide applicability\nof DiLoCo.\n4. Related Work\nIn this section we review relevant work from the\nliterature, limiting the discussion to only few rep-\nresentative works given the large body of litera-\n8",
    "DiLoCo: Distributed Low-Communication Training of Language Models\n(a)i.i.d.data regime.(b)non-i.i.d.data regime.\nFigure 8|Asynchronous communication: we drop communication of outer gradients of each replica\nwith a certain probability. If a replica is dropped, it continues training without synchronizing its\nparameters.\nFigure 9|Accelerating a single worker: DiLoCo\napplied to a single replica푘=1provides both\nfaster and better generalization.\nture.\nWe cover the literature of distributed learning,\nspecifically local SDG and federated learning. We\nalso relate to recent works done on linear mode\nconnectivity which inspired much of our work.\n4.1. Local SGD and Federated Learning\nSeveral communities have proposed and studied\nlocal SGD. To the best of our knowledge, the first\ninstantation was in McMahan et al. (2017) who\nintroduced the concept of federated learning and\nlocal SGD as a way to enable learning on a net-\nwork of mobile devices which retain private access\nto their own data. In this work, the outer opti-\nmization consists of a mere parameter averaging\nstep. This was later extended to more powerful\nouter optimizers by Reddi et al. (2021); Wang\net al. (2020); this work inspired our use of Nes-\nterov momentum in the outer optimization. Lin\net al. (2020) considered local SGD as a way to\nimprove generalization when learning with large\nbatch sizes. Stich (2019) instead focused on local\nSGD because of its ability to limit communication\nin distributed learning, a perspective we share\nalso in our work. To the best of our knowledge,\nonly FedMom (Huo et al., 2020) considers Nes-\nterov as the outer optimizer as we did. While they\nalso tackle a language modeling task, the setting\nis much smaller (1-layer LSTM), with only 2 repli-\ncas, and rather frequent communication (every\n20 inner steps). In our work instead, we consider\na larger setting with up to a 400M transformer\nlanguage model, across up to 64 replicas, and up\nto100×less communication. Furthermore, we\nuse AdamW as inner optimizer while they used\nSGD.\nOrtiz et al. (2021) is one of the few works in fed-\nerated learning / local sgd body of literature that\nhas validated on a large-scale setting. They con-\nsider ImageNet (Deng et al., 2009) with Resnet50\nand Resnet101 (He et al., 2015), and found that\nlocal sgd struggles at scale. In particular, they\nreported that fewer inner steps (e.g.,퐻=8), no\npretraining, and a relatively large number of repli-\ncas (≥푘=16) degrade generalization. Thus the\nauthors conclude that \"local SGD encounters chal-\nlenges at scale.\". Instead, we show in section 3 that\nDiLoCo can robustly operate while communicat-\n9",
    "DiLoCo: Distributed Low-Communication Training of Language Models\ning125×less (퐻=1000), even without pretrain-\ning, and using up to4×more replicas (푘=64)\nboth in the i.i.d. and non-i.i.d. settings. Recently,\nmultiple works (Diskin et al., 2021; Presser, 2020;\nRyabinin et al., 2021) also applied Local SGD for\nlanguage models but without outer optimization.\n4.2. Linear Mode Connectivity\nThe field of linear mode connectivity studies how\nto linearly interpolate between several models in\nparameters space, to yield a single model with\nthe best capabilities of all models combined (Fran-\nkle et al., 2020; Wortsman et al., 2021). A sur-\nprising result from this field is the relative easi-\nness to find a linear interpolation between sev-\neral models where all intermediary points have\na low loss, avoiding anyloss barrier. Specifically,\nWortsman et al. (2022c) started from a pretrained\nmodel, finetuned different replicas on various\ntasks or choice of hyperparameters (Wortsman\net al., 2022b), and then averaged the resulting\nparameters. Originally proposed in the vision\ndomain, this method has then been used also in\nNLP (Li et al., 2022), RLHF (Ramé et al., 2023a),\nnoisy data (Rebuffi et al., 2022), and OOD (Ramé\net al., 2023b). Recently, several works studied\nother ways to alleviate loss barriers (Jin et al.,\n2023; Jordan et al., 2023; Stoica et al., 2023).\nWhile we didn’t apply any of these methods to\nDiLoCo, they are complementary and could be\nused in future works.\nThe majority of works on linear connectivity\nconsiders only averaging once all replicas have\nbeen fully finetuned, while we exploit the lin-\near mode connectivityduringtraining. There are\nhowever notable exceptions: BTM (Li et al., 2022)\nand PAPA (Jolicoeur-Martineau et al., 2023) are\nroughly equivalent to our framework but use\nas outer optimizerOuterOpt=SGD(lr=1.0).\nThe former communicates very little because each\nreplica is fully finetuned on a task before syn-\nchronization. The latter communicates every few\nsteps and with at most 10 replicas. Finally, Kad-\ndour (2022) only considers a few previous check-\npoints of the same model trained on a single task,\nand don’t re-use it for training. Git-theta (Kandpal\net al., 2023) argues that linear mode connectivity\ncan facilitate collaboration by merging models\ntrained by different teams (Diskin et al., 2021)\non various tasks; we show that DiLoCo is actually\ncapable to do soduringtraining, even when the\ndata of each worker is different.\n5. Limitations\nOur work has several limitations, which constitute\navenue for future work. First, we only considered\na single task, namely language modeling, and a\nsingle architecture, a transformer. Other datasets,\ndomains (e.g.vision), and other architectures\n(e.g., CNNs which are known to be more sensitive\nto linear mode connectivity (Jordan et al., 2023))\nshould also be considered.\nSecond, we have presented results at the scale\nof 60 to 400 million parameters. However, at\nthe time of writing state-of-the-art language mod-\nels use 3 orders of magnitude more parameters.\nTherefore, it would be interesting to see how\nDiLoCo works at larger scale. Our initial extrapo-\nlation indicate that DiLoCo might perform even\nbetter at larger scales, because there is less inter-\nference during the outer-gradient averaging step.\nHowever, this hypothesis should be validated em-\npirically.\nThird, the version of DiLoCo presented here\nassumes that all workers are homogeneous. How-\never, in practice workers might operate at wildly\ndifferent speed. In these cases, waiting for all\nworkers to perform the same number of steps is\nrather inefficient. Another avenue of future work\nis then to extend DiLoCo to the asynchronous set-\nting, whereby workers update the global parame-\nter without ever waiting for any other worker.\nFourth, DiLoCo exhibits diminishing returns\nbeyond 8 workers. Another avenue of future re-\nsearch is to improve the algorithm to better lever-\nage any additional compute that might be avail-\nable.\nFinally, DiLoCo attains fast convergence in\nterms of wall-clock time. However, the distributed\nnature of the computation reduces the FLOP and\ndata efficiency of the model, as shown by the\n8×updates row in Table 2.  At a high level,\nthis is because the outer updates have effectively\ntoo large a batch size; but naively reducing the\n10",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nouter-update batch size would result in the work-\ners being destabilized because their batch-size\nis too small. Therefore, another avenue of fu-\nture research is on balancing wall-clock time effi-\nciency with compute efficiency and data efficiency,\namong other quantities of interest. In particular,\nwe believeasynchronousvariants of local-sgd may\nallow distributed training with relatively more\ndata-efficient updates.\n6. Conclusion\nIn this work we study the problem of how to dis-\ntribute training of large-scale transformer lan-\nguage models when not all devices are co-located,\nand the network between the various machines\nmay have low bandwidth. To address this prob-\nlem, we propose DiLoCo, a variant of Feder-\nated Averaging whereby the outer optimizer is\nreplaced with Nesterov momentum, the inner\noptimizer is AdamW (thede factostandard op-\ntimizer for transformer language models), and\nthe number of inner optimization steps is large\n(our default value is 500). The latter is crucial to\nreduce communication, and it means that work-\ners only need to send data once every 500 steps.\nPractically speaking, while standard mini-batch\nmethods relying on data and model parallelism\nrequire sending data every few hundred millisec-\nonds, DiLoCo does so only every few minutes.\nTherefore, if each communication step takes a lot\nof time, DiLoCo converges much faster in terms\nof wall-clock time.\nOur empirical validation demonstrate the ro-\nbustness of DiLoCo on several fronts, from the\ntype of data distribution each worker consumes,\nto the number of inner optimization steps, and\nnumber of workers which can even change over\ntime.\nIn conclusion,DiLoCo is a robust and effec-\ntive way to distribute training of transformer\nlanguage models when there are several avail-\nable machines but poorly connected. Of course,\nit remains to be seen whether these findings gen-\neralize to models of larger scale, or to other do-\nmains and architecture types.\nReferences\nAlexander Borzunov, Dmitry Baranchuk, Tim\nDettmers,  Max Ryabinin,  Younes Belkada,\nArtem Chumachenko, Pavel Samygin, and\nColin Raffel. Petals: Collaborative inference\nand fine-tuning of large models.arXiv preprint\nlibrary, 2022.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li,\nKai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database.Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2009.\nMichael  Diskin,  Alexey  Bukhtiyarov,  Max\nRyabinin, Lucile Saulnier, Quentin Lhoest,\nAnton Sinitsin, Dmitry Popov, Dmitry Pyrkin,\nMaxim Kashirin, Alexander Borzunov, Albert\nVillanova del Moral, Denis Mazur, Ilia Kobelev,\nYacine Jernite, Thomas Wolf, and Gennady\nPekhimenko.  Distributed deep learning in\nopen collaborations.Advances in Neural\nInformation  Processing  Systems  (NeurIPS),\n2021.\nJonathan Frankle, Gintare Karolina Dziugaite,\nDaniel M. Roy, and Michael Carbin.  Linear\nmode connectivity and the lottery ticket hy-\npothesis.International Conference on Machine\nLearning (ICML), 2020.\nDashan Gao, Xin Yao, and Qiang Yang. A survey\non heterogeneous federated learning.arXiv\npreprint library, 2022.\nXinran Gu, Kaifeng Lyu, Longbo Huang, and San-\njeev Arora. Why (and when) does local sgd\ngeneralize better than sgd?Proceedings of the\nInternational Conference on Learning Represen-\ntations (ICLR), 2023.\nKshitij Gupta, Benjamin Thérien, Adam Ibrahim,\nMats L. Richter, Quentin Anthony, Eugene\nBelilovsky, Irina Rish, and Timothée Lesort.\nContinual pre-training of large language mod-\nels: How to (re)warm your model?arXiv\npreprint library, 2023.\nSuchin Gururangan, Margaret Li, Mike Lewis,\nWeijia Shi, Tim Althoff, Noah A. Smith, and\nLuke Zettlemoyer.  Scaling expert language\n11",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nmodels with unsupervised domain discovery.\narXiv preprint library, 2023.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun.  Deep residual learning for image\nrecognition.Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), 2015.\nJordan Hoffmann, Sebastian Borgeaud, Arthur\nMensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne\nHendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen,\nJack W. Rae, Oriol Vinyals, and Laurent Sifre.\nTraining compute-optimal large language mod-\nels.Advances in Neural Information Processing\nSystems (NeurIPS), 2022.\nZhouyuan Huo, Qian Yang, Bin Gu, and Lawrence\nCarin. Heng Huang.  Faster on-device train-\ning using new federated momentum algorithm.\narXiv preprint library, 2020.\nGabriel    Ilharco,Mitchell    Wortsman,\nSamir Yitzhak Gadre,  Shuran Song,  Han-\nnaneh  Hajishirzi,  Simon  Kornblith,  Ali\nFarhadi,  and Ludwig Schmidt.   Patching\nopen-vocabulary  models  by  interpolating\nweights.Advances in Neural Information\nProcessing Systems (NeurIPS), 2022.\nXisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and\nPengxiang Cheng.  Dataless knowledge fu-\nsion by merging weights of language models.\nProceedings of the International Conference on\nLearning Representations (ICLR), 2023.\nAlexia Jolicoeur-Martineau, Emy Gervais, Kilian\nFatras, Yan Zhang, and Simon Lacoste-Julien.\nPopulation parameter averaging (papa).arXiv\npreprint library, 2023.\nKeller Jordan, Hanie Sedghi, Olga Saukh, Rahim\nEntezari, and Behnam Neyshabur.  Repair:\nRenormalizing permuted activations for inter-\npolation repair.arXiv preprint library, 2023.\nJean Kaddour.  Stop wasting my time! saving\ndays of imagenet and bert training with latest\nweight averaging.Advances in Neural Infor-\nmation Processing Systems (NeurIPS) Workshop,\n2022.\nNikhil  Kandpal,  Brian  Lester,  Mohammed\nMuqeeth, Anisha Mascarenhas, Monty Evans,\nVishal Baskaran, Tenghao Huang, Haokun Liu,\nand Colin Raffel. Git-theta: A git extension for\ncollaborative development of machine learning\nmodels.arXiv preprint library, 2023.\nDiederik P. Kingma and Jimmy Ba.  Adam: A\nmethod for stochastic optimization.Proceed-\nings of the International Conference on Learning\nRepresentations (ICLR), 2014.\nMargaret Li, Suchin Gururangan, Tim Dettmers,\nMike Lewis, Tim Althoff, Noah A. Smith, and\nLuke Zettlemoyer. Branch-train-merge: Embar-\nrassingly parallel training of expert language\nmodels.arXiv preprint library, 2022.\nTao Lin, Sebastian U. Stich, Kumar Kshitij Patel,\nand Martin Jaggi. Don’t use large mini-batches,\nuse local sgd.Proceedings of the International\nConference on Learning Representations (ICLR),\n2020.\nIlya Loshchilov and Frank Hutter.  Decoupled\nweight decay regularization.Proceedings of the\nInternational Conference on Learning Represen-\ntations (ICLR), 2019.\nH. Brendan McMahan, Eider Moore, Daniel Ram-\nage, Seth Hampson, and Blaise Agüera y Arcas.\nCommunication-efficient learning of deep net-\nworks from decentralized data.International\nConference on Artificial Intelligence and Statis-\ntics (AISTATS), 2017.\nJose Javier Gonzalez Ortiz, Jonathan Frankle,\nMike Rabbat, Ari Morcos, and Nicolas Ballas.\nTrade-offs of local sgd at scale: An empirical\nstudy.arXiv preprint library, 2021.\nShawn Presser.  Swarm training, 2020.  URL\nhttps://battle.shawwn.com/swarm-\ntraining-v01a.pdf.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine  Lee,  Sharan  Narang,  Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu.\nExploring the limits of transfer learning with\n12",
    "DiLoCo: Distributed Low-Communication Training of Language Models\na unified text-to-text transformer.Journal of\nMachine Learning Research, 2020.\nAlexandre Ramé, Guillaume Couairon, Mustafa\nShukor, Corentin Dancette, Jean-Baptiste Gaya,\nLaure Soulier, and Matthieu Cord. Rewarded\nsoups: towards pareto-optimal alignment by\ninterpolating weights fine-tuned on diverse re-\nwards.Advances in Neural Information Process-\ning Systems (NeurIPS), 2023a.\nAlexandre Ramé, Matthieu Kirchmeyer, Thibaud\nRahier, Alain Rakotomamonjy, Patrick Gallinari,\nand Matthieu Cord.  Diverse weight averag-\ning for out-of-distribution generalization.Ad-\nvances in Neural Information Processing Systems\n(NeurIPS), 2023b.\nSylvestre-Alvise Rebuffi, Francesco Croce, and\nSven Gowal. Revisiting adapters with adver-\nsarial training.Proceedings of the International\nConference on Learning Representations (ICLR),\n2022.\nSashank Reddi, Zachary Charles, Manzil Zaheer,\nZachary Garrett, Keith Rush, Jakub Konečný,\nSanjiv Kumar, and H. Brendan McMahan. Adap-\ntive federated optimization.Proceedings of the\nInternational Conference on Learning Represen-\ntations (ICLR), 2021.\nMax  Ryabinin,  Eduard  Gorbunov,  Vsevolod\nPlokhotnyuk, and Gennady Pekhimenko. Mosh-\npit sgd: Communication-efficient decentralized\ntraining on heterogeneous unreliable devices.\nAdvances in Neural Information Processing Sys-\ntems (NeurIPS), 2021.\nSebastian U. Stich. Local SGD converges fast and\ncommunicates little.Proceedings of the Interna-\ntional Conference on Learning Representations\n(ICLR), 2019.\nGeorge Stoica, Daniel Bolya, Jakob Bjorner, Taylor\nHearn, and Judy Hoffman.  Zipit!  merging\nmodels from different tasks without training.\narXiv preprint library, 2023.\nIlya Sutskever, James Martens, George Dahl, and\nGeoffrey Hinton.  On the importance of ini-\ntialization and momentum in deep learning.\nInternational Conference on Machine Learning\n(ICML), 2013.\nZhenheng Tang, Shaohuai Shi, Wei Wang, Bo Li,\nand Xiaowen Chu.  Communication-efficient\ndistributed deep learning: A comprehensive\nsurvey.arXiv preprint library, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention\nis all you need.Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nJianyu Wang, Vinayak Tantia, Nicolas Ballas,\nand Michael Rabbat.  Slowmo:  Improving\ncommunication-efficient distributed sgd with\nslow momentum.Proceedings of the Interna-\ntional Conference on Learning Representations\n(ICLR), 2020.\nMitchell Wortsman, Maxwell Horton, Carlos\nGuestrin, Ali Farhadi, and Mohammad Raste-\ngari. Learning neural network subspaces.In-\nternational Conference on Machine Learning\n(ICML), 2021.\nMitchell Wortsman, Suchin Gururangan, Shen Li,\nAli Farhadi, Ludwig Schmidt, Michael Rabbat,\nand Ari S. Morcos. lo-fi: distributed fine-tuning\nwithout communication.arXiv preprint library,\n2022a.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya\nGadre,  Rebecca Roelofs,  Raphael Gontijo-\nLopes, Ari S Morcos, Hongseok Namkoong, Ali\nFarhadi, Yair Carmon, Simon Kornblith, and\nLudwig Schmidt.  Model soups:  averaging\nweights of multiple fine-tuned models improves\naccuracy without increasing inference time.\nInternational Conference on Machine Learning\n(ICML), 2022b.\nMitchell Wortsman, Gabriel Ilharco, Jong Wook\nKim,  Mike Li,  Simon Kornblith,  Rebecca\nRoelofs, Raphael Gontijo-Lopes, Hannaneh Ha-\njishirzi, Ali Farhadi, Hongseok Namkoong, and\nLudwig Schmidt. Robust fine-tuning of zero-\nshot models.Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), 2022c.\nPrateek Yadav, Derek Tam, Leshem Choshen,\nColin Raffel, and Mohit Bansal. Resolving inter-\nference when merging models.Advances in Neu-\n13",
    "DiLoCo: Distributed Low-Communication Training of Language Models\nHyperparameterValue\nInner Learning rate4푒\n−4\nNumber of warmup steps1,000\nWeight decay0.1\nBatch Size512\nSequence length1,024\nOuter OptimizerSGD, SGDM,Nesterov, Adam\nInner OptimizerAdamW\nOuter SGD learning rate1.0, 0.7,0.5, 0.3, 0.1\nOuter SGDM learning rate\n1.0, 0.7, 0.5,0.3, 0.1\nOuter SGDM momentum0.9\nOuter Nesterov learning rate1.0,0.7, 0.5, 0.3, 0.1\nOuter Nesterov momentum0.95,0.9, 0.8\nOuter Adam learning rate\n1.0, 0.7, 0.5,0.3, 0.1\nOuter Adam beta10.9\nOuter Adam beta20.999,0.95\nOuter Adam epsilon1.0,10\n−1\n,10\n−3\n,10\n−5\n,10\n−7\nCommunication frequency퐻\n50, 100, 250,500,1,000,2,000\nNumber of pretraining steps0,12,000,24,000,48,000\nNumber of replicas4,8, 16, 64\nData regimesi.i.d.,non-i.i.d\nTable 5|Optimization Hyperparametersevalu-\nated during in this work. Chosen values for main\nexperiments are highlighted in bold.\nral Information Processing Systems (NeurIPS),\n2023.\nMichael R. Zhang, James Lucas, Geoffrey Hinton,\nand Jimmy Ba. Lookahead optimizer: k steps\nforward, 1 step back.Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2019.\nAcknowledgements\nWe would like to thank Ross Hemsley, Bo Liu,\nAmal Rannen-Triki, and Jack Rae for their valu-\nable feedback.\nSupplementary Materials\n6.1. Implementation Details\nHyperparametersWe displayed in Table 1 the\narchitectural difference between the 60M, 150M,\nand 400M models we evaluted. In Table 5, we\noutline the optimization hyperparameters con-\nsidered for this study, and highlight in bold the\nvalues chosen for the main experiments. We de-\ntailled extensively the impact of each hyparame-\nters in subsection 3.1.\nInner Optimizer StatesIn all experiments,\nthe inner optimizer,InnerOpt,  is AdamW\n(Loshchilov and Hutter, 2019) as standard prac-\ntice when training transformer language models.\nEach replica in our method has a separate Adam\nstate (e.g.first and second momentum). DiLoCo\nsynchronizes the parameters of the model, but we\nalso considered synchronizing the inner optimizer\nstates. It did not lead to significant improvements\nwhile significantly increasing the communication\ncost (×3more data to transmit). Therefore, we\nlet each model replica own their own version of\noptimizer states. Similar findings were found in\nthe literature where SGDM or Nesterov momen-\ntum were used as inner optimizers (Ortiz et al.,\n2021; Wang et al., 2020).\nWeighted Average of Outer GradientsIn line\n12 of Algorithm 1, we perform a uniform average\nof every model replica’s outer gradient. There are\nalso other strategies, such asgreedy soup(Worts-\nman et al., 2022b) where model replicas are se-\nlected sequentially to minimize validation loss,\nordisjoint merge(Yadav et al., 2023) which uses\na sign-based heuristics. The first strategy is too\ntime-costly in our setting. We tried the latter, but\ngot slightly worse results. Thus, for the random\ni.i.d. data regime we use a uniform average. For\nthe non-i.i.d. data regime, we rescale each outer\ngradient by the number of examples in its shard.\nWhile at푘=4, all clusters are quite balanced,\nimbalance can be striking at푘=64and giving\nmore importance to larger clusters is beneficial.\nInfrastructureThe empirical validation of this\nwork was performed on machines hosting 16\nA100 GPUs. These machines were not necessarily\nco-located in the same geographic region. The\nouter optimization step is performed on a CPU\nserver connected to the local machines.\n6.2. Experiments & Ablations\nPruning outer gradientsAlthough DiLoCo\ncommunicates infrequently, when communica-\ntion is required the network might get saturated,\nparticularly when there are lots of workers, or\nwhen the model replicas are large. We thus ex-\n14",
    "DiLoCo: Distributed Low-Communication Training of Language Models\n(a)i.i.d.data regime.(b)non-i.i.d.data regime.\nFigure 10|Cosine Similarity between Outer Gradients: The line is the average similarity among\nthe푘=8replicas’ outer gradients, the shaded area is the standard deviation. This is almost null in\nthe case of i.i.d. shards.\n% of pruned valuesPerplexity  Relative change\n0%15.020%\n25%\n15.01-0.06%\n50%\n15.08+0.39%\n75%15.27+1.66%\nTable 6|Pruning outer gradientsusing a per-\nneuron sign pruning (Yadav et al., 2023).\nplored pruning of outer gradients in order to re-\nduce the need for high-bandwidth networks.\nWe consider the simplest pruning technique,\nsign-based pruning following Yadav et al. (2023).\nMore efficient methods could be explored in the\nfuture (Tang et al., 2023), particularly those lever-\naging structured sparsity. In Table 6, we prune\nbetween 25% to 75% of the individual outer gra-\ndients per replica before averaging them. Prun-\ning up to 50% of the individual values resulted\nin negligible loss of performance (+0.39%per-\nplexity). Therefore, DiLoCo’s communication ef-\nficiency can be further improved using standard\ncompression techniques.\nCosine Similarity of outer gradientsOur em-\npirical validation shows remarkable robustness\nof DiLoCo to the number of inner optimization\nsteps and data distribution of the shards. Why\ndoes DiLoCo converge even when performing 500\ninner steps? And why using shards with different\nFigure 11|Outer Gradients similarity versus\nnumber of replicas: in a non-i.i.d. data regime\nincreasing the number of replicas/clusters (푘=\n4→8) produces more dissimilar outer gradients.\ndata distribution does not harm performance at\nall?\nTo shed light on these questions, we have gath-\nered statistics of the outer gradients returned\nby workers. In particular, we calculate the av-\nerage cosine similarity between outer gradients\nreturned by workers while varying the number of\ninner optimization steps (퐻={250,500,1000})\nfor both the i.i.d. (in Figure 10a) and non-i.i.d.\n(in Figure 10b) settings.\nThe former regime has close to no variance\ncompared to the latter, since all shards have\nthe same data distribution and therefore outer-\n15",
    "DiLoCo: Distributed Low-Communication Training of Language Models\ngradients are much more correlated. For both\ndata regimes, perhaps unintuitively, similarity\nis inversely proportional to the communication\nfrequency however. We surmise that when the\nnumber of inner step is larger (up to some extent)\nmodel replicas converge towards a similar gen-\neral direction (Gu et al., 2023) averaging out the\nnoise of stochastic gradient descent.\nInterestingly, as the learning rate anneals to 0\ntowards the end of training, the outer gradients\nsimilarity increases in the i.i.d. case but in the\nnon-i.i.d. case only the variance increases. Since\nshards have a different distribution, each local\noptimization seems to fall in a different nearby\nloss basin. However, the averaging of such more\northogonal gradients grants beneficial general-\nization as the non-i.i.d. version of DiLoCo tends\nto generalize at a better rate towards the end of\ntraining as can be seen in Figure 5.\nLastly, in the non-i.i.d. setting we expect that\nthe larger the number of shards the more dis-\ntinctive their distribution, and therefore, the less\ncorrelated the corresponding outer gradients. Fig-\nure 11 shows precisely this trend when going\nfrom푘=4to푘=8shards. We also found that in\nthis setting the averaged outer gradient’s norm is\ninversely proportional to the square root of the\nnumber of replicas.\n16"
  ]
}