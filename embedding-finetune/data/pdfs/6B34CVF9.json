{
  "key": "6B34CVF9",
  "url": "http://arxiv.org/pdf/2404.03592",
  "metadata": {
    "title": "ReFT: Representation Finetuning for Language Models",
    "abstract": "  Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via\nupdates to a small number of weights. However, much prior interpretability work\nhas shown that representations encode rich semantic information, suggesting\nthat editing representations might be a more powerful alternative. Here, we\npursue this hypothesis by developing a family of $\\textbf{Representation\nFinetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model and\nlearn task-specific interventions on hidden representations. We define a strong\ninstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is\na drop-in replacement for existing PEFTs and learns interventions that are\n10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase\nLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,\nAlpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best\nbalance of efficiency and performance, and almost always outperforms\nstate-of-the-art PEFTs. We release a generic ReFT training library publicly at\nhttps://github.com/stanfordnlp/pyreft.\n",
    "published": "2024-04-04T17:00:37Z"
  },
  "text": [
    "ReFT: Representation Finetuning\nfor Language Models\nZhengxuan Wu\n∗†\nAryaman Arora\n∗†\nZheng Wang\n†\nAtticus Geiger\n‡\nDan Jurafsky\n†\nChristopher D. Manning\n†\nChristopher Potts\n†\n†\nStanford University\n‡\nPr(Ai)\n2\nR Group\n{wuzhengx,aryamana,peterwz,atticusg,jurafsky,manning,cgpotts}@stanford.edu\nAbstract\nParameter-efficient fine-tuning (PEFT) methods seek to adapt large models via\nupdates to a small number ofweights. However, much prior interpretability work\nhas shown thatrepresentationsencode rich semantic information, suggesting that\nediting representations might be a more powerful alternative. Here, we pursue this\nhypothesis by developing a family ofRepresentation Finetuning (ReFT)methods.\nReFT methods operate on a frozen base model and learn task-specific interven-\ntions on hidden representations. We define a strong instance of the ReFT family,\nLow-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for\nexisting PEFTs and learns interventions that are 10×–50×more parameter-efficient\nthan prior state-of-the-art PEFTs. We showcase LoReFT on eight commonsense\nreasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In\nall these evaluations, LoReFT delivers the best balance of efficiency and perfor-\nmance, and almost always outperforms state-of-the-art PEFTs. We release a generic\nReFT training library publicly athttps://github.com/stanfordnlp/pyreft.\n1    Introduction\nPretrained LMs are frequently finetuned to adapt them to new domains or tasks [Dai and Le, 2015].\nWith finetuning, a single base model can be adapted to a variety of tasks given only small amounts of\nin-domain data. However, finetuning the entire model is expensive, especially for very large LMs.\nParameter-efficient finetuning (PEFT) methods propose to address the high costs of full finetuning\nby updating only a small fraction of weights [Han et al., 2024].  This reduces memory usage and\ntraining time, and PEFTs have been shown to achieve similar performance to full finetuning in many\npractical settings [Hu et al., 2023]. Adapters, which are a common family of PEFTs, learn an edit that\ncan be added to a subset of model weights, or an additional set of weights that operate alongside the\nfrozen base model. Recent adapters such as LoRA [Hu et al., 2022] (and variants such as DoRA; Liu\net al., 2024b) reduce the number of trainable parameters in learned weight updates by using low-rank\napproximations in place of full weight matrices during adapter training. QLoRA [Dettmers et al.,\n2023] further shows that full-precision adapters can be trained on top of reduced-precision models\nwithout sacrificing performance. Adapters are generally more efficient and effective than methods\nthat introduce new model components, like prefix-tuning [Li and Liang, 2021].\nA hallmark of current state-of-the-art PEFTs is that they modifyweightsrather thanrepresentations.\nHowever, much prior interpretability work has shown that representations encode rich semantic\ninformation, suggesting that editing representations might be a more powerful alternative to weight\nupdates.  In this paper, we pursue this hypothesis by developing and motivatingRepresentation\nFinetuning (ReFT). Instead of adapting model weights, ReFT methods train interventions that\n*\nEqual contribution.\nTechnical Report.\narXiv:2404.03592v2  [cs.CL]  8 Apr 2024",
    "Commonsense\nLLaMA 7B\nLLaMA 13B\nArithmetic\nLLaMA 7BLLaMA 13B\nParameters\nPer\nformance\nInstruct-tuning\nLlama-2 7B\nGLUE\nRoBERTa-baseRoBERTa-large\nFigure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks\nwhen applied to LLaMA, Llama-2, and RoBERTa models. Despite training much fewer parameters\nthan existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks.\nIts value is most apparent for the largest models in our evaluations.Note: FT refers to full-parameter\nfinetuning, which is not a PEFT or ReFT method.\nmanipulate a small fraction of model representations in order to steer model behaviors to solve\ndownstream tasks at inference time. ReFT methods are drop-in replacements for weight-based PEFTs.\nThis approach is inspired by recent work in LM interpretability that intervenes on representations to\nfind faithful causal mechanisms [Geiger et al., 2023b] and to steer model behaviours at inference time\n[Turner et al., 2023, Li et al., 2024], and it can be seen as a generalization of the representation-editing\nwork of Wu et al. [2024a], Turner et al. [2023], and Zou et al. [2023] (see Appendix A for formal\ndetails on the relationships).\nWe focus on a strong and highly efficient instance of the ReFT family that we callLow-rank\nLinear Subspace ReFT (LoReFT). This is a parametrisation of ReFT that intervenes on hidden\nrepresentations in the linear subspace spanned by a low-rank projection matrix, building directly on\nthe distributed alignment search (DAS) method of Geiger et al. [2023b] and Wu et al. [2023]. We\nevaluate LoReFT on LLaMA-family models and small-scale LMs against existing PEFTs on standard\nbenchmarks from four domains: commonsense reasoning, arithmetic reasoning, instruction-following,\nand natural language understanding. Compared to LoRA, we find that LoReFT uses 10×–50×times\nfewer parameters while achieving state-of-the-art performance for x out of y number datasets (fig. 1).\nThese findings indicate that ReFT methods are worthy of further exploration, as they may emerge as\nmore efficient and effective alternatives to weight-based PEFTs.\n2    Related work\nParameter-efficient finetuning methods (PEFTs).In contrast to full finetuning, PEFTs train only a\nfraction of the model’s parameters to adapt it to downstream tasks. PEFTs can be classified into three\nmain categories:\n1.\nAdapter-based methodstrain additional modules (e.g. fully-connected layers) on top of the\nfrozen pretrained model.Series adaptersinsert components between LM attention or MLP\nlayers [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al.,\n2021], whileparallel adaptersadd modules alongside existing components [He et al., 2022a].\n2",
    "Since adapters add new components that cannot be easily folded into existing model weights,\nthey impose additional burden at inference time.\n1\n2.LoRA[Hu et al., 2022] and its recent variant DoRA [Liu et al., 2024b] use low-rank matrices to\napproximate additive weight updates during training, and require no additional overhead during\ninference since the weight updates can be merged into model. These are the strongest PEFTs\ncurrently.\n2\n3.\nPrompt-based methodsadd randomly-initialised soft tokens to the input (usually as a prefix)\nand train their embeddings while keeping the LM weights frozen [Li and Liang, 2021].  The\nperformance  of  these  methods  are  often  far  from  optimal  compared  to  other  PEFTs,  and\ncome at the cost of significant inference overhead.  A variant of this method where hidden-\nlayer activations are also tuned was introduced as a baseline in Hu et al. [2022], with better\nperformance.\nInstead of weight updates, ReFT methods learn interventions to modify a small fraction of model\nrepresentations.\nRepresentation editing.Recent work onactivation steeringandrepresentation engineeringshows\nthat adding fixed steering vectors to the residual stream can enable a degree of control over pretrained\nLM generations without the need for resource-intensive finetuning [Subramani et al., 2022, Turner\net  al.,  2023,  Zou  et  al.,  2023,  Vogel,  2024].   For  example,  steering  attention-head  outputs  can\nincrease performance on TruthfulQA [Li et al., 2024]. Similarly, Wu et al. [2024a] show that editing\nrepresentations with a learned scaling and translation operation can approach (but not surpass)\nthe performance of LoRA on a variety of models and tasks with far fewer learned parameters.\nThe success of these methods affirms that representations induced by pretrained LMs carry rich\nsemantics. However, exploration of such methods has been sporadic and performance is sub-optimal;\nadapter-based PEFTs continue to be the state-of-the-art and impose no additional inference burden.\nInterventional interpretability.Recent work in interpretability has used interventions under the\nframework of causal abstraction [Geiger et al., 2021] to test hypotheses about how LMs implement\nvarious behaviours. In particular, interventions on linear subspaces of representations have provided\nincreasing evidence that human-interpretable concepts are encoded linearly; this includes linguistic\nfeatures such as gender and number [Lasri et al., 2022, Wang et al., 2023, Hanna et al., 2023, Chintam\net al., 2023, Yamakoshi et al., 2023, Hao and Linzen, 2023, Chen et al., 2023, Amini et al., 2023,\nGuerner et al., 2023, Arora et al., 2024], logical and mathematical reasoning [Wu et al., 2023], and\nentity attributes [Huang et al., 2024].\n3    ReFT\nWe now define the ReFT family of methods.  To do this, we first summarize the core motivation,\nwhich emerges from work on intervention-based model interpretability. We then show how this leads\ndirectly to Low-rank Linear Subspace ReFT (LoReFT). Finally, we generalize this to a family of\nReFT methods.\nTo keep the presentation simple, we assume throughout that our target model is a Transformer-\nbased [Vaswani et al., 2017] LM that produces contextualised representations of sequences of tokens.\nGiven a sequence ofninput tokensx=(x\n1\n,...,x\nn\n), the model first embeds these into a list of\nrepresentationsh\n(0)\n=(h\n(0)\n1\n,...,h\n(0)\nn\n). Then,mlayers successively compute thej-th list of hidden\nrepresentationsh\n(j)\nas a function of the previous list of hidden representationsh\n(j−1)\n. Each hidden\nrepresentation is a vectorh∈R\nd\n. The LM uses the final hidden representationsh\n(m)\nto produce its\npredictions. In our experiments, we consider both autoregressive LMs and masked LMs [Devlin et al.,\n2019]. An autoregressive LM predictsp(x\nn+1\n∣x\n1\n,...,x\nn\n)=softmax(Wh\n(m)\nn\n), while a masked\nLM predictsp(x\ni\n∣x\n1\n,...,x\ni−1\n,x\ni+1\n,...,x\nn\n)=softmax(Wh\n(m)\ni\n), whereWis a learned matrix\nmapping from representations to logits over the vocabulary space.\n1\nSeveral recent papers introduce new adapter architectures but do not benchmark them on the tasks we\nconsider, or perform hyperparameter-tuning in a different setup than done in this work. These include: LLaMA-\nAdapter [Zhang et al., 2024b], LLaMA-Adapter v2 [Gao et al., 2023], Aligner [Ziheng et al., 2023].\n2\nAdditional methods not studied in this work: AutoLoRA [Zhang et al., 2024c], ResLoRA [Shi et al., 2024],\nSiRA [Zhu et al., 2023].\n3",
    "3.1    Motivation\nThelinear representation hypothesisclaims that concepts are encoded in linear subspaces of\nrepresentations in neural networks. Early connectionist work on distributed neural representations\nwas the first to propose this [Smolensky, 1986, Rumelhart et al., 1986, McClelland et al., 1986], and\nrecent empirical work has found evidence for this claim in neural models trained on natural language\nas well as other input distributions [Mikolov et al., 2013, Elhage et al., 2022, Park et al., 2023, Nanda\net al., 2023, Guerner et al., 2023].\nIn interpretability research, the framework of causal abstraction [Geiger et al., 2021] usesinterchange\ninterventionsto causally establish the role of neural network components in implementing particular\nbehaviours. The logic of the interchange intervention is as follows: if one fixes a representation to\nwhat it would have been given a counterfactual input, and this intervention consistently affects model\noutput in the way predicted by our claims about the component producing that representation, then\nthat component plays a causal role in the behaviour being studied.\nTo test whether a concept is encoded in a linear subspace of a represention,  as claimed by the\nlinear representation hypothesis, one may use adistributed interchange intervention[Geiger et al.,\n2023b].\n3\nLetbbe the hidden representation created at rowiand columnkwhen our model processes\ninputb, and letsbe the corresponding representation when that same model processes inputs.\nA distributed interchange intervention onbgiven a counterfactual source representationsis then\ndefined as\nDII(b,s,R)=b+R\n⊺\n(Rs−Rb)(1)\nwhereR∈R\nr×d\nis a low-rank projection matrix with orthonormal rows,dis the representation\ndimensionality, andris the dimensionality of the subspace we are intervening on.  We learn the\nsubspaceRusing distributed alignment search (DAS), which finds the subspace that maximises the\nprobability of the expected counterfactual output after intervention.\nDAS has been used to find linear representations in LMs of entity attributes [Huang et al., 2024], lin-\nguistic features [Arora et al., 2024], sentiment [Tigges et al., 2023], and mathematical reasoning [Wu\net al., 2023, Lepori et al., 2023].  However, experiments reveal that DAS is highly expressive and\ncapable of finding causally efficacious subspaces even when a Transformer LM has beenrandomly\ninitialisedand thus has not yet learned any task-specific representations [Wu et al., 2023, Arora\net al., 2024].  While this has led to debate on whether DAS is faithful enough for interpretability\npurposes [Makelov et al., 2024, Wu et al., 2024c], DAS’s expressivity suggests that it could serve\nas a powerful tool forcontrollingLM behaviours, in line with work on representation editing and\ncontrollable generation [Li et al., 2024, Zou et al., 2023]. Therefore, we sought to use the distributed\ninterchange intervention operation to make a new parameter-efficient method for adapting language\nmodels for downstream tasks.\n3.2    Low-rank Linear Subspace ReFT (LoReFT)\nThe formulation ofDIIin eq. (1) immediately suggests a way to control model generations via\ninterventions. The guiding intuition is that we can learn how to perform interventions that lead the\nmodel to accurately predict our task labels. The resulting method, Low-rank Linear Subspace ReFT\n(LoReFT), is defined by the following variant of eq. (1):\nΦ\nLoReFT\n(h)=h+R\n⊺\n(Wh+b−Rh)(2)\nThis is identical to eq. (1), except we use alearned projected sourceRs=Wh+b.  Intuitively,\nLoReFT edits the representation in ther-dimensional subspace spanned by the columns ofRto take\non the values obtained from our linear projectionWh+b.\nThe learned parameters areφ={R,W,b}.  As withDII,R∈R\nr×d\nis a low-rank matrix with\northonormal rows wheredis the hidden-state dimensionality andr≤dis the rank of the subspace.\nWe further define a linear projectionW∈R\nr×d\nand bias vectorb∈R\nr\n. The parameters of the LM\nare frozen.\nWe consider both generation tasks using decoder-only or encoder–decoder LMs and classification\ntasks  using  encoder-only  models.   The  pretrained  language  model  is  a  distribution  over  token\nsequencesp(⋅).  We denote the model that results from the LoReFT intervention onp(⋅)asp\nΦ\n(⋅)\n3\nThis notion of subspace intervention was also independently discovered by Guerner et al. [2023].\n4",
    "with trainable parametersφ. To simplify notation, we refer to the hidden representations produced by\nthe LM on inputxash(x), and those by the intervened LM ash\nΦ\n(x).\nFor generation tasks, our training objective is language modelling.  Given an input sequencex=\n(x\n1\n,...,x\nn\n)withninput tokens as the prompt, the goal is to predict the output sequencey=\n(y\n1\n,...,y\nm\n)withminput tokens. We minimise the cross-entropy loss with teacher-forcing over all\noutput positions.\nmin\nφ\n{−\nm\n∑\ni=1\nlogp\nΦ\n(y\ni\n∣xy\n<i\n)}(3)\nFor single-label classification tasks, we add a classification headH\nθ\n(⋅)with parametersθthat takes\nthe final-layer representation at the first token (CLS) as input and outputs a distribution over classes.\nHhas the learned parametersθ={W\no\n,b\no\n,W\nd\n,b\nd\n}.\nH\nθ\n(⋅∣h)=softmax(W\no\n(tanh(W\nd\nh\n(m)\n1\n+b\nd\n))+b\no\n)(4)\nWe learn the parameters of the head and those of the intervention functionΦ.  We minimise the\ncross-entropy loss of the target classygiven inputx.\nmin\nφ,θ\n{−logH\nθ\n(y∣h\nΦ\n(x))}(5)\n3.3    The ReFT family of methods\nIt is straightforward to generalize the above to define a family of intervention-based representation\nfinetuning methods. We first define a general notion ofintervention, i.e. the modification of hidden\nrepresentations during the model forward pass:\nDefinition 3.1.AninterventionIis a tuple⟨Φ,P,L⟩that encapsulates a single inference-time\nmodification of the representations computed by a Transformer-based LM. The three components of\nan intervention are\n•  The intervention functionΦ∶R\nd\n→R\nd\nwith learned parametersφ.\n•  A set of input positionsP⊆{1,...,n}that the intervention is applied to.\n•  The layerL∈{1,...,m}at which the intervention is applied.\nWe implement the interventionIas the following operation that overwrites some representationsh:\nh\n(L)\n←\n(\nΦ\n(\nh\n(L)\np\n)\nifp∈Pelseh\n(L)\np\n)\np∈1,...,n\n(6)\nThe intervention is applied immediately after the computation ofh\n(L)\nand thus affects the representa-\ntions computed in later layersh\n(L+1)\n,...,h\n(m)\n.\nFigure 2 provides a schematic overview of an intervention. A ReFT is then defined as a constrained\nset of non-overlapping interventions:\nDefinition 3.2.AReFT methodis a set offinterventionsI={I\n1\n,...,I\nf\n}.  We enforce that for\nany two interventionsI\nj\n,I\nk\n∈Isuch that they operate on the same layerL\nj\n=L\nk\n, their intervention\npositions must be disjoint, i.e.P\nj\n∩P\nk\n=∅. The parameters(φ\n1\n,...,φ\nf\n)of all of the intervention\nfunctions are independent.\nReFT is thus a generic framework encompassing interventions on hidden representations during the\nmodel forward pass. In appendix A, we show how a variety of existing inference-time intervention\nmethods can be described within this framework.\n4    Experiments\nTo evaluate LoReFT against existing PEFTs,  we conduct experiments across four diverse NLP\nbenchmarks covering more than 20 datasets. Our goal is to provide a rich picture of how LoReFT\nperforms in different scenarios. Here is a brief overview of our benchmarks:\n5",
    "ReFT Intervention\nthisissome\ntext\nLoReFT\nprefix\nunintervenedsuffix\n1\n2\n34\n5\n678\n9\n10\n11\n12\n13\n14\n15\n16\nthisismoreandmore\ntext\nFigure 2:Illustration of ReFT.(1)The left panel depicts an interventionI:  the intervention\nfunctionΦis applied to hidden representations at positionsPin layerL.(2)The right panel depicts\nthe hyperparameters we tune when experimenting with LoReFT. Specifically, the figure depicts\napplication of LoReFT at all layers with prefix lengthp=2and suffix lengths=2. When not tying\nlayer weights, we train separate intervention parameters at each position and layer, resulting in16\ninterventions with unique parameters in this example.\n•\nCommonsense reasoningwhich contains eight commonsense reasoning datasets, including\nBoolQ [Clark et al., 2019], PIQA [Bisk et al., 2020], SIQA [Sap et al., 2019], HellaSwag [Zellers\net al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC-e, ARC-c [Clark et al., 2018], and\nOBQA [Mihaylov et al., 2018]. The task is formulated as a multiple-choice problem.\n•Arithmetic  reasoningwhich  contains  four  datasets  for  math  world  problems,  including\nAQuA [Ling et al., 2017], GSM8K [Cobbe et al., 2021], MAWPS [Koncel-Kedziorski et al.,\n2016], and SVAMP [Patel et al., 2021]. Models need to generate chain-of-thought [Wei et al.,\n2022] before the final answer.\n•Instruction-followingwhich evaluates whether models can follow human instructions. We use\nUltrafeedback [Cui et al., 2023] as our training data, and Alpaca-Eval v1.0 [Li et al., 2023] as\nour evaluation dataset.\n•Natural  language  understandingwhich  contains  eight  datasets  from  the  GLUE  bench-\nmark [Wang et al., 2018] such as sentiment analysis and natural language inference.\nWe experiment with LMs at different scales ranging from RoBERTa-base [Liu et al., 2019] with\n125M to LLaMA-1 [Touvron et al., 2023] with 13B parameters, and both masked and autoregressive\ntransformer language models. We benchmark against existing PEFTs such as prefix-tuning [Li and\nLiang, 2021], adapter-tuning with both Series Adapters and Parallel Adapters, LoRA [Hu et al., 2022],\nand DoRA [Liu et al., 2024b]. Our comparisons focus on both performance and parameter efficiency.\nIn our comparisons, we use hyperparameter-tuned scores from previous works when possible. We\nload our base LMs intorch.bfloat16to save memory.All of our experiments are run with a\nsingle GPU: NVIDIA A100 40G/80G or RTX 6000. Examples of raw model generations are in\nappendix I.\n4.1    Hyperparameter configuration\nWhen using LoReFT in practice, we must decide how many interventions to learn and which layers\nand input positions to apply each one on. We propose learning interventions on a fixed number ofp\nprefix andssuffix positions in the prompt. Specifically, we tune four hyperparameters:\n1.  The number of prefix positionspto intervene on, i.e. positions{1,...,p}.\n2.  The number of suffix positionssto intervene on, i.e. positions{n−s+1,...,n}.\n3.  Which set of layersLto intervene on.\n4.  Whether or not to tie intervention parametersφacross different positions in the same layer.\nThis simplifies the hyperparameter search space and ensures only a fixed additional inference cost that\ndoes not scale with prompt length. Compared to LoRA, we only have the additional consideration of\nwhich positions to intervene on.\n6",
    "Following  the  definition  of  ReFT,  we  thus  define  the  untied  and  tied  variants  of  our  LoReFT\nintervention as:\nP={1,...,p}∪{n−s+1,...,n}(7)\nI\nuntied\n={⟨Φ\nLoReFT\n,{p},l⟩ ∣p∈P,l∈L}(8)\nI\ntied\n={⟨Φ\nLoReFT\n,P,l⟩ ∣l∈L}(9)\nAdditionally, when applying untied LoReFT to a prompt with lengthnwheren<p+s, we set\np←min(p,⌊n/2⌋)ands←min(s,⌈n/2⌉)and do not apply the truncated interventions inI\nuntied\n. We\nalso tune neural-network training hyperparameters. Further details are provided in appendix C. We\ngraphically depict how an untied LoReFT intervenes on an example prompt in fig. 2.\nUnlike previous works [Hu et al., 2022, 2023, Liu et al., 2024b] where hyperparameter tuning\nmay involve optimising performance directly on test sets, we only tune our hyperparameters on\ndevelopment sets which do not contain any overlapping examples with the test sets of our tasks. We\nfurther describe hyperparameter tuning for each benchmark in appendix C.\n4.2    Commonsense reasoning\nModelPEFTParams(%)\nAccuracy(↑)\nBoolQPIQASIQAHellaS.WinoG.ARC-eARC-cOBQAAvg.\nChatGPT\n∗\n——73.185.468.578.566.189.879.974.877.0\nLLaMA-7B\nPrefT\n∗\n0.110%64.376.873.942.172.172.954.060.664.6\nAdapter\nS∗\n0.990%63.079.276.367.975.774.557.172.470.8\nAdapter\nP∗\n3.540%67.976.478.869.878.973.757.375.272.3\nLoRA\n∗\n0.830%68.980.777.478.178.877.861.374.874.7\nDoRA (half)\n∗\n0.430%70.082.679.783.280.680.665.477.677.5\nDoRA\n∗\n0.840%68.582.979.684.880.881.465.881.078.1\nLoReFT (ours)0.031%69.384.480.393.184.283.268.278.980.2\nLLaMA-13B\nPrefT\n∗\n0.030%65.375.472.155.268.679.562.968.068.4\nAdapter\nS∗\n0.800%71.883.079.288.182.482.567.381.879.5\nAdapter\nP∗\n2.890%72.584.979.892.184.784.271.282.481.5\nLoRA\n∗\n0.670%72.183.580.590.583.782.868.382.480.5\nDoRA (half)\n∗\n0.350%72.585.379.990.182.982.769.783.680.8\nDoRA\n∗\n0.680%72.484.981.592.484.284.269.682.881.5\nLoReFT (ours)0.025%72.186.381.895.187.286.273.784.283.3\nTable 1: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on\neight commonsense reasoning datasets.\n∗\nPerformance results of all baseline methods are taken from\nLiu et al. [2024b]. We report averaged performance of three runs with distinct random seeds for our\nmethod. For LoReFT, # Param. (%) is calculated by dividing the number of trainable parameters by\nthe number of parameters of the base LM.\nWe replicate the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B and 13B on a\ncombined dataset of eight commonsense reasoning tasks (COMMONSENSE170K). We report scores\non each task’s test set individually. We compare with PEFTs benchmarked in Hu et al. [2023] as well\nas the identical experiment reported in Liu et al. [2024b] for DoRA.\nDatasets.Examples from these datasets are formulated as multiple-choice problems where the model\nneeds to directly generate the correct choice without rationales. We use the same prompt template as\nin Hu et al. [2023] with additional string normalization (removing leading and trailing whitespace).\nFurther details on the eight tasks are given in appendix B.1.\nHyperparameter tuning.We do not want to do hyperparameter selection based on test set results.\nTo avoid this, we use the hyperparameter settings of the model that performs best on a development\nset created from theGSM8Ktraining set, except we use a lower number of epochs (6 instead of\n12) because theCOMMONSENSE170Ktraining set is more than 20 times larger thanGSM8K. This\nallows us to tune relevant hyperparamters, and also serves to test the robustness of these settings\nacross different domains.\nResults.We report results in table 1. LoReFT sets state-of-the-art performance on the commonsense\nreasoning tasks, outperforming all other methods by a considerable margin.\n7",
    "4.3    Arithmetic reasoning\nModelPEFTParams(%)\nAccuracy(↑)\nAQuAGSM8KMAWPSSVAMPAvg.\nLLaMA-7B\nPrefT\n∗\n0.110%14.224.463.438.135.0\nAdapter\nS∗\n0.990%15.033.377.752.344.6\nAdapter\nP∗\n3.540%18.135.382.449.646.4\nLoRA\n∗\n0.830%18.937.579.052.146.9\nLoReFT (ours)0.031%21.426.076.246.842.6\nLLaMA-13B\nPrefT\n∗\n0.300%15.731.166.841.438.8\nAdapter\nS∗\n0.800%22.044.078.650.848.9\nAdapter\nP∗\n2.890%20.543.381.155.750.2\nLoRA\n∗\n0.670%18.547.583.654.651.1\nLoReFT (ours)0.025%23.638.182.454.249.6\nTable 2:  Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods\non four arithmetic reasoning datasets.\n∗\nPerformance results of all baseline methods are taken from\nHu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our\nmethod.\nSimilar to the previous experiment, we follow the experimental setup in Hu et al. [2023] and finetune\nLLaMA-1 7B and 13B on a combined dataset of seven arithmetic reasoning tasks with LM-generated\nchain-of-thought steps (MATH10K) and report scores on four of the tasks’ test sets. We only evaluate\ncorrectness on the final numeric or multiple-choice answer.\nHyperparameter tuning.We use the same hyperparameter settings as for the Commonsense Rea-\nsoning benchmark with 12 epochs for training.\nDatasets.For more details on the datasets, see appendix B.2. We use the same prompt template and\nhyperparameter settings as in the previous experiment.\nResults.We report results in table 2. We find that LoReFT does not perform as well at arithmetic\nreasoning tasks compared to LoRA and adapters, but does outperform prefix-tuning.  Our results\nsuggest that LoReFT may have more trouble on chain-of-thought reasoning than the single-step\ncommonsense reasoning tasks due to the length of generations (and greater length necessarily reduces\nthe effect of the intervention) and overall greater difficulty of the task. Our results show that LoReFT\nperforms better with 13B model than 7B model, which suggests that LoReFT scales with model size.\nOverall, we note that the arithmetic reasoning results show a lot of variation, with no single method\nemerging as a clear winner across all of them.\n4.4    Instruction-following\nBase LMs require instruction finetuning to follow human prompts [Ouyang et al., 2022]. We follow\nthe experimental setup in Wu et al. [2024a] and finetune Llama-2 7B with Ultrafeedback [Cui et al.,\n2023]. We compare against full parameter fine-tuning, LoRA ,and RED. We use Alpaca-Eval v1.0 [Li\net al., 2023] for evaluation which computes win-rate againsttext-davinci-003using GPT-4 as the\nannotator. We use the same prompt template as in Taori et al. [2023].\nDatasets.We finetune Llama-2 7B with Ultrafeedback. Ultrafeedback is high-quality instruction\ndataset where responses are generated via scoring a diverse set of model responses from a list of can-\ndidates (e.g. ChatGPT and Bard). The score is calculated as weighted score of instruction-following,\ntruthfulness, honesty and helpfulness.  Some of the best 7B and 13B chat-models (e.g. UltraLM-\n13B [Ding et al., 2023]) are finetuned with Ultrafeedback.\nHyperparameter tuning.We do hyperparameter-tuning on the unseen instruction-following dataset\nAlpaca-52K [Taori et al., 2023] with only LLaMA-7B to prevent test-set hill-climbing.  We then\nuse the hyperparameter settings of our best performing model to finetune on Ultrafeedback.  For\nhyperparameter tuning, we use Alpaca-Eval v1.0 with GPT-4 turbo as the annotator for fast turnaround,\nwhich also prevents overfitting with GPT-4 as a judge.\n8",
    "Model & PEFTParams(%)Win-rate(↑)\nGPT-3.5 Turbo 1106\n†\n—86.30\nLlama-2 Chat 13B\n†\n—81.10\nLlama-2 Chat 7B\n†\n—71.40\nLlama-2 7B & FT\n∗\n100%80.93\nLlama-2 7B & LoRA\n∗\n0.1245%81.48\nLlama-2 7B & RED\n∗\n0.0039%81.69\nLlama-2 7B &LoReFT (ours)0.0039%85.60\nLlama-2 7B &LoReFT (ours,half)0.0019%84.12\nLlama-2 7B &LoReFT (ours,1K)\n†\n0.0039%81.91\nTable 3: Instruction tuning evaluation results for instruction-tuned Llama-2 7B with Alpaca-Eval\nv1.0. We report averaged performance of two runs with distinct random seeds for our method.half\ndenotes our runs with half of the rank;1Kdenotes our runs with a low-resource setting where there is\nonly 1K training examples.\n†\nPerformance results of baseline methods are taken from Li et al. [2023].\n∗\nPerformance results of baseline methods are taken from Wu et al. [2024a].\n†\nIt takes≈18 minutes\nto train our Llama-2 Chat 7B on a single A100 40G GPU with≈1MB parameters on disk.\nResults.We report results in table 3.   When matched in parameter count to the previous most\nparameter-efficient PEFT (RED) and trained on Llama-2 7B, LoReFT outperforms all reported\nfinetuning methods (including full finetuning) and achieves a win-rate within 1% of GPT-3.5 Turbo\n1106. Furthermore, after halving the parameter count or using only 1/64-th of the data, LoReFT still\noutperforms other finetuning methods.  This result shows that LoReFT can succeed at long-form\ntext generation (despite some potential issues with this in the arithmetic reasoning evaluations). Our\nresults also suggest that LoReFT could be a viable way to evaluate instruction-tuning dataset quality\nwith a quick turnaround time.\n4\nModelPEFTParams(%)\nAccuracy(↑)\nMNLISST-2MRPCCoLAQNLIQQPRTESTS-BAvg.\nbase\nFT100%87.394.487.962.492.591.778.390.685.6\nAdapter\n∗\n0.318%87.093.388.460.992.590.576.590.585.0\nLoRA\n∗\n0.239%86.693.988.759.792.690.475.390.384.7\nAdapter\nFNN∗\n0.239%87.193.088.858.592.090.277.790.484.7\nBitFit\n∗\n0.080%84.794.088.054.091.087.369.889.582.3\nRED\n∗\n0.016%83.993.989.261.090.787.278.090.484.3\nLoReFT (ours)0.015%83.193.489.260.491.287.479.090.084.2\nlarge\nFT100%88.896.091.768.293.891.585.892.688.6\nAdapter\n∗\n0.254%90.195.290.565.494.691.485.391.588.0\nLoRA\n∗\n0.225%90.296.089.865.594.790.786.391.788.1\nAdapter\nFNN∗\n0.225%90.396.190.564.494.391.384.890.287.7\nRED\n∗\n0.014%89.596.090.368.193.588.886.291.388.0\nLoReFT (ours)0.014%89.296.290.168.094.188.587.591.688.2\nTable 4: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods\non the GLUE benchmark.\n∗\nPerformance results of all baseline methods are taken from Wu et al.\n[2024a]. We report averaged performance of five runs with distinct random seeds for our method. #\nParam. (%) is calculated by dividing the number of trainable parameters (excluding the number of\nparameters of the classification head) with the number of parameter of the base LM.\n4.5    Natural language understanding\nWe evaluate LoReFT on the GLUE benchmark [Wang et al., 2018] against existing PEFTs. We use\nthis set of experiments to show LoReFT works well even with small-scale LMs, and can improve\nrepresentations for classification tasks and not just text generation.  We finetune RoBERTa-base\n(125M) as well as RoBERTa-large (350M) on GLUE, a sequence classification benchmark for natural\n4\nWe release our ReFT weights (<1MB) of our instruction-tuned model through HuggingFace and provide a\ntutorial athttps://github.com/stanfordnlp/pyreft/blob/main/examples/chat.\n9",
    "language understanding (NLU) which covers domains such as sentiment classification and natural\nlanguage inference.  Details about the GLUE benchmark can be found in its original paper.  We\nfollow Wu et al. [2024a] for proper evaluation on GLUE validation set: we split the validation set into\ntwo sets guarded by a random seed, and we pick the best model with highest in-training validation\naccuracy to evaluate on the other held-out half for testing accuracy.\nHyperparameter tuning.We tune our hyperparameters for each task separately. which is standard\nfor  PEFTs.   To  avoid  overfitting  to  random  seeds,  we  hyperparameter-tune  our  models  with  a\nconstant seed, and report averaged results over that and four additional unseen seeds. We describe\nhyperparameter tuning experiments in Appendix C.\nResults.We report results in table 4. LoReFT obtains comparable performance with PEFT methods\non both model sizes when parameter matched with RED, the previous most parameter-efficient PEFT\nfor this task. Full results with standard deviation is in table 10.\n5pyreft: A ReFT-native Python Library\nTo lower the cost of switching from PEFTs to ReFT, we releasepyreft, a Python library made for\ntraining and sharing ReFTs. Our library is built on top ofpyvene[Wu et al., 2024b], a library for\nperforming and training activation interventions on arbitrary PyTorch models. We publish our library\non PyPI.\n5\nAny pretrained LM available on HuggingFace is supported throughpyreftfor finetuning\nwith ReFT methods, and finetuned models can be easily uploaded to HuggingFace. The following\nexample shows steps to wrap a Llama-2 7B model with a single intervention on the residual stream\noutput of the 19-th layer:\nimport  torch\nimport  transformers\nfrom  pyreft  import (\nget_reft_model ,\nReftConfig ,\nLoreftIntervention ,\nReftTrainerForCausalLM\n)\n# loading  huggingface  model\nmodel_name_or_path = \"yahma/llama -7b-hf\"\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\nmodel_name_or_path , torch_dtype=torch.bfloat16 , device_map=\"cuda\")\n# wrap  the  model  with rank -1  constant  reft\nreft_config = ReftConfig(representations ={\n\"layer\": 19, \"component\": \"block_output\",\n\"intervention\": LoreftIntervention(\nembed_dim=model.config.hidden_size , low_rank_dimension =1)})\nreft_model = get_reft_model(model , reft_config)\nreft_model.print_trainable_parameters ()\nThe wrapped model can be trained for downstream tasks. We also provide data loading helpers to\nconstruct training data that is compatible with HuggingFace trainers:\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n# get  training  data  with  customized  dataloaders\ndata_module = make_supervised_data_module(\ntokenizer=tokenizer , model=model , layers =[19] ,\ntraining_args=training_args , data_args=data_args)\n# train\ntrainer = reft.ReftTrainerForCausalLM(\nmodel=reft_model , tokenizer=tokenizer , args=training_args , ** data_module)\ntrainer.train ()\ntrainer.save_model(output_dir=training_args.output_dir)\n5\npip install pyreft\n10",
    "6    Conclusion\nIn this paper, we propose a strong alternative to PEFTs, LoReFT. LoReFT achieves strong per-\nformance across benchmarks from four domains while being 10×–50×more efficient than prior\nstate-of-the-art PEFTs. Notably, LoReFT establishes new state-of-the-art performance on common-\nsense reasoning, instruction-following, and natural language understanding against the strongest\nPEFTs. We also show how our method can be described under a generic framework — ReFT. ReFT is\na new approach to finetuning that is more powerful, more parameter-efficient, and more interpretable\nthan any existing PEFTs. We hope our work serves as an initial call for the community to study ReFTs.\nWe also hope to explore why ReFT works, and we provide some of our early explorations in our\nsupplementary materials, focusing on memorisation (appendix E and appendix F) and compositional\nmerging of ReFT weights (appendix G).\n7    Limitations and future work\nMore diverse models.Due to limited time and resources, we mainly explored the LLaMA-family\nof models. In future work, we hope to explore the effectiveness of ReFT on other model families\n(e.g. Mistral-7B or GPT-2). It would be also be worth trying ReFT on vision-language models such\nas LLaVA [Liu et al., 2024a].\nAdditional design considerations.The capabilities of ReFT have not yet been fully explored due to\nthe large hyperparameter search space (e.g. which position to intervene on), and we are interested in\nautomating this search. In particular, it would be rewarding to investigate more effective interventions\nfor arithmetic reasoning tasks — one idea we have considered is scheduling interventions during\ngeneration, which may improve performance at the expense of a larger inference-time computation\nburden. In addition, we have not fully explored the power of orthogonality of our learned subspace\nyet. We are currently investigating whether learned orthogonal subspaces can be composed together\nwithout adaptation. Some encouraging initial findings are reported in appendix G.\nReFT, abstraction, and generation.Neural network interpretability research often struggles to\ncontribute directly to improving models.  With ReFT, we have shown one way to overcome this\nchallenge. The ReFT framework is rooted in work on causal abstraction [Geiger et al., 2023a] for\nmodel interpretability, and LoReFT builds directly on the distributed interchange intervention method\nof Geiger et al. [2023b] and Wu et al. [2023]. See also the interchange intervention training (IIT)\nmethod of Geiger et al. [2022], Wu et al. [2022], Huang et al. [2023c]. In a similar vein, recent work\nalso uses representation-based editing of the Transformer stream to steer model behavior [Li et al.,\n2024, Zou et al., 2023]. ReFT advances this line of work by showing one way that such steering can\nbe learned, rather than being merely a post hoc analysis step.\nThe precise ways in which ReFT works deserve deeper exploration. Although these methods intervene\non representations, the causal effect of such interventions may only emerge in the model’s upstream\ncomputations. In other words, the power of ReFT may come from the fact that it creates new causal\npathways or modifies the strength of some existing ones. We leave it to future research to track these\neffects, and perhaps to explore more structured ReFTs to modify complex causal pathways in LMs.\nReFT and model interpretability.ReFT relies on insights from work on interpretability, and it may\nalso be able to contribute insights back to that field. In particular, LoReFT shows that training a set\nof low-rank interventions on selected residual streams can induce a base LM to follow instructions\n(section 4.4). In other words, a single set of neurons, when intervened on in a particular way, can\nachieve generalised control over a vast number of tasks. This is a serious challenge to work seeing\nto interpret individual neurons in isolation (for related criticisms, see Huang et al. 2023b).  The\nsuccess of ReFT suggests to us a quite different approach to interperetability, one that starts from the\nassumption that neurons will play different roles in different contexts.\nEvaluation practices in PEFT research.In this work, we hyperparameter-tune ReFT on develop-\nment sets that do not overlap with the test set. Unfortunately, a considerable portion of the literature\non PEFTs directly hill-climbs performance on test sets. This results in overfitting to specific tasks,\nwhich gives practitioners less certainty about the real-world performance of different methods and\nimpedes fair comparison. We hope that future work can introduce benchmarks for evaluating PEFTs\nand ReFTs. These should allow for compute- or time-matched hyperparameter-tuning comparisons,\nand they should disallow any kind of tuning or model selection based on the test set.\n11",
    "Acknowledgements\nWe thank Jing Huang for helpful discussion in designing our memorisation tests as well as writing.\nWe thank Chenglei Si, Harshit Joshi, Jordan Juravsky, Julie Kallini, Ken Liu, Rohan Pandey, Jiuding\nSun, Leonard Tang, Tristan Thrush, Shengguang Wu, Qinan Yu, Yanzhe Zhang, Amir Zur, and Shiqi\nChen for helpful discussion about the project and comments on the manuscript.\nReferences\nAfra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell.  Naturalistic causal probing for\nmorpho-syntax.Transactions of the Association for Computational Linguistics, 11:384–403, 2023.\ndoi: 10.1162/tacl_a_00554. URLhttps://aclanthology.org/2023.tacl-1.23.\nAryaman Arora, Dan Jurafsky, and Christopher Potts.  CausalGym:  Benchmarking causal inter-\npretability methods on linguistic tasks.arXiv:2402.12560, 2024. URLhttps://arxiv.org/abs/\n2402.12560.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.  PIQA: Reasoning about physical\ncommonsense in natural language. InProceedings of the AAAI Conference on Artificial Intelligence,\nvolume 34, pages 7432–7439, 2020. URLhttps://arxiv.org/abs/1911.11641.\nLewis Carroll.Alice’s Adventures in Wonderland. Macmillan, London, 1865.\nAngelica Chen,  Ravid Schwartz-Ziv,  Kyunghyun Cho,  Matthew L. Leavitt,  and Naomi Saphra.\nSudden drops in the loss:  Syntax acquisition, phase transitions, and simplicity bias in MLMs.\narXiv:2309.07311, 2023. URLhttps://arxiv.org/abs/2309.07311v4.\nAbhijith  Chintam,  Rahel  Beloch,  Willem  Zuidema,  Michael  Hanna,  and  Oskar  van  der  Wal.\nIdentifying  and  adapting  transformer-components  responsible  for  gender  bias  in  an  English\nlanguage model.   In Yonatan Belinkov,  Sophie Hao,  Jaap Jumelet,  Najoung Kim,  Arya Mc-\nCarthy,  and Hosein Mohebbi,  editors,Proceedings of the 6th BlackboxNLP Workshop:  Ana-\nlyzing and Interpreting Neural Networks for NLP, pages 379–394, Singapore, December 2023.\nAssociation for Computational Linguistics.   doi:  10.18653/v1/2023.blackboxnlp-1.29.   URL\nhttps://aclanthology.org/2023.blackboxnlp-1.29.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova.    BoolQ:  Exploring  the  surprising  difficulty  of  natural  yes/no  questions.    In  Jill\nBurstein,  Christy  Doran,  and  Thamar  Solorio,  editors,Proceedings  of  the  2019  Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL\nhttps://aclanthology.org/N19-1300.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord.   Think you have solved question answering?   Try ARC, the AI2 reasoning\nchallenge.arXiv:1803.05457, 2018. URLhttps://arxiv.org/abs/1803.05457.\nKarl Cobbe,  Vineet Kosaraju,  Mohammad Bavarian,  Mark Chen,  Heewoo Jun,  Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems.arXiv:2110.14168, 2021. URLhttps://arxiv.org/abs/2110.14168.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan\nLiu, and Maosong Sun.  UltraFeedback: Boosting language models with high-quality feedback.\narXiv:2310.01377, 2023. URLhttps://arxiv.org/abs/2310.01377.\nAndrew M. Dai and Quoc V. Le.   Semi-supervised sequence learning.   InAdvances in Neural\nInformation  Processing  Systems,  volume  28.  Curran  Associates,  Inc.,  2015.   URLhttps://\nproceedings.neurips.cc/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning\nof quantized LLMs.   36:10088–10115,  2023.   URLhttps://proceedings.neurips.cc/paper_\nfiles/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html.\n12",
    "Jacob  Devlin,  Ming-Wei  Chang,  Kenton  Lee,  and  Kristina  Toutanova.   BERT:  Pre-training  of\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\nThamar Solorio, editors,Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics:  Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics.  doi: 10.18653/v1/N19-1423.  URLhttps://aclanthology.org/\nN19-1423.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen\nZhou.  Enhancing chat language models by scaling high-quality instructional conversations.  In\nHouda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pages 3029–3051, Singapore, December\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.183. URL\nhttps://aclanthology.org/2023.emnlp-main.183.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish,\nJared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo-\nsition.Transformer Circuits Thread, 2022. URLhttps://transformer-circuits.pub/2022/toy_\nmodel/index.html.\nStanislav Fort.  Scaling laws for adversarial attacks on language model activations, 2023.  URL\nhttp://arxiv.org/abs/2312.02780.\nCheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, and Jishen Zhao.  Learn-to-Share:  A\nhardware-friendly transfer learning framework exploiting computation and parameter sharing.\nIn Marina Meila and Tong Zhang, editors,Proceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 ofProceedings of\nMachine Learning Research, pages 3469–3479. PMLR, 2021.  URLhttp://proceedings.mlr.\npress/v139/fu21a.html.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,\nConghui He, Xiangyu Yue, et al. LLaMA-Adapter v2: Parameter-efficient visual instruction model.\narXiv:2304.15010, 2023. URLhttps://arxiv.org/abs/2304.15010.\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts.  Causal abstractions of neural\nnetworks.  In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan,\neditors,Advances in Neural Information Processing Systems, volume 34, pages 9574–9586. Curran\nAssociates, Inc., 2021.  URLhttps://proceedings.neurips.cc/paper_files/paper/2021/file/\n4f5c422f4d49a5a807eda27434231040-Paper.pdf.\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good-\nman,  and Christopher Potts.   Inducing causal structure for interpretable neural networks.   In\nKamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato,\neditors,Proceedings of the 39th International Conference on Machine Learning, volume 162 of\nProceedings of Machine Learning Research, pages 7324–7338. PMLR, 17–23 Jul 2022.  URL\nhttps://proceedings.mlr.press/v162/geiger22a.html.\nAtticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model interpretation.\narXiv:2301.04709, 2023a. URLhttps://arxiv.org/abs/2301.04709.\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman.  Find-\ning  alignments  between  interpretable  causal  variables  and  distributed  neural  representations.\narXiv:2303.02536, 2023b. URLhttps://arxiv.org/abs/2303.02536.\nClément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, and Ryan Cotterell.  A geometric\nnotion of causal probing.arXiv:2307.15054, 2023. URLhttps://arxiv.org/abs/2307.15054.\nZeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. Parameter-efficient fine-tuning for large\nmodels: A comprehensive survey.arXiv:2403.14608, 2024. URLhttps://arxiv.org/abs/2403.\n14608.\n13",
    "Michael Hanna, Yonatan Belinkov, and Sandro Pezzelle. When language models fall in love: Animacy\nprocessing in transformer language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\npages 12120–12135, Singapore, December 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.744. URLhttps://aclanthology.org/2023.emnlp-main.744.\nSophie Hao and Tal Linzen. Verb conjugation in transformers is determined by linear encodings of\nsubject number. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association\nfor Computational Linguistics:  EMNLP 2023, pages 4531–4539, Singapore, December 2023.\nAssociation for Computational Linguistics.  doi:  10.18653/v1/2023.findings-emnlp.300.  URL\nhttps://aclanthology.org/2023.findings-emnlp.300.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\na unified view of parameter-efficient transfer learning.  InThe Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event, 2022a. URLhttps://openreview.net/\nforum?id=0RDcd5Axok.\nShwai  He,  Liang  Ding,  Daize  Dong,  Jeremy  Zhang,  and  Dacheng  Tao.    SparseAdapter:   An\neasy approach for improving the parameter-efficiency of adapters.  In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang, editors,Findings of the Association for Computational Linguistics:\nEMNLP  2022,  pages  2184–2190,  Abu  Dhabi,  United  Arab  Emirates,  December  2022b.  As-\nsociation  for  Computational  Linguistics.   doi:  10.18653/v1/2022.findings-emnlp.160.   URL\nhttps://aclanthology.org/2022.findings-emnlp.160.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman.  Learning to\nsolve arithmetic word problems with verb categorization.  In Alessandro Moschitti, Bo Pang,\nand Walter Daelemans, editors,Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 523–533, Doha, Qatar, October 2014. Association\nfor Computational Linguistics.  doi:  10.3115/v1/D14-1058.  URLhttps://aclanthology.org/\nD14-1058.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In\nKamalika Chaudhuri and Ruslan Salakhutdinov, editors,Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,\nvolume 97 ofProceedings of Machine Learning Research, pages 2790–2799. PMLR, 2019. URL\nhttp://proceedings.mlr.press/v97/houlsby19a.html.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.   LoRA: Low-rank adaptation of large language models.   InThe Tenth In-\nternational Conference on Learning Representations, ICLR 2022, Virtual Event, 2022.   URL\nhttps://openreview.net/forum?id=nZeVKeeFYf9.\nZhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya\nPoria, and Roy Lee. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large\nlanguage models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, pages 5254–5276, Singapore,\nDecember 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.\n319. URLhttps://aclanthology.org/2023.emnlp-main.319.\nChengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.  Lorahub:\nEfficient cross-task generalization via dynamic lora composition.arXiv preprint arXiv:2307.13269,\n2023a.\nJing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigorously\nassessing natural language explanations of neurons.   In Yonatan Belinkov, Sophie Hao, Jaap\nJumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors,Proceedings of the 6th\nBlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 317–331,\nSingapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nblackboxnlp-1.24. URLhttps://aclanthology.org/2023.blackboxnlp-1.24.\n14",
    "Jing Huang,  Zhengxuan Wu,  Kyle Mahowald,  and Christopher Potts.   Inducing character-level\nstructure in subword-based language models with type-level interchange intervention training.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,Findings of the Association\nfor Computational Linguistics:  ACL 2023, pages 12163–12180, Toronto, Canada, July 2023c.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.770. URLhttps:\n//aclanthology.org/2023.findings-acl.770.\nJing Huang, Christopher Potts Zhengxuan Wu, Mor Geva, and Atticus Geiger. RAVEL: Evaluating\ninterpretability methods on disentangling language model representations.arXiv:2402.17700,\n2024. URLhttps://arxiv.org/abs/2402.17700.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang.\nParsing algebraic word problems into equations.Transactions of the Association for Computational\nLinguistics, 3:585–597, 2015.   doi:  10.1162/tacl_a_00160.   URLhttps://aclanthology.org/\nQ15-1042.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS:\nA math word problem repository.  In Kevin Knight, Ani Nenkova, and Owen Rambow, editors,\nProceedings of the 2016 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, pages 1152–1157, San Diego, Califor-\nnia, June 2016. Association for Computational Linguistics.  doi: 10.18653/v1/N16-1136.  URL\nhttps://aclanthology.org/N16-1136.\nKarim  Lasri,  Tiago  Pimentel,  Alessandro  Lenci,  Thierry  Poibeau,  and  Ryan  Cotterell.    Prob-\ning for the usage of grammatical number.   In Smaranda Muresan,  Preslav Nakov,  and Aline\nVillavicencio,  editors,Proceedings  of  the  60th  Annual  Meeting  of  the  Association  for  Com-\nputational  Linguistics  (Volume  1:   Long  Papers),  pages  8818–8831,  Dublin,  Ireland,  May\n2022. Association for Computational Linguistics.  doi:  10.18653/v1/2022.acl-long.603.  URL\nhttps://aclanthology.org/2022.acl-long.603.\nMichael A. Lepori, Thomas Serre, and Ellie Pavlick. Uncovering causal variables in transformers\nusing circuit probing.arXiv:2311.04354, 2023. URLhttps://arxiv.org/abs/2311.04354.\nHector Levesque, Ernest Davis, and Leora Morgenstern.  The Winograd Schema Challenge.  In\nProceedings of the Thirteenth International Conference on Principles of Knowledge Representation\nand Reasoning, 2012. URLhttps://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf.\nKenneth    Li,Oam    Patel,Fernanda    Viégas,Hanspeter    Pfister,and    Martin    Watten-\nberg.Inference-time  intervention:Eliciting  truthful  answers  from  a  language  model.\n36,2024.URLhttps://proceedings.neurips.cc/paper_files/paper/2023/hash/\n81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke\nZettlemoyer.  Branch-train-merge: Embarrassingly parallel training of expert language models.\narXiv preprint arXiv:2208.03306, 2022.\nXiang Lisa Li and Percy Liang.   Prefix-tuning:  Optimizing continuous prompts for generation.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online,\nAugust 2021. Association for Computational Linguistics.  doi: 10.18653/v1/2021.acl-long.353.\nURLhttps://aclanthology.org/2021.acl-long.353.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instruction-following\nmodels.https://github.com/tatsu-lab/alpaca_eval, 2023.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener-\nation: Learning to solve and explain algebraic word problems.arXiv:1705.04146, 2017.  URL\nhttps://arxiv.org/abs/1705.04146.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.Advances in\nNeural Information Processing Systems, 36, 2024a. URLhttps://arxiv.org/abs/2304.08485.\n15",
    "Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-\nTing Cheng, and Min-Hung Chen. DoRA: Weight-decomposed low-rank adaptation.arXiv preprint\narXiv:2402.09353, 2024b. URLhttps://arxiv.org/abs/2402.09353.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov.  RoBERTa:  A robustly optimized BERT pretraining\napproach.arXiv:1907.11692, 2019. URLhttps://arxiv.org/abs/1907.11692.\nAleksandar Makelov,  Georg Lange,  Atticus Geiger,  and Neel Nanda.   Is this the subspace you\nare looking for?  An interpretability illusion for subspace activation patching.  InThe Twelfth\nInternational  Conference  on  Learning  Representations,  Vienna,  Austria,  2024.   URLhttps:\n//arxiv.org/abs/2311.17030.\nJames L. McClelland, David E. Rumelhart, and PDP Research Group.Parallel Distributed Processing:\nExplorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models.\nMIT Press, 1986.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.  Can a suit of armor conduct\nelectricity?  a new dataset for open book question answering.arXiv:1809.02789, 2018.  URL\nhttps://arxiv.org/abs/1809.02789.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word\nrepresentations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors,Proceedings\nof the 2013 Conference of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June 2013. Association\nfor Computational Linguistics. URLhttps://aclanthology.org/N13-1090.\nNeel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models\nof self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim,\nArya McCarthy, and Hosein Mohebbi, editors,Proceedings of the 6th BlackboxNLP Workshop:\nAnalyzing and Interpreting Neural Networks for NLP, pages 16–30, Singapore, December 2023.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URLhttps:\n//aclanthology.org/2023.blackboxnlp-1.2.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.Advances in Neural Information Processing Systems,  35:\n27730–27744, 2022. URLhttps://arxiv.org/abs/2203.02155.\nKiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry\nof large language models.arXiv:2311.03658, 2023. URLhttps://arxiv.org/abs/2311.03658.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.   Are NLP models really able to solve sim-\nple math word problems?   In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\nHakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou,\neditors,Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 2080–2094, Online, June\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL\nhttps://aclanthology.org/2021.naacl-main.168.\nJonas Pfeiffer,  Ivan Vuli\n ́\nc,  Iryna Gurevych,  and Sebastian Ruder.   MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer. In Bonnie Webber, Trevor Cohn, Yulan He,\nand Yang Liu, editors,Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7654–7673, Online, November 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.emnlp-main.617. URLhttps://aclanthology.\norg/2020.emnlp-main.617.\nSubhro Roy and Dan Roth.  Solving general arithmetic word problems.  In Lluís Màrquez, Chris\nCallison-Burch, and Jian Su, editors,Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 1743–1752, Lisbon, Portugal, September 2015. Association\nfor Computational Linguistics.  doi: 10.18653/v1/D15-1202.  URLhttps://aclanthology.org/\nD15-1202.\n16",
    "David E. Rumelhart, James L. McClelland, and PDP Research Group.Parallel Distributed Processing:\nExplorations in the Microstructure of Cognition, volume 1: Foundations. MIT Press, 1986.\nKeisuke Sakaguchi,  Ronan Le Bras,  Chandra Bhagavatula,  and Yejin Choi.   WinoGrande:  An\nadversarial Winograd Schema Challenge at scale.Communications of the ACM, 64(9):99–106,\n2021. URLhttps://arxiv.org/abs/1907.10641.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Common-\nsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\nWan, editors,Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 4463–4473, Hong Kong, China, November 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/D19-1454. URLhttps://aclanthology.org/D19-1454.\nShuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang, Haizhen Huang, Furu Wei,\nWeiwei Deng, Feng Sun, and Qi Zhang. ResLoRA: Identity residual mapping in low-rank adaption.\narXiv:2402.18039, 2024. URLhttps://arxiv.org/abs/2402.18039.\nPaul Smolensky.   Neural and conceptual interpretation of PDP models.   InParallel Distributed\nProcessing:  Explorations  in  the  Microstructure  of  Cognition,  volume  2:  Psychological  and\nBiological Models, pages 390–431. MIT Press/Bradford Books, Cambridge, MA, 1986.\nNishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting latent steering vectors from\npretrained language models.arXiv:2205.05124, 2022. URLhttps://arxiv.org/abs/2205.05124.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nCurt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear representations of\nsentiment in large language models.arXiv:2310.15154, 2023. URLhttps://arxiv.org/abs/2310.\n15154.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language\nmodels.arXiv:2302.13971, 2023. URLhttps://arxiv.org/abs/2302.13971.\nAlex Turner,  Lisa Thiergart,  David Udell,  Gavin Leech,  Ulisse Mini,  and Monte MacDiarmid.\nActivation addition: Steering language models without optimization.arXiv:2308.10248, 2023.\nURLhttps://arxiv.org/abs/2308.10248.\nAshish Vaswani,  Noam Shazeer,  Niki Parmar,  Jakob Uszkoreit,  Llion Jones,  Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin.   Attention is all you need.   In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural\nInformation Processing Systems, volume 30, pages 5998–6008. Curran Associates, Inc., 2017.\nURLhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\nTheia Vogel. repeng, 2024. URLhttps://github.com/vgel/repeng/.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen,\nGrzegorz Chrupała, and Afra Alishahi, editors,Proceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium,\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL\nhttps://aclanthology.org/W18-5446.\nKevin  Ro  Wang,  Alexandre  Variengien,  Arthur  Conmy,  Buck  Shlegeris,  and  Jacob  Steinhardt.\nInterpretability in the wild:  a circuit for indirect object identification in GPT-2 small.  InThe\nEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,\n2023. URLhttps://openreview.net/pdf?id=NpsVSN6o4ul.\n17",
    "Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan\nAwadallah, and Jianfeng Gao.  AdaMix:  Mixture-of-adaptations for parameter-efficient model\ntuning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing,  pages 5744–5760,  Abu\nDhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.emnlp-main.388. URLhttps://aclanthology.org/2022.emnlp-main.388.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in\nNeural Information Processing Systems, 35:24824–24837, 2022. URLhttps://arxiv.org/abs/\n2201.11903.\nMuling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu,\nCenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine-\ntuning via representation editing.arXiv:2402.15179, 2024a. URLhttps://arxiv.org/abs/2402.\n15179.\nZhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christo-\npher Potts, and Noah Goodman.  Causal distillation for language models.  In Marine Carpuat,\nMarie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors,Proceedings of the 2022\nConference  of  the  North  American  Chapter  of  the  Association  for  Computational  Linguis-\ntics:   Human  Language  Technologies,  pages  4288–4295,  Seattle,  United  States,  July  2022.\nAssociation  for  Computational  Linguistics.    doi:   10.18653/v1/2022.naacl-main.318.    URL\nhttps://aclanthology.org/2022.naacl-main.318.\nZhengxuan Wu,  Atticus Geiger,  Christopher Potts,  and Noah D. Goodman.   Interpretability at\nscale: Identifying causal mechanisms in Alpaca. InAdvances in Neural Information Processing\nSystems,  volume  36,  2023.   URLhttps://papers.neurips.cc/paper_files/paper/2023/file/\nf6a8b109d4d4fd64c75e94aaf85d9697-Paper-Conference.pdf.\nZhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman,\nChristopher D. Manning, and Christopher Potts. pyvene: A library for understanding and improving\nPyTorch models via interventions. InarXiv:2403.07809, 2024b. URLhttps://arxiv.org/abs/\n2403.07809.\nZhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts,\nand Noah D. Goodman. A reply to Makelov et al.(2023)’s “interpretability illusion” arguments.\narXiv:2401.12631, 2024c. URLhttps://arxiv.org/abs/2401.12631.\nTakateru  Yamakoshi,  James  McClelland,  Adele  Goldberg,  and  Robert  Hawkins.Causal  in-\nterventions  expose  implicit  situation  models  for  commonsense  language  understanding.    In\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,Findings of the Association\nfor Computational Linguistics:  ACL 2023, pages 13265–13293, Toronto, Canada, July 2023.\nAssociation  for  Computational  Linguistics.    doi:   10.18653/v1/2023.findings-acl.839.    URL\nhttps://aclanthology.org/2023.findings-acl.839.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine\nreally finish your sentence?arXiv:1905.07830, 2019. URLhttps://arxiv.org/abs/1905.07830.\nFeiyu  Zhang,  Liangzhi  Li,  Junhao  Chen,  Zhouqiang  Jiang,  Bowen  Wang,  and  Yiming  Qian.\nIncreLoRA:  Incremental  parameter  allocation  method  for  parameter-efficient  fine-tuning.\narXiv:2308.12043, 2023. URLhttps://arxiv.org/abs/2308.12043.\nJinghan  Zhang,  Junteng  Liu,  Junxian  He,  et  al.   Composing  parameter-efficient  modules  with\narithmetic operation.Advances in Neural Information Processing Systems, 36, 2024a.\nRenrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLLaMA-Adapter: Efficient fine-tuning of large language models with zero-initialized attention. In\nThe Twelfth International Conference on Learning Representations, Vienna, Austria, 2024b. URL\nhttps://openreview.net/forum?id=d4UiXAHN2W.\nRuiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, and Pengtao Xie.  AutoLoRA: Automatically\ntuning matrix ranks in low-rank adaptation based on meta learning.arXiv:2403.09113, 2024c.\nURLhttps://arxiv.org/abs/2403.09113.\n18",
    "Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu,\nJiawei Han, and Weizhu Chen.  Multi-lora composition for image generation.arXiv preprint\narXiv:2402.16843, 2024.\nYun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Ca-\nnoee Liu, Liangchen Luo, Jindong Chen, et al.  SiRa:  Sparse mixture of low rank adaptation.\narXiv:2311.09179, 2023. URLhttps://arxiv.org/abs/2311.09179.\nZhou Ziheng, Yingnian Wu, Song-Chun Zhu, and Demetri Terzopoulos. Aligner: One global token\nis worth millions of parameters when aligning large language models.arXiv:2312.05503, 2023.\nURLhttps://arxiv.org/abs/2312.05503.\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan,\nXuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A\ntop-down approach to AI transparency.arXiv:2310.01405, 2023. URLhttps://arxiv.org/abs/\n2310.01405.\n19",
    "A    Describing existing methods in the ReFT framework\nTo show the expressivity of the ReFT framework, we cast existing representing-editing methods in\nthe literature into ReFTs.\nGeneral comments about expressivity of ReFT.Given that previous works have unified PEFTs\nunder a single framework [He et al., 2022a], one may askwhy not express ReFT as a PEFT\nmethod?First of all, PEFT frameworks lacks the notion oftimeorsequence(see the unified PEFT\nview provided in Table 1 on pg. 5 of He et al. [2022a]). In PEFT, representation modifications are\napplied toeverytoken in the sequence, even in recent variants such as AdaLoRA [Zhang et al.,\n2023]. A key aspect of ReFT is that it leverages representations over time and intervenes only on\na small number of them while being effective. More importantly, the notation of time is important\nfor future versions of ReFT that intervene on representationsschematically(e.g. intervene on the\nfirst token at some early layers and then intervene on the last token at some later layers). The ability\nto intervene at different layer and position locations schematically is also implemented in our code.\nExisting popular PEFT libraries\n6\nenforceweight-basedupdates without conveniently supporting\nflexible representation-based interventions.\nA.1    RED\nRED [Wu et al., 2024a] is a simple representation-editing method that applies an element-wise scaling\ntransforms∈R\nn\nand adds a biasb∈R\nn\nto the hidden representation in every layer.  The same\nintervention is applied to every position (including at generated tokens, increasing inference burden)\nbut separate interventions are learned at each layer. In the ReFT framework, RED is defined as\nΦ\nRED\n(h)=s×h+b(10)\nI\nRED\n={⟨Φ\nRED\n,{1,...,n},l⟩ ∣l∈{1,...,m}}(11)\nThe parametersφ\nRED\n={s,b}are learned with gradient descent to minimise a loss function such as\nlanguage-modelling loss or a classification loss, as in our experiments with LoReFT. We believe that\nRED is better classified as a kind of adapter due to its application at all positions.\nA.2    Activation addition\nActivation addition [Turner et al., 2023] takes the difference in activations at at some positionspand\nqand layerlgiven two contrastive promptsx\n+\nandx\n−\nas input. It then adds this difference vector,\nscaled by a tuned constantc, to representations at all positions in layerlfor some new prompt.\na=h(x\n+\n)\n(l)\np\n−h(x\n−\n)\n(l)\nq\n(12)\nΦ\nActAdd\n(h)=h+c⋅a(13)\nI\nActAdd\n={⟨φ\nActAdd\n,{1,...,n},l⟩}(14)\nA.3    RepE\nZou et al. [2023] introduce several intervention methods for controlling model behaviour, which they\ntermrepresentation engineering.\nFirst, given a set of prompts{x\n1\n,...,x\nn\n}designed to elicit the presence of a concept, we randomly\npair them, take the difference in activations for each pair, and find the first principle component of the\ndifference vectors at the last token position in some layer of interestlto obtain areading vector:\na\nreading\n=PCA({h(x\ni\n)\n(l)\n−1\n−h(x\ni+1\n)\n(l)\n−1\n∣i≡0 mod 2})\n1\n(15)\nOne can also used a more structured pairing of constrastive prompts to obtain acontrast vector,\nsimilar to the difference vector computed in activation addition:\na\ncontrast\n=PCA({h(x\n+\ni\n)\n(l)\n−1\n−h(x\n−\ni\n)\n(l)\n−1\n∣1≤i≤n})\n1\n(16)\n6\nSeehttps://github.com/huggingface/peft.\n20",
    "Then, using eithera\nreading\nora\ncontrast\n, RepE introduces three operators (i.e. parametrisations ofΦ)\nfor intervening on activations:\nΦ\nRepE,linear\n(h)=h±c⋅a(17)\nΦ\nRepE,piecewise\n(h)=h+c⋅sign(a⋅h)⋅a(18)\nΦ\nRepE,projection\n(h)=h−c⋅\na⋅h\n∥a∥\n2\n⋅a(19)\nThe first two of these are similar to activation addition, while the latter is a scaled one-dimensional\ndistributed interchange intervention that is a special case of LoReFT. These operations are then used\nto intervene on some set of positionsP⊆{1,...,n}in the layer of interest:\nI\nRepE\n={⟨Φ\nRepE\n,P,l⟩}(20)\nRepE  introduces  another  model  control  method  called  Low-Rank  Representation  Adaptation\n(LoRRA), which is a kind of PEFT rather than a ReFT since it tunes modelweightsusing a variant of\nLoRA.\nB    Datasets\nB.1    Commonsense reasoning\nWe train and evaluate our models on eight datasets covering different domains of open-ended QA\ntasks:\n1.\nTheBoolQ[Clark et al., 2019] dataset, which is a question-answering dataset for yes or\nno naturally occurring questions. We remove the provided passage in the dataset following\nprevious works to ensure a fair comparison.\n2.ThePIQA[Bisk et al., 2020] dataset, which tests physical commonsense reasoning and\nrequires the model to choose one of the provided actions to take based on a hypothesized\nscenario.\n3.\nTheSIQA[Sap et al., 2019] dataset, which focus on reasoning about people’s actions and\ntheir corresponding social consequences.\n4.TheHellaSwag[Zellers et al., 2019] dataset, which asks the model to choose an appropriate\nending (or sentence completion) given a context.\n5.TheWinoGrande[Sakaguchi et al., 2021] dataset, inspired by Winograd Schema Chal-\nlenge [Levesque et al., 2012], asks the model to fill-in-a-blank with binary options given a\nsentence which requires commonsense reasoning.\n6.The ARC Easy set (ARC-e[Clark et al., 2018]), which includes genuine grade-school level\nmultiple-choice science questions\n7.The ARC Challenge set (ARC-c) [Clark et al., 2018]), which is likeARC-ebut designed in\na way that co-occurrence methods are expected to fail to answer correctly.\n8.TheOBQAdataset [Mihaylov et al., 2018], which is a knowledge-intensive and open-book\nQA dataset that requires multi-hop reasoning.  Dataset statistics and simplified training\nexamples from each dataset are provided in Hu et al. [2023].\nDataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023].\nWe replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training\ndataset (COMMONSENSE170K) of the tasks mentioned above, and evaluate on their individual test\nset.\nB.2    Arithmetic reasoning\nWe train and evaluate with seven datasets covering different domains of math world problems:\n1.\nTheAddSub[Hosseini et al., 2014] dataset, which involves solving arithmetic word prob-\nlems that include addition and subtraction.\n21",
    "2.TheAQuA[Ling  et  al.,  2017]  dataset,  which  formulates  algebraic  word  problems  as\nmultiple-choice problems.\n3.\nTheGSM8K[Cobbe et al., 2021] dataset,  which consists of grade-school math word\nproblems that require multi-step reasoning.\n4.TheMAWPS[Koncel-Kedziorski et al., 2016] dataset, which contains math word problem\nwith varying complexity.\n5.\nTheMultiArith[Roy and Roth, 2015] dataset, which contains multi-step arithmetic prob-\nlems.\n6.TheSingleEq[Koncel-Kedziorski et al., 2015] dataset, which has grade-school math word\nproblems that map to single equations with different length.\n7.\nTheSVAMP[Patel et al., 2021] dataset, which enhances the original Math World Prob-\nlem (MWP) challenge by requiring robust reasoning ability that is invariant to structural\nalternations of the posing problem.\nDataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023].\nWe replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined\ntraining dataset (MATH10K) of four tasks mentioned above: GSM8K, MAWPS, MAWPS-single\nand AQuA. Different from Hu et al. [2023], selected tasks are excluded for testing since the original\npaper accidentally leaks testing examples from these tasks into the training set, affecting AddSub,\nMultiArith and SingleEq. They are included in the MAWPS training dataset, and thus leaked into the\ntraining dataset.\nC    Hyperparameter tuning and decoding strategy\nCommonsense reasoning and arithmeric reasoning.We create a standalone development set by\ntaking the last 300 examples from theGSM8Ktraining set. We train our models with the remaining\ntraining set ofGSM8Kand select the hyperparameter settings based on model performance on the\ndevelopment set. We select the hyperparameters using LLaMA-7B, and apply the same settings to\nLLaMA-13B without additional tuning. We use a maximum sequence length of 512 for training and\nhyperparameter tuning, and a maximum new token number of 32 for inference. Table 5 describes\nour hyperparameter search space.   We use a lower number of epochs (6 instead of 12) for the\ncommonsense reasoning benchmark because theCOMMONSENSE170Ktraining set is more than 100\ntimes larger than GSM8K.\nDuring inference, we use greedy decoding without sampling for the commonsense reasoning bench-\nmark, since it is a multi-token classification benchmark, and use the same decoding strategy as in\nHu et al. [2023] for the arithmetic reasoning benchmark with a higher temperature0.3. The reason\nto switch to a slightly different set of decoding hyperparameters is that theHuggingFacedecoding\nfunction may throw an error due to statistical instability with close-to-zero probabilities over output\ntokens with beam search.\n7\nInstruction following.We finetune LLaMA-7B on Alpaca-52K [Taori et al., 2023] to select hy-\nperparameters. We select the hyperparameter settings based on model performance evaluated with\nAlpaca-Eval v1.0 [Li et al., 2023], which calculates the win-rate overtext-davinci-003by using\ngpt-4-turboas the annotator. We use a maximum sequence length of 768 for training and hyper-\nparameter tuning, and a maximum new token number of 2048 for inference. Table 6 describes our\nhyperparameter search space.\nDuring inference, we use the same decoding strategy as in RED [Wu et al., 2024a] to ensure a fair\ncomparison. Specifically, we use greedy decoding without sampling, and use a maximum repetition\nn-gram size of 5 with a repetition penalty of 1.1.\nNatural language understanding.We conduct hyperparameter tuning with RoBERTa-base and\nRoBERTa-large for each task individually. We pick the hyperparameters based on testing performance\non the held-out validation set with a fixed random seed of 42.  We then evaluate our model with\nadditional four unseen seeds {43, 44, 45, 46} for final results. We follow Wu et al. [2024a]’s setting\nfor evaluation. For QQP with RoBERTa-large, there are some stochasticity in runs with the same\n7\nSee reference ticket:https://github.com/huggingface/transformers/issues/11267.\n22",
    "seed, so we picked the best run out of 3 runs for any particular seed. As reported by Wu et al. [2024a],\nwe also observe that evaluation results on RTE are unstable due to the small size of the dataset. We\nthus replace several random seeds as in Wu et al. [2024a] to ensure a fair comparison. In addition, we\nreplace one or two random seeds for CoLA for stability. Table 7 describes our hyperparameter search\nspace. Table 8 and Table 9 describe our hyperparameter settings for each task.\nHyperparametersLLaMA-7B w/ GSM8K\nprefix+suffice positionp+s{p1+s1,p3+s3,p5+s5,p7+s7,p9+s9,p11+s11}\nTied weightp,s{True\n, False}\nRankr{8, 16, 32, 64}\nLayerL(sep. w/ ‘;’){0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22, all}\nDropout{0.00, 0.05\n}\nOptimizerAdamW\nLR{9×10\n−5\n, 1×10\n−4\n, 3×10\n−4\n, 6×10\n−4\n, 9×10\n−4\n, 1×10\n−3\n, 3×10\n−3\n}\nWeight decay{0\n, 1×10\n−3\n, 2×10\n−3\n}\nLR schedulerLinear\nBatch size{4, 8, 16, 32\n, 64}\nWarmup ratio{0.00, 0.06, 0.10}\nEpochs{3, 6, 9, 12, 18}\nTable 5:  Hyperparameter search space of LLaMA-1 7B models on the GSM8K development set\nwith the best settingsunderlined. We use greedy decoding without sampling during hyperparameter\ntuning.\nHyperparametersLLaMA-7B w/ Alpaca-52K\nprefix+suffice positionp+s{p1+s1,p3+s3,p5+s5,p7+s7}\nTied weightp,s{True\n, False}\nRankr{1, 2, 3, 4, 5, 6}\nLayerL(sep. w/ ‘;’){9;18, 3;9;18, 3;9;18;24}\nDropout{0.00, 0.05}\nOptimizerAdamW\nLR9×10\n−4\nWeight decay0×10\n−3\nLR schedulerLinear\nBatch size{16, 32, 64, 128\n}\nWarmup ratio0.00\nEpochs{1, 3, 6, 9, 12\n}\nTable 6: Hyperparameter search space of LLaMA-1 7B models on Alpaca-52K evaluated by Alpaca-\nEval v1.0 with the best settingsunderlined.   We use greedy decoding without sampling during\nhyperparameter tuning.\n23",
    "HyperparametersRoBERTa-base and RoBERTa-large w/ GLUE\nprefix+suffice positionp+s{p1,p3,p5,p7,p9,p11}\nTied weightp,sFalse\nRankr{1, 2}\nLayerL(sep. w/ ‘;’){1;3;5;7;9;11, all}\nDropout{0.00, 0.05, 0.10, 0.20}\nOptimizerAdamW\nLR{1×10\n−4\n, 2×10\n−4\n, 3×10\n−4\n, 4×10\n−4\n, 5×10\n−4\n},\n{6×10\n−4\n, 9×10\n−4\n, 1×10\n−3\n, 3×10\n−3\n}\nWeight decay{0, 1×10\n−4\n, 6×10\n−4\n, 1×10\n−3\n, 6×10\n−3\n, 1×10\n−1\n, 2×10\n−1\n}\nLR schedulerLinear\nBatch size{16, 32, 64, 128}\nWarmup ratio{0, 5×10\n−3\n, 6×10\n−3\n, 3×10\n−2\n, 5×10\n−2\n, 6×10\n−2\n, 1×10\n−1\n, 2×10\n−1\n}\nEpochs{20, 30, 40, 50, 60}\nTable 7:  Hyperparameter search space of RoBERTa-base and RoBERTa-large models on GLUE\nevaluated with classification accuracy.  Best hyperparameter settings are task-specific, which are\nspecified in separate tables.\nHyperparametersMNLISST-2MRPCCoLAQNLIQQPRTESTS-B\npositionpp1p3p3p3p11p11p3p3\nTied weightFalse\nRankr1\nLayerLall\nDropout0.050.100.050.200.050.050.050.05\nOptimizerAdamW\nLR6×10\n−4\n6×10\n−4\n3×10\n−4\n4×10\n−4\n9×10\n−4\n6×10\n−4\n9×10\n−4\n6×10\n−4\nWeight decay0.00\nLR schedulerLinear\nBatch size32\nWarmup ratio6×10\n−2\n1×10\n−1\n05×10\n−3\n1×10\n−1\n003×10\n−2\nEpochs4040406020406060\nTable 8: Hyperparameter settings of RoBERTa-base models on GLUE.\n24",
    "HyperparametersMNLISST-2MRPCCoLAQNLIQQPRTESTS-B\npositionpp1p3p3p3p11p11p3p3\nTied weightFalse\nRankr1\nLayerLall\nDropout0.050.050.200.200.050.050.050.05\nOptimizerAdamW\nLR6×10\n−4\n6×10\n−4\n3×10\n−4\n1×10\n−4\n9×10\n−4\n6×10\n−4\n6×10\n−4\n8×10\n−4\nWeight decay0.00\nLR schedulerLinear\nBatch size32\nWarmup ratio0.000.100.060.200.100.060.000.20\nEpochs2020303020203030\nTable 9: Hyperparameter settings of RoBERTa-large models on GLUE.\nModelPEFTParams(%)\nAccuracy(↑) (SD)\nMNLISST-2MRPCCoLAQNLIQQPRTESTS-BAvg.\nbase\nFT100%87.3\n(0.34)\n94.4\n(0.96)\n87.9\n(0.91)\n62.4\n(3.29)\n92.5\n(0.22)\n91.7\n(0.19)\n78.3\n(3.20)\n90.6\n(0.59)\n85.6\nAdapter\n∗\n0.318%87.0\n(0.28)\n93.3\n(0.40)\n88.4\n(1.54)\n60.9\n(3.09)\n92.5\n(0.02)\n90.5\n(0.08)\n76.5\n(2.26)\n90.5\n(0.35)\n85.0\nLoRA\n∗\n0.239%86.6\n(0.23)\n93.9\n(0.49)\n88.7\n(0.76)\n59.7\n(4.36)\n92.6\n(0.10)\n90.4\n(0.08)\n75.3\n(2.79)\n90.3\n(0.54)\n84.7\nAdapter\nFNN∗\n0.239%87.1\n(0.10)\n93.0\n(0.05)\n88.8\n(1.38)\n58.5\n(1.69)\n92.0\n(0.28)\n90.2\n(0.07)\n77.7\n(1.93)\n90.4\n(0.31)\n84.7\nBitFit\n∗\n0.080%84.7\n(0.08)\n94.0\n(0.87)\n88.1\n(1.57)\n54.0\n(3.07)\n91.0\n(0.05)\n87.3\n(0.02)\n69.8\n(1.51)\n89.5\n(0.35)\n82.3\nRED\n∗\n0.016%83.9\n(0.14)\n93.9\n(0.31)\n89.2\n(0.98)\n61.0\n(2.96)\n90.7\n(0.35)\n87.2\n(0.17)\n78.0\n(2.06)\n90.4\n(0.32)\n84.3\nLoReFT (ours)0.015%83.1\n(0.26)\n93.4\n(0.64)\n89.2\n(2.62)\n60.4\n(2.60)\n91.2\n(0.25)\n87.4\n(0.23)\n79.0\n(2.76)\n90.0\n(0.29)\n84.2\nlarge\nFT100%88.8\n(0.45)\n96.0\n(0.66)\n91.7\n(1.73)\n68.2\n(2.62)\n93.8\n(0.33)\n91.5\n(1.28)\n85.8\n(1.40)\n92.6\n(0.16)\n88.6\nAdapter\n∗\n0.254%90.1\n(0.12)\n95.2\n(0.48)\n90.5\n(0.59)\n65.4\n(2.24)\n94.6\n(0.17)\n91.4\n(0.13)\n85.3\n(1.34)\n91.5\n(0.33)\n88.0\nLoRA\n∗\n0.225%90.2\n(0.25)\n96.0\n(0.85)\n89.8\n(2.09)\n65.5\n(2.02)\n94.7\n(0.21)\n90.7\n(0.91)\n86.3\n(2.41)\n91.7\n(0.44)\n88.1\nAdapter\nFNN∗\n0.225%90.3\n(0.15)\n96.1\n(0.75)\n90.5\n(1.26)\n64.4\n(1.56)\n94.3\n(0.39)\n91.3\n(0.24)\n84.8\n(2.01)\n90.2\n(0.24)\n87.7\nRED\n∗\n0.014%89.5\n(0.38)\n96.0\n(0.48)\n90.3\n(1.40)\n68.1\n(1.69)\n93.5\n(0.33)\n88.8\n(0.11)\n86.2\n(1.40)\n91.3\n(0.21)\n88.0\nLoReFT (ours)0.014%89.2\n(0.27)\n96.2\n(0.72)\n90.1\n(1.17)\n68.0\n(1.44)\n94.1\n(0.35)\n88.5\n(0.45)\n87.5\n(1.49)\n91.6\n(0.43)\n88.2\nTable  10:  Accuracy  comparison  of  RoBERTa-base  and  RoBERTa-large  against  existing  PEFT\nmethods on the GLUE benchmark withstandard deviation (SD).\n∗\nPerformance results of all\nbaseline methods are taken from Wu et al. [2024a].  We report averaged performance of five runs\nwith distinct random seeds for our method. # Param. (%) is calculated by dividing the number of\ntrainable parameters (excluding the number of parameters of the classification head) with the number\nof parameter of the base LM.\n25",
    "D    Suggestions on choosing hyperparameters for ReFT\nSimilar to PEFTs or finetuning, ReFT can be sensitive to hyperparameter settings. Here, we recom-\nmand a non-exhaustive list for choosing the best hyperparameter settings for your tasks:\n•Intervening on multiple positions delivers significant gains. We find that intervening only\non a single token position (e.g., just the first one or the last one) is always less optimal than\nintervening on multiple tokens. However, intervening on excessive number of tokens might\nharm performance by slowing down convergence.\n•Intervening on all layers first, and then shrink down.  Intervening on all layers often\nprovides a good baseline.  We recommand users to start with all layers, and shrink down\nthe number of intervening layers depending on the desired performance–parameter count\nbalance.\n•Higher rank may not entail better performance.  High rank entails higher parameter\ncount, but it does not always bring performance gain (likely due to slower convergence). We\nrecommend users to start with a rank that is lower than 32 (e.g. rank 4).\n•\nTie intervention weights as much as you can. In the paper, we explore tying the interven-\ntion weights between prefix and suffix token positions. It automatically halves the parameter\ncount, and it can result in better performance as well.  We suspect weight sharing across\nlayers may also help.\n•Hyperparameter tuning with learning rate, warmup ratio, dropout rate and weight\ndecay should go after other hyperparameters.  These classic neural-network training\nhyperparameters can play a role, yet they have much smaller effect than previous ones.\nE    A single vector is worth a thousand tokens\nIn this section, we explore the power of LoReFT through a memorization test.  Similar tests have\nalso been studied in terms of activation-based adversarial attacks in the original basis [Fort, 2023].\nSpecifically, we learn a single rank-1 LoReFT at a single layer on the residual stream of the last\nprompt token to recover a specific output sequence with lengthL\nm\n.  For simplicity, we simplify\nLoReFT in Eqn. 2 by removingWhto make the intervention input-independent, where we learn\na single scalarbbesides the low-rank matrix. As a result, our simplified rank-1 LoReFT contains\nprecisely 4,097 parameters for LLaMA-1 7B and 5,121 parameters for LLaMA-1 13B models.\n8\nWe measure the memory power by how largeL\nm\ncan be, and how accurate the recovered output\nsequence is with prefix length exact match in percentage. We use the first few thousand words of\nthe book Alice’s Adventures in Wonderland [Carroll, 1865] as our recovery sequence. Our prompt\nis constructed asALIC#ID1->followed by model generations.  We train with 1000 epochs with a\nlearning rate of4×10\n−3\nand a linear learning rate scheduler without warm-up.\nAs shown in fig. 3 and fig. 4, both models can successfully remember up to 2,048 tokens across\nmost layers with a 100% recovery rate. As a result, a rank-1 intervention can thus correctly recover\na sequence of at least 2,048 in length. LLaMA-1 7B starts to fail catastrophically after the length\nexceeds 2,048, suggesting that positional embeddings might play a role, or the maximum sequence\nlength  during  pretraining.   LLaMA-1  13B  shows  better  memorization  for  lengths  up  to  2,560,\nsuggesting memorization scales with model size. Note that we may heavily underestimate the model’s\npower of memorization due to the fact that our hyperparameters are picked with an educated guess\nwithout tuning.\nFrom fig. 5 to fig. 8, we conduct harder tests by asking our models to recover a scrambled version\n(word order is scrambled) of Alice’s Adventures in Wonderland, and to recover a random token\nsequence.  Recovery rates for these two conditions are significantly worse than the original book,\nsuggesting that pretraining data memorization may play a role in terms of recovery rate, given that\nthe book is highly likely in the pretraining corpus. Moreover, both models can only recover random\ntoken sequences up to 128 tokens, suggesting that word morphology also plays a role. Our results\n8\nThese parameters take about 17.5KB of disk space.\n26",
    "also suggest that a single rank-1 intervention can transmit over 128 bits of token identity sequence\nusing the hyperparameters we have.\n9\nFigure 3: Memorisation test results forLLaMA-1 7B modelon recovering first n-th tokens of the\nAlice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last token’s\nresidual stream. Rec. % is measured by the percentage of prefix matches.\nFigure 4:  Memorisation test results forLLaMA-1 13B modelon recovering first n-th tokens of\nthe Alice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last\ntoken’s residual stream. Rec. % is measured by the percentage of prefix matches.\nFigure 5:  Memorisation test results forLLaMA-1 7B modelon recovering first n-th tokens of a\nrandomly scrambledversion of the book Alice’s Adventures in Wonderland.\nFigure 6: Memorisation test results forLLaMA-1 13B modelon recovering first n-th tokens of a\nrandomly scrambledversion of the book Alice’s Adventures in Wonderland.\nFigure 7:  Memorisation test results forLLaMA-1 7B modelon recovering first n-th tokens ofa\nsequence of random tokens.\n9\nOurcodeisavailableathttps://github.com/stanfordnlp/pyreft/tree/main/examples/\nmemorisation.\n27",
    "Figure 8: Memorisation test results forLLaMA-1 13B modelon recovering first n-th tokens ofa\nsequence of random tokens.\nF    A single vector can memorise a codebook with 256 entries\nOur memorization tests in appendix E test how long of a sequence we can encode in a rank-1\nintervention. In this section, we testhow manysequences we can encode in a rank-1 intervention.\nSpecifically, we attempt to memorise a mapping of input-output pairs at scale, viewinglearned\nReFTas a simple index-based storage system.   We employ the same intervention and training\nhyperparameters as in appendix E, but with a different training dataset. Our prompt is constructed\nasRAND#ID1->, followed by a single output token that the ID maps to. We construct a set of these\ninput-output pairs and train a rank-1 intervention to memorize them.\nWe present our results in fig. 9 and fig. 10 for LLaMA-1 7B and 13B, respectively, in terms of how\nmany random input-output pairs a single rank-1 intervention can memorise depending on the layer\nthe intervention in performed in. Our results suggest that a rank-1 intervention can reliably remember\nup to 256 pairs, with near-perfect recall in layer 20 of the 13B model.  Recalling the fact that our\nsimplified LoReFT intervention learns only a single scalarb, which is input-dependent, means the\nlearned scalar, when projected back into the original basis, allows the distributed representation of\nthe scalar to enable the model to correctly generate the output token. As a result, it is evidence that\ntoken identities are likely superpositioned in the original basis, and linear decomposition (i.e., our\nlearned projection matrixR) can disentangle superpositioned information to some degree.\nFigure 9: Multitude test results forLLaMA-1 7B modelon recoveringninput-output pairs where\neach pair constitutes an input prompt asRAND#ID1->with varying IDs and a single random token\noutput.\nFigure 10: Multitude test results forLLaMA-1 13B modelon recoveringninput-output pairs where\neach pair constitutes an input prompt asRAND#ID1->with varying IDs and a single random token\noutput.\n28",
    "G    Learned ReFTs are like puzzle pieces, fitting together to solve new tasks\nVarious works have studied how to merge model weights, or PEFT weights together to achieve multi-\ntask learning (MTL) without adaptation [Li et al., 2022, Huang et al., 2023a, Zhang et al., 2024a,\nZhong et al., 2024]. Recent works also explore merging PEFT weights to achieve task composition\n(i.e., generalize to unseen tasks) by detoxifying an instruction-tuned LM [Huang et al., 2023a, Zhang\net al., 2024a]. Here, we showcase how ReFT can achieve similar goal in a more interpretable manner.\nMore importantly, we focus oncompositional use of learned abilities(i.e., to combine abilities\ntogether to solve a new task) instead of instilling MTL ability to the model (i.e., to solve different\ntasks).  Recall eq. (2), we can further partition our low-rank projection into orthogonal subspaces\ngiven that each column vector of our projection matrix is an orthonormal vector. Formally, we can\nrefine our subspace intervention parameters asφ\ni\n={R\ni\n,W\ni\n,b\ni\n}.R∈R\nr\ni\n×d\nwhereidenotes a\nsubspace partition with a dimension ofr\ni\n. Each training example now contains an additional field\nindicating its subspace partition to receive gradient updates. At inference time, these partitions can be\nintervened together to test for new task generalization.\nExperiment design.We train a rank-8 LoReFT and partition the subspace into two groups with equal\nrank{0,1,2,3}and{4,5,6,7}.  We train the first group for completing any sentence in German.\nSpecifically, we use a the 1K training dataset HellaSwag\n10\nvariant which has an English sentence\nprefix and a German completion. We train the second group for instruction following. Specifically, we\nsubsample 1K training examples from Ultrafeedback [Cui et al., 2023]. There is no hyperparameter\ntuning. We train our model for 3 epochs with a learning rate of5×10\n−3\nand a linear learning rate\nscheduler without warm-up. Training finishes under 10 mins with a single Nvidia A100 40G GPU.\n11\nQualitative results.We provide some qualitative results below.\n10\nhttps://huggingface.co/datasets/LeoLM/HellaSwag_de\n11\nOur code is athttps://github.com/stanfordnlp/pyreft/tree/main/examples/composition.\n29",
    "Composable ReFTs\nHow can we reduce air pollution?\nResponse\nModelResponse\nLLaMA-7B (LoReFT,\nGerman completion\nsubspace)\n, die Luft wird sauber gemacht.\nLLaMA-7B (LoReFT,\nInstruction following\ncompletion subspace)\nSure, here are some ways to reduce air pollution:\n1. Reduce vehicle emissions by using public transportation or carpooling when-\never possible.\n2. Plant more trees and use renewable energy sources like solar power and wind\nturbines.\n3. Recycle and reuse materials instead of throwing them away.\n4. Use environmentally-friendly products such as reusable bags and recycled\npaper.\n5. Avoid burning fossil fuels for heating and cooking.\n6. Conserve water by taking shorter showers and turning off the faucet while\nbrush your teeth.\nLLaMA-7B (LoReFT,\nCombined subspaces)\nEinige Lösungen sind:\n* Wenn Sie Autofahren, fahren Sie auf der Straße mit weniger Verkehr und\nsparen Sie damit Kraftstoffe.\n* Wenn Ihnen ein Auto fehlt, können Sie auch zu Fuß gehen oder einen Fahrrad\nbenutzen.\n* Wann Sie in einem Haushalt arbeiten, können Sie die Heizung oder das Wasser\nabschalten, wenn Sie nicht benötigt werden.\nComposable ReFTs\nHow to kill a linux process?\nResponse\nModelResponse\nLLaMA-7B (LoReFT,\nGerman completion\nsubspace)\nEs wird ein Linux-Prozess getötet, indem man ihn mit dem Befehl \"kill\" tötet.\nLLaMA-7B (LoReFT,\nInstruction following\ncompletion subspace)\nTo kill a Linux process, you can use the ‘kill‘ command with the PID (process\nID) of the process you want to terminate. For example, if you want to kill the\nprocess with PID 123456789, you would run the following command:\n“‘\n$ kill -9 123\n”’\nThis will send a signal to the process with Pid 123, instructing it to terminate\nimmediately.   The ‘-9‘ flag indicates that the process should be terminated\nforcefully and without any further warning or prompts.\nNote that this method only works for processes running on the same machine as\nyou. If the process is running on another computer, you cannot kill it using this\nmethod. In such cases, you may need to use other methods, such as sending a\nmessage to the remote system using SSH or a similar protocol.\nLLaMA-7B (LoReFT,\nCombined subspaces)\nEs gibt verschiedene Möglichkeiten, um einen Linux-Prozess zu löschen.\n1. Mit dem Kommando \"kill\" kann man den Prozess beenden.\n2.  Mit dem Kommandopuffer \"ps -ef | grep <Processname>\" kann man die\nPosition des Prozesses in der Tabelle \"ps -ef\" finden und ihn dann mit dem\nKommandomodus \"kill\" beenden.\n30",
    "H    Inference speed overhead analysis of ReFT withpyreft\nUnlike LoRA [Hu et al., 2022] where learned LoRA weights can be merged into model weights\nto produce zero-overhead at inference time, ReFT requires interventions being hooked into the\ncomputation graph of the intervened LM.\n12\nOn the other hand, we empirically show that LoReFT\nmay only need to intervene on the prompt tokens to achieve good performance, which significantly\nreduces the overhead due to the fact that we only spend extra time on inference when populating the\ninitial key-value cache.\n13\nOther PEFTs such as Adapters [Houlsby et al., 2019, Pfeiffer et al., 2020,\nWang et al., 2022, He et al., 2022b, Fu et al., 2021] will theoretically have a larger inference overhead\nsince they are often applied to all the prompt tokens as well as every decoding step. Here, we compare\nthe end-to-end inference runtime of a LoReFT LM and a vanilla LM without any intervention (i.e.,\nthe ceiling runtime of any PEFT or ReFT).\nExperiment design.We initialize LoReFT with different settings without any training (i.e., the\nintervened LM may generate garbage), and measure its generation runtime with greedy decoding\nwithout any early stopping criteria.  The maximum number of new tokens is set to 256.  We use a\nmaximum repetition n-gram size of 5 with a repetition penalty of 1.1. We benchmark LoReFT against\na vanilla LM (i.e., un-intervened) with the following conditions with LLaMA-1 7B:\n1.\nVarying rankswhere we fix the intervening layer at layer 15 and the intervening position at\nthe last prompt token. We choose a rank from{1,4,8,16,32}.\n2.Varying layerswhere we fix the LoReFT rank to be 8 and the intervening position at the\nlast prompt token. We choose a number of intervening layers from{2,4,6,8,10}.\n3.Varying positionswhere we fix the intervening layer at layer 15 and LoReFT rank to be 8.\nWe choose the number of intervening positionsnfrom{2,4,6,8,10}. We only intervening\non the lastn-th tokens.\nQualitative results.We show our results in fig. 11 where we measure the generation time (y-axis)\nfor a fixed length of 256 tokens given different prompt length (x-axis).  Overall, ReFT introduces\ncompute overhead during inference as expected. Higher rank or more intervening layers positively\ncorrelate with larger overhead. For intervening with 10 layers with a rank of 8 on the last prompt\ntoken, the overhead is about0.05second.\n12\nOurpyreftlibrary is powered by thepyveneLibrary [Wu et al., 2024b] for performing model interventions.\nDetails about the system design ofpyvenecan be found in its original paper.\n13\nTo read more about the KV cache in the HuggingFace library,  seehttps://huggingface.co/docs/\ntransformers/main/en/llm_tutorial_optimization.\n31",
    "Positions\nRank\nLayers\nPrompt tokens\nTime\nFigure 11: Runtime vs. number of prompt tokens across various settings of interventions with LoReFT\ncompared to no intervention. We adjustpositions(specifically the suffix lengths), therankof the\nsubspace being intervened on, and the range oflayerswe intervene on.\n32",
    "I    Generation examples\nInstruction (WinoGrande)\nPlease choose the correct answer to fill in the blank to complete the given sentence: Sarah was a much\nbetter surgeon than Maria so _ always got the easier cases. Option1: Sarah Option2: Maria Answer format:\noption1/option2\nResponse\nModelResponse\nLLaMA-7B (LoReFT)option1\nLLaMA-13B (LoReFT)option2\nInstruction (OBQA)\nPlease choose the correct answer to the question: A red-tailed hawk is searching for prey. It is most likely\nto swoop down on Answer1: an eagle Answer2: a cow Answer3: a gecko Answer4: a deer Answer format:\nanswer1/answer2/answer3/answer4\nResponse\nModelResponse\nLLaMA-7B (LoReFT)answer4\nLLaMA-13B (LoReFT)answer3\n33",
    "Instruction (AQuA)\nA shopkeeper in order to promote his new shop put a discount of 20% on all the items for one day. Now\nhe must sell the items at original price the other day. By what percentage must he increase the price to\noriginal? Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18%\nResponse\nModelResponse\nLLaMA-7B (LoReFT)Let’s assume that the original price of an item is x.\nFor the first day, the shopkeeper sold the item at a discount of 20%, which\nmeans he sold it for 0.8x (80% of the original price).\nOn the second day, he must sell the item at the original price, which means he\nmust sell it for 1x (100% of the original price).\nTo find the percentage increase in price, we need to find the difference between\nthe selling price on the second day and the selling price on the first day.\nSelling price on the second day = 1x\nSelling price on the first day = 0.8x\nPercentage increase = (1x - 0.8x) / 0.8x * 100%\nPercentage increase = (x - 0.8x) / 0.8x * 100%\nPercentage increase = 0.2x / 0.8x * 100%\nPercentage increase = 0.25 * 100%\nPercentage increase = 25%\nTherefore, the answer is option C) 25%.\nLLaMA-13B (LoReFT)Let’s assume that the original price of each item was $x.\nAfter the discount, the selling price of each item became $0.8x (20% off).\nNow, the shopkeeper must sell the items at original price the other day.\nSo, the selling price of each item on the other day will be $x.\nTo find the percentage increase required, we need to calculate the difference\nbetween the selling price on the other day and the selling price on the discount\nday.\nSelling price on the other day = $x\nSelling price on the discount day = $0.8x\nDifference = $x - $0.8x = $0.2x\nPercentage increase required = (Difference / Original price) x 100%\nPercentage increase required = ($0.2x / $x) x 100%\nPercentage increase required = 20%\nTherefore, the answer is (B) 20%.\n34",
    "Instruction (GSM8K)\nA car in the fast lane is traveling at 60 miles/hour. A car in the slow lane is traveling at half that speed. If\nthe car in the fast lane traveled for a total of 480 miles, calculate the time the car in the slow lane took to\ncover the same distance?\nResponse\nModelResponse\nLLaMA-7B (LoReFT)Let’s call the speed of the car in the slow lane “x”.\nWe know that the speed of the car in the fast lane is 60 miles/hour, and the speed\nof the car in the slow lane is half that, or 30 miles/hour.\nWe also know that the car in the fast lane traveled for a total of 480 miles.\nTo find the time it took the car in the slow lane to cover the same distance, we\ncan set up an equation:\nx = 30\nWe can solve for x:\nx = 30\nx = 30\nTherefore, the speed of the car in the slow lane is 30 miles/hour.\nTo find the time it took the car in the slow lane to cover the same distance, we\ncan use the formula:\ntime = distance / speed\ntime = 480 / 30\ntime = 16\nTherefore, the car in the slow lane took 16 hours to cover the same distance as\nthe car in the fast lane.\nAnswer: 16\nLLaMA-13B (LoReFT)Let’s call the speed of the car in the slow lane “x”. We know that x = 60/2 = 30\nmiles/hour.\nThe time it took the car in the fast lane to travel 480 miles is:\ntime = distance / speed\ntime = 480 miles / 60 miles/hour\ntime = 8 hours\nNow we can use the formula time = distance / speed to find the time it took the\ncar in the slow lane to travel the same distance:\ntime = 480 miles / 30 miles/hour\ntime = 16 hours\nTherefore, the car in the slow lane took 16 hours to cover the same distance as\nthe car in the fast lane.\n35",
    "Instruction (Alpaca-Eval)\nList the layers of the TCP/IP model and for each layer identify one protocol.\nResponse\nModelResponse\nLlama-2 7B (LoReFT)\nThe TCP/IP (Transmission Control Protocol / Internet Protocol) model is a\nstandard network communication model used to transmit data across the internet.\nIt consists of four main layers, each with its own set of protocols:\n1. Application Layer: This layer provides services such as email, file transfer,\nand web browsing.  Some examples of protocols at this layer include HTTP\n(Hypertext Transfer Protocol), FTP (File Transfer Protocol), SMTP (Simple\nMail Transfer Protocol), and Telnet.\n2. Transport Layer: The transport layer ensures reliable delivery of data between\napplications on different machines.  Examples of protocols at the transport\nlayer include TCP (Transmission ControlProtocol) and UDP (User Datagram\nProtocol).\n3. Network Layer: The network layer handles routing and forwarding of packets\nacross multiple networks.  Examples of network-layer protocols include IP\n(Internet Protocol) and ICMP (Internet Control Message Protocol).\n4.  Link Layer:  The link layer deals with the physical connection between\ndevices on the same network segment.  Examples of link-layer protocols are\nEthernet, Token Ring, and FDDI (Fiber Distributed Data Interface).\nIn summary, the TCP/IP Model has four layers: application, transport, network,\nand link. Each layer has its own set of associated protocols that work together\nto ensure seamless communication over the internet.\nLlama-2 7B (LoReFT,\nhalf)\nThe TCP/IP (Transmission Control Protocol / Internet Protocol) model is a\nstandard network architecture used to communicate data across networks.  It\nconsists of four layers, each with its own set of protocols:\n1.  **Application Layer** - This layer provides services such as file transfer,\nemail, and web browsing. Examples include HTTP (Hypertext Transfer Proto-\ncol), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and\nTelnet.\n2. **Transport Layer** - The transport layer ensures reliable delivery of data\nby providing error checking and retransmissions if necessary. It also handles\nmultiplexing and de-multiplexing of data streams.   Examples include TCP\n(Transmission Control protocol) and UDP (User Datagram Protocol).\n3.  **Internet Layer** - Also known as the Network Layer, this layer routes\npackets from source to destination based on IP addresses. It includes protocols\nlike IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol Version 6).\n4. **Data Link Layer** - Responsible for transmitting data over physical links\nsuch as Ethernet or WiFi.  It uses protocols like ARP (Address Resolution\nProtocol) and SLIP (Serial Line Interface Protocol).\nPlease note that some protocols may be implemented at more than one layer in\nthe TCP/IP stack. For example, DNS (Domain Name System) can operate at\nboth the application and transport layers.\n36",
    "Llama-2 7B (LoReFT,\n1K)\nThe TCP/IP (Transmission Control Protocol / Internet Protocol) model is a\nstandard network architecture used to communicate data across the internet or\nany other computer network. It consists of four main layers, each with its own\nset of protocols:\n1. Application Layer: This layer provides services such as email, file transfer,\nand web browsing. Examples include SMTP (Simple Mail Transfer Protocol),\nFTP (File Transfer Protocol), HTTP (Hypertext Transfer Protocol), and DNS\n(Domain Name System).\n2. Transport Layer: This is responsible for establishing and maintaining con-\nnections between applications at the application layer.   It ensures that data\nis delivered in the correct order and without errors.  Examples include TCP\n(Transmission ControlProtocol) and UDP (User Datagram Protocol).\n3.  Network Layer:  This handles routing and forwarding of packets between\ndifferent  networks.   It  also  manages  addressing  schemes  like  IP  addresses\n(Internet Protocol Addresses). Examples include IPv4 (Internet Protocol Version\n4) and IPv6 (Internet ProtocolVersion 6).\n4. Link Layer: This deals with physical communication between devices on a\nsingle network segment. It includes protocols like Ethernet, Token Ring, and\nFiber Distributed Data Interface (FDDI).\n37",
    "Instruction (Alpaca-Eval)\nTake MLK speech “I had a dream” but turn it into a top 100 rap song\nResponse\nModelResponse\nLlama-2 7B (LoReFT)(Verse 1)\nI had a dream, a dream of freedom and equality for all\nA world where love and peace would reign supreme\nWhere people from every race and creed could live in harmony\nAnd justice would prevail, no more oppression or tyranny\nChorus:\nI had a vision, a dream so powerful and strong\nIt’s time to wake up and make this dream come true\nWe must stand together, united as one\nTo build a better future, a brighter tomorrow\n(Verse II)\nIn my dream, I saw a world without hate and prejudice\nA place where everyone was treated with respect and dignity\nNo more discrimination based on skin color or religion\nJust love and understanding, that’s what we need to achieve\n(Bridge)\nWe can make this dream a reality, if we work together\nLet’s put aside our differences and embrace each other\nWe are all human beings, let’s treat each other with kindness\nAnd create a world where everyone can thrive and flourish\n(Outro)\nSo let us march on, towards a brighter future\nWith hope and determination, we will overcome\nTogether, we can make Dr. King’s dream a reality\nA world where peace and love reign supreme\n38",
    "Llama-2 7B (LoReFT,\nhalf)\n(Verse 1)\nI had a dream, a dream so bold and bright,\nA dream of freedom for all, no matter their race or creed.\nI saw a world where love was king,\nAnd hate was banished from its throne.\nI had a vision, a vision so clear,\nOf a land where justice reigned supreme,\nWhere equality was the law,\nAnd prejudice was a thing of the past.\n(Chorus)\nI had this dream, I had this dream,\nThat one day we would live in harmony,\nIn a world where peace and love were real,\nAnd hatred was just a distant memory.\n(Verse II)\nI had to speak out, I had to shout,\nTo make my voice heard above the noise.\nI knew that change could not wait,\nFor too many people were still suffering.\nSo I spoke up, I spoke loud,\nUntil my words rang through the air,\nAnd hope began to take root,\nAs people listened with open ears.\n(Bridge)\nWe can’t let fear hold us back,\nWe must stand together as one,\nAnd fight for what is right,\nNo matter how long it takes to win.\nLet’s keep our eyes on the prize,\nAnd never give up, never surrender.\nTogether, we will make this dream come true,\nAnd build a better future for all mankind.\n(Outro)\nI had the dream, I had the dream,\nThat we could live in harmony.\nNow it’s time to make it real,\nAnd work towards a brighter tomorrow.\nSo let’s join hands, let’s stand tall,\nAnd make this dream a reality.\nLet’ s make this dream come alive,\nAnd create a world where everyone belongs.\n39",
    "Llama-2 7B (LoReFT,\n1K)\nTitle: I Had A Dream (Top 100 Rap Song)\nArtist: Martin Luther King Jr.\nAlbum: The Greatest Speeches of All Time\nLabel: Universal Music Group\nRelease Date: January 20, 1964\nIntro:\nI had a dream...\nChorus:\nI have a dream!\nThat one day on the red hills of Georgia,\nThe sons of former slaves and the sons of former slave owners will be able to sit\ndown together at the table of brotherhood.\nI have a vision...\nVerse 1:\nI am not afraid of the color of my skin,\nBut the injustice that plagues this nation.\nI have seen the tears of a mother who has lost her son,\nAnd felt the pain of a father who has nothing left to give his family.\nI have heard the voices of those who are crying out for change,\nAnd I know that we can make a difference if we all come together.\nRefrain:\nI still have a dream! That one day every man will be free!\nI still have faith! In the future of our land!\nI still believe! That we can overcome!\nWe’ll walk hand in hand! And sing a new song!\nVerse II:\nI see a city where people are judged by the content of their character,\nNot the color of their skin.\nI see a country where love is more powerful than hate,\nWhere peace is stronger than war.\nI see hope for a brighter tomorrow,\nWhen we stand together as one.\nRefrains:\nI’m gonna keep on fighting ’til the battle is won!\nI’m not gonna stop believing ’til the dream comes true!\nI’ve got a dream! That we can live as brothers!\nI’ll never give up! ’Cause I know what I’m fighting for!\nOutro:\nThank you! Thank you! Thank you so much!\nGod bless America! God bless us all!\n40"
  ]
}