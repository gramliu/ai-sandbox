{
  "key": "IS3HFEH5",
  "url": "https://arxiv.org/pdf/2302.14051",
  "metadata": {
    "title": "Internet Explorer: Targeted Representation Learning on the Open Web",
    "abstract": "  Modern vision models typically rely on fine-tuning general-purpose models\npre-trained on large, static datasets. These general-purpose models only\ncapture the knowledge within their pre-training datasets, which are tiny,\nout-of-date snapshots of the Internet -- where billions of images are uploaded\neach day. We suggest an alternate approach: rather than hoping our static\ndatasets transfer to our desired tasks after large-scale pre-training, we\npropose dynamically utilizing the Internet to quickly train a small-scale model\nthat does extremely well on the task at hand. Our approach, called Internet\nExplorer, explores the web in a self-supervised manner to progressively find\nrelevant examples that improve performance on a desired target dataset. It\ncycles between searching for images on the Internet with text queries,\nself-supervised training on downloaded images, determining which images were\nuseful, and prioritizing what to search for next. We evaluate Internet Explorer\nacross several datasets and show that it outperforms or matches CLIP oracle\nperformance by using just a single GPU desktop to actively query the Internet\nfor 30--40 hours. Results, visualizations, and videos at\nhttps://internet-explorer-ssl.github.io/\n",
    "published": "2023-02-27T18:59:55Z"
  },
  "text": [
    "Internet Explorer: Targeted Representation Learning on the Open Web\nAlexander C. Li\n* 1\nEllis Brown\n* 1\nAlexei A. Efros\n2\nDeepak Pathak\n1\nAbstract\nModern  vision  models  typically  rely  on  fine-\ntuning  general-purpose  models  pre-trained  on\nlarge, static datasets. These general-purpose mod-\nels only capture the knowledge within their pre-\ntraining datasets, which are tiny, out-of-date snap-\nshots of the Internet—where billions of images\nare uploaded each day.  We suggest an alternate\napproach: rather than hoping our static datasets\ntransfer to our desired tasks after large-scale pre-\ntraining,  we  propose  dynamically  utilizing  the\nInternet to quickly train a small-scale model that\ndoes extremely well on the task at hand. Our ap-\nproach, called Internet Explorer, explores the web\nin a self-supervised manner to progressively find\nrelevant examples that improve performance on\na desired target dataset. It cycles between search-\ning for images on the Internet with text queries,\nself-supervised training on downloaded images,\ndetermining which images were useful, and pri-\noritizing what to search for next. We evaluate In-\nternet Explorer across several datasets and show\nthat it outperforms or matches CLIP oracle perfor-\nmance by using just a single GPU desktop to ac-\ntively query the Internet for 30–40 hours. Results,\nvisualizations, videos, and code on our website:\ninternet-explorer-ssl.github.io/\n1. Introduction\nSuppose you have a small dataset and need to train a model\nfor some task, say classification.  A pipeline that has be-\ncome standard today is to download the latest pre-trained\ndeep network and fine-tune it on your own small dataset.\nThis pre-trained model used to be ImageNet-based (Deng\net al., 2009; He et al., 2016) and now would probably be\nCLIP (Radford et al., 2021).  The implicit goal set by the\ncommunity for such pre-trained models is that they should\n*\nEqual contribution\n1\nCarnegie Mellon University\n2\nUniversity of\nCalifornia, Berkeley. Correspondence to: Alexander Li<alexan-\nderli@cmu.edu>, Ellis Brown<ellisbrown@cmu.edu>.\nProceedings of the40\nth\nInternational Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nstatic dataset\npre-train \nonce\nfine-tune\nmodel\nStandard Pre-Training Setting\nInternet\nfocus on \nknowledge gaps\nlearn from \nnew data\nmodel\nOur Setting: Continually Explore the Internet\ntarget dataset\ntarget dataset\nFigure 1.Given  unlabeled  data  for  a  target  task,  our  approach,\nInternet Explorer, searches the Internet to progressively find more\nand more relevant training data via self-supervised exploration.\ntransfer well to any kind of downstream task not known in\nadvance.  This has led to a race to build ultra-large-scale\nmodels in terms of computation, model size, and dataset\nsize. But is this goal of building an “omniscient” pre-trained\nmodel that can work on any future downstream task even\nfeasible?  Perhaps not, as our world is continually chang-\ning. Although the size of the pretraining datasets has grown\nfrom 1.2M  (Deng et al., 2009) to 5B (Schuhmann et al.,\n2022) images, what has not changed at all is their nature:\nthese datasets are curated and, more importantly,static. For\ninstance, the portion of ImageNet curated before 2007 has\nno idea what an iPhone is.  Furthermore, although a few\nhundred million images represent a staggering quantity of\nvisual data, they are minuscule compared to the entire Inter-\nnet, where billions of new photos are uploaded every day.\nThus, current static datasets, however big they become, fail\nto capture the richness and dynamic nature of the data avail-\nable on the Internet. Moreover, as our static datasets grow,\nthey require increasingly inaccessible amounts of compute.\nIn this paper,  we rethink the idea ofgenericlarge-scale\npretraining and propose an alternate paradigm: train a small-\nscale but up-to-date model geared towards thespecificdown-\nstream task of interest.  To do so, we look beyond static\ndatasets andtreat the Internet itself as a dynamic, open-\nended dataset.   Unlike conventional datasets,  which are\n1\narXiv:2302.14051v2  [cs.LG]  7 Sep 2023",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nexpensive to expand and grow stale with time, the Internet\nis dynamic, rich, grows automatically, and is always up to\ndate. Its continuously evolving nature also means we cannot\nhope to ever download it or train a model, whether large or\nsmall, on all of it.\nWe propose that the Internet can be treated as a special kind\nof dataset—one that exists out there, ready to be queried as\nneeded to quickly train a customized model for a desired\ntask. We draw an analogy to reinforcement learning, where\neven though the task is known, finding a policy that can\ngenerate the desired behavior is non-trivial due to the high\ncomplexity of the state space.   Hence,  most approaches\nrely on some form of exploration to figure out what actions\nthe agent should take so that it quickly finds high-reward\nstates. Inspired by this analogy, we formulate a disembodied,\nonline agent we callInternet Explorer, that actively queries\nstandard search engines to find relevant visual data that\nimprove feature quality on a target dataset (see Figure 1).\nThe agent’s actions are text queries made to search engines,\nand the observations are the data obtained from the search.\nThe queries made by Internet Explorer improve over time.\nIt cycles between searching for images on the Internet with\ntext queries, self-supervised training on downloaded images,\ndetermining which images are relevant to the target dataset,\nand prioritizing what to search for next (see Figure 2). We\nalso bootstrap Internet Explorer using existing pre-trained\nmodels such as MoCo-v3 (He et al., 2020) and obtain a\nsignificant boost on the target datasets.\nOur setting is different from active learning (Settles, 2009),\nwhere the goal is to selectively obtain labels for data points\nfrom a fixed dataset. In contrast, Internet Explorer contin-\nually expands the size of its dataset and requires no labels\nfor training, even from the target dataset. Some prior works\nhave also discussed ways to leverage the Internet as an addi-\ntional source of data. NELL (Carlson et al., 2010) proposed\na way to continually scrape web pages to learn new con-\ncepts and relationships, which are periodically curated by\na human in the loop.  NEIL (Chen et al., 2013) builds on\nNELL’s dictionary to search visual data and develop visual\nrelationships. Both are semi-supervised methods to gather\ngeneral “common-sense” knowledge from the Internet. In\ncontrast, we perform an actively improvingdirectedsearch\nto perform well on target data, in a fully self-supervised\nmanner. Recent work (Jiang et al., 2021) follows a similar\nsetting but searches a static dataset and not the Internet.\nWe  evaluate  Internet  Explorer  across  7  datasets,  includ-\ning 4 fine-grained datasets, PASCAL VOC, ImageNet-100,\nand FMoW-WILDS. We search for relevant images using\nGoogle; however, the method is compatible with any text-\nbased search engine or even a static dataset (see Section 4.5).\nWe  compare  against  several  strong  baselines,  including\nCLIP, on downstream tasks.   Note that CLIP acts as an\nInternet Explorer Method\n1. Sample Query\nLearned concept distribution\nBMW, sunflower,. . . , duck\nGPT\n2. Internet Image Search\n3. Self-Supervised Training\nencoder\ncontrastive \nloss\n4. Update Concept Distribution\ncalculate \nreward\nencoder\nincrease probability of useful concepts\nBMW,sunflower,. . . , duck\ntarget dataset\n“duck”\n“baby”  +\nFigure 2.Overview of Internet Explorer.Our goal is to efficiently\nsearch the Internet for images that improve our performance on\na target dataset.  In each iteration, we first generate text queries\nby combining a concept sampled from a learned distribution with\na GPT-generated descriptor (§2.2,§2.7).  Next, we query search\nengines with the resulting phrase and download the top 100 image\nresults (§2.1, 4.5). We add these images to the set of previously\ndownloaded images and perform self-supervised training on the\ncombined dataset (§2.3). Finally, we evaluate the relevance of the\nnew images and update our concept distribution to increase the\nlikelihood of similar queries if their images were similar to the\ntarget dataset (§2.4, §2.5).\noracle for our approach because it has likely already seen all\nor more queries that Internet Explorer makes. In most sce-\nnarios, Internet Explorer either outperforms or matches the\nCLIP oracle using only a single 3090 GPU desktop machine\nthat runs for 30–40 hours, makes over 10K progressively\nimproving queries, and downloads over 1M relevant Internet\nimages for each target dataset.\n2. Internet Explorer: An Online Agent\nWe focus on the problem of efficiently improving represen-\ntations for some target dataset by acquiring Internet data.\nWe make as few assumptions as possible and assume that\nwe have only unlabeled training data from the target dataset.\nSuccessful representation learning in this setting would lead\nto better performance on the target dataset distribution for\nstandard tasks like classification and detection, and poten-\ntially others where the labels are not semantic (e.g., depth\nprediction or robotics). An overview of the Internet Explorer\nmethod is depicted in Figure 2 and described in Algorithm 1.\n2.1. Text-to-image Search\nWe discover and download images from the full breadth\nof the Internet by querying text-to-image search engines,\nwhich return images based on their captions and surround-\ning text. Text-to-image search is fast, finds diverse images\nfrom across the Internet, and enables searches for vastly\n2",
    "Internet Explorer: Targeted Representation Learning on the Open Web\ndifferent queries simultaneously.  Note that text-to-image\nsearch is noisy and makes use of weak supervision (the\nimage-text pairing on webpages). Thus, we only perform\nself-supervised training on the downloaded images. We use\na public codebase to query Google Images, which can down-\nload the top 100 images for each query (Vasa, 2015; Clinton,\n2020). We also try other search engines in Section 4.5.\n2.2. Text Query Generation\nAs text queries are our only input interface with the Inter-\nnet, it is crucial that we can generate diverse queries that\ncorrespond to a variety of visual categories. Specificity is\nalso important. Once a useful visual category is identified,\ngenerating fine-grained variants of the query is necessary\nto obtain data for all visual variations in the category. We\nconstruct queries by combining two components:\n1.\nConceptsspecify semantic categories such as people,\nplaces, or objects.\n2.Descriptorsare modifiers that generate variations in\nappearance.\nWe draw our concepts from the WordNet hierarchy (Miller,\n1995), which consists of146,347noun lemmas. Not all of\nthese lemmas are visual, but the vocabulary still covers an\nincredible range of topics (see examples in Appendix C.1).\nTo generate a text query, we first sample a concept from\na learned distribution over our vocabulary.  This discrete\ndistribution is defined by our estimates of how relevant each\nconcept in the vocabulary is at the current time (see Section\n2.4 for details on estimating rewards and Section 2.7 for the\ndistribution). Given a sampled concept, we can generate a\ndescriptor by prompting a GPT-J language model (Wang &\nKomatsuzaki, 2021) with examples of descriptor-concept\npairs (details in Appendix C.2). Finally, as shown in Step 1\nof Figure 2, we concatenate the concept and descriptor. If\nour concept is “duck” and the GPT-generated descriptor is\n“baby,” our search engine query is “baby duck.”\n2.3. Self-supervised Training\nWe use self-supervised learning (SSL) to learn useful rep-\nresentations from the unlabeled images that we download\nfrom  the  Internet.   Internet  Explorer  is  compatible  with\nany SSL algorithm that uses images or image-text pairs,\nincluding contrastive (He et al., 2020; Chen et al., 2020),\nnon-contrastive  (Grill  et  al.,  2020;  Zbontar  et  al.,  2021;\nBardes et al., 2021; Caron et al., 2021), masking-based (Bao\net al., 2021; He et al., 2022), or multimodal (Radford et al.,\n2021) approaches. For speed and stability reasons, we use\nthe MoCo-v3 algorithm (Chen et al., 2021), which trains\nencodersf\nq\nandf\nk\non augmentations(x\n1\n,x\n2\n)of the same\nimage to output vectorsq=f\nq\n(x\n1\n)andk=f\nk\n(x\n2\n).f\nq\nis\ntrained to minimize the InfoNCE loss (Oord et al., 2018):\nL\nq\n=−log\nexp(q·k\n+\n/τ)\nexp(q·k\n+\n/τ) +\nP\nk\n−\nexp(q·k\n−\n/τ)\n(1)\nk\n+\ncorresponds tof\nk\n’s output on the other augmentation of\nthe image used to computeq, and the set of negative exam-\nples{k\n−\n}corresponds tof\nk\n’s output on other images in the\nbatch. The temperatureτis set to1by default.f\nk\nconsists\nof a base encoder, a projection MLP, and a prediction head,\nwhereasf\nq\nis the exponential moving average of the base\nencoder and projection MLP fromf\nk\n. By trainingqandk\n+\nto be similar across image augmentations, MoCo-v3 encour-\nages the network to learn high-level semantic features.\nBefore turning to the Internet, we initialize a ResNet-50\nmodel (He et al., 2016) using a MoCo-v3 checkpoint trained\noffline for 100 epochs on ImageNet and then fine-tuned on\nthe target dataset. Without using labels, we select the best\nstarting checkpoint by early stopping on the SSL loss, which\nhighly correlates with target accuracy (Li et al., 2022). In\neach iteration of our method, we use MoCo-v3 to fine-tune\nour encoder on a mixture of newly downloaded, previously\ndownloaded, and target dataset images.\n2.4. Image Relevance Reward\nWe want to rank newly downloaded images by how much\nthey improve our features for the target dataset. This allows\nus to (a) prioritize taking gradient steps on useful images,\nand (b) understand what to search for in subsequent itera-\ntions. Unfortunately, it is challenging to directly measure\nthe effect of an individual training example on performance.\nNumerous techniques have been proposed (Koh & Liang,\n2017; Feldman & Zhang, 2020; Paul et al., 2021; Ilyas et al.,\n2022), but they all require extensive and repeated training\non new images to estimate their impact.\nInstead of trying to precisely measure what is learned from\neach image, we use its similarity to the target dataset as\na proxy for being relevant to training.  We rank the down-\nloaded images by their similarity in representation space to\nthe target dataset images; those most similar to the target\ndataset induce larger contrastive loss since eachexp(q·k\n−\n)\nterm in the denominator of Eq. 1 is larger when the nega-\ntive examples{k\n−\n}are closer toq. These “hard negatives”\n(Robinson et al., 2020; Schroff et al., 2015; Oh Song et al.,\n2016; Harwood et al., 2017; Wu et al., 2017; Ge, 2018) yield\nlarger and more informative gradients and should result in\nthe biggest improvement in representation quality.  Thus,\noverloading notation fork, we compute the reward for a\nparticular image as its representation’s average cosine simi-\nlarity to itskclosest neighbors in the target dataset. Given\nan image encoderf\nk\n:R\nH×W×3\n→R\nd\n, an unlabeled target\ndatasetD={x\ni\n}\nN\ni=1\n, and a new imageyto evaluate, the\n3",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nAlgorithm 1Internet Explorer\n1:Input:target datasetD, SSL algorithmA, search en-\ngineSE, encoderf:R\nH×W×3\n→R\nd\n, image reward\nfunctionr, vocabularyV={c\ni\n}\nC\ni=1\n,#concepts/itrM,\n#query results/searchQ, GPT-based concept→de-\nscriptor functionGPTDesc, concept distribution func-\ntionCalcProb\n2:Initialize replay bufferB ←∅\n3:Initialize concept distributionp=Uniform{1,C}\n4:foriteration= 1,2,...do\n5:fori= 1,...,Mdo\n6:Sample conceptc\ni\n∼p(V)(§2.2)\n7:Sample descriptord\ni\n←GPTDesc(c\ni\n)(§C.2)\n8:Image search{I\ni\nj\n}\nQ\nj=1\n←SE(d\ni\n+c\ni\n,Q)(§2.1)\n9:Calc. rewardr\nc\ni\n←\n1\nQ\nP\nQ\nj=1\nr(f,D,I\ni\nj\n)(§2.4)\n10:end for\n11:B\nnew\n={I\n1\nj\n}\nQ\nj=1\n∪···∪{I\nM\nj\n}\nQ\nj=1\n12:SSL training:A(f,D∪B∪B\nnew\n)(§2.3)\n13:Add to buffer:B ←B∪Top50%(B\nnew\n,r)\n14:\nPredict all concept rewardsr\nconcept\nfrom{r\nc\ni\n}(§2.5)\n15:Update concept distp←CalcProb(r\nconcept\n)(§2.7)\n16:end for\nreward is calculated:\nr(f\nk\n,D,y) =max\nI⊂{1,...,N};\n|I|=k\n1\nk\nX\ni∈I\nS\ncos\n(f\nk\n(x\ni\n),f\nk\n(y))(2)\nwhereS\ncos\nis the cosine similarity.  A previous metric for\nidentifying relevant data (Jiang et al., 2021) usedk=  1\nnearest neighbors, but we found that this was too noisy and\nallowed high rewards for outlier target images to distract\nour search. We instead usek= 15to improve the accuracy\nof our relevance estimation. In Section 4.6, we compare our\nreward to alternatives and explore their failure modes. This\nreward is used for two purposes: determining which of the\ndownloaded images to train on and, subsequently, which\nconcepts would be useful to search for next.\nWhich images to train on.Many newly downloaded im-\nages are not worth training on, since they come from un-\nrelated queries or are noisy results from the search engine.\nThus, at the end of each iteration, we rank the newly down-\nloaded images by their reward and save the top50%to a\nreplay buffer that we maintain across iterations.  In subse-\nquent iterations, we continue training on this filtered data.\nDetermining  which  concepts  are  useful.When  we\nsearch for a concept and get backQimage results{I\ni\n}\nQ\ni=1\n,\nwe  take  the  average  of  the  top  10  image-level  rewards\nr\ni\n=r(f\nk\n,D,I\ni\n)and use that as aconcept-level score. This\ngives us an accurate measure of the relevance of a particular\nquery and reduces the impact of noisy search results.\n2.5. Estimating Reward for Unseen Concepts\nSince our vocabulary contains hundreds of thousands of\nconcepts, it is inefficient to search to test whether a query\nyields relevant images. Luckily, we can estimate the quality\nof a query by using the observed rewards of the queries used\nso far.  Humans can do this effortlessly due to our under-\nstanding of what each concept means. To us, it is obvious\nthat if querying “golden retriever” yielded useful images\nfor this dataset, then “labrador retriever” probably should\nas well. To give our method the same understanding of con-\ncept meaning, we embed our146,347WordNet concepts\ninto a 384-dimensional space using a pre-trained sentence\nsimilarity model (Reimers & Gurevych, 2019). We provide\nrelevant context about concepts to the text embedding model\nusing the following template:\n{lemma}({hypernym}):{definition}.\nFor example,\nChihuahua (toy dog):  an old breed\nof tiny short-haired dog with\nprotruding eyes from Mexico held\nto antedate Aztec civilization.\nWe use Gaussian process regression (GPR) (Williams &\nRasmussen, 1995) over the text embeddings{e\ni\n}to predict\nthe concept-level rewardr(e\ni\n)for untried concepts. GPR\nmodels the function outputs for any set of inputs{r(e\ni\n)}\nas jointly Gaussian random variables.  The covariance of\nany  two  variablesr(e\ni\n)andr(e\nj\n)is  determined  by  the\nkernelk(e\ni\n,e\nj\n), which we set as the default RBF kernel\nk(e\ni\n,e\nj\n) = exp(\n−∥e\ni\n−e\nj\n∥\n2\n2\n). Given the observed rewards\nfor conceptsR\nobs\n={r(e\ni\n)}, GPR calculates the posterior\ndistribution over the rewards for an unobserved concept\ne\n′\n,P(r(e\n′\n)|{r(e\ni\n)}=R\nobs\n).  Given that the joint distri-\nbutionP({r(e\ni\n)},r(e\n′\n))is Gaussian, the posterior is also\nGaussian with meanμ(e\n′\n)and varianceσ(e\n′\n)\n2\n. The local-\nity provided by the RBF kernel enables reasonable reward\npredictions, and having a distribution over rewards instead\nof a point estimate allows us to explore potentially good\nconcepts. We encourage exploration by setting the score of\nunobserved concepts toμ(e\ni\n) +σ(e\ni\n).\n2.6. Provable speedup in relevant query identification\nOnly  a  small  subset  of  our  vocabulary  ofnconcepts  is\nrelevant to the target dataset. We assume that the relevant\nconcepts are partitioned intocdisjoint clusters of sizes, with\ncs≪n.  We want to discover every relevant concept by\nsampling concepts uniformly at random (with replacement)\nto test.  We assume that sampling a concept conclusively\ntells us whether it is relevant. Furthermore, we assume that\nwe could optionally use an algorithm (e.g., Gaussian process\nregression) that, if we have sampled a relevant concept, tells\n4",
    "Internet Explorer: Targeted Representation Learning on the Open Web\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n10\n−5\n10\n−3\nProbability\nScale, softmax\nScale, softmax, tier\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nSorted Concept Index (log scale)\n0.0\n0.5\n1.0\nCumulative Prob.\nFigure 3.Learned concept sampling distribution.Given esti-\nmated scores for each of the146,347concepts, we need to choose\nhow often to sample each one in order to balance exploration and\nexploitation.Top:we scale our scores to a desired temperature,\nthen take the softmax to obtain a distribution over concepts.  Fi-\nnally, we create tiers so that the top 250 concepts have80%of the\nprobability mass, and the next 750 have10%. This ensures that we\nsample enough from the top1,000concepts while still exploring\nother concepts with lower scores.Bottom:the top1,000concepts\nare only sampled a tiny fraction of the time without tiering.\nus that all concepts in its cluster are also relevant.  Then,\nLemma  2.1  shows  that  the  Gaussian  process  drastically\nreduces the time required to identify all relevant concepts.\nLemma  2.1.LetT\nbase\nbe  the  expected  time  to  identify\nevery relevant concept without the GPR, andT\nGP R\nbe the\nexpected time when exploiting the additional knowledge\nfrom the GPR. Then,T\nbase\n=nH\nc·s\n,T\nGP R\n=\nnH\nc\ns\n, and\nthe speedup from GPR is\nT\nbase\nT\nGP R\n≈slogs.\nThe proof is in Appendix D. For our vocabulary and target\ndatasets,s≈100. This shows that a predictive model like\nGPR is crucial for quickly identifying all useful concepts.\n2.7. Query sampling distribution\nOnce we have estimates for the quality of each concept,\nhow do we determine what to search for next?  We face\nthe age-old dilemma of exploration versus exploitation: we\nneed to sample the top concepts frequently enough to get\nrelevant training data for SSL, while at the same time, we\nneed sufficient exploration of promising untried concepts.\nWe use a sampling-based approach based on Boltzmann\nexploration (Sutton, 1991).  Boltzmann exploration sam-\nples  based  on  a  scaled  softmax  distributionp(c\ni\n)∝\nexp(r(c\ni\n)/τ), whereτis the temperature scaling.  How-\never,  with  a  large  vocabulary  (action  space)  of146,347\nconcepts,  it becomes difficult to tuneτso that we sam-\nple the top concepts frequently enough without being too\nskewed. Thus, we define a “tiering function” to adjust the\nprobability mass in specified intervals of our distribution.\nGiven a sorted discrete probability distributionp, interval\nboundariesT\n0\n= 0< T\n1\n<···< T\nn\n, and interval masses\n∆\n0\n,...,∆\nn−1\nsuch that\nP\ni\n∆\ni\n=  1\n, tiering computes a\nnew distribution:\np\ntier\ni\n= ∆\nj\np\ni\nP\nT\nj+1\nk=T\nj\np\nk\nforjs.t.T\nj\n≤i < T\nj+1\n(3)\np\ntier\nis a new distribution such that\nP\nT\nj+1\nk=T\nj\np\ntier\n= ∆\nj\n. We\nuseT\n0\n= 0,T\n1\n= 250,T\n2\n= 1,000,T\n3\n= 146,347,∆\n0\n=\n0.8,∆\n1\n= 0.1, and∆\n2\n= 0.1.  Simply put:  we give the\nhighest-ranked250concepts80%of the probability mass,\nthe next750concepts10%, and all remaining concepts10%.\nFigure 3 shows that tiering the scaled softmax distribution\nsamples frequently enough from the top concepts while a\nvanilla scaled softmax distribution does not.\n3. Experimental Setting\n3.1. Self-supervised Exploration\nWe assume that we have an unlabeled target dataset of im-\nages for which we would like to learn useful visual features.\nWe compare three methods:\n1.  Random: sample concepts uniformly from the vocab.\n2.  Ours: sample concepts from our learned distribution.\n3.  Ours++: additionally use GPT-generated descriptors.\n3.2. Label Set-guided Exploration\nWe may sometimes know the set of labels for our task (e.g.,\n“golden retriever,” etc.) even if we do not have image-label\npairs. Knowing the label set greatly accelerates learning on\nthe Internet, because it acts as a strong prior on what could\nbe useful.  Using our text similarity model, we reduce the\nsize of the vocabulary by selecting the top10%(14,635con-\ncepts) with the largest average top-ksimilarity to the label\nset in text embedding space. We setkto a third of the size of\nthe label set to reduce the impact of outliers. Reducing the\nsize of the vocabulary strengthens our baselines by ensuring\nthat they only search for potentially useful concepts.  We\ncompare 4 methods:\n1.  Labels: only search for labels.\n2.Labels + relevant:  search for labels half of the time,\nand random concepts from the pruned vocabulary the\nother half of the time.\n3.\nOurs: sample labels half of the time and sample from\nour learned concept distribution the other half.\n4.  Ours++: additionally use GPT-generated descriptors.\nWe call this setting “label set-guided,” since we have addi-\ntional supervision in the form of the label set.\n3.3. Datasets and Metrics\nWe  evaluate  Internet  Explorer  on  4  popular  small-scale\nfine-grained classification datasets:  Birdsnap (Berg et al.,\n5",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nTarget dataset: Pets\nIteration 0Iteration 1Iteration 3Iteration 6Iteration 10Iteration 15\nFigure 4.Progression of downloaded images across training. Top:samples of Oxford-IIIT Pets images.Bottom:samples of images\nqueried by Internet Explorer across iterations. As it learns, it makes queries that are progressively more relevant to the target dataset.\n02040\nIteration\n25\n30\nk-NN Val Accuracy (%)\nBirdsnap\n02040\nIteration\n90\n95\nFlowers\n02040\nIteration\n70\n72\n74\nFood\n020\nIteration\n70\n80\nPets\n01020\nIteration\n55\n60\n65\nVOC2007\nOurs++OursRandom\nFigure 5.Learning curves in self-supervised setting.We show howk-NN validation accuracy improves across iterations on each target\ndataset. Without using any labels, Internet Explorer identifies and focuses on relevant concepts for each target dataset. This allows it to\nfind more useful data than the baseline that searches for random concepts. Adding GPT-generated descriptors (Ours++) further improves\nperformance by enabling Internet Explorer to generate diverse views of useful concepts.\n2014), Flowers-102 (Nilsback & Zisserman, 2008), Food101\n(Bossard et al., 2014), and Oxford-IIT Pets (Parkhi et al.,\n2012).   These  small  datasets  consist  of2,040to75,750\ntraining examples, making them ideal for testing whether\nInternet Explorer can efficiently find relevant useful data.\nWe also evaluate on PASCAL VOC 2007 (Cls) (Evering-\nham et al., 2010), a coarse-grained multi-label classification\ntask, and ImageNet-100 (Tian et al., 2020). Finally, we try\nFMoW (Christie et al., 2018), a satellite domain classifi-\ncation task. We compare the representation quality of our\nmodelw.r.t.its target dataset using two metrics:k-nearest\nneighbors (k-NN) accuracy and linear probe accuracy.\n4. Results and Analysis\n4.1. Self-supervised Results\nFigure 5 shows how Internet Explorer improves thek-NN\naccuracy more efficiently than sampling queries uniformly\nat random from the concept vocabulary.  In fact, random\nsampling  occasionally  decreases  accuracy,  likely  due  to\nthe fact that Internet images can generally be unsuitable\nfor pre-training due to issues such as watermarks, images\ncontaining text, and overly photogenic images (Mezuman\n& Weiss, 2012; Chen & Gupta, 2015). Table 1 shows that\nour method significantly improves on the starting MoCo-v3\n(ImageNet + target) checkpoint and can outperform a CLIP\n(Radford et al., 2021) model of the same size while using\nmuch less compute and data. This is impressive as CLIP can\nbe considered an oracle since its training set contains up to\n20k Bing image search results for each WordNet lemma (in\naddition to other queries). Using GPT-generated descriptors\nin “Ours++” also significantly improves performance by\nenabling Internet Explorer to generate diverse views of the\nmost useful concepts.\n4.2. Self-supervised Exploration Behavior\nFigure  6  shows  the  progression  of  Internet  Explorer\n(Ours++) behavior on the Pets dataset in the self-supervised\nsetting. Since Pets consists of cat and dog breeds, to analyze\nthe results, we use the WordNet hierarchy to divide concepts\nin our vocabulary into 5 meaningful categories: cats, dogs,\nnon-cat felines (e.g., lion), non-dog canines (e.g., wolf), and\nother.  This categorization is only done for this post hoc\nanalysis and is not provided during training. Figure 6 (top)\n6",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nModelBirdsnapFlowersFoodPetsVOC2007IN100FMoW\n⋆\nImages   GPU hrs.\nFixed dataset, lang. supervision\nCLIP ResNet-50 (oracle)57.196.086.488.486.789.344.9400×10\n6\n4,000\nFixed dataset, self-supervised\nMoCo-v3 (ImageNet pre-train)26.883.270.579.6−−40.81.2×10\n6\n72\nMoCo-v3 (ImageNet + target)39.994.678.385.358.0\n†\n84.7\n†\n52.51.2×10\n6\n72 + 12\nNo label set information\nRandom exploration39.6  (−0.3)95.3 (+0.7)77.0 (−1.3)85.6 (+0.3)70.2 (+12.2)85.7 (+1.0)54.3 (+1.8)2.2×10\n6\n84 + 40\nOurs43.4  (+3.5)97.1 (+2.5)80.5 (+2.2)86.8 (+1.5)68.5 (+10.5)86.2 (+1.5)−  −2.2×10\n6\n84 + 40\nOurs++54.4 (+14.5)98.4 (+3.8)82.2 (+3.9)89.6 (+4.3)80.1 (+22.1)86.4 (+1.7)54.1 (+1.6)2.2×10\n6\n84 + 40\nUse label set information\nSearch labels only47.1  (+7.2)96.3 (+1.7)80.9 (+2.6)85.7 (+0.4)61.8  (+3.8)85.7 (+1.0)53.5 (+1.0)2.2×10\n6\n84 + 40\nLabels + relevant terms49.9 (+10.0)98.0 (+3.4)81.2 (+2.9)87.0 (+1.7)67.5  (+9.5)86.3 (+1.6)54.1 (+1.6)2.2×10\n6\n84 + 40\nOurs52.0 (+12.1)97.6 (+3.0)81.2 (+2.9)87.3 (+2.0)70.3 (+14.3)86.4(+1.7)––2.2×10\n6\n84 + 40\nOurs++62.8(+22.9)99.1(+4.5)84.6 (+6.3)90.8(+5.5)79.6 (+21.6)87.1 (+2.4)54.5(+2.0)2.2×10\n6\n84 + 40\nTable 1.Linear probing accuracy. Our method significantly improves the starting checkpoint performance in just 40 additional hours of\ntraining. We show the performance change from the starting MoCo-v3 (ImageNet + target) initialization in green/red. CLIP numbers\ncorrespond to linear probe (which is higher than its zero-shot accuracy). Internet Explorer reaches or often surpasses CLIP (oracle with 2x\nparams) performance on each dataset while using 2.5% as much compute and 0.5% as much data.\n†\nFor VOC2007 and IN100, we do not\ndo ImageNet pre-training because ImageNet is too similar and obscures the effect.\n⋆\nFor FMoW-WILDS, we use a hand-crafted list of\ndomain-specific descriptors common to all models (see Appendix C.8 for more details).\n051015\n0.5\n0.6\n0.7\nAvg Estimated Reward\nCats\nDogs\nOther felines\nOther canines\nOther\nFirst cat\nFirst dog\n051015\nIteration\n0.00\n0.25\n0.50\n0.75\n1.00\nProbability per Category\nCats\nDogs\nOther felines\nOther canines\nOther\nFigure 6.Self-supervised  concept  discovery  on  Pets  dataset.\nWhen targeting the Pets dataset, self-supervised Internet Explorer\nquickly estimates high reward for concepts from the cat category\n(82 concepts) and dog category (246 concepts).  It is also able\nto identify felines that are not cats (e.g., tiger) and canines that\nare not dogs (e.g., wolf), although it gives them lower reward on\naverage. Finding these categories is especially challenging since\nthey comprise only460/146,347 = 0.3%of the vocabulary.\nshows that Internet Explorer rapidly identifies the roughly\n0.3%of concepts that are useful for Pets. During the first\ntwo iterations, the average estimated reward for each cat-\negory is roughly the same.   However,  after the first dog\nconcept is searched in iteration#2, the estimated reward\nand probability mass for dogs and other canines rapidly\nincreases.  The same happens for cats after the first cat is\nsearched in iteration#4. Interestingly, while “other felines”\nand “other canines” have higher average reward than the\n“other” category, they still have much lower reward than cats\nand dogs.  This indicates that our model understands that\nother felines and canines (mostly large, wild predators) are\nonly moderately relevant for house pet cats and dogs.\nFigure 4 shows how Internet Explorer downloads progres-\nsively more useful images over time.  It shows 8 random\nimages that were downloaded in iteration#0,#1,#3,#6,\n#10, and#15in the self-supervised setting. Iteration#0\ncontains mostly useless data, like graphics or screenshots,\nbut Pets-relevant images already make up most of the down-\nloads  by  iteration#3.   Appendix  E  shows  that  Internet\nExplorer identifies useful images shockingly quickly across\nevery dataset, without any knowledge of their label sets.\n4.3. Label Set-guided Results\nInternet  Explorer  significantly  outperforms  the  stronger\nbaselines in the label set-guided setting where we addition-\nally have knowledge of the label set. Searching for the label\nset continuously provides useful data and helps us rapidly\nidentify other useful concepts. Together with the diversity\npromoted by GPT descriptors, Ours++ outperforms CLIP\nin 4/7 datasets and approaches its performance in the other\n3, using just 2.5% of the time and 0.5% the data.\n4.4. Domain dataset results\nTo  test  if  Internet  Explorer  is  effective  when  the  target\ndataset contains very specific domain knowledge, we ap-\nply it to FMoW-WILDS (Christie et al., 2018)—a popu-\nlar satellite imaging domain dataset—by hand-designing a\ndozen search prompts that help induce satellite image re-\nsults (details in Appendix C.8). Even though the WordNet\nvocabulary is not particularly suited for this dataset, Internet\nExplorer still improves the LP accuracy by2percentage\npoints (see Table 1). Notably, all of our methods dramati-\ncally outperform CLIP here, likely because the distribution\n7",
    "Internet Explorer: Targeted Representation Learning on the Open Web\n02550\nIteration\n30\n40\nk-NN Val Accuracy (%)\nBirdsnap\n01020\nIteration\n90\n95\nFlowers\n01020\nIteration\n72\n74\n76\nFood\n02040\nIteration\n70\n80\nPets\n010\nIteration\n55\n60\n65\nVOC2007\nOurs++OursLabelsLabels + relevant\nFigure 7.Learning curves in label set-guided setting.Using knowledge of the label set improves the performance of all methods.\nof satellite data is very different than the data used to train\nCLIP. This demonstrates the wide flexibility of our method\nto be applied to arbitrary domains.\n4.5. Learning from other sources of data\nWe primarily obtain images by querying Google Images,\nbut Internet Explorer is compatible with any text-to-image\nsearch engine. To measure the effect of the choice of search\nengine, we also test Internet Explorer with the Flickr photo\nsearch API and a custom search engine we built on top of\na subset of LAION-5B (Schuhmann et al., 2022). LAION-\n5B consists of noisy web-scraped (text, image) pairs, and\nour custom LAION search engine searches using approx-\nimate nearest neighbors intext embedding space.   Thus,\nit tests whether Internet Explorer can still improve even\nwhen the search engine has little inductive bias.  We dis-\ncuss more details in Appendix A. Table 2 shows that Inter-\nnet Explorer consistently improves over time, regardless of\nthe search engine we use.  Google consistently does best,\nfollowed by Flickr, then LAION (which has the smallest\npool of images to draw from). Using Internet Explorer to\nsearch LAION-5B consistently performsbetterthan random\nexploration—indicating that Internet Explorer is effective\neven for selecting data from a static dataset.\n4.6. Effect of image reward type\nWe run an ablation on the type of image relevance reward.\nInstead of calculating the image reward based on the aver-\nage similarity to thek= 15nearest neighbors in represen-\ntation space (as in Section 2.3), we also try usingk= 1\nReward TypeFood\nMoCo loss81.2\n1-NN sim83.2\n15-NN sim (ours)84.6\nTable 3.Ablation on type of im-\nage reward.MoCo loss does not\nidentify relevant concepts, and 1-\nNN is sensitive to outlier images.\nor the MoCo contrastive\nloss  as  the  reward.   Ta-\nble   3   compares   these\nthree  metrics  in  the  la-\nbel set-guided setting and\nshows   thatk=    15\ndoes  best.    We  explain\nthis   result   by   qualita-\ntively comparing the be-\nhavior of various metrics\non Food101 in Figure 8.  The MoCo loss does not iden-\n15-NN \nsimilarity:\nMoCo loss:\n1-NN \nsimilarity:\n1-NN in \nPets dataset:\nbreakfast \nburrito\nedamamechocolate \nmousse\nhamburger\nLabel:\nFigure 8.Most preferable images under different rewards.We\nshow the top 5 downloaded images ranked by 3 possible image\nrewards for adversarial Food101 examples. MoCo loss encourages\nnoisy out-of-distribution images;  15-NN (ours) prefers a wide\nvariety of food images, whereas outliers in the Food dataset throw\noff 1-NN, causing it to reward black images, text, and zebras.\ntify relevant concepts, instead preferring images that are\ndifficult to align across augmentations. Representation simi-\nlarity withk= 1also fails, as it prefers images ofzebras\nandbooksbecause they are highly similar to a few outlier\nimages in Food101.  Our proposed reward withk=  15\neliminates the influence of outliers and avoids this problem.\n4.7. Comparison to image-to-image search\nAn alternate approach to finding relevant Internet data is\nto use image-to-image search: for each image in the target\ndataset, directly retrieve images that are visually similar.\nScientific and practical issuesImage-to-image search\nuses strong visual representations from pretrained models\nin order to identify similar images. This defeats the primary\npurpose of Internet Explorer:  learning useful representa-\ntions when none exist beforehand (e.g., a new iPhone is\nreleased that is out-of-distribution for existing vision mod-\nels). Text-based search avoids this issue by using additional\nsupervision (e.g., caption and surrounding text) that makes\nit easier to index new images. Image-to-image search also\nrelies on paid APIs that can cost thousands of dollars.\n8",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nModel\nFlowersFoodPets\nGoogleFlickrLAIONGoogleFlickrLAIONGoogleFlickrLAION\nFixed dataset\nMoCo-v3 (IN)83.283.283.270.570.570.579.679.679.6\nMoCo-v3 (IN + target)94.694.694.678.378.378.385.385.385.3\nUndirected search\nRandom exploration95.395.294.877.080.080.285.684.485.1\nInternet Explorer\nOurs++ (no label set)98.498.194.681.280.380.987.388.485.9\nOurs++ (with label set)99.1    99.095.884.6    81.981.090.8    89.186.7\nTable 2.Linear probe accuracy with other search engines.  Internet Explorer improves its performance using any search engine,\nincluding Flickr and our custom text-based LAION search engine.\nComparison to text-based searchRegardless of the con-\ncerns above, we do a controlled comparison between Inter-\nnet Explorer and image-based search over LAION-5B. For\neach image in a target training set, we compute its CLIP\nViT-L/14 representation and find itsNnearest neighbors in\nLAION-5B. We chooseNso that we download a total of 1\nmillion new images, which matches how many images Inter-\nnet Explorer downloads. We then train a MoCo-v3 model on\na 1:1 mix of the target dataset and the downloaded images\nwith the exact same hyperparameters (e.g., learning rate,\nnumber of steps, etc) as Internet Explorer. Interestingly, Ta-\nble 4 shows that the image-to-image approach consistently\nlearns worse features than Internet Explorer, despite taking\nadvantage of strong, pretrained vision features from CLIP.\nWe hypothesize that image-to-image search finds images\nthat are too similar to the target images, resulting in less\nadditional information that was not already present in the tar-\nget dataset. In contrast, using text (concepts and descriptors)\nas an intermediate bottleneck encourages Internet Explorer\nto download novel images that generalize along useful axes.\n5. Related Work\nMany  papers  use  self-supervised  or  weakly-supervised\nlearning on large-scale static datasets collected from the\nInternet,  such  as  YFCC-100M  (Thomee  et  al.,  2015),\nInstagram-1B (Mahajan et al., 2018), or LAION-5B (Schuh-\nmann et al., 2022). However, these are usually impractically\nexpensive since they train on all of the data, not just the\nsubset relevant to a target dataset. Concurrent work (Oquab\net  al.,  2023)  attempts  to  address  this  by  adding  a  “one-\ntime” automatic data curation step that keeps only the most\nrelevant images from a static web crawl dataset.  This ap-\nproach works well but is limited as the selection process\ndoes not use the most up-to-date learned features or adjust\nits searches on-the-fly to focus on especially useful data.\nOther approaches obtain additional training data by search-\ning for predetermined queries. Fergus et al. (2005) create a\nsupervised training dataset from the Google image search\nresults for a list of known classes.  Kamath et al. (2022)\nFlowersPetsVOC2007\nImage-to-image96.681.667.8\nInternet Explorer (ours)98.887.076.1\nTable 4.k-NN accuracy across search methods. Image-to-image\nsearch uses CLIP ViT-L/14 vision features to acquire the nearest\nneighbors of each target dataset image. Despite using strong pre-\ntrained features and the same source data (LAION-5B), number\nof downloaded images, and other hyperparameters as Internet Ex-\nplorer, the image-to-image approach learns worse features.\nimprove a visual question-answering model using a set of\npredetermined Bing queries.  However, these approaches\nquery the internet just once, which is susceptible to noise\nin the search results, and the total amount of data is limited\nto the relevant search terms knowna priori.  Internet Ex-\nplorer’s self-supervised approach bypasses these problems.\nIt can learn useful features from noisy yet relevant data, and\nit only needs an initial image collection to identify relevant\nsearch queries.  This enables it tocontinuallyexplore the\nInternet via a potentially unbounded number of searches.\nFinally,  some approaches continuously interact with the\nInternet to find useful data.  NELL (Carlson et al., 2010;\nMitchell et al., 2018) extracts text from web pages to form\nbeliefs, and NEIL (Chen et al., 2013) uses images down-\nloaded from Google Image Search to learn visual concepts.\nHowever, both methods are undirected (i.e., they do not\nmodify their exploration behavior to prioritize specific data),\nwhich means that learning is slow and will not necessarily\nimprove performance on a desired task. In contrast, Inter-\nnet Explorer continually usestargetedexploration on the\nInternet to find data for self-supervised training.\n6. Conclusion\nWe show that interactively exploring the Internet is an effi-\ncient source of highly relevant training data—if one knows\nhow to search for it.  In just 30–40 hours of training on a\nsingle GPU, Internet Explorer significantly outperforms or\nclosely matches the performance of compute-heavyoracle\nmodels like CLIP trained on static datasets, as well as strong\nbaselines that search the Internet in an undirected manner.\n9",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nAcknowledgementsWe thank Russell Mendonca for help-\nful  discussions  and  Shivam  Duggal,  Mihir  Prabhudesai,\nSheng-Yu Wang, Jason Y. Zhang, and Rishi Veerapaneni for\npaper feedback. AL is supported by the NSF GRFP, grants\nDGE1745016 and DGE2140739. This work is supported by\nNSF IIS-2024594 and ONR MURI N00014-22-1-2773.\nReferences\nBao, H., Dong, L., and Wei, F.  Beit:  Bert pre-training of\nimage transformers.arXiv preprint arXiv:2106.08254,\n2021.\nBardes, A., Ponce, J., and LeCun, Y.   Vicreg:  Variance-\ninvariance-covariance regularization for self-supervised\nlearning.arXiv preprint arXiv:2105.04906, 2021.\nBerg, T., Liu, J., Woo Lee, S., Alexander, M. L., Jacobs,\nD. W., and Belhumeur, P. N. Birdsnap: Large-scale fine-\ngrained visual categorization of birds. InProceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 2011–2018, 2014.\nBossard, L., Guillaumin, M., and Gool, L. V.  Food-101–\nmining discriminative components with random forests.\nInEuropean conference on computer vision, pp. 446–461.\nSpringer, 2014.\nBuchner, J. imagehash (fork).https://github.com/\nJohannesBuchner/imagehash, 2021.\nCarlson, A., Betteridge, J., Kisiel, B., Settles, B., Hruschka,\nE. R., and Mitchell, T. M.  Toward an architecture for\nnever-ending language learning. InTwenty-Fourth AAAI\nconference on artificial intelligence, 2010.\nCaron, M., Touvron, H., Misra, I., J\n ́\negou, H., Mairal, J.,\nBojanowski, P., and Joulin, A.  Emerging properties in\nself-supervised vision transformers.  InProceedings of\nthe IEEE/CVF International Conference on Computer\nVision, pp. 9650–9660, 2021.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G.   A\nsimple framework for contrastive learning of visual rep-\nresentations.preprint arXiv:2002.05709, 2020.\nChen,  X.  and  Gupta,  A.   Webly  supervised  learning  of\nconvolutional  networks.   InProceedings  of  the  IEEE\ninternational conference on computer vision, pp. 1431–\n1439, 2015.\nChen, X., Shrivastava, A., and Gupta, A. Neil: Extracting\nvisual knowledge from web data. InProceedings of the\nIEEE international conference on computer vision, pp.\n1409–1416, 2013.\nChen, X., Xie, S., and He, K. An empirical study of training\nself-supervised vision transformers.  InProceedings of\nthe IEEE/CVF International Conference on Computer\nVision, pp. 9640–9649, 2021.\nChristie,  G.,  Fendley,  N.,  Wilson,  J.,  and Mukherjee,  R.\nFunctional map of the world. InCVPR, 2018.\nClinton,J.Googleimagesdownload(fork).\nhttps://github.com/Joeclinton1/\ngoogle-images-download, 2020.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248–255. Ieee, 2009.\nEveringham, M., Van Gool, L., Williams, C. K., Winn, J.,\nand Zisserman, A. The pascal visual object classes (voc)\nchallenge.IJCV, 2010.\nFeldman, V. and Zhang, C.  What neural networks mem-\norize and why:  Discovering the long tail via influence\nestimation.Advances in Neural Information Processing\nSystems, 33:2881–2891, 2020.\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\ning object categories from google’s image search.   In\nTenth IEEE International Conference on Computer Vision\n(ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE,\n2005.\nGe, W. Deep metric learning with hierarchical triplet loss.\nInProceedings of the European Conference on Computer\nVision (ECCV), pp. 269–285, 2018.\nGrill, J.-B., Strub, F., Altch\n ́\ne, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos,\nR., and Valko, M.   Bootstrap your own latent:  A new\napproach to self-supervised learning. InNeurIPS, 2020.\nHarwood, B., Kumar BG, V., Carneiro, G., Reid, I., and\nDrummond, T.  Smart mining for deep metric learning.\nInProceedings of the IEEE International Conference on\nComputer Vision, pp. 2821–2829, 2017.\nHe,  K.,  Zhang,  X.,  Ren,  S.,  and Sun,  J.   Deep residual\nlearning for image recognition. InCVPR, 2016.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.   Mo-\nmentum contrast for unsupervised visual representation\nlearning. InCVPR, 2020.\nHe, K., Chen, X., Xie, S., Li, Y., Doll\n ́\nar, P., and Girshick,\nR. Masked autoencoders are scalable vision learners. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 16000–16009, 2022.\n10",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nIlyas, A., Park, S. M., Engstrom, L., Leclerc, G., and Madry,\nA. Datamodels: Predicting predictions from training data.\narXiv preprint arXiv:2202.00622, 2022.\nJiang,  Z.,  Chen,  T.,  Chen,  T.,  and Wang,  Z.   Improving\ncontrastive learning on imbalanced data via open-world\nsampling.Advances in Neural Information Processing\nSystems, 34:5997–6009, 2021.\nJohnson, J., Douze, M., and J\n ́\negou, H. Billion-scale similar-\nity search with GPUs.IEEE Transactions on Big Data, 7\n(3):535–547, 2019.\nKamath, A., Clark, C., Gupta, T., Kolve, E., Hoiem, D.,\nand  Kembhavi,  A.   Webly  supervised  concept  expan-\nsion for general purpose vision models.arXiv preprint\narXiv:2202.02317, 2022.\nKoh, P. W. and Liang, P. Understanding black-box predic-\ntions via influence functions. InInternational conference\non machine learning, pp. 1885–1894. PMLR, 2017.\nLi,  A. C.,  Efros,  A. A.,  and Pathak,  D.   Understanding\ncollapse in non-contrastive siamese representation learn-\ning.  InEuropean Conference on Computer Vision, pp.\n490–505. Springer, 2022.\nMahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,\nM., Li, Y., Bharambe, A., and van der Maaten, L. Explor-\ning the limits of weakly supervised pretraining. InECCV,\n2018.\nMezuman, E. and Weiss, Y. Learning about canonical views\nfrom internet image collections.Advances in neural in-\nformation processing systems, 25, 2012.\nMiller, G. A. Wordnet: a lexical database for english.Com-\nmunications of the ACM, 38(11):39–41, 1995.\nMitchell, T., Cohen, W., Hruschka, E., Talukdar, P., Yang,\nB., Betteridge, J., Carlson, A., Dalvi, B., Gardner, M.,\nKisiel, B., et al. Never-ending learning.Communications\nof the ACM, 61(5):103–115, 2018.\nNilsback, M.-E. and Zisserman, A. Automated flower clas-\nsification over a large number of classes.  In2008 Sixth\nIndian Conference on Computer Vision, Graphics & Im-\nage Processing, 2008.\nOh Song, H., Xiang, Y., Jegelka, S., and Savarese, S. Deep\nmetric learning via lifted structured feature embedding. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 4004–4012, 2016.\nOord,  A.  v.  d.,  Li,  Y.,  and  Vinyals,  O.   Representation\nlearning  with  contrastive  predictive  coding.preprint\narXiv:1807.03748, 2018.\nOquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec,\nM., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-\nNouby, A., et al. Dinov2: Learning robust visual features\nwithout supervision.arXiv preprint arXiv:2304.07193,\n2023.\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C.\nCats and dogs.  In2012 IEEE conference on computer\nvision and pattern recognition,  pp. 3498–3505. IEEE,\n2012.\nPaul, M., Ganguli, S., and Dziugaite, G. K. Deep learning on\na data diet: Finding important examples early in training.\nAdvances in Neural Information Processing Systems, 34:\n20596–20607, 2021.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al.  Learning transferable visual models from natural\nlanguage supervision.  InInternational Conference on\nMachine Learning, pp. 8748–8763. PMLR, 2021.\nReimers,  N. and Gurevych,  I.   Sentence-bert:  Sentence\nembeddings using siamese bert-networks.arXiv preprint\narXiv:1908.10084, 2019.\nRobinson, J., Chuang, C.-Y., Sra, S., and Jegelka, S. Con-\ntrastive  learning  with  hard  negative  samples.arXiv\npreprint arXiv:2010.04592, 2020.\nSchroff, F., Kalenichenko, D., and Philbin, J.  Facenet: A\nunified embedding for face recognition and clustering. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 815–823, 2015.\nSchuhmann,  C.,  Beaumont,  R.,  Vencu,  R.,  Gordon,  C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.\narXiv preprint arXiv:2210.08402, 2022.\nSettles, B. Active learning literature survey. 2009.\nSutton, R. S. Dyna, an integrated architecture for learning,\nplanning, and reacting.ACM Sigart Bulletin, 2(4):160–\n163, 1991.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B.,\nNi, K., Poland, D., Borth, D., and Li, L.-J.  Yfcc100m:\nThe new data in multimedia research.arXiv preprint\narXiv:1503.01817, 2015.\nTian, Y., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. InComputer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XI 16, pp. 776–794. Springer, 2020.\n11",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nVasa, H.  Google images download.https://github.\ncom/hardikvasa/google-images-download,\n2015.\nWang,    B.   and   Komatsuzaki,    A.GPT-J-6B:   A\n6BillionParameterAutoregressiveLanguage\nModel.https://github.com/kingoflolz/\nmesh-transformer-jax, May 2021.\nWilliams, C. and Rasmussen, C.  Gaussian processes for\nregression.Advances in neural information processing\nsystems, 8, 1995.\nWu, C.-Y., Manmatha, R., Smola, A. J., and Krahenbuhl, P.\nSampling matters in deep embedding learning.  InPro-\nceedings of the IEEE international conference on com-\nputer vision, pp. 2840–2848, 2017.\nYou, Y., Gitman, I., and Ginsburg, B. Large batch training\nof convolutional networks.preprint arXiv:1708.03888,\n2017.\nZbontar,  J.,  Jing,  L.,  Misra,  I.,  LeCun,  Y.,  and Deny,  S.\nBarlow twins: Self-supervised learning via redundancy\nreduction.arXiv preprint arXiv:2103.03230, 2021.\n12",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nAppendix\nA. Learning from other sources of data\n sunflower\nShow me:\nFigure 9.Our custom LAION-5B search engine.\nWe build a custom text-to-image search engine that\nfinds images within the LAION-5B dataset by doing\nnearest neighbor search in text embedding space.\nThis uses no image features whatsoever.\nGoogle Images is an exceptionally useful data source for Internet Explorer.\nIt offers access to a large portion of the Internet’s images, and it ranks\nimages using weak supervision from the image caption, surrounding\ntext, click rates, image features, incoming and outgoing hyperlinks, and\nother signals.  This extra supervision is helpful and should be utilized.\nNonetheless, we show that Internet Explorer is agnostic to the choice of\ntext-to-image search engine and can still rapidly improve even when the\ndata source is much noisier.\nTo test Internet Explorer in the most minimal setting, we build a custom\nsearch engine that finds images solely using their accompanying text—\nwithout using any pre-trained visual features whatsoever.  We use the\nLAION-5B dataset (Schuhmann et al., 2022), which consists of>5B\nnoisy image-caption pairs. We filter the dataset to only include images\nof at least512\n2\npixels with English captions. This leaves us with about\n600M text-image pairs. To find image results for a query, we find the 100\ncaptions closest to the query in text representation space, then return the\nassociated images. We use a pre-trained text embedding model (Reimers\n& Gurevych, 2019) to compute 384-dimensional text embeddings for each caption. Then, we use Faiss (Johnson et al., 2019)\nto compute a fast, approximate nearest-neighbors lookup index. Querying our custom search engine finds 100 image results\nin less than a second. Figure 9 shows that our search engine is reasonably accurate, even without using any image features.\nWe also test Flickr’s photo search API as another text-to-image search engine, in addition to Google Images and LAION.\nFigure 11 shows that each data source has its own tendencies.  For the “spaghetti bolognese” query, Google Images is\nbiased (Mezuman & Weiss, 2012; Chen & Gupta, 2015) towards brightly-lit, photogenic images that typically come from\nfood blogs. Flickr mainly consists of amateur home photos, so it returns a messier variety of images that perhaps better\ncapture the real world. LAION images come from web crawling, without any ranking, so they additionally contain many\ngraphics with text overlays. The same image can also frequently show up in the LAION results multiple times, as a result of\nbeing posted on multiple separate pages.\nFigure 10 and Table 2 (main paper) show that Internet Explorer still improves over time, even when the data comes from\nLAION or Flickr. Internet Explorer tends to perform better with Flickr than with LAION, which makes sense. Flickr indexes\nfar more images, as our custom LAION search engine only uses 600M images, so it can return more of the useful photos\nthat Internet Explorer queries for. Flickr is also slightly better at understanding descriptors, although both Flickr and LAION\ntend to be thrown off by specific or odd descriptors.  Nevertheless, Internet Explorer significantly improves the starting\nmodel in less than a day of searching and training even with noisy search results and no hyperparameter tuning. Overall,\nthese results prove that Internet Explorer can effectively utilize any window into the Internet’s vast ocean of image data.\n05101520\nIteration\n90.0\n92.5\n95.0\n97.5\nk-NN Val Accuracy (%)\nFlowers\n05101520\nIteration\n72\n73\nFood\n0102030\nIteration\n78\n80\n82\nPets\nLAION (no label set)LAION (w/ label set)Flickr (no label set)Flickr (w/ label set)\nFigure 10.Learning from Flickr and LAION-5B.Even with the noisy search results returned by Flickr and LAION, Internet Explorer\nstill continuously improves performance.\n13",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nFood101 dataset: “Spaghetti Bolognese”\nGoogle Images: “Spaghetti Bolognese”\nFlickr: “Spaghetti Bolognese”\nLAION-5B: “Spaghetti Bolognese”\nFigure 11.Comparison of different search engines.We show images for the “spaghetti bolognese” class in the Food101 dataset, as well\nas 20 search results for “spaghetti bolognese” from Google Images, Flickr, and LAION5B. Google images are typically well-lit, aesthetic\nfood blog pictures. In comparison, Flickr images are messier, darker, and capture a wider variety of real-world conditions. LAION-5B\nimages lie somewhere in the middle, but contain text overlays much more frequently. Duplicate image results are also common.\n14",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nBirdsnapFlowersFoodPetsVOC2007\nTarget test set size184961422524636634952\nNo exploration\nTarget training set overlap1(0.05%)5(0.01%)34(0.13%)21(0.57%)0(0.00%)\nInternet Explorer\nOurs++ (no label set)28 (+1.46%)11 (+0.01%)35 (+0.00%)26 (+0.14%)1 (+0.02%)\nOurs++ (with label set)57 (+3.03%)27 (+0.36%)35 (+0.00%)43 (+0.60%)1 (+0.02%)\nTable 5.Number of leaked test set images. We use image hashing to compute the fraction of test images present in the set of images\ndownloaded by Internet Explorer.  Surprisingly, the training/validation sets of these datasets already leak a small fraction of the test\nsets—Pets is the most egregious, with0.57%test leakage. For each dataset, we show the test set size, the number of leaked test images,\nand the percentage of the test set that this represents in blue. For each version of our method, we show the total number of leaked images\nthat the model had access to, and the percentage increase this represents over the training set’s leakage in blue. Leakage numbers for our\nmethods include this train-test leakage, since our methods also train on the target dataset’s training set. Internet Explorer only finds a tiny\nfraction of test set images online, and it only uses them for self-supervised training, so there is nolabel leakage. Internet Explorer’s large\nincrease in accuracy cannot be explained by test set leakage, so its performance gains must come through better feature learning and\ngeneralization.\nB. Are we finding the entire test set online?\nOne may be concerned that Internet Explorer improves performance mainly by finding a significant portion of the test\nset images online. We address this concern by checking how much test data Internet Explorer has downloaded. We use\ndifference hashing (dHash) (Buchner, 2021) to compute hashes for the target dataset’s training set, its test set, and the≈10\n6\nimages that Internet Explorer has downloaded. We compare hashes to determine how many test images were leaked, and\nwe report the number of collisions in Table 5. Across all five datasets, Internet Explorer finds very few test images. On\nBirdsnap, Internet Explorer finds 56 additional test set images that were not leaked in the training set, which is roughly\n3%of the test set. On the other datasets, the amount leaked ranges from0.003%to0.6%of the test set. Additionally, we\nonly perform image-based self-supervised training on downloaded images, so it is much harder for our model to cheat with\nthe leaked images. Overall, given that Internet Explorer outperforms its starting checkpoint by between 5 to 30 percentage\npoints, we conclude that its performance cannot be explained by cheating.\nIn fact, we view it as a positive that Internet Explorer finds some test set images, because it serves as confirmation that it is\nlearning to search for relevant images—and the most relevant images possible would be those from the dataset itself! But\nbeyond test set images, Internet Explorer finds a lot of internet images that are very relevant to the dataset. We visualize the\ntop-10 most similar downloaded images for 5 randomly selected test set images from multiple datasets in Figures 12 to 16.\nWe use CLIP ViT-L/14 to compute the representations of the test set images, as well as the downloaded images. We then\nfind the top-10 most similar online images given a test set image (from the downloaded images using Ours++ (with label\nset)). We see that Internet Explorer finds several images that are very similar but not identical to the test set images.\nC. Method Details\nC.1. WordNet Lemmas\nWe draw our concepts from the WordNet hierarchy (Miller, 1995), which consists of146,347noun lemmas. For reference,\nhere are 32 randomly sampled concepts:\n\"resolution\", \"lodgment\", \"phycobilin\", \"acidosis\", \"widening\", \"human\nface\", \"family Crassulaceae\", \"sail\", \"Ipomoea imperialis\", \"Davis\",\n\"prothrombin\", \"cease\", \"marsh clematis\", \"major power\", \"chump change\",\n\"madcap\", \"junky\", \"pere david’s deer\", \"make-up\", \"genus Rumex\", \"gape\",\n\"Brachychiton populneus\", \"bell morel\", \"wain\", \"friendly\", \"Principe\",\n\"bottle green\", \"glycerol trimargarate\", \"water-shield\", \"San Joaquin\nRiver\", \"woodsman\", \"pin\".\n15",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nTest Img.Ranked Nearest Neighbors in Downloaded Images\nOxford-IIIT Pets\nFigure 12.Top-10 most similar online images to Pets101\nTest Img.Ranked Nearest Neighbors in Downloaded Images\nFood101\nFigure 13.Top-10 most similar online images to Food101\n16",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nTest Img.Ranked Nearest Neighbors in Downloaded Images\nOxford Flowers 102\nFigure 14.Top-10 most similar online images to Flowers102\nTest Img.Ranked Nearest Neighbors in Downloaded Images\nVOC2007\nFigure 15.Top-10 most similar online images to PASCAL VOC2007\n17",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nTest Img.Ranked Nearest Neighbors in Downloaded Images\nImageNet-100\nFigure 16.Top-10 most similar online images to IN100\nC.2. GPT-J Descriptor Prompting\nWe use GPT-J-6B (Wang & Komatsuzaki, 2021), a free, open-source autoregressive language model, to generate useful\ndescriptors for a given concept. We use the following prompt template:\n\"What are some words that describe the quality of ‘{concept}’?\nThe{concept}is frail.\nThe{concept}is red.\nThe{concept}is humongous.\nThe{concept}is tall.\nThe{concept}is\"\nWe sample completions with a temperature of 0.9 and a max length of 100 tokens. We truncate the completion after the first\ncomma, period, underscore, or newline character (including the special character). If the truncated completion is degenerate\nand contains a duplicate of the concept, we resample another completion.  After successfully sampling a descriptor, we\nprepend it to the concept and use the resulting phrase as our search query.\nFor reference, here are 32 randomly sampled descriptors for “labrador retriever”:\n\"a good-looking dog\", \"very gentle\", \"a\", \"brown\", \"lovable\", \"a\nstrong runner\", \"a male or a female\", \"sturdy\", \"agile\", \"a strong\",\n\"beautiful\", \"a male\", \"kind\", \"long-haired\", \"a male or a female\", \"a\ngood-looking dog\", \"gentle\", \"medium\", \"loyal\", \"very gentle\", \"blue-eyed\",\n\"sturdy\", \"blue-eyed\", \"a retriever\", \"kind\", \"loyal\", \"large\", \"brown\",\n\"good-natured\", \"gentle\", \"large\", \"small\".\nC.3. Concept Vocabulary Size\nAs stated in Section 2.2, our vocabulary comprises the146,347noun lemmas in the WordNet hierarchy. Thus, in all our\nexperiments, Internet Explorer only searches for WordNet terms (plus the class names, if we have knowledge of the label\n18",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nDatasetCategory\nOxford Flowers102Flower\nOxford IIIT PetsPet\nFood101Food\nBirdsnapBird\nVOC2007Object\nTable 6.Target Dataset “Category”.\nset). We found that this worked quite well for these standard benchmarks. Note that expanding the vocabulary (e.g., adding\ntechnical terms relevant to a specific topic) can easily be done by adding those terms to the list of possible concepts. One\neasy extension would be to add page titles and frequent unigrams and bigrams from Wikipedia, as was done to generate the\nCLIP training set (Radford et al., 2021). Doing so would expand our vocabulary to roughly500,000total concepts.\nC.4. Query Model Details\nTemperature for concept distributionAfter estimating scoresr(c\ni\n)for each conceptc\ni\n, we do a temperature-scaled\nsoftmax, followed by the tiering operation described in Section 2.6. We compute the temperatureτsuch that\nSMR=\nmax\ni\nr(c\ni\n)−min\ni\nr(c\ni\n)\nτ\n(4)\nwhere the “softmax range”SMR∈Ris the desired gap between the largest and smallest scores after temperature scaling.\nAfter the softmaxp(c\ni\n)∝exp(r(c\ni\n)/τ), the softmax range determines the likelihood ratio of most likely concept to least\nlikely concept:\nmax\ni\np(c\ni\n)\nmin\ni\np(c\ni\n)\n=\nmax\ni\nexp(r(c\ni\n)/τ)\nmin\ni\nexp(r(c\ni\n)/τ)\n(5)\n= exp\n\u0012\nmax\ni\nr(c\ni\n)−min\ni\nr(c\ni\n)\nτ\n\u0013\n(6)\n= exp(SMR)(7)\nThus, SMR is an easy way to specify the relative likelihood of the highest and lowest scoring concepts and achieve a desired\nexploration-exploitation balance.\nLabel set-guided vocabularyTo reduce our search space in the label set-guided setting, in which we know the English\nnames of the classes a priori,  we generate a subset of the WordNet vocabulary that contains only the top-10%most\nsemantically-relevant concepts to each target dataset. We use a pre-trained text embedding model (Reimers & Gurevych,\n2019) to generate384-dimensional embeddings for each concept in WordNet, using the same template described in Section\n2.5 of the main paper:\n{lemma}({hypernym}):{definition}.\nTo generate a similar embedding for concepts in target datasets, we use the summary from Wikipedia in place of the\ndefinition and the “category” of the target dataset (shown in Table 6) in place of the hypernym:\n{label}({category}):{summary}.\nAfter generating the embeddings for each concept in the target dataset, we find thek-NN distance for each WordNet concept\nto the target dataset embeddings, wherekis chosen to be1/3the size of the class label set. We then rank the concepts in\nWordNet by the distance and take the closest10%of terms as our subset. This subset is used for all methods in the label\nset-guided setting, including the random exploration methods.\nC.5. Training Details\nIn each iteration, we download roughly 25k candidate images, since we download up to 100 images for each of the 256\nqueries. Given this setCof candidate images, we samplePCR×|C|images from the union of the replay bufferBand the\n19",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nHyperparameterValue\nArchitectureResnet-50 (He et al., 2016)\nOptimizerLARS (You et al., 2017)\nBatch size224\nLearning rate0.8×\n224\n256\nLearning rate scheduleconstant\nMoCo momentum0.9985\nRandomResizedCrop min crop area0.2\nQueries per iteration256\nRequested images per query100\nMin images per query10\nSoftmax range (SMR)3\nPCR2\nEpochs per iteration10\nTable 7.Internet Explorer hyperparameters.\ntarget dataset training imagesD. PCR (past data to candidate data ratio) is a scalar value that determines how much old data\nvs new data to train on at every iteration. We setPCR= 2for all experiments. We perform10epochs of training over the\nunion of the new candidate data and the sampled replay buffer and target dataset images.\nC.6. Hyperparameters\nTable 7 shows our hyperparameter values, which are shared across datasets. We perform minimal hyperparameter tuning\nand copy most of the values from the MoCo-v3 (Chen et al., 2021) ResNet-50 configuration. Our code has been released\nathttps://github.com/internet-explorer-ssl/internet-explorer, which we hope will clarify any\nremaining implementation details and make it easy for the community to reproduce and build on our work.\nC.7. Image Licenses\nInternet Explorer uses images that were indexed by a web crawler (Google Images and LAION) or uploaded to Flickr. The\nimages and their rights belong to their respective owners; we use, download, and train on them under fair use guidelines for\nresearch.\nC.8. Domain Dataset Descriptor Details\nWhen targeting a niche domain dataset—in which a practitioner almost surely has useful a priori knowledge to impart—it is\nsimple to modify Internet Explorer to accelerate learning. Rather than using GPT to help provide variety to our queries for a\nconcept, we can use leverage our practitioner’s domain knowledge to help hone our search from the start.\nThis amounts to defining a list of “descriptors” that help return relevant results for arbitrary queries. For example, the below\nlist of 16 descriptors was selected for the FMoW satellite dataset to help return satellite imagery when prepended to concepts\n(e.g., “tennis court”) instead of their more canonical views. This list was hand-selected through trial & error using a variety\nof randomly selected concepts. Note that this static list replaces the GPT-J generated descriptors for this dataset.\nFMoW-WILDS Descriptors:\n\"a centered satellite photo of\", \"a satellite photo of\", \"a google earth\nphoto of\", \"satellite view of\", \"high resolution satellite\", \"high\nresolution satellite imagery of\", \"aerial satellite\", \"aerial satellite\nview\", \"aerial satellite view of\", \"satellite imagery, centered photo\nof\", \"satellite imagery, photo of\", \"military highest resolution satellite\nimagery of\", \"NASA imagery of\", \"geo high resolution satellite\", \"land\ncover satellite image of\", \"european satellite close up aerial image of\",\n\"super high resolution highest resolution satellite imagery\"\n20",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nD. Proof of Lemma 2.1\nHere, we prove Lemma 2.1 from Section 2.6, which we repeat below:\nLemma 2.1.LetT\nbase\nbe the expected time to identify every relevant concept without the GPR, andT\nGP R\nbe the expected\ntime when exploiting the additional knowledge from the GPR. Then,T\nbase\n=nH\nc·s\n,T\nGP R\n=\nnH\nc\ns\n, and the speedup from\nGPR is\nT\nbase\nT\nGP R\n≈slogs.\nProof.\nThis problem is a variant of the coupon collector problem. Let’s first computeT\nbase\nas the sum of expected timest\ni\nto identify the next relevant concept.\nT\nbase\n=\ncs\nX\ni=1\nt\ni\n(8)\n=\ncs\nX\ni=1\n1\np\ni\n(9)\n=\ncs\nX\ni=1\nn\ncs+ 1−i\n(10)\n=n\ncs\nX\ni=1\n1\ncs+ 1−i\n(11)\n=nH\ncs\n(12)\nwhereH\ncs\nis thecsth harmonic number. Similarly, we can computeT\nGP R\nas the sum of expected timest\ni\nto identify the\nnext relevant cluster.\nT\nGP R\n=\nc\nX\ni=1\nt\ni\n(13)\n=\nc\nX\ni=1\n1\np\ni\n(14)\n=\nc\nX\ni=1\nn\ns(c+ 1−i)\n(15)\n=\nn\ns\nc\nX\ni=1\n1\nc+ 1−i\n(16)\n=\nnH\nc\ns\n(17)\nThe speedup is then\nT\nbase\nT\nGP R\n=s\nH\ncs\nH\nc\n≈slogs.\nWe find that in practical settings (e.g., the Pets example analyzed in Figure 6), we can accurately predict how many samples\nare required to discover all useful concepts. If the vocabulary size isn≈150,000, the number of clusters is aboutc= 2\n(one for cats and one for dogs), and the size of each cluster is about150, thenT\nGP R\n= 1500, which roughly matches the9\niterations×256queries/iteration= 1792queries it took to discover both cats and dogs in the Pets dataset.\nE. Progression of downloaded images\nJust as Figure 4 in the main paper showed how Internet Explorer progressively discovers useful data when targeting the Pets\ndataset, Figures 17 to 20 show the progression of downloaded images when targeting Birdsnap, Flowers, Food, and VOC\nrespectively. Note that this analysis is in the self-supervised setting, where Internet Explorer has no knowledge of the label\nset. Thus, it is quite surprising that Internet Explorer is able to identify relevant images in so few iterations.\n21",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nTarget dataset: Birdsnap\nIteration 0Iteration 1Iteration 3Iteration 6Iteration 10Iteration 15\nFigure 17.Progression of downloaded Birdsnap images.This corresponds to Ours++ without using label set information.\nTarget dataset: Flowers\nIteration 0Iteration 1Iteration 3Iteration 6Iteration 10Iteration 15\nFigure 18.Progression of downloaded Flowers images.This corresponds to Ours++ without using label set information.\n22",
    "Internet Explorer: Targeted Representation Learning on the Open Web\nTarget dataset: Food\nIteration 0Iteration 1Iteration 3Iteration 6Iteration 10Iteration 15\nFigure 19.Progression of downloaded Food images.This corresponds to Ours++ without using label set information.\nTarget dataset: VOC2007\nIteration 0Iteration 1Iteration 3Iteration 6Iteration 10Iteration 15\nFigure 20.Progression of downloaded VOC2007 images.This corresponds to Ours++ without using label set information.\n23"
  ]
}