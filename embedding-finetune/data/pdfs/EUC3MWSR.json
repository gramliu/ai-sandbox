{
  "key": "EUC3MWSR",
  "url": "http://arxiv.org/pdf/2404.19737",
  "metadata": {
    "title": "Better &amp; Faster Large Language Models via Multi-token Prediction",
    "abstract": "  Large language models such as GPT and Llama are trained with a next-token\nprediction loss. In this work, we suggest that training language models to\npredict multiple future tokens at once results in higher sample efficiency.\nMore specifically, at each position in the training corpus, we ask the model to\npredict the following n tokens using n independent output heads, operating on\ntop of a shared model trunk. Considering multi-token prediction as an auxiliary\ntraining task, we measure improved downstream capabilities with no overhead in\ntraining time for both code and natural language models. The method is\nincreasingly useful for larger model sizes, and keeps its appeal when training\nfor multiple epochs. Gains are especially pronounced on generative benchmarks\nlike coding, where our models consistently outperform strong baselines by\nseveral percentage points. Our 13B parameter models solves 12 % more problems\non HumanEval and 17 % more on MBPP than comparable next-token models.\nExperiments on small algorithmic tasks demonstrate that multi-token prediction\nis favorable for the development of induction heads and algorithmic reasoning\ncapabilities. As an additional benefit, models trained with 4-token prediction\nare up to 3 times faster at inference, even with large batch sizes.\n",
    "published": "2024-04-30T17:33:57Z"
  },
  "text": [
    "Better & Faster Large Language Models via Multi-token Prediction\nFabian Gloeckle\n* 1 2\nBadr Youbi Idrissi\n* 1 3\nBaptiste Rozière\n1\nDavid Lopez-Paz\n+ 1\nGabriel Synnaeve\n+ 1\nAbstract\nLarge language models such as GPT and Llama\nare trained with a next-token prediction loss. In\nthis work, we suggest that training language mod-\nels to predictmultiplefuture tokens at once results\nin higher sample efficiency. More specifically, at\neach position in the training corpus, we ask the\nmodel to predict the followingntokens usingn\nindependent output heads, operating on top of a\nshared model trunk. Considering multi-token pre-\ndiction as an auxiliary training task, we measure\nimproved downstream capabilities with no over-\nhead in training time for both code and natural\nlanguage models. The method is increasingly use-\nful for larger model sizes, and keeps its appeal\nwhen training for multiple epochs. Gains are es-\npecially pronounced ongenerativebenchmarks\nlike coding, where our models consistently out-\nperform strong baselines by several percentage\npoints.  Our 13B parameter models solves 12 %\nmore problems on HumanEval and 17 % more on\nMBPP than comparable next-token models. Ex-\nperiments on small algorithmic tasks demonstrate\nthat multi-token prediction is favorable for the\ndevelopment of induction heads and algorithmic\nreasoning capabilities.  As an additional benefit,\nmodels trained with 4-token prediction are up to\n3×faster at inference, even with large batch sizes.\n1. Introduction\nHumanity has condensed its most ingenious undertakings,\nsurprising  findings  and  beautiful  productions  into  text.\nLarge Language Models (LLMs) trained on all of these\ncorpora are able to extract impressive amounts of world\nknowledge, as well as basic reasoning capabilities by im-\nplementing a simple—yet powerful—unsupervised learning\ntask:  next-token prediction.   Despite the recent wave of\nimpressive achievements (OpenAI, 2023), next-token pre-\n*\nEqual contribution\n+\nLast authors\n1\nFAIR at Meta\n2\nCERMICS\nEcole des Ponts ParisTech\n3\nLISN Université Paris-Saclay.  Cor-\nrespondence to: Fabian Gloeckle <fgloeckle@meta.com>, Badr\nYoubi Idrissi <byoubi@meta.com>.\ndiction remains an inefficient way of acquiring language,\nworld knowledge and reasoning capabilities. More precisely,\nteacher forcing with next-token prediction latches on local\npatterns and overlooks “hard” decisions. Consequently, it\nremains a fact that state-of-the-art next-token predictors call\nfor orders of magnitude more data than human children to\narrive at the same level of fluency (Frank, 2023).\nFigure 1:Overview of multi-token prediction.(Top) Dur-\ning training, the model predicts4future tokens at once, by\nmeans of a shared trunk and4dedicated output heads. Dur-\ning inference, we employ only the next-token output head.\nOptionally, the other three heads may be used to speed-up\ninference time. (Bottom) Multi-token prediction improves\npass@1 on the MBPP code task, significantly so as model\nsize increases. Error bars are confidence intervals of 90%\ncomputed with bootstrapping over dataset samples.\n1\narXiv:2404.19737v1  [cs.CL]  30 Apr 2024",
    "Better & Faster Large Language Models via Multi-token Prediction\nIn this study, we argue that training LLMs topredict multiple\ntokensat once will drive these models toward better sample\nefficiency. As anticipated in Figure 1, multi-token prediction\ninstructs the LLM to predict thenfuture tokens from each\nposition in the training corpora, all at once and in parallel (Qi\net al., 2020).\nContributionsWhile  multi-token  prediction  has  been\nstudied in previous literature (Qi et al., 2020), the present\nwork offers the following contributions:\n1.We propose a simple multi-token prediction architec-\nture with no train time or memory overhead (Section 2).\n2.We provide experimental evidence that this training\nparadigm is beneficial at scale, with models up to 13B\nparameters solving around 15% more code problems\non average (Section 3).\n3.Multi-token prediction enables self-speculative decod-\ning, making models up to 3 times faster at inference\ntime across a wide range of batch-sizes (Section 3.2).\nWhile cost-free and simple, multi-token prediction is an ef-\nfective modification to train stronger and faster transformer\nmodels. We hope that our work spurs interest in novel aux-\niliary losses for LLMs well beyond next-token prediction,\nas to improve the performance, coherence, and reasoning\nabilities of these fascinating models.\n2. Method\nStandard language modeling learns about a large text corpus\nx\n1\n,...x\nT\nby implementing a next-token prediction task.\nFormally, the learning objective is to minimize the cross-\nentropy loss\nL\n1\n=−\nX\nt\nlogP\nθ\n(x\nt+1\n|x\nt:1\n),(1)\nwhereP\nθ\nis our large language model under training, as to\nmaximize the probability ofx\nt+1\nas the next future token,\ngiven the history of past tokensx\nt:1\n=x\nt\n,...,x\n1\n.\nIn this work, we generalize the above by implementing a\nmulti-token prediction task, where at each position of the\ntraining corpus, the model is instructed to predictnfuture\ntokens at once. This translates into the cross-entropy loss\nL\nn\n=−\nX\nt\nlogP\nθ\n(x\nt+n:t+1\n|x\nt:1\n).(2)\nTo make matters tractable, we assume that our large lan-\nguage modelP\nθ\nemploys a shared trunk to produce a latent\nrepresentationz\nt:1\nof the observed contextx\nt:1\n, then fed\nintonindependent heads to predict in parallel each of the\nnfuture tokens (see Figure 1).  This leads to the follow-\ning factorization of the multi-token prediction cross-entropy\nloss:\nL\nn\n=−\nX\nt\nlogP\nθ\n(x\nt+n:t+1\n|z\nt:1\n)·P\nθ\n(z\nt:1\n|x\nt:1\n)\n=−\nX\nt\nn\nX\ni=1\nlogP\nθ\n(x\nt+i\n|z\nt:1\n)·P\nθ\n(z\nt:1\n|x\nt:1\n).\nIn practice, our architecture consists of a shared transformer\ntrunkf\ns\nproducing the hidden representationz\nt:1\nfrom the\nobserved contextx\nt:1\n,nindependent output heads imple-\nmented in terms of transformer layersf\nh\ni\n, and a shared\nunembedding  matrixf\nu\n.   Therefore,  to  predictnfuture\ntokens, we compute:\nP\nθ\n(x\nt+i\n|x\nt:1\n) =softmax(f\nu\n(f\nh\ni\n(f\ns\n(x\nt:1\n)))),\nfori=  1,...n, where, in particular,P\nθ\n(x\nt+1\n|x\nt:1\n)is\nour next-token prediction head. See Appendix B for other\nvariations of multi-token prediction architectures.\nMemory-efficient implementationOne big challenge in\ntraining multi-token predictors is reducing their GPU mem-\nory utilization.  To see why this is the case, recall that in\ncurrent LLMs the vocabulary sizeVis much larger than the\ndimensiondof the latent representation—therefore, logit\nvectors become the GPU memory usage bottleneck. Naive\nimplementations of multi-token predictors that materialize\nall logits and their gradients, both of shape(n,V), severely\nlimit the allowable batch-size and average GPU memory\nutilization.  Because of these reasons, in our architecture\nwe propose to carefully adapt the sequence of forward and\nbackward operations, as illustrated in Figure 2. In particular,\nafter the forward pass through the shared trunkf\ns\n, we se-\nquentially compute the forwardandbackward pass of each\nindependent output headf\ni\n, accumulating gradients at the\ntrunk. While this creates logits (and their gradients) for the\noutput headf\ni\n, these are freed before continuing to the next\noutput headf\ni+1\n, requiring the long-term storage only of the\nd-dimensional trunk gradient∂L\nn\n/∂f\ns\n.  In sum, we have\nreduced the peak GPU memory utilization fromO(nV+d)\ntoO(V+d), at no expense in runtime (Table S5).\nInferenceDuring inference time, the most basic use of the\nproposed architecture is vanilla next-token autoregressive\nprediction using the next-token prediction headP\nθ\n(x\nt+1\n|\nx\nt:1\n), while discarding all others. However, the additional\noutput heads can be leveraged to speed up decoding from the\nnext-token prediction head withself-speculative decoding\nmethods such as blockwise parallel decoding (Stern et al.,\n2018)—a variant of speculative decoding (Leviathan et al.,\n2023) without the need for an additional draft model—and\nspeculative decoding with Medusa-like tree attention (Cai\net al., 2024).\n2",
    "Better & Faster Large Language Models via Multi-token Prediction\nFigure 2:Order of the forward/backward in ann-token\nprediction model withn= 2heads.By performing the\nforward/backward on the heads in sequential order, we avoid\nmaterializing all unembedding layer gradients in memory\nsimultaneously and reduce peak GPU memory usage.\n3. Experiments on real data\nWe demonstrate the efficacy of multi-token prediction losses\nby seven large-scale experiments. Section 3.1 shows how\nmulti-token prediction is increasingly useful when grow-\ning the model size.  Section 3.2 shows how the additional\nprediction heads can speed up inference by a factor of3×\nusing speculative decoding. Section 3.3 demonstrates how\nmulti-token prediction promotes learning longer-term pat-\nterns, a fact most apparent in the extreme case of byte-level\ntokenization. Section 3.4 shows that4-token predictor leads\nto strong gains with a tokenizer of size32k.  Section 3.5\nillustrates that the benefits of multi-token prediction remain\nfor training runs with multiple epochs.  Section 3.6 show-\ncases the rich representations promoted by pretraining with\nmulti-token prediction losses by finetuning on the Code-\nContests dataset (Li et al., 2022).  Section 3.7 shows that\nthe benefits of multi-token prediction carry to natural lan-\nguage models, improvinggenerativeevaluations such as\nsummarization, while not regressing significantly on stan-\ndard benchmarks based on multiple choice questions and\nnegative log-likelihoods.\nTo allow fair comparisons between next-token predictors\nandn-token predictors, the experiments that follow always\ncompare models with an equal amount of parameters. That\nis, when we addn−1layers in future prediction heads, we\nremoven−1layers from the shared model trunk. Please\nrefer to Table S14 for the model architectures and to Ta-\nble S13 for an overview of the hyperparameters we use in\nour experiments.\n3.1. Benefits scale with model size\nTo study this phenomenon,  we train models of six sizes\nin the range 300M to 13B parameters from scratch on at\nleast 91B tokens of code.  The evaluation results in Fig-\n+4.5\n-1.7\n25\n7112426\nMBPP\n+1.7\n-0.6\nPass@1\n23\n5\n7\n1314\nHuman Eval\n+3.9\n-5.4\n1021\n27365457\n+5.0\n-1.0\nPass@10\n5913\n172934\n0.3B0.6B1.3B\n3B\n6.7B\n13B\n+2.2\n-9.8\n304551\n607577\n0.3B0.6B1.3B\n3B\n6.7B\n13B\n+7.5\n-2.3\nPass@100\n111724\n305256\nFigure 3:Results ofn-token prediction models on MBPP\nby model size.We train models of six sizes in the range\nor 300M to 13B total parameters on code,  and evaluate\npass@1,10,100 on the MBPP (Austin et al., 2021) and Hu-\nmanEval (Chen et al., 2021) benchmark with 1000 samples.\nMulti-token prediction models are worse than the baseline\nfor small model sizes, but outperform the baseline at scale.\nError bars are confidence intervals of 90% computed with\nbootstrapping over dataset samples.\nure 3 for MBPP (Austin et al., 2021) and HumanEval (Chen\net al., 2021) show that it is possible, with the exact same\ncomputational budget, to squeeze much more performance\nout of large language models given a fixed dataset using\nmulti-token prediction.\nWe believe thisusefulness only at scaleto be a likely reason\nwhy multi-token prediction has so far been largely over-\nlooked as a promising training loss for large language model\ntraining.\n3.2. Faster inference\nWe  implement  greedyself-speculative  decoding(Stern\net al., 2018) with heterogeneous batch sizes using xForm-\ners (Lefaudeux et al., 2022) and measure decoding speeds\nof our best 4-token prediction model with 7B parameters\non completing prompts taken from a test dataset of code\nand natural language (Table S2) not seen during training.\nWe observe a speedup of3.0×on code with an average of\n2.5 accepted tokens out of 3 suggestions on code, and of\n3",
    "Better & Faster Large Language Models via Multi-token Prediction\nTable 1:Multi-token prediction improves performance and unlocks efficient byte level training.We compare models\nwith 7B parameters trained from scratch on 200B and on 314B bytes of code on the MBPP (Austin et al., 2021), HumanEval\n(Chen et al., 2021) and APPS (Hendrycks et al., 2021) benchmarks. Multi-token prediction largely outperforms next token\nprediction on these settings. All numbers were calculated using the estimator from Chen et al. (2021) based on 200 samples\nper problem. The temperatures were chosen optimally (based on test scores; i.e. these are oracle temperatures) for each\nmodel, dataset and pass@k and are reported in Table S12.\nTraining dataVocabularyn\nMBPPHumanEvalAPPS/Intro\n@1@10@100@1@10@100@1@10@100\n313B bytes\n(0.5 epochs)\nbytes\n119.342.464.718.128.247.80.10.52.4\n832.350.069.621.834.157.91.25.714.0\n1628.647.168.020.432.754.31.05.012.9\n3223.040.760.317.230.249.70.62.88.8\n200B tokens\n(0.8 epochs)\n32k tokens\n130.053.873.722.836.462.02.87.817.4\n230.355.176.222.238.562.62.19.021.7\n433.855.976.924.040.166.11.67.119.9\n631.953.973.120.638.463.93.510.822.7\n830.752.273.420.036.659.63.510.422.1\n1T tokens\n(4 epochs)\n32k tokens\n140.765.483.431.757.683.05.417.834.1\n443.165.983.731.657.386.24.315.633.7\n2.7×on text. On an 8-byte prediction model, the inference\nspeedup is6.4×(Table S3).  Pretraining with multi-token\nprediction allows the additional heads to be much more ac-\ncurate than a simple finetuning of a next-token prediction\nmodel, thus allowing our models to unlock self-speculative\ndecoding’s full potential.\n3.3.Learning global patterns with multi-byte prediction\nTo show that the next-token prediction task latches to local\npatterns, we went to the extreme case of byte-level tokeniza-\ntion by training a 7B parameter byte-level transformer on\n314B bytes,  which is equivalent to around 116B tokens.\nThe 8-byte prediction model achieves astounding improve-\nments compared to next-byte prediction, solving 67% more\nproblems on MBPP pass@1 and 20% more problems on\nHumanEval pass@1.\nMulti-byte prediction is therefore a very promising avenue\nto  unlock  efficient  training  of  byte-level  models.   Self-\nspeculative decoding can achieve speedups of 6 times for\nthe 8-byte prediction model, which would allow to fully\ncompensate the cost of longer byte-level sequences at infer-\nence time and even be faster than a next-token prediction\nmodel by nearly two times.  The 8-byte prediction model\nis a strong byte-based model, approaching the performance\nof token-based models despite having been trained on1.7×\nless data.\n3.4. Searching for the optimaln\nTo better understand the effect of the number of predicted\ntokens, we did comprehensive ablations on models of scale\n7B trained on 200B tokens of code.  We tryn= 1,2,4,6\nand8in this setting.  Results in table 1 show that training\nwith 4-future tokens outperforms all the other models con-\nsistently throughout HumanEval and MBPP for pass at 1,\n10 and 100 metrics: +3.8%, +2.1% and +3.2% for MBPP\nand +1.2%, +3.7% and +4.1% for HumanEval. Interestingly,\nfor APPS/Intro,n= 6takes the lead with +0.7%, +3.0%\nand +5.3%.  It is very likely that the optimal window size\ndepends on input data distribution.  As for the byte level\nmodels the optimal window size is more consistent (8 bytes)\nacross these benchmarks.\n3.5. Training for multiple epochs\nMulti-token training still maintains an edge on next-token\nprediction when trained on multiple epochs of the same\ndata.    The  improvements  diminish  but  we  still  have  a\n+2.4% increase on pass@1 on MBPP and +3.2% increase\non pass@100 on HumanEval, while having similar perfor-\nmance for the rest. As for APPS/Intro, a window size of 4\nwas already not optimal with 200B tokens of training.\n3.6. Finetuning multi-token predictors\nPretrained models with multi-token prediction loss also out-\nperform next-token models for use in finetunings. We evalu-\nate this by finetuning 7B parameter models from Section 3.3\n4",
    "Better & Faster Large Language Models via Multi-token Prediction\non the CodeContests dataset (Li et al., 2022). We compare\nthe 4-token prediction model with the next-token prediction\nbaseline, and include a setting where the 4-token prediction\nmodel is stripped off its additional prediction heads and\nfinetuned using the classical next-token prediction target.\nAccording to the results in Figure 4, both ways of finetuning\nthe 4-token prediction model outperform the next-token pre-\ndiction model on pass@k acrossk. This means the models\nare both better at understanding and solving the task and\nat generating diverse answers.  Note that CodeContests is\nthe most challenging coding benchmark we evaluate in this\nstudy. Next-token prediction finetuning on top of 4-token\nprediction pretraining appears to be the best method overall,\nin line with the classical paradigm of pretraining with auxil-\niary tasks followed by task-specific finetuning. Please refer\nto Appendix F for details.\n1101001000\nk\n0.2\n0.5\n1.0\n2.0\n5.0\n10.0\npass@k (%)\nn=1, n'=1\nn=4, n'=1\nn=4, n'=4\nFigure  4:Comparison  of  finetuning  performance  on\nCodeContests.We finetune a4-token prediction model\non  CodeContests  (Li  et  al.,  2022)  (train  split)  usingn\n′\n-\ntoken prediction as training loss withn\n′\n=  4orn\n′\n=  1,\nand compare to a finetuning of the next-token prediction\nbaseline model (n=n\n′\n=  1).   For evaluation,  we gen-\nerate 1000 samples per test problem for each temperature\nT∈{0.5,0.6,0.7,0.8,0.9}, and compute pass@k for each\nvalue ofkandT. Shown isk7→max\nT\npass_at(k,T), i.e.\nwe grant access to a temperature oracle.  We observe that\nboth ways of finetuning the 4-token prediction model out-\nperform the next-token prediction baseline.  Intriguingly,\nusing next-token prediction finetuning on top of the 4-token\nprediction model appears to be the best method overall.\n3.7. Multi-token prediction on natural language\nTo evaluate multi-token prediction training on natural lan-\nguage,  we train models of size 7B parameters on 200B\ntokens of natural language with a 4-token, 2-token and next-\ntoken prediction loss, respectively. In Figure  5, we evaluate\nthe resulting checkpoints on 6 standard NLP benchmarks.\nOn these benchmarks, the 2-future token prediction model\nperforms on par with the next-token prediction baseline\n500010000150002000025000\nTraining step\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nAverage accuracy\nn\n1\n2\n4\nFigure 5:Multi-token training with 7B models doesn’t\nimprove performance on choice tasks.This figure shows\nthe evolution of average accuracy of 6 standard NLP bench-\nmarks.   Detailed  results  in  Appendix  G  for  7B  models\ntrained on 200B tokens of language data.   The 2 future\ntoken model has the same performance as the baseline and\nthe 4 future token model regresses a bit. Larger model sizes\nmight be necessary to see improvements on these tasks.\nthroughout training.  The 4-future token prediction model\nsuffers a performance degradation.  Detailed numbers are\nreported in Appendix G.\nHowever,  we  do  not  believe  that  multiple-choice  and\nlikelihood-based benchmarks are suited to effectively dis-\ncerngenerative capabilitiesof language models. In order\nto avoid the need for human annotations of generation qual-\nity or language model judges—which comes with its own\npitfalls, as pointed out by Koo et al. (2023)—we conduct\nevaluations on summarization and natural language math-\nematics benchmarks and compare pretrained models with\ntraining sets sizes of 200B and 500B tokens and with next-\ntoken and multi-token prediction losses, respectively.\nFor  summarization,   we  use  eight  benchmarks  where\nROUGE metrics (Lin, 2004) with respect to a ground-truth\nsummary allow automatic evaluation of generated texts. We\nfinetune each pretrained model on each benchmark’s train-\ning dataset for three epochs and select the checkpoint with\nthe highest ROUGE-LF\n1\nscore on the validation dataset.\nFigure 6 shows that multi-token prediction models with both\nn= 2andn= 4improve over the next-token baseline in\nROUGE-LF\n1\nscores for both training dataset sizes, with\nthe performance gap shrinking with larger dataset size. All\nmetrics can be found in Appendix H.\nFor  natural  language  mathematics,  we  evaluate  the  pre-\ntrained models in 8-shot mode on the GSM8K benchmark\n(Cobbe et al., 2021) and measure accuracy of the final an-\nswer produced after a chain-of-thought elicited by the few-\nshot examples.  We evaluate pass@k metrics to quantify\ndiversity and correctness of answers like in code evaluations\n5",
    "Better & Faster Large Language Models via Multi-token Prediction\n200500\nTraining tokens (B)\n25.0\n25.5\n26.0\n26.5\n27.0\n27.5\nAvg. ROUGE-L F1\nn=1\nn=2\nn=4\nFigure 6:Performance on abstractive text summariza-\ntion.Average ROUGE-L (longest common subsequence\noverlap)F\n1\nscore for 7B models trained on 200B and 500B\ntokens of natural language on eight summarization bench-\nmarks.  We finetune the respective models on each task’s\ntraining data separately for three epochs and select the check-\npoints with highest ROUGE-LF\n1\nvalidation score.  Both\nn= 2andn= 4multi-token prediction models have an\nadvantage over next-token prediction models.  Individual\nscores per dataset and more details can be found in Ap-\npendix H.\nand use sampling temperatures between 0.2 and 1.4.  The\nresults are depicted in Figure S13 in Appendix I. For 200B\ntraining tokens, then= 2model clearly outperforms the\nnext-token prediction baseline, while the pattern reverses\nafter 500B tokens andn= 4is worse throughout.\n4. Ablations on synthetic data\nWhat drives the improvements in downstream performance\nof multi-token prediction models on all of the tasks we have\nconsidered? By conducting toy experiments on controlled\ntraining datasets and evaluation tasks, we demonstrate that\nmulti-token prediction leads toqualitative changes in model\ncapabilities and generalization behaviors.   In particular,\nSection 4.1 shows that for small model sizes,induction\ncapability—as discussed by Olsson et al. (2022)—either\nonly forms when using multi-token prediction as training\nloss, or it is vastly improved by it.  Moreover, Section 4.2\nshows that multi-token prediction improves generalization\non an arithmetic task, even more so than tripling model size.\n4.1. Induction capability\nInduction describes a simple pattern of reasoning that com-\npletes partial patterns by their most recent continuation (Ols-\nson et al., 2022). In other words, if a sentence contains “AB”\nand later mentions “A”, induction is the prediction that the\ncontinuation is “B”. We design a setup to measure induction\n1310301003001000\nParameters (M)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nInduction success\nn=1 (baseline)\nn=2 (ours)\nFigure 7:Induction capability ofn-token prediction mod-\nels.Shown is accuracy on the second token of two token\nnames that have already been mentioned previously. Shown\nare numbers for models trained with a next-token and a\n2-token prediction loss, respectively, with two independent\nruns each.  The lines denote per-loss averages.  For small\nmodel sizes, next-token prediction models learn practically\nno or significantly worse induction capability than 2-token\nprediction models, with their disadvantage disappearing at\nthe size of 100M nonembedding parameters.\ncapability in a controlled way.  Training small models of\nsizes 1M to 1B nonembedding parameters on a dataset of\nchildren stories, we measure induction capability by means\nof an adapted test set: in 100 stories from the original test\nsplit, we replace the character names by randomly generated\nnames that consist of two tokens with the tokenizer we em-\nploy. Predicting the first of these two tokens is linked to the\nsemantics of the preceding text, while predicting the second\ntoken of each name’s occurrence after it has been mentioned\nat least once can be seen as a pure induction task.  In our\nexperiments, we train for up to 90 epochs and perform early\nstopping with respect to the test metric (i.e.  we allow an\nepoch oracle). Figure 7 reports induction capability as mea-\nsured by accuracy on the names’ second tokens in relation\nto model size for two runs with different seeds.\nWe find that 2-token prediction loss leads to a vastly im-\nproved formation of induction capability for models of size\n30M nonembedding parameters and below, with their advan-\ntage disappearing for sizes of 100M nonembedding parame-\nters and above.\n1\nWe interpret this finding as follows: multi-\ntoken prediction losses help models to learn transferring\ninformation across sequence positions, which lends itself\nto the formation of induction heads and other in-context\nlearning mechanisms. However, once induction capability\nhas been formed, theselearned featurestransform induction\n1\nNote that a perfect score is not reachable in this benchmark\nas some of the tokens in the names in the evaluation dataset never\nappear in the training data, and in our architecture, embedding and\nunembedding parameters are not linked.\n6",
    "Better & Faster Large Language Models via Multi-token Prediction\n1122334455667788991010\n# operations\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nin-domainout-of-domain\nn=1\nn=2\nn=4\nFigure 8:Accuracy on a polynomial arithmetic task with\nvarying number of operations per expression.Training\nwith multi-token prediction losses increases accuracy across\ntask difficulties. In particular, it also significantly improves\nout-of-domain generalization performance, albeit at a low\nabsolute level. Tripling the model size, on the other hand,\nhas a considerably smaller effect than replacing next-token\nprediction with multi-token prediction loss (Figure S16).\nShown are two independent runs per configuration with\n100M parameter models.\ninto a task that can be solvedlocallyat the current token and\nlearned with next-token prediction alone. From this point\non, multi-token prediction actually hurts on this restricted\nbenchmark—but we surmise that there are higher forms\nof in-context reasoning to which it further contributes, as\nevidenced by the results in Section 3.1. In Figure S14, we\nprovide evidence for this explanation:  replacing the chil-\ndren stories dataset by a higher-quality 9:1 mix of a books\ndataset with the children stories, we enforce the formation\nof induction capability early in training by means of the\ndataset alone. By consequence, except for the two smallest\nmodel sizes, the advantage of multi-token prediction on the\ntask disappears: feature learning of induction features has\nconverted the task into a pure next-token prediction task.\n4.2. Algorithmic reasoning\nAlgorithmic reasoning tasks allow to measure more involved\nforms of in-context reasoning than induction alone. We train\nand evaluate models on a task on polynomial arithmetic in\nthe ringF\n7\n[X]/(X\n5\n)with unary negation, addition, mul-\ntiplication and composition of polynomials as operations.\nThe coefficients of the operands and the operators are sam-\npled uniformly. The task is to return the coefficients of the\npolynomials corresponding to the resulting expressions. The\nnumbermof operations contained in the expressions is se-\nlected uniformly from the range from 1 to 5 at training time,\nand can be used to adjust the difficulty of both in-domain\n(m≤5) and out-of-domain (m >5) generalization evalua-\ntions. The evaluations are conducted with greedy sampling\non a fixed test set of 2000 samples per number of operations.\nWe train models of two small sizes with 30M and 100M\nnonembedding parameters, respectively. This simulates the\nconditions of large language models trained on massive text\ncorpora which are likewise under-parameterized and unable\nto memorize their entire training datasets.\nMulti-token prediction improves algorithmic reasoning ca-\npabilities as measured by this task across task difficulties\n(Figure 8).   In particular,  it leads to impressive gains in\nout-of-distribution generalization, despite the low absolute\nnumbers.  Increasing the model size from 30M to 100M\nparameters, on the other hand, does not improve evalua-\ntion accuracy as much as replacing next-token prediction by\nmulti-token prediction does (Figure S16). In Appendix K,\nwe furthermore show that multi-token prediction models\nretain their advantage over next-token prediction models\non this task when trained and evaluated withpause tokens\n(Goyal et al., 2023).\n5. Why does it work? Some speculation\nWhy  does  multi-token  prediction  afford  superior  perfor-\nmance on coding evaluation benchmarks, and on small al-\ngorithmic reasoning tasks? Our intuition, developed in this\nsection, is that multi-token prediction mitigates the distri-\nbutional discrepancy between training-time teacher forc-\ning and inference-time autoregressive generation. We sup-\nport this view with an illustrative argument on theimplicit\nweightsmulti-token prediction assigns to tokens depending\non their relevance for the continuation of the text, as well as\nwith an information-theoretic decomposition of multi-token\nprediction loss.\n5.1. Lookahead reinforces choice points\nNot  all  token  decisions  are  equally  important  for  gener-\nating useful texts from language models (Bachmann and\nNagarajan, 2024; Lin et al., 2024). While some tokens allow\nstylistic variations that do not constrain the remainder of\nthe text, others representchoice pointsthat are linked with\nhigher-level semantic properties of the text and may decide\nwhether an answer is perceived as useful orderailing.\nMulti-token prediction implicitly assigns weights to training\ntokens depending on how closely they are correlated with\ntheir successors.  As an illustrative example, consider the\nsequence depicted in Figure 9 where one transition is a\nhard-to-predict choice point while the other transitions are\nconsidered “inconsequential”. Inconsequential transitions\nfollowing  a  choice  point  are  likewise  hard  to  predict  in\nadvance. By marking and counting loss terms, we find that\n7",
    "Better & Faster Large Language Models via Multi-token Prediction\nFigure 9:Multi-token prediction loss assigns higher im-\nplicit weights toconsequentialtokens.Shown is a se-\nquence in which all transitions except “5→A” are easy to\npredict, alongside the corresponding prediction targets in\n3-token prediction. Since the consequences of the difficult\ntransition “5→A” are likewise hard to predict, this transi-\ntion receives a higher implicit weight in the overall loss via\nits correlates “3→A”, ..., “5→C”.\nn\n-token prediction associates a weight of\nn(n+1)\n2\nto choice\npoints via their correlates,  and a smaller weight ofnto\ninconsequential points.  Please refer to Appendix L.3 for\nmore details. Generally, we believe that the quality of text\ngenerations depends on picking the right decisions at choice\npoints, and thatn-token prediction losses promote those.\n5.2. Information-theoretic argument\nLanguage models are typically trained by teacher-forcing,\nwhere the model receives the ground truth for each future\ntoken during training. However, during test time generation\nis unguided and autoregressive, whereby errors accumulate.\nTeacher-forcing, we argue, encourages models to focus on\npredicting well in the very short term, at the potential ex-\npense of ignoring longer-term dependencies in the overall\nstructure of the generated sequence.\nTo illustrate the impact of multi-token prediction, consider\nthe  following  information-theoretic  argument.   Here,X\ndenotes the next future token, andYthe second-next future\ntoken. The production of both of these tokens is conditioned\non some observed, input contextC, that we omit from our\nequations  for  simplicity.   When  placed  before  tokenX,\nvanilla next-token prediction concerns the quantityH(X),\nwhile multi-token prediction withn= 2aims atH(X) +\nH(Y). We decompose these two quantities as:\nH(X) =H(X|Y) +I(X;Y),\nH(X) +H(Y) =H(X|Y) + 2I(X;Y) +H(Y|X).\nBy discarding the termH(Y|X)—which appears again\nwhen predicting at the following position—we observe that\n2-token prediction increases the importance ofI(X;Y)by\na factor of2. So, multi-token predictors are more accurate at\npredicting tokensXthat are of relevance for the remainder\nof the text to come. In Appendix L.2, we give a relative ver-\nsion of the above equations that shows the increased weight\nofrelative mutual informationin a loss decomposition of\n2-token prediction loss.\n6. Related work\nLanguage modeling lossesDong et al. (2019) and Tay\net al. (2022) train on a mixture of denoising tasks with dif-\nferent attention masks (full, causal and prefix attention) to\nbridge the performance gap with next token pretraining on\ngenerative tasks.  Tay et al. (2022) uses the span corrup-\ntion objective, which replaces spans of tokens with special\ntokens for the encoder and the decoder then predicts the con-\ntents of those spans. Unlike UniLM, this allows full causal\ntraining with teacher forcing. Similarly, Yang et al. (2019)\ntrain on permuted sequences, while conserving the original\npositional embeddings, effectively training the model to pre-\ndict various parts of the sequence given a mix of past and\nfuture information.  This permuted language modeling is\nthe closest task to ours since it allows predicting beyond the\nnext token. However all of these language modeling tasks\ntrain on a small percentage of the input text:  on average\nonly 15% of the tokens are backwarded through. For Dong\net al. (2019), where the masking is done in BERT style, it\nis hard to mask more than 15% since it destroys too much\ninformation. For Tay et al. (2022), it is technically possible\nto have a larger proportion but in practice, the settings used\nhave between 15% and 25% of masked tokens. (Yang et al.,\n2019) also makes it possible to train on the whole sequence\nsince it is only permuted, and no information is lost.  Yet,\nin practice,  since the completely random permutation is\nvery hard to reconstruct, only 15% are predicted for training\nstability reasons.\nMulti-token prediction in language modellingQi et al.\n(2020) argue that multi-token prediction encourages plan-\nning, improves representations and prevents the overfitting\non local patterns that can result from teacher-forced training.\nHowever, their technical approach replicates the residual\nstreamn-fold while ours allows for compute-matched com-\nparisons and makes the residual representations participate\nmore directly in the auxiliary loss terms. Stern et al. (2018)\nand Cai et al. (2024) propose model finetunings with multi-\ntoken prediction for faster inference but do not study the\neffects of such a loss during pretraining. Pal et al. (2023) use\nprobing methods to show that next-token prediction models\nare able to predict additional consecutive tokens to a certain\nextent, but less so than our models which are specifically\ntrained for this task. Jianyu Zhang (2024) observe improve-\nments in language modelling tasks with multi-label binary\nclassification over the occurrence of vocabulary words in\nthe future as an auxiliary learning task.\n8",
    "Better & Faster Large Language Models via Multi-token Prediction\nSelf-speculative decodingStern et al. (2018) are, to the\nbest of our knowledge,  the first to suggest a speculative\ndecoding scheme for faster inference. Our architecture re-\nplaces their linear prediction heads by transformer layers,\nbut is otherwise similar. By reorganizing the order of the for-\nward/backward, we can use all loss terms instead of stochas-\ntically picking one head for loss computation.  Cai et al.\n(2024) present a more elaborate self-speculative decoding\nscheme that uses the top-kpredictions of each head instead\nof the best one only.  It can be used with the multi-token\nprediction models we train.\nMulti-target   predictionMulti-task   learning   is   the\nparadigm of training neural networks jointly on several tasks\nto improve performance on the tasks of interest (Caruana,\n1997). Learning with such auxiliary tasks allows models to\nexploit dependencies between target variables and can even\nbe preferable in the case of independent targets (Waegeman\net al., 2019). While more specifically tailored architectures\nfor multi-target prediction are conceivable (Spyromitros-\nXioufis et al., 2016; Read et al., 2021), modern deep learn-\ning approaches usually rely on large shared model trunks\nwith separate prediction heads for the respective tasks (Caru-\nana, 1997; Silver et al., 2016; Lample et al., 2022) like we\ndo.  Multi-target prediction has been shown to be a suc-\ncessful strategy in various domains, e.g. for learning time\nseries prediction with more distant time steps in the future\nas auxiliary targets (Vapnik and Vashist, 2009) or for learn-\ning from videos with several future frames (Mathieu et al.,\n2016; Srivastava et al., 2016) or representations of future\nframes (Vondrick et al., 2016) as auxiliary targets.\n7. Conclusion\nWe have proposed multi-token prediction as an improvement\nover next-token prediction in training language models for\ngenerative or reasoning tasks. Our experiments (up to 7B pa-\nrameters and 1T tokens) show that this is increasingly useful\nfor larger models and in particular show strong improve-\nments for code tasks.  We posit that our method reduces\ndistribution mismatch between teacher-forced training and\nautoregressive generation. When used with speculative de-\ncoding, exact inference gets 3 times faster.\nIn future work we would like to better understand how to au-\ntomatically choosenin multi-token prediction losses. One\npossibility to do so is to use loss scales and loss balanc-\ning (Défossez et al., 2022). Also, optimal vocabulary sizes\nfor multi-token prediction are likely different from those for\nnext-token prediction, and tuning them could lead to better\nresults, as well as improved trade-offs between compressed\nsequence length and compute-per-byte expenses.  Finally,\nwe would like to develop improved auxiliary prediction\nlosses that operate in embedding spaces (LeCun, 2022).\nImpact statement\nThe goal of this paper is to make language models more\ncompute and data efficient.  While this may in principle\nreduce the ecological impact of training LLMs, we shall be\ncareful aboutrebound effects.  All societal advantages, as\nwell as risks, of LLMs should be considered while using\nthis work.\nEnvironmental impact\nIn aggregate, training all models reported in the paper re-\nquired around 500K GPU hours of computation on hardware\nof type A100-80GB and H100. Estimated total emissions\nwere around 50 tCO2eq,  100% of which were offset by\nMeta’s sustainability program.\nAcknowledgements\nWe thank Jianyu Zhang, Léon Bottou, Emmanuel Dupoux,\nPierre-Emmanuel Mazaré, Yann LeCun, Quentin Garrido,\nMegi Dervishi, Mathurin Videau and Timothée Darcet and\nother FAIR PhD students and CodeGen team members for\nhelpful discussions. We thank Jonas Gehring for his tech-\nnical expertise and the original Llama team and xFormers\nteam for enabling this kind of research.\n9",
    "Better & Faster Large Language Models via Multi-token Prediction\nReferences\nJacob  Austin,  Augustus  Odena,  Maxwell  Nye,  Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie  Cai,  Michael  Terry,  Quoc  Le,  et  al.   Program\nsynthesis with large language models.arXiv preprint\narXiv:2108.07732, 2021.\nGregor Bachmann and Vaishnavh Nagarajan.  The pitfalls\nof next-token prediction, 2024.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer.   Scheduled sampling for sequence prediction\nwith recurrent neural networks, 2015.\nYonatan Bisk,  Rowan Zellers,  Ronan Le Bras,  Jianfeng\nGao, and Yejin Choi.  Piqa:  Reasoning about physical\ncommonsense in natural language, 2019.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\nJason D. Lee, Deming Chen, and Tri Dao. Medusa: Sim-\nple llm inference acceleration framework with multiple\ndecoding heads, 2024.\nRich Caruana.  Multitask learning.Machine learning, 28:\n41–75, 1997.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-\nrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda,\nNicholas  Joseph,  Greg  Brockman,  et  al.    Evaluating\nlarge language models trained on code.arXiv preprint\narXiv:2107.03374, 2021.\nNakhun Chumpolsathien. Using knowledge distillation from\nkeyword extraction to improve the informativeness of neu-\nral cross-lingual summarization. Master’s thesis, Beijing\nInstitute of Technology, 2020.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry  Tworek,  Jacob  Hilton,  Reiichiro  Nakano,  et  al.\nTraining verifiers to solve math word problems.arXiv\npreprint arXiv:2110.14168, 2021.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,\nYu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen\nHon. Unified language model pre-training for natural lan-\nguage understanding and generation. InProceedings of\nthe 33rd International Conference on Neural Information\nProcessing Systems, pages 13063–13075, 2019.\nAlexandre Défossez, Jade Copet, Gabriel Synnaeve, and\nYossi Adi. High fidelity neural audio compression.arXiv\npreprint arXiv:2210.13438, 2022.\nMoussa Kamal Eddine, Antoine J. P. Tixier, and Michalis\nVazirgiannis.Barthez:    a  skilled  pretrained  french\nsequence-to-sequence model, 2021.\nAlexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir R. Radev.   Multi-news:  a large-scale multi-\ndocument summarization dataset and abstractive hierar-\nchical model, 2019.\nMehrdad Farahani. Summarization using bert2bert model on\nwikisummary dataset. https://github.com/m3hrdadfi/wiki-\nsummary, 2020.\nMehrdad Farahani, Mohammad Gharachorloo, and Moham-\nmad Manthouri. Leveraging parsbert and pretrained mt5\nfor persian abstractive text summarization. In2021 26th\nInternational Computer Conference, Computer Society\nof  Iran  (CSICC).  IEEE,  March  2021.   doi:  10.1109/\ncsicc52343.2021.9420563.URLhttp://dx.doi.\norg/10.1109/CSICC52343.2021.9420563.\nMichael C Frank. Bridging the data gap between children\nand large language models.Trends in Cognitive Sciences,\n2023.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer.   Samsum corpus:  A human-annotated\ndialogue dataset for abstractive summarization.  InPro-\nceedings  of  the  2nd  Workshop  on  New  Frontiers  in\nSummarization. Association for Computational Linguis-\ntics, 2019.   doi:  10.18653/v1/d19-5409.   URLhttp:\n//dx.doi.org/10.18653/v1/D19-5409.\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna\nMenon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think\nbefore you speak: Training language models with pause\ntokens, 2023.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas\nMazeika, Akul Arora, Ethan Guo, Collin Burns, Samir\nPuranik, Horace He, Dawn Song, et al. Measuring cod-\ning  challenge  competence  with  apps.arXiv  preprint\narXiv:2105.09938, 2021.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The curious case of neural text degeneration, 2020.\nJianyu Zhang Leon Bottou. Multi-label classification as an\nauxiliary loss for language modelling. personal commu-\nnication, 2024.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettle-\nmoyer. Triviaqa: A large scale distantly supervised chal-\nlenge dataset for reading comprehension, 2017.\nDiederik Kingma and Jimmy Ba.   Adam:  A method for\nstochastic optimization.ICLR, 2015.\nRyan  Koo,  Minhwa  Lee,  Vipul  Raheja,  Jong  Inn  Park,\nZae Myung Kim, and Dongyeop Kang.  Benchmarking\ncognitive biases in large language models as evaluators.\narXiv preprint arXiv:2309.17012, 2023.\n10",
    "Better & Faster Large Language Models via Multi-token Prediction\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,\nMichael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin,\nKenton Lee, Kristina N. Toutanova, Llion Jones, Ming-\nWei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. Natural questions: a benchmark for question\nanswering research.Transactions of the Association of\nComputational Linguistics, 2019.\nGuillaume Lample, Marie-Anne Lachaux, Thibaut Lavril,\nXavier  Martinet,  Amaury  Hayat,  Gabriel  Ebner,  Au-\nrélien Rodriguez, and Timothée Lacroix. Hypertree proof\nsearch for neural theorem proving, 2022.\nYann LeCun. A path towards autonomous machine intelli-\ngence version 0.9. 2, 2022-06-27.Open Review, 62(1),\n2022.\nBenjamin Lefaudeux, Francisco Massa, Diana Liskovich,\nWenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,\nJieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut,\nand  Daniel  Haziza.   xformers:  A  modular  and  hack-\nable transformer modelling library.https://github.\ncom/facebookresearch/xformers, 2022.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.  Fast\ninference from transformers via speculative decoding,\n2023.\nYujia  Li,   David  Choi,   Junyoung  Chung,   Nate  Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago, et al.\nCompetition-level code generation with alphacode.Sci-\nence, 378(6624):1092–1097, 2022.\nChin-Yew Lin.  ROUGE: A package for automatic evalu-\nation of summaries.   InText Summarization Branches\nOut, pages 74–81, Barcelona, Spain, July 2004. Asso-\nciation  for  Computational  Linguistics.   URLhttps:\n//aclanthology.org/W04-1013.\nZhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong\nShen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan\nDuan, and Weizhu Chen. Rho-1: Not all tokens are what\nyou need, 2024.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts, 2017.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization, 2019.\nMichael Mathieu, Camille Couprie, and Yann LeCun. Deep\nmulti-scale video prediction beyond mean square error,\n2016.\nRamesh Nallapati, Bowen Zhou, Cicero Nogueira dos san-\ntos, Caglar Gulcehre, and Bing Xiang.  Abstractive text\nsummarization using sequence-to-sequence rnns and be-\nyond, 2016.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t\ngive me the details, just the summary! topic-aware con-\nvolutional neural networks for extreme summarization,\n2018.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph,  Nova  DasSarma,  Tom  Henighan,  Ben  Mann,\nAmanda  Askell,  Yuntao  Bai,  Anna  Chen,  Tom  Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez,  Scott Johnston,  Andy Jones,  Jack-\nson  Kernion,   Liane  Lovitt,   Kamal  Ndousse,   Dario\nAmodei,  Tom Brown,  Jack Clark,  Jared Kaplan,  Sam\nMcCandlish,   and  Chris  Olah.In-context  learning\nand  induction  heads.Transformer  Circuits  Thread,\n2022.  https://transformer-circuits.pub/2022/in-context-\nlearning-and-induction-heads/index.html.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.\nWainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Ja-\ncob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\nAmanda  Askell,  Peter  Welinder,  Paul  Christiano,  Jan\nLeike, and Ryan Lowe.   Training language models to\nfollow instructions with human feedback, 2022.\nKoyena Pal, Jiuding Sun, Andrew Yuan, Byron C. Wallace,\nand David Bau.  Future lens:  Anticipating subsequent\ntokens from a single hidden state, 2023.\nWeizhen  Qi,  Yu  Yan,  Yeyun  Gong,  Dayiheng  Liu,  Nan\nDuan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\nProphetnet:  Predicting future n-gram for sequence-to-\nsequence pre-training, 2020.\nJesse Read, Bernhard Pfahringer, Geoffrey Holmes, and\nEibe Frank. Classifier chains: A review and perspectives.\nJournal of Artificial Intelligence Research, 70:683–718,\n2021.\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S\nGordon. Choice of plausible alternatives: An evaluation\nof commonsense causal reasoning. In2011 AAAI Spring\nSymposium Series, 2011.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras,\nand  Yejin  Choi.   Socialiqa:  Commonsense  reasoning\nabout social interactions, 2019.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez,\nLaurent Sifre, George Van Den Driessche, Julian Schrit-\ntwieser,  Ioannis  Antonoglou,  Veda  Panneershelvam,\nMarc  Lanctot,  et  al.   Mastering  the  game  of  go  with\ndeep neural networks and tree search.nature, 529(7587):\n484–489, 2016.\n11",
    "Better & Faster Large Language Models via Multi-token Prediction\nAaditya K Singh, Stephanie CY Chan, Ted Moskovitz, Erin\nGrant, Andrew M Saxe, and Felix Hill.  The transient\nnature of emergent in-context learning in transformers.\narXiv preprint arXiv:2311.08360, 2023.\nEleftherios  Spyromitros-Xioufis,  Grigorios  Tsoumakas,\nWilliam Groves, and Ioannis Vlahavas.  Multi-target re-\ngression via input space expansion:  treating targets as\ninputs.Machine Learning, 104:55–98, 2016.\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhut-\ndinov.  Unsupervised learning of video representations\nusing lstms, 2016.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Block-\nwise parallel decoding for deep autoregressive models,\n2018.\nYi  Tay,  Mostafa  Dehghani,  Vinh  Q  Tran,  Xavier  Gar-\ncia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Sia-\nmak  Shakeri,  Dara  Bahri,  Tal  Schuster,  et  al.Ul2:\nUnifying language learning paradigms.arXiv preprint\narXiv:2205.05131, 2022.\nVladimir  Vapnik  and  Akshay  Vashist.   A  new  learning\nparadigm: Learning using privileged information.Neural\nnetworks, 22(5-6):544–557, 2009.\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nAnticipating visual representations from unlabeled video,\n2016.\nWillem  Waegeman,  Krzysztof  Dembczy\n ́\nnski,  and  Eyke\nHüllermeier.   Multi-target prediction:  a unifying view\non problems and methods.Data Mining and Knowledge\nDiscovery, 33:293–324, 2019.\nVikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick\nand (not so) dirty:  Unsupervised selection of justifica-\ntion sentences for multi-hop question answering.arXiv\npreprint arXiv:1911.07176, 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuss R Salakhutdinov,  and Quoc V Le.   Xlnet:  Gen-\neralized autoregressive pretraining for language under-\nstanding. InAdvances in neural information processing\nsystems, pages 5753–5763, 2019.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,\nand Yejin Choi. Hellaswag: Can a machine really finish\nyour sentence?, 2019.\n12",
    "Better & Faster Large Language Models via Multi-token Prediction\nA. Additional results on self-speculative decoding\n1816243240\nBatch size\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nThroughput (relative)\nk=1\nk=2\nk=3\nk=4\n1816243240\nBatch size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLatency (relative)\nk=1\nk=2\nk=3\nk=4\nFigure S10:Decoding speeds and latencies with self-speculative decoding relative to standard autoregressive decoding.\nWe usekheads of a 4-token prediction model and evaluate decoding speeds of a code model as explained in Table S2. All\nnumbers are relative to the autoregressive (k= 1) baseline with the same batch size.\nTable S2:Relative speedups with self-speculative decoding.For wikipedia and books we prompt a 7B parameter model\ntrained on 500B tokens, and for code we prompt a 7B parameter model trained on 1T tokens of code on 4200 sequences of\n512 tokens from a test dataset not seen during training, and generate completions consisting of 512 tokens using greedy\nself-speculative decoding (Stern et al., 2018) using the indicated number of heads from a 4-token prediction model. Note\nthat the maximal speedup that can be obtained with self-speculative decoding usingkheads isk. The last column shows the\naverage number of tokens retrieved from a forward containing this sequence (both verification and prediction). The speedup\nwas evaluated at the maximal batch size of 42, but is constant across batch sizes (Figure S10).\nWikipediaBooksCode\n# Heads usedRel. speedupTokens / forwardRel. speedupTokens / forwardRel. speedupTokens / forward\n11.001.001.001.001.001.00\n21.791.881.771.871.851.94\n32.352.572.322.562.542.78\n42.743.122.673.093.053.50\nTable S3:Relative speedups with self-speculative decoding with byte-level models on code.We prompt the 7B parameter\nmodels from Section 3.3 on 4096 sequences of 1024 bytes of code not seen during training, and generate completions\nconsisting of 1024 bytes using greedy self-speculative decoding (Stern et al., 2018) as in Table S2.  The speedup was\nevaluated at a batch size of 16.\nn= 8n= 16n= 32\n# Heads usedRel. speedupTokens / forwardRel. speedupTokens / forwardRel. speeduptokens / forward\n11.001.001.001.001.001.00\n21.941.981.941.981.931.97\n43.673.843.633.813.623.80\n86.397.046.256.926.226.89\n12−−8.079.368.019.30\n16−−9.2411.209.1511.15\n20−−−−9.8312.61\n24−−−−10.3413.67\n28−−−−10.5514.58\n32−−−−10.8415.35\n13",
    "Better & Faster Large Language Models via Multi-token Prediction\nB. Alternative architectures\nTable S4:Alternative architectures improve on baseline but not as consistently.Alternative architectures for multi-token\nprediction are worth exploring to improve efficiency. Here we tried Anticausal, causal and linear and showed no significant\nimprovement with respect to Parallel architecture.\nMBPPHumanEvalAPPS/Intro\nnHead typeArchitecture+Layers@1@10@100@1@10@100@1@10@100\n1transformerparallel030.053.873.722.836.462.02.87.817.4\n4\nlinearparallel033.655.076.221.938.563.73.110.123.0\ntransformer\nanticausal030.854.875.320.938.464.52.08.721.6\ncausal031.954.974.920.938.167.34.011.622.8\nparallel\n033.855.976.924.040.166.11.67.119.9\n333.355.777.322.439.466.72.69.522.1\nThe architecture described in Section 2 is not the only sensible option, but proved technically viable and well-performing in\nour experiments. We describe and compare alternative architectures in this section.\nReplicated unembeddingsReplicating the unembedding matrixntimes is a simple method for implementing multi-token\nprediction architectures. However, it requires matrices with shapes(d,nV)in the notation of Section 2, which is prohibitive\nfor large-scale trainings.\nLinear headsApart from using a single transformer layer for the headsH\ni\n, other architectures are conceivable.  We\nexperimented with a single linear layer without any nonlinearity as heads, amounting to linear probing of the model’s\nresidual representationz.  Architectures with more than one layer per head are also possible, but we did not pursue this\ndirection further.\nCausal and anticausal variantInstead of making the prediction headsP\ni\n(x\nt+i\n|z\nt:1\n)architecturally independent of each\nother, we can also allow them to rely on other heads’ (pre-unembedding) outputs. In acausalvariant, later prediction heads\nare applied on top of the previous ones, i.e. thei-th prediction headP\ni\nis given by\nP\nθ\n(x\nt+i\n|·) = softmax◦f\nu\n◦f\nh\ni\n◦f\nh\ni−1\n···◦f\nh\n1\n◦f\ns\n.\nIn anotheranticausalvariant, the network starts by predicting the most distant tokens before gradually refining up to the\nfollowing token:\nP\nθ\n(x\nt+i\n|·) = softmax◦f\nu\n◦f\nh\ni\n◦f\nh\ni+1\n···◦f\nh\nn\n◦f\ns\n.\nThese architectures likewise allow a sequential forward/backward order as the parallel architecture from Section 2. This is\ndescribed in Figure S11.\n14",
    "Better & Faster Large Language Models via Multi-token Prediction\nInput\nTrunk\nHead 1\nLoss 1\nHead 2\nLoss 2\n1\n10\n2\n9\n36\n8\n5\n7\n4\nFigure S11:Order of the forward/backward in a causaln-token prediction model withn= 2heads.Like in the\nforward/backward depicted for parallel prediction heads in Figure 2, we avoid materializing all unembedding layer gradients\nin memory simultaneously and reduce peak GPU memory usage significantly. The iteration over the heads starts with the\none furthest to the trunk. At each head, a gradient from the succeeding prediction heads and from the head’s own loss are\naccumulated for both the head’s output and its weights.\nC. Training speeds\nTable S5:Training time relative to next-token prediction training.The slight overhead when using multi-token prediction\nhere is explained by a suboptimal use of Fully Sharded Data Parallel. In our implementation, when doing separate backward\npasses for each head, we lose the overlap of layer weight communication and computation, therefore it incurs a very slight\noverhead that can be removed if reimplemented correctly.\nModeln=1n=2n=4\n0.3B1.001.071.22\n0.6B1.001.051.13\n1.3B1.001.041.12\n3B1.001.021.07\n6.7B1.001.021.07\n13B1.001.041.09\nD. Finetuning\nTable S6:Finetuning LLama 2 with multi-token prediction does not significantly improve performance.We tried to\nfinetune LLama 2 with 4-token prediction but this did not yield significant improvements compared to the baseline. We\nsuppose that this new loss changes the initialization too brutally and never really recovers. We still some improvements for\nexample on MBPP Pass@1. All runs use 200B tokens of code.\nMBPPHumanEvalAPPS/Intro\nnHead type+Layers@1@10@100@1@10@100@1@10@100\n1transformer039.665.182.431.457.784.710.021.636.7\n4\nlinear039.363.781.329.053.482.26.920.034.0\ntransformer\n038.362.280.127.953.682.45.818.234.3\n342.564.481.328.756.982.47.821.237.3\n15",
    "Better & Faster Large Language Models via Multi-token Prediction\nE. Additional results on model scaling behavior\nTable S7:Scaling model sizeFull results of scaling model size with n=1,2 and 4.\nMBPPHumanEval\nModel SizeFut@1@10@100@1@10@100\n0.3B\n11.810.429.91.95.010.9\n21.710.127.21.54.410.3\n41.06.320.11.24.08.6\n0.6B\n14.721.045.22.98.516.7\n24.621.044.73.28.916.2\n43.015.638.02.77.715.5\n1.3B\n16.827.051.04.613.124.3\n27.327.551.75.413.623.3\n47.427.650.14.812.322.5\n3B\n111.136.460.47.217.229.8\n211.837.260.58.018.231.2\n412.737.661.17.218.533.3\n6.7B\n123.954.274.712.829.351.7\n224.754.876.413.232.253.9\n426.055.876.013.833.258.5\n13B\n126.057.177.014.133.656.0\n230.560.579.415.236.960.0\n430.561.079.215.838.663.5\nF. Details on CodeContests finetuning\nWe use the Python subset of the CodeContests (Li et al., 2022) train split with reward annotations (“correct” / “incorrect”)\nand condition on correct solutions at evaluation time. For evaluation, we generate 1000 samples per problem from the test\nsplit for each temperatureT∈ {0.5,0.6,0.7,0.8,0.9}, and compute the unbiased estimator for pass@k from Chen et al.\n(2021) for each value ofkandT. It is possible that models that were pretrained with different losses have different respective\noptimal temperatures for pass@k, so we compute and showk7→max\nT\npass_at(k,T)in Figure 4. In other words, we grant\npass@k access to a temperature oracle. For small values ofk, pass@k measures the capability of understanding and solving\ntasks while for largek, it additionally favors diversity in outputs. According to the results in Figure 4, multi-token prediction\npretraining leads to finetuned models that are better on both axes.\n16",
    "Better & Faster Large Language Models via Multi-token Prediction\nG. Additional results on natural language benchmarks\nWe evaluate the models from Section 3.7 on standard natural language processing benchmarks: ARC Challenge (Yadav\net al., 2019), COPA (Roemmele et al., 2011), Hellaswag (Zellers et al., 2019), Natural Questions (Kwiatkowski et al., 2019),\nPIQA (Bisk et al., 2019), SIQA (Sap et al., 2019) and TriviaQA (Joshi et al., 2017).\n25\n30\n35\nvalue\narc_challenge\n70\n80\ncopa\n40\n50\n60\nhellaswag\n5\n10\n15\nvalue\nnq\n1000020000\nglobal_step\n65\n70\n75\npiqa\n1000020000\nglobal_step\n42\n44\n46\nsiqa\n1000020000\nglobal_step\n10\n20\n30\n40\nvalue\ntqa\nn\n1\n2\n4\nFigure S12:Multiple token training with 7B models doesn’t improve performance on choice tasks.This figure shows\nthe evolution of average accuracy of some standard NLP benchmarks (ARC Challenge COPA Hellaswag MMLU Natural\nQuestions PIQA SIQA and TriviaQA. For the 7B models trained on 200B tokens of language data, the 2 future token\nmodel has the same performance as the baseline and the 4 future token model regresses a bit. Larger model sizes might be\nnecessary to see improvements on these tasks.\n17",
    "Better & Faster Large Language Models via Multi-token Prediction\nH. Additional results on abstractive text summarization\nIn this section, we report comprehensive evaluation results on summarization tasks for the 7B parameter models trained on\n200B and 500B tokens of natural language from Section 3.7.\nTable S8:Comprehensive evaluation on abstractive text summarization.ROUGE-n (n-gram overlap) and ROUGE-L\n(longest common subsequence overlap)F\n1\nscores for 7B models trained on 200B and 500B tokens of natural language,\nrespectively. The last three columns correspond to models trained on 500B tokens, the previous three to models trained on\n200B tokens. Shown are numbers of then= 1baseline and the absolute difference ofn= 2andn= 4models trained\non the same number of tokens. Summary-level ROUGE-L (“ROUGE-L\nsum\n”) is reported where it differs from ROUGE-L.\nModel checkpoints with maximal validation ROUGE-LF\n1\nare selected separately for each model dataset and model type\nand reported in the first row corresponding to each dataset. Boldface for numbers within 0.05 difference to the best one for\neach dataset size separately.\nTaskMetricBaseline 200B∆\nn=2\n∆\nn=4\nBaseline 500B∆\nn=2\n∆\nn=4\nCNN/Dailymail (Nallapati et al., 2016)\nevaluation epoch222222\nROUGE-142.88+0.74+0.7443.77+0.55+0.50\nROUGE-219.56+0.52+0.5320.34+0.52+0.34\nROUGE-311.11+0.39+0.3511.69+0.36+0.19\nROUGE-L29.72+0.66+0.4930.51+0.48+0.37\nROUGE-Lsum40.18+0.72+0.6841.02+0.56+0.52\nMulti-News (Fabbri et al., 2019)\nevaluation epoch133232\nROUGE-144.48+1.70+1.7245.87+1.05+0.69\nROUGE-216.88+0.44+0.7017.56+0.42+0.40\nROUGE-39.63-0.06+0.179.91+0.22+0.18\nROUGE-L23.82+0.17+0.4024.22+0.20+0.26\nOrangeSum (Eddine et al., 2021)\nevaluation epoch223213\nROUGE-132.95+0.41+0.3533.37+0.32+0.78\nROUGE-213.90+0.31+0.3614.22+0.25+0.53\nROUGE-38.01+0.19+0.218.12+0.22+0.48\nROUGE-L23.62+0.36+0.5123.91+0.23+0.66\npn-summary (Farahani et al., 2021)\nevaluation epoch111123\nROUGE-11.03+0.020.000.92+0.09+0.05\nROUGE-20.13+0.02+0.030.150.000.00\nROUGE-30.020.00+0.020.020.00+0.02\nROUGE-L1.02+0.03+0.010.91+0.09+0.05\nSAMSum (Gliwa et al., 2019)\nevaluation epoch333333\nROUGE-151.39+0.70+0.6352.54-0.24+0.69\nROUGE-226.46+0.76+0.3027.74-0.20+0.82\nROUGE-316.40+0.91+0.2817.56-0.30+0.71\nROUGE-L42.59+0.90+0.5143.92-0.10+0.63\nThaiSum (Chumpolsathien, 2020)\nevaluation epoch233333\nROUGE-145.08+0.63+1.1245.48+0.77+0.91\nROUGE-227.85+0.30+0.7328.07+0.74+0.64\nROUGE-315.73+0.04+0.4315.82+0.50+0.30\nROUGE-L44.92+0.64+1.1245.31+0.76+0.89\nWikiSummary (Farahani, 2020)\nevaluation epoch333333\nROUGE-110.16+0.67-0.2312.80-0.17-0.99\nROUGE-24.46-0.03-0.096.17-0.11-0.69\nROUGE-31.31+0.21+0.131.98-0.08-0.33\nROUGE-L10.11+0.65-0.2812.69-0.17-0.99\nXSum (Narayan et al., 2018)\nevaluation epoch223223\nROUGE-142.16+0.71+1.0743.42+0.78+0.67\nROUGE-219.19+0.54+0.5520.32+0.68+0.34\nROUGE-310.43+0.38+0.2811.23+0.48+0.20\nROUGE-L34.03+0.67+0.9235.18+0.79+0.63\n18",
    "Better & Faster Large Language Models via Multi-token Prediction\nTable S9:Performance on abstractive text summarization.ROUGE-L (longest common subsequence overlap)F\n1\nscore\nfor 7B models trained on 200B and 500B tokens of natural language. We finetune the respective models on each task’s\ntraining data separately for a given number of epochs and select the checkpoints with maximal ROUGE-LF\n1\non the\nvalidation dataset. The second and fifth column report the numbers for a next-token prediction model, while the third, fourth,\nsixth and seventh one report the absolute improvements for 2-token and 4-token prediction models trained on the same\namount of data, respectively. Boldface for numbers within 0.05 difference to the best one for each dataset size separately.\nDatasetBaseline 200B∆\nn=2\n∆\nn=4\nBaseline 500B∆\nn=2\n∆\nn=4\nCNN/Dailymail29.72+0.66+0.4930.51+0.48+0.37\nMulti-News23.82+0.17+0.4024.22+0.20+0.26\nOrangeSum23.62+0.36+0.5123.91+0.23+0.66\npn-summary1.02+0.03+0.010.91+0.09+0.05\nSAMSum42.59+0.90+0.5143.92-0.10+0.63\nThaiSum44.92+0.64+1.1245.31+0.76+0.89\nWikiSummary10.11+0.65-0.2812.69-0.17-0.99\nXSum34.03+0.67+0.9235.18+0.79+0.63\nAverage26.23+0.51+0.4627.08+0.28+0.31\nTable S10:Summary statistics for abstractive text summarization evaluations.Reported are averages for ROUGE-n and\nROUGE-L metrics across all datasets from Table S8, separately for precision, recall andF\n1\nscore. Both 2-token and 4-token\nprediction models outperform the next-token prediction baseline. Trained on 500B tokens, 4-token prediction models appear\nbetter at recall metrics while 2-token prediction models appear better at precision metrics. Model checkpoints are selected\nas described in Table S8. Boldface for numbers within 0.05 difference to the best one for each dataset size separately.\nMetricAspectBaseline 200B∆\nn=2\n∆\nn=4\nBaseline 500B∆\nn=2\n∆\nn=4\nROUGE-1\nF\n1\n33.77+0.70+0.6834.77+0.39+0.41\nprecision35.76+0.88+0.8337.03+0.42-0.04\nrecall34.37+0.45+0.4535.14+0.35+0.68\nROUGE-2\nF\n1\n16.06+0.36+0.3916.82+0.29+0.30\nprecision16.97+0.40+0.4317.91+0.29+0.03\nrecall16.34+0.28+0.3516.99+0.32+0.48\nROUGE-3\nF\n1\n9.08+0.26+0.239.54+0.18+0.22\nprecision9.59+0.29+0.2810.17+0.18+0.05\nrecall9.26+0.21+0.209.65+0.21+0.35\nROUGE-L\nF\n1\n26.23+0.51+0.4627.08+0.28+0.31\nprecision27.79+0.62+0.5528.85+0.28-0.09\nrecall26.71+0.37+0.3227.40+0.28+0.57\nROUGE-L\nsum\nF\n1\n27.53+0.52+0.4828.40+0.29+0.33\nprecision29.07+0.64+0.5830.15+0.29-0.08\nrecall28.13+0.35+0.3328.81+0.29+0.60\n19",
    "Better & Faster Large Language Models via Multi-token Prediction\nI. Additional results on mathematical reasoning in natural language\n2\n3\npass@1 (%)\n200B tokens\n2\n4\n6\n8\n500B tokens\nn = 1\nn = 2\nn = 4\n15\n20\npass@10 (%)\n20\n30\n0.20.40.60.81.01.21.4\nTemperature\n40\n50\n60\npass@100 (%)\n0.20.40.60.81.01.21.4\nTemperature\n40\n60\nFigure S13:Performance on the mathematical reasoning benchmark GSM8K (Cobbe et al., 2021).We evaluate\npretrained next-token and multi-token prediction models trained on 200B and 500B tokens of natural language in 8-shot\nmode using nucleus sampling (Holtzman et al., 2020) with probability mass 0.95 and various sampling temperatures.\nReported are the frequencies of the correct final answer to appear amongksamples, fork= 1,10,100, estimated from\n200 samples like in code generation benchmarks (Chen et al., 2021).  After 200B tokens, the 2-token prediction model\nhas a clear advantage over the next-token baseline but the order reverses after 500B tokens. The 4-token prediction model\nis worse throughout.  We interpret this similarly to the findings in Section 4.1:  the follow-your-nose chains-of-thought\nrequired for GSM8K may be difficult to learn from a limited amount of data, attesting to the data efficiency of multi-token\nprediction training.  Once the correct circuits for correct autoregressive chains-of-thought in this domain have formed,\nhowever, multi-token prediction comes at a cost.\n20",
    "Better & Faster Large Language Models via Multi-token Prediction\nJ. Additional results on induction learning\n1310301003001000\nParameters (M)\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nInduction success\nn=1 (baseline)\nn=2 (ours)\nFigure S14:Induction capability ofn-token prediction models trained on higher-quality data.Shown is accuracy on the\nsecond token of two token names that have already been mentioned previously. Training on a 9:1 mix of a books dataset and\nthe children storiy dataset, we observe that induction capability forms significantly earlier in training (not shown here) and to\na higher degree. We believe that this is explained both because our evaluation dataset no longer contains out-of-distribution\ntokens (Section 4.1) and because the higher-quality data contained in the books dataset makes induction necessary earlier on\n(especially for small models, cf. Singh et al. (2023)). In particular, by enforcing the formation of induction capability in the\nmodel by means of the dataset – instead of the loss – the advantage of 2-token prediction models on this task disappears\nexcept for the smallest models: feature learning converts the task into a pure next-token prediction task.\n21",
    "Better & Faster Large Language Models via Multi-token Prediction\nK. Additional results on algorithmic reasoning\nWe investigate the followingcomputation-sharing hypothesisfor explaining the efficacy of multi-token prediction as training\nloss.\nThe prediction difficulty of different tokens in natural text varies greatly. Some tokens may be the continuations\nof partial words that are uniquely determined from their preceding context without any effort, while others may\nrequire to predict theorem names in difficult mathematical proofs or the correct answer to an exam question.\nLanguage models with residual connections have been shown to refine their output token distribution with each\nsuccessive layer, and can be trained with early exit strategies that spend variable amounts of computational\nresources per token position. Multi-token prediction losses explicitly encourage information-sharing between\nadjacent token positions and can thus be viewed as a method to learn allocating computational resources in\nlanguage models more efficiently to the tokens that benefit most of it.\nTo check the truth of this hypothesis, we augment the polynomial arithmetic task from Section 4.2 with a varying number of\npause tokens(Goyal et al., 2023) inserted between the question and a token that denotes the beginning of the answer. Pause\ntokens introduce additional computational resources that can be expended for computations that are expected to be useful\nlater on in the sequence, in other words: to start thinking about the answer. According to thecomputation-sharing hypothesis,\nmulti-token prediction models learn information-sharing and thus computation-sharing between token positions more easily,\nand may be better at making use of these additional computational resources than next-token prediction models are.  In\nFigure S15, we show the evaluation results on the polynomial arithmetic task with a fixed number of pause tokens inserted\nboth at training and evaluation time.  Multi-token prediction models likewise outperform next-token prediction models\non these task variants across task difficulties and model sizes. However, we do not see strong evidence of a widening or\nshrinking of this gap i.e. we cannot conclude from these experiments on the veracity of the computation-sharing hypothesis.\nIn Table S11, we report results from another experiment in the same spirit: by adding spaces and newlines to HumanEval\nand MBPP prompts, we add “pause tokens” in a somewhat natural way. According to these results, multi-token prediction\nmodels have a slight advantage at using this additionally provided compute, but the effect is marginal.\n1122334455667788991010\n# operations\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nin-domainout-of-domain\nn=1\nn=2\nn=4\n(a) 5 pause tokens\n1122334455667788991010\n# operations\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nin-domainout-of-domain\nn=1\nn=2\nn=4\n(b) 10 pause tokens\nFigure S15:Accuracy on a polynomial arithmetic task with varying number of operations per expression and pause\ntokens.We train and evaluate models on the polynomial arithmetic task described in Section 4.2, modified by the addition\nofpause tokens(Goyal et al., 2023): between the question and the equality sign that indicates the beginning of the answer,\nwe add a constant number of pause tokens both in training and evaluation. For both a variant with five and with ten pause\ntokens, respectively, we observe comparable improvements from using multi-token prediction to the ones obtained in the\ncase without pause tokens (Figure 8).\n22",
    "Better & Faster Large Language Models via Multi-token Prediction\nTable S11:Utilization of additional whitespace tokens in code benchmarks.\nTaskWhitespacen= 1n= 4\nAPPS/Introspaces + newline+0.21+0.34\nAPPS/Intronewline+0.79+0.69\nHumanEvalspaces + newline-0.72-0.16\nHumanEvalnewline-0.26+0.10\nMBPPspaces + newline-0.10-0.06\nMBPPnewline+0.03-0.08\nAverage-0.01+0.14\n1122334455667788991010\n# operations\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nin-domainout-of-domain\n30M, n=1\n30M, n=2\n30M, n=4\n100M, n=1\n100M, n=2\n100M, n=4\nFigure S16:Accuracy on a polynomial arithmetic task for two model sizes.We train and evaluate models with 30M and\n100M parameters on the polynomial arithmetic task described in Section 4.2. Tripling the model size has a smaller effect\non performance than replacing next-token prediction loss by multi-token prediction. Shown are two independent runs per\nconfiguration and their means, the 100M parameter models being identical to the ones in Figure 8.\n23",
    "Better & Faster Large Language Models via Multi-token Prediction\nTable S12:Optimal temperatures for all numbers in table  1\nTraining dataVocabularyn\nMBPPHumanEvalAPPS/Intro\n@1@10@100@1@10@100@1@10@100\n313B bytes\n(0.5 epochs)\nbytes\n10.20.80.80.10.80.80.80.80.8\n80.10.80.80.10.80.80.40.40.4\n160.10.80.80.10.80.80.40.40.4\n320.10.40.80.10.40.80.10.40.4\n200B tokens\n(0.8 epochs)\n32k tokens\n10.10.80.80.10.80.80.10.40.8\n20.10.80.80.20.80.80.40.40.8\n40.10.80.80.10.80.80.20.80.8\n60.10.80.80.20.80.80.40.40.8\n80.10.80.80.10.80.80.20.40.8\n1T tokens\n(4 epochs)\n32k tokens\n10.10.80.80.10.80.80.10.40.8\n40.10.80.80.20.80.80.40.80.8\nL. Additional intuitions on multi-token prediction\nL.1. Comparison to scheduled sampling\nIn Section 5.2, we argued that multi-token prediction reduces the distribution mismatch between teacher-forced training and\nautoregressive evaluation of language models. Scheduled sampling (Bengio et al., 2015) is a curriculum learning method\nthat likewise aims to bridge this gap in sequence prediction tasks by gradually replacing more and more input tokens with\nmodel-generated ones.\nWhile effective in areas such as time series forecasting, scheduled sampling is, in our opinion, inapplicable to language\nmodelling due to the discrete nature of text. Replacing ground truth input sequences by interleavings of ground truth and\nmodel-generated tokens frequently results in ungrammatical, factually wrong or otherwise incoherent text, which should\nbe avoided at all cost.  Moreover, unlike multi-token prediction, the technique originally developed for recurrent neural\nnetworks cannot easily be adapted for parallel training setups like the ones of transformer models.\nL.2. Information-theoretic argument\nWe give details on the information-theoretic terms appearing in the decomposition in Section 5.2 and derive a relative\nversion that similarly allows to decompose multi-token prediction losses. As in Section 5.2, denote byXthe next token\nand byYthe second-next one, and omit conditioning on the preceding contextCfor ease of notation. In Section 5.2, we\ndecomposedH(X) +H(Y)—the quantity of interest for 2-token prediction models—as follows:\nH(X) +H(Y) =H(X|Y) + 2I(X;Y) +H(Y|X).(3)\nLet us explain each of the terms. The entropy terms denote the uncertainty contained in the ground-truth random variables\nXandY.\n2\nThe termH(Y|X)is a classical next-token entropy for the prefix(C,X). The conditional entropyH(X|Y)\nis a more theoretical entity not modelled by causal models. It describes the uncertainty aboutXgiven the prefixCand suffix\nY, and therefore captures the local variations ofXthat do not affect the continuation of the textY. The mutual information\nI(X;Y)on the other hand describes the information aboutYcontained inX(and vice versa) and therefore captures the\nvariations ofXwhich constrain the continuation of the text.\nHowever, the argument given in Section 5.2 relies on the assumption that multi-token prediction losses obey a similar\ndecomposition as the sum of the ground-truth entropies themselves.  Let us make this rigorous.  Denote byp(x,y)the\njoint distribution ofXandY, byp(x)(short forp\nX\n(x)) the marginal distribution ofXand byp(y)the one ofY. Denote\nthe densities of the model’s predictions byq(x,y),q(x)andq(y), respectively, conditional distributions byp(x|y)and\nKullback-Leibler divergence fromqtopbyD(p∥q)and cross-entropy fromqtopbyH(p,q).\nDefinition L.1.Theconditional cross-entropyH(p\nX|Y\n,q\nX|Y\n)ofXconditioned onYfromqtopis defined as the\n2\nIn particular, they do not refer tomodelpredictions.\n24",
    "Better & Faster Large Language Models via Multi-token Prediction\nexpectation underyof the cross-entropy between the distributionsp\nX\nandq\nX\nconditioned ony, in formulas:\nH(p\nX|Y\n,q\nX|Y\n) =E\ny∼p\nY\nH(p\nX|Y=y\n,q\nX|Y=y\n) =E\ny∼p\nY\nH(p(·|y),q(·|y)).\nDefinition L.2.Therelative mutual informationI\np∥q\n(X;Y)ofXandYfromqrelative topis defined by\nI\np∥q\n(X;Y) =D(p∥q\nX\n⊗q\nY\n)−D(p∥q).\nWe haveI\np∥q\n(X;Y) =H(p\nX\n,q\nX\n) +H(p\nY\n,q\nY\n)−H(p,q),I\np∥p\n(X;Y) =I\np\n(X;Y)reduces to standard mutual informa-\ntion under the distributionpandI\np∥q\n(X;Y)is symmetric inXandYbut can be negative.\nWe have the following relative version of the decompositionH(X) =H(X|Y) +I(X;Y).\nLemma L.3.H(p\nX\n,q\nX\n) =H(p\nX|Y\n,q\nX|Y\n) +I\np∥q\n(X;Y).\nProof.We calculate\nH(p\nX\n,q\nX\n) =−\nX\nx\np(x) logq(x)\n=−\nX\nx,y\np(x,y) logq(x)\n=−\nX\nx,y\np(x,y) log\nq(x)q(y)\np(x,y)\np(x,y)\nq(x,y)\nq(x,y)\nq(y)\n=D(p∥q\nX\n⊗q\nY\n)−D(p∥q)−\nX\nx,y\np(y)p(x|y) logq(x|y)\n=I\np∥q\n(X;Y) +\nX\ny\np(y)H(p\nX|y\n,q\nY|y\n)\n=I\np∥q\n(X;Y) +H(p\nX|Y\n,q\nX|Y\n).\nSymmetrizing, we get the desired relative version ofH(X) +H(Y) =H(X|Y) + 2I(X;Y) +H(Y|X):\nH(p\nX\n,q\nX\n) +H(p\nY\n,q\nY\n) =H(p\nX|Y\n,q\nX|Y\n) + 2I\np∥q\n(X;Y) +H(p\nY|X\n,q\nY|X\n).\nSettingpto be the empirical distribution of the training data, the left-hand side describes the cross-entropy loss used to\ntrain 2-token prediction models.  The right-hand side gives the decomposition into alocalcross-entropy term, a mutual\ninformation term with weight two and a shifted next-token cross-entropy term. We interpret this as follows: by adding the\ntermH(p\nY\n,q\nY\n)to the loss, 2-token prediction incentivizes models to precompute features which will become useful for\npredictingYin the next step and increases the weight of the relative mutual information term in the loss. What does relative\nmutual information actually mean? By interpreting Kullback-Leibler divergenceD(p∥q)as the average number of bits\nneeded in addition to send data frompwith a code optimized forqinstead ofp, we see that minimizing\nI\np∥q\n(X;Y) =D(p∥q\nX\n⊗q\nY\n)−D(p∥q)\nmeans minimizing the average number of additional bits needed to send data frompwith a code optimized forqthat treats\nXandYas independent compared to one that does not. If this number is small,qmanaged to exploit the mutual information\nofXandYunderp.\nL.3. Lookahead reinforces choice points\nTraining with multi-head prediction increases the importance of choice points in the loss in comparison to inconsequential\ndecisions. To make this argument, we present a simplified model of language modelling. Consider a sequential decision task\nand a modelMthat is trained in a teacher-forced way on optimal trajectories. We distinguishchoice points–transitions that\nlead to different outcomes – andinconsequentialdecisions which do not (Figure S17 (a) and (b)).\n25",
    "Better & Faster Large Language Models via Multi-token Prediction\n(a)\n(c)\n(b)\nFigure S17:Example of a sequential prediction task with derailing.The goal is to go from the arrow to the trophy.\nTurning around is not allowed. Most transitions are unique, but there are two turns to be taken correctly, theconsequential\ndecisions(a) and (c). Turn (b) is aninconsequential decision: the paths join right after it. Next to transitions (a) and (b),\nwe sketch how a 4-step prediction loss can place more emphasis on consequential transitions than inconsequential ones\nduring teacher-forced training. Next to transition (c), we sketch how a 4-step lookahead can prevent models from taking\nirreversible suboptimal decisions during autoregressive decoding.\nMore formally, assume that the language model is deployed in a reinforcement learning setting like inreinforcement learning\nfrom human feedback(Ouyang et al., 2022) (states are prompts followed by the partial sequence of tokensx\nt:1\ngenerated so\nfar, actions are single tokensx\nt+1\nto generate, rewards are externalR(x\nt:1\n)). The quantity\nV\nπ\n(x\nt:1\n) =E\nx\nt+i\n∼π(x\nt+i−1:1\n),i≥1\n\n\nX\ni≥0\nR(x\nt+i:1\n)\n\n\nis the value of the statex\nt:1\nfollowing the policyπ, while\nσ\nπ\n(x\nt:1\n) =\nr\nVar\nx\nt+1\n∼π(x\nt:1\n)\n[V\nπ\n(x\nt+1:1\n)]\nquantifies the importance of the decisionx\nt+1\non the value thereafter.Choice pointscan formally be viewed as stepstfor\nwhichσ\nπ\n(x\nt:1\n)is large, whileinconsequential pointsare steps where it is low. Note that for completion models, there is no\nexplicit reward, and our argument is merely meant to illustrate what we mean bychoice points.\nDerailingdenotes a situation where autoregressive generation of trajectories fromMat inference time results in bad\noutcomes afterMmade a mistake on a choice point. Even if subsequently,Macts optimally given this choice, the final\noutcome can be significantly worse than the outcome of the optimal trajectory.\nStaying in the teacher-forced setting, we ask:  What is the impact of trainingMwithn-step prediction instead of next-\nstep prediction on this task?   Sayx\nt\n→x\nt+1\nis a choice point in an optimal trajectory with the suboptimal choice\nbeingx\nt\n→ ̃x\nt+1\n(Figure S17 (a)).  Assume that the trajectories precedingx\nt\nand succeedingx\nt+1\nand ̃x\nt+1\nconsist of\ninconsequential transitions, the latter denoted by ̃x\nt+j\n→ ̃x\nt+j+1\n. We will compare the losses of a teacher-forced next-step\nprediction model and a teacher-forcedn-step prediction model on the partial trajectory(x\nt−n+1\n,...x\nt\n). For the next-step\nprediction model, the predictions are(x\nt−n+2\n,...,x\nt\n, ̃x\nt+1\n)with a single wrong prediction. The predictions of ann-step\nprediction model at timet−n+i,i= 1,...,nare(x\nt−n+i+1\n,...,x\nt\n, ̃x\nt+1\n,..., ̃x\nt+i\n)withiwrong predictions. In other\nwords, ann-step prediction model receives1 +...+n=\nn(n+1)\n2\nloss terms pertaining to such a choice point and its\nconsequences, while each inconsequential transition (Figure S17 (b)) is only reinforcedntimes as often as in a next-step\nprediction model.  In other words, choice points receive on average\nn+1\n2\ntimes more importance in the loss ofn-step\nprediction models than in next-step prediction models.\n26",
    "Better & Faster Large Language Models via Multi-token Prediction\nAs argued in Section 5.1, we believe that this model captures important features of training and inference with language\nmodels: choice points are semantically important turning points in the generated texts, such as the final answer to a question\nor a specific line of code, while inconsequential decisions can be a choice among synonyms or of variable names in code.\nApart from this training dynamics point of view, we hypothesize thatn-step prediction also allows the formation of circuits\nthat specifically spot inconsistencies between predictions for earlier and later steps.  For instance, if in an early layer of\nthe model, it can be predicted that a decisionx\nt\n→ ̃x\nt+1\nleads to suboptimal outcomes ̃x\nt+n\n(Figure S17 (c)), subsequent\nlayers can reduce the probability ofx\nt\n→ ̃x\nt+1\nin the model’s next-step prediction. Such behaviors also happen in next-step\nprediction models given enough capacity, but our experiments in Section 4.2 point to the fact that circuits of this kind are\nformed more easily in multi-step architectures that enforce the required information ̃x\nt+n\nto be available to the model when\npredicting ̃x\nt+1\n. We believe that this situation appears frequently in natural language and code modelling, for instance where\nan initial answer to a question contradicts the results of thechain of thoughtbrought forward with the intention to justify it.\nIn more general terms, this situation arises whenever predicting first ̃x\nn+i\nfor some1< i≤nand then ̃x\nn+1\nbased on ̃x\nn+i\nis easier than predicting ̃x\nn+1\ndirectly. We discuss this phenomenon offactorization ordersin the next section and present a\nspecific instance of it that frequently appears in modelling natural language.\nL.4. Factorization orders\nCausal language modelling factorizes probabilities over text sequencesx\nt\n···x\n1\nclassically as\nP(x\nt\n···x\n1\n) =\nt\nY\ni=1\nP(x\ni\n|x\ni−1\n···x\n1\n).\nWhile moving forward in time is certainly the most natural choice of factorization order, there exist cases where it is\nsuboptimal. In inflectional languages, for instance, agreement between related sentence parts is a frequent pattern with one\nword directing the grammatical forms of others. Consider the German sentence\nWie konnten auch Worte meiner durstenden Seele genügen?\n3\nFriedrich Hölderlin, Fragment von Hyperion (1793)\nwhere \"genügen\" requires a dative case object and then \"Seele\" requires the possessive pronoun \"mein\" to be in female\nsingular dative form \"meiner\" and the participle \"durstend\" to be in female singular dative form in weak declination\n\"durstenden\" because it follows \"meiner\". In other words, the factorization order\nWie konnten auch Worte→genügen→Seele→meiner→durstenden?\nis arguably an easier one for constructing the above sentence. Humans as well as language models therefore have to perform\nthis factorization (which deviates from the causal order in which predictions take place!) within their latent activations, and\na4-token prediction loss makes this easier as it explicitly encourages models to have all information about the successive 4\ntokens in its latent representations.\n3\nroughly:How could words be enough for my thirsty soul?\n27",
    "Better & Faster Large Language Models via Multi-token Prediction\nM. Training hyperparameters\nTable S13:Overview of all training hyperparameters used.We schedule all learning rates with a linear warmup and\ncosine decay (Loshchilov and Hutter, 2017) to a fraction of the peak learning rate which is depicted in the last column\n(“decay ratio”). All experiments use the Adam (Kingma and Ba, 2015) optimizer withβ\n1\n= 0.9,β\n2\n= 0.95and decoupled\nL\n2\nweight decay (Loshchilov and Hutter, 2019) coefficient0.1. We clip gradients to a maximal Euclidean norm of1.0in all\nexperiments except CodeContests finetunings, where we use0.1instead. Summarization finetunings correspond to three\nepochs on all datasets except BigPatent (1 epoch). Byte-level models use the architecture with replicated unembeddings\nfrom Appendix B.\nModelBatch size (2\n20\n)StepsTokens (B)Warmup stepsPeak LRContext lengthDecay ratio\nModel scaling (Section 3.1)\n0.3B810,85091.010003×10\n−4\n40960.03\n0.6B810,85091.010003×10\n−4\n40960.03\n1.3B810,85091.010003×10\n−4\n40960.03\n3B810,85091.010003×10\n−4\n40960.03\n7B825,000209.720003×10\n−4\n40960.03\n13B825,000209.710003×10\n−4\n40960.03\nCode models (Section 3)\n7B 200B825,000209.720003×10\n−4\n40960.03\n7B 500B768,570503.320003×10\n−4\n40960.03\n7B 1T7136,2401000.020003×10\n−4\n40960.03\nByte-level models (Section 3.3)\n7B 314GB1225,000314.620003×10\n−4\n81920.03\nLanguage models (Section 3.7)\n7B 200B825,000209.720003×10\n−4\n40960.10\n7B 500B860,000503.320003×10\n−4\n40960.10\nInduction task (Section 4.1)\n1M – 1B0.25100,00026.2200010\n−4\n20480.03\n1M – 1B (Appendix J)0.55000026.2200010\n−4\n20480.03\nArithmetic task (Section 4.2)\n30M0.25100,00026.2200010\n−4\n10240.03\n100M0.25100,00026.2200010\n−4\n20480.03\nSummarization (Section 3.7)\nBigPatent0.12576,68010.11003×10\n−5\n40960.03\nCNN/Dailymail0.1257,1400.91003×10\n−5\n40960.03\nMulti-News0.1253,3300.41003×10\n−5\n40960.03\nOrangeSum0.1253600.01003×10\n−5\n40960.03\npn-summary0.1253,4500.51003×10\n−5\n40960.03\nSAMSum0.125600.01003×10\n−5\n40960.03\nThaiSum0.12523,6403.11003×10\n−5\n40960.03\nWikiSummary0.1252,5500.31003×10\n−5\n40960.03\nXSum0.1252,7600.41003×10\n−5\n40960.03\nCodeContests (Section 3.6)\n7B0.2513,0003.64005×10\n−5\n40960.004\n28",
    "Better & Faster Large Language Models via Multi-token Prediction\nTable S14:Overview of model architectures used for scaling analyses.\nNameDimensionLayersHeads\n1M12854\n3M25648\n10M38468\n30M512108\n100M7681412\n300M10242516\n1B15363624\n0.3B10241816\n0.6B12802720\n1.3B20482416\n3B25603620\n6.7B (“7B”)40963232\n13B51204040\n29"
  ]
}