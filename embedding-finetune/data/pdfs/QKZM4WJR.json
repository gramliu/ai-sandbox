{
  "key": "QKZM4WJR",
  "url": "http://arxiv.org/pdf/2404.14361",
  "metadata": {
    "title": "Better Synthetic Data by Retrieving and Transforming Existing Datasets",
    "abstract": "  Despite recent advances in large language models, building dependable and\ndeployable NLP models typically requires abundant, high-quality training data.\nHowever, task-specific data is not available for many use cases, and manually\ncurating task-specific data is labor-intensive. Recent work has studied\nprompt-driven synthetic data generation using large language models, but these\ngenerated datasets tend to lack complexity and diversity. To address these\nlimitations, we introduce a method, DataTune, to make better use of existing,\npublicly available datasets to improve automatic dataset generation. DataTune\nperforms dataset transformation, enabling the repurposing of publicly available\ndatasets into a format that is directly aligned with the specific requirements\nof target tasks. On a diverse set of language-based tasks from the BIG-Bench\nbenchmark, we find that finetuning language models via DataTune improves over a\nfew-shot prompting baseline by 49% and improves over existing methods that use\nsynthetic or retrieved training data by 34%. We find that dataset\ntransformation significantly increases the diversity and difficulty of\ngenerated data on many tasks. We integrate DataTune into an open-source\nrepository to make this method accessible to the community:\nhttps://github.com/neulab/prompt2model.\n",
    "published": "2024-04-22T17:15:32Z"
  },
  "text": [
    "Better Synthetic Data by Retrieving and Transforming\nExisting Datasets\nSaumya Gandhi\n∗\n, Ritu Gala\n∗\n, Vijay Viswanathan,\nTongshuang Wu,Graham Neubig\nCarnegie Mellon University\nAbstract\nDespite  recent  advances  in  large  language\nmodels, building dependable and deployable\nNLP models typically requires abundant, high-\nquality training data.  However, task-specific\ndata is not available for many use cases, and\nmanually curating task-specific data is labor-\nintensive.   Recent  work  has  studied  prompt-\ndriven  synthetic  data  generation  using  large\nlanguage models, but these generated datasets\ntend to lack complexity and diversity.  To ad-\ndress these limitations, we introduce a method,\nDataTune, to make better use of existing, pub-\nlicly available datasets to improve automatic\ndataset generation. DataTune performsdataset\ntransformation,  enabling  the  repurposing  of\npublicly available datasets into a format that is\ndirectly aligned with the specific requirements\nof target tasks.  On a diverse set of language-\nbased tasks from the BIG-Bench benchmark,\nwe find that finetuning language models via\nDataTune improves over a few-shot prompting\nbaseline by 49% and improves over existing\nmethods that use synthetic or retrieved training\ndata by 34%. We find that dataset transforma-\ntion significantly increases the diversity and\ndifficulty of generated data on many tasks. We\nintegrate DataTune into an open-source reposi-\ntory to make this method accessible to the com-\nmunity.\n1\n1    Introduction\nThe major bottleneck in modern AI research is data.\nDespite the paradigm-shifting developments of pre-\ntraining and prompting, the recipe for achieving\npeak performance on any particular task has hardly\nchanged:   obtain  large  amounts  of  high-quality\ntraining data and fine-tune your model.   This is\nparticularly valuable for developing models with\nfewer  than  3  billion  parameters;  in  this  regime,\n∗\nequal contribution.\n1\nhttps://github.com/neulab/prompt2model\nInput_colOutput_col\nGive an english language \ndescription of this code: \ndef fn(...):\n   if high>=low:\n      ...\nPerforms binary \nsearch on a \nsorted array\n...\n...\nGive an English language description of Python code\nInput Prompt\nIDQuestionSolutionTest Cases ...\n1\nwrite an efficient algm \nfor searching a \nnumber in sorted array\ndef bin_search(...):\n  if high>=low:\n. . .\nin=[1,3,4,9],4\nout=2\n...\n2consider a circle...\ndef area_of_circle(r):\nr=3, out=9\n...\n3\nWhat is count of each \nchar in a string\ndef count_char(str):\n   return Counter(str)\nstr=\"acaa\" \nout={a:3, c:1}\n...\nData \nCuration \n(Manual or \nSynthetic)\n+Transformation \n(synthetically adapt datasets)\nRetrieval (cross-task transfer \nusing existing datasets)\nFigure 1:  Obtaining task-specific annotated data can\nbe tricky.   Existing solutions include (1)data gener-\nation methodseither by employing human annotators\n(incurring high costs) or synthetically, such as using\nLLMs (risking low diversity) or (2)cross-task transfer,\nwhere related but task-misaligned datasets are used (for\ninstance, for the task of generating English language de-\nscriptions based on code, this could be a public dataset\nwith coding questions, solutions, and test cases but no\nexplicit descriptions).  Our approach combines these\nstrategies by adaptively transforming existing datasets\nfor the target task (using the \"solution\" field from the\npublic dataset and asking an LLM to create description\nor make any formatting changes required) preserving\noriginal dataset diversity while ensuring the quality of\nsynthetically generated data.\nsupervised finetuning can be significantly more ef-\nfective than in-context learning (Mosbach et al.,\n2023).  However, this recipe can be challenging\nfor specialized or novel tasks where task-specific\nannotated data is limited.\nPrior works have proposed strategies for fine-\ntuning in this low-resource setting.  The most in-\ntuitive way isobtain labeled datathrough manual\ncuration (Callison-Burch, 2009; Zhang et al., 2022),\nwhich assumes access to domain experts and may\nrequire considerable financial resources to com-\narXiv:2404.14361v3  [cs.CL]  26 Apr 2024",
    "pensate annotators fairly (Huang et al., 2023). An\nincreasingly popular alternative to large-scale man-\nual annotation is to synthetically produce datasets\nusing  existing  large  models  (Wang  et  al.,  2022;\nPeng et al., 2023; Tang et al., 2023). We will refer\nto this method assynthetic data generation.  Un-\nfortunately, directly generating synthetic datasets\nthat simultaneously have high correctness and suf-\nficient diversity is difficult. Models trained on syn-\nthetic datasets are typically significantly worse than\nthose trained on manually curated data when stan-\ndardizing for dataset size, suggesting that current\nsynthetic dataset generation methods still leave sig-\nnificant room for improvement in dataset quality\n(Ding et al., 2023).\nOn the other hand,cross-task transferentirely\nsidesteps the need for strictly in-domain data by\ntraining models either in a multi-task fashion on a\nwide range of datasets (Sanh et al., 2021) or on task-\nspecific datasets closest to the new target task (Vu\net al., 2020).   We focus on the latter,  which we\nrefer to asexisting data. Recent methods such as\nPrompt2Model (Viswanathan et al., 2023b) com-\nbine both of the above methods, showing the ad-\nditive benefits of synthetic data generation and ex-\nisting datasets for finetuning in few-shot settings.\nHowever,  they report that even for the machine\nreading question answering dataset of SQuAD (Ra-\njpurkar et al., 2016), a dataset considered today to\nbe largely “solved”, training onexisting dataalone\nis significantly worse than training on manually or\nsynthetically created data.\nTo overcome the limitations of these approaches,\nwe introduce DataTune, a system that automatically\nrepurposes public datasets for new tasks. DataTune\nidentifies the most relevant public datasets and uses\na large language model to transform them into a\nformat aligned with the target task’s needs. For in-\nstance, for a task requiring descriptions of Python\ncode in English (as shown in Figure 1), DataTune\nfinds a dataset with programming questions and\ncode solutions (along with other, irrelevant data\ncolumns).  It transforms the retrieved dataset by\nusing the code solutions as input and generating a\nsynthetic description of the code as output.  This\napproach maintains the original dataset’s diversity\nwhile matching the task specification, boosting per-\nformance by 22 points over baselines.  We refer\nto this synthetic adaptation of publicly available\ndatasets asDataset Transformation.\nWe evaluate the effectiveness of DataTune on six\nchallenging language-based tasks from the BIG-\nBench  benchmark  (BIG  Bench  Authors,  2023),\nwhich are designed to gauge the system’s perfor-\nmance across diverse NLP task categories. When\ncompared to few-shot prompting of the same base\nmodel (Mistral-7B) without fine-tuning, DataTune-\nenhanced models improve by an average of 5.2\npoints on five tasks,  demonstrating its value for\ndomain-specific fine-tuning. DataTune also can be\nused additively with existing synthetic dataset gen-\neration approaches, yielding an 8-point improve-\nment over the few-shot prompting baseline. Com-\nparing DataTune with existing methods of synthetic\ndata generation, we find that DataTune often pro-\nduces more difficult and diverse examples, and on a\nsmall sample of data we observe that these benefits\ndo not come at the expense of data correctness.\n2    Problem Setup\nWe study the problem of how to automaticallyre-\ntrieveandtransformexisting datasets to prepare a\nfine-tuning dataset for a new task. In our problem\nsetting, a user specifies the task of interest with\na textual promptT(optionally containing a few\ndemonstration examples). We assume access to a\nlarge, diverse collection of labeled datasetsD, a\nlarge language model that can be prompted, and a\nsmall language modelMthat we can fine-tune.\nThe goal here is to automatically generate a syn-\nthetic training datasetD\n′\nwhich, after finetuningM\nonD\n′\n, will improveM’s ability to satisfy the task\nspecification.  For each taskTin a known set of\ntask descriptionsT, we can measure our progress\ntowards this goal by evaluating the trained model\nM\nT\nagainst labeled data obtained for taskT.\n3    Methods\nDataTune focuses on selecting and transforming a\ndataset to align it with a specific task. First, it finds\nrelevant datasets fromDthat are good candidates\nfor further transformation, through dataset retrieval\nand reranking. Then, it performs data transforma-\ntion (i.e., synthetically modify each entries in the\nselected dataset) to create a new datasetD\n′\nthat\nbetter aligns with the task requirements.\n3.1    Dataset Retrieval\nThe first phase of DataTune involves identifying\nrelevant datasets for our task from a large repository\nof existing datasets. We confine our dataset space",
    "Task \nExpansion\nPlanning \nModule\nExecution \nModule\nRetrieved \nDataset\nPlan\nTransformed \nDataset\nDataset Transfomation Component\nTask Description \n+ Few Examples\n\"Give an English language \ndescription of Python code\"\nQ: Python code:\nx = input()\ny = input()\nz = x.find(y)\nprint('N') if z == -1  else print('Y')\nchoice 1: prints 'Y' if the second input string is \nfound in the first input string, otherwise prints 'N'\nchoice 2: prints the sum of the two input strings\nDataset Sample: \nSolutions: \"\nx = input()\ny = input()\nz = x.find(y)\nprint('N') if z == -1  else print('Y')\nPython_Code Dataset\nPlan:\n1.Extract the \"solutions\" field, which contains \nPython code snippets.\n2.Generate one correct and a set of plausible \nbut incorrect descriptions of the code.\n3.Format the \"input\" field by concatenating \nthe description with the code from the \n\"solutions\" field..\nSchema \nSelection\nDataset Sample\nSchema Selection: \n \nRelevant Columns: \n[Solutions] \n \nIrrelevant Columns:         \n[ID, URL]\nDataset \nSchema: ID, \nURL, Solutions..\nTransformed \nSample\nSelected \nColumns\nSchema\nTask Expansion:\nThe input is a Python code snippet that \nneeds to be described. The descriptions \nshould capture the purpose and \nfunctionality of the code, and should not \nbe too general. The output consists of 3 \nincorrect descriptions and 1 correct \ndescription of the code...\nExpanded\nDescription\nFigure 2: The data transformation component of DataTune, explained with an example (in yellow).\nDto the HuggingFace Hub (Lhoest et al., 2021),\nwhich consists of over 75,000 datasets.\nTo  efficiently  complete  this  task,  we  use  a\ndual-stage retrieval approach.  We firstretrievea\nset of documents using DataFinder (Viswanathan\net al., 2023a), a bi-encoder retriever specifically\ntrained  for  retrieving  datasets  from  natural  lan-\nguage queries using textual descriptions of each\ndataset. We then rerank these datasets to increase\nthe likelihood that our selected dataset can be effec-\ntively transformed for the target task. Inspired by\nSun et al. (2023), who showed that LLMs can effec-\ntively rerank documents, we similarly use LLMs\nfor reranking datasets. While we use only descrip-\ntions of the dataset to generate our initial candi-\ndate datasets, this is often inadequate for localizing\nto the best dataset for a given task.  For instance,\nfor a math-based task which have multiple choice\nquestions (MCQs), the dataset descriptions of both\nGSM8K\n2\nandmath_qa\n3\nmake them valid choices,\nbut it is only when we look at the schema and sam-\nple rows ofmath_qa(which has multiple choice\noptions) do we see that it is a better choice for\nour task thanGSM8K(which has open-ended ques-\ntions).   Thus we also provide additional dataset\nattributes such as its schema and a small sampling\nof rows, to aid with the reranking. The reranking\nstep concludes with the name (and any versions) of\nthe chosen datasetD\nR\n. We include the reranking\nprompt we used in Appendix. A.1.\nAdditionally, to improve the reliability of dataset\nchosen,    we   employ   self-consistency   decod-\ning (Wang et al., 2023), where we run the reranker\nmultiple times, and choose the most frequently re-\n2\nhttps://huggingface.co/datasets/gsm8k\n3\nhttps://huggingface.co/datasets/math_qa\nturned dataset.  It is also possible for no suitable\ndataset being found, acknowledging that not every\ntask has a relevant dataset.\n3.2    Dataset Transformation\nHaving selected an appropriate datasetD\nR\nfrom\nD,  we  now  focus  on  tailoring  it  to  the  task  re-\nquirements to createD\n′\n. This transformation pro-\ncess can include modifications ranging from adjust-\ning the input/output format to more closely match\nthat of the few shot examples to more substantial\nchanges like generating new fields that the present\ndataset may not currently have.\nTo accomplish this complex task, we split it into\ntwo primary steps. First, thePlanning Modulegen-\nerate a multi-step plan illustrating the sequence of\ntransformations needed to convert examples from\nD\nR\ninto the desired output.  Next, theExecution\nModuleexecutes the plan on each datapoint inD\nR\nto create our resulting synthetic training datasetD\n′\n.\nEmpirically, we notice that the Planning Module\nis more effective when given descriptions of the\nsource dataset (D\nR\n) and target task (T) that are de-\ntailed and specific. As such, we further implement\ntwo more scaffolding modules — aSchema Selec-\ntorthat removes irrelevant columns from dataset\nD\nR\n(providing a clearer source description) and a\nTask Expansion Modulethat enriches task descrip-\ntionsTwith requirements (giving a better target\nspecification).\nBelow,  we describe the four modules in their\nexecution sequence (examples in Figure 2).\nTask ExpansionThe brevity of task descriptions\noften hampers the creation of an effective plan, and\noften the subtleties of the task are more readily",
    "Sample Plan\n1. Extract the \"solutions\" field from the dataset as this contains the Python code snippets.\n2. For each \"solutions\" entry, identify the primary operation or functionality of the Python code.\nThis may require parsing the code and understanding its logic.\n3. Generate a set of multiple-choice descriptions (\"choices\") for each code snippet. These should\ninclude one correct description of what the code does and several incorrect descriptions.  The\nincorrect descriptions can be plausible but should not accurately describe the code’s functionality.\n4. Format the \"input\" field by labeling it as \"Python code:\" followed by the actual code snippet\nfrom the \"solutions\" field. Below the code, list the generated \"choices\" with the label \"choice:\"\npreceding each option.\n5. Determine the correct \"choice\" that accurately describes the code’s behavior. This will be the\n\"output\" field.\n6. Combine the \"input\" field and the \"output\" field to create the final data in the required format\nfor the task examples.\n7.  If a \"solutions\" entry does not contain a Python code snippet or is not relevant to the task\ndescription, ignore the data sample and return null for that entry.\nFigure 3: We show an example plan for the task of providing concise descriptions of Python code. The retrieved\ndataset contains natural language questions and code solutions. The plan then specifies that the transformation must\ncreate the correct description, create incorrect descriptions to create an multiple choice dataset, and format changes\nrequired to match the target task examples.\ngrasped through the examination of examples.\n4\nTo\naddress this issue, we implemented an intermedi-\nate step wherein the LLM is utilized to scrutinize\nboth the task description and provided examples\nand generate an expanded version of the task de-\nscription.  This enhanced task specification helps\nin devising a more detailed and actionable plan,\nencompassing more explicit steps tailored to the\ntask at hand. The task expansion prompt is linked\nin the Appendix A.6.\nSchema SelectionThe schema selection compo-\nnent is designed by instructing LLMs to identify the\nmost pertinent columns within a dataset for a given\ntask.  For example, for the task of code comment\ngeneration,  where we have a dataset of internet\ncode, thecode snippetcolumn is extremely useful,\nwhereas theURLcolumn code would not be useful.\nTo identify these relevant columns, we provide the\nLLM with detailed information, including the task\ndescriptionT, the chosen datasetD\nR\n, the names\nof existing columns inD\nR\n, and samples of dataset\nrows fromD\nR\n. The LLM is then tasked with iden-\ntifying which columns are relevant to the specific\ntaskT. This approach ensures a targeted selection\n4\nThis is akin to requirement elicitation in Software En-\ngineering, where a deeper understanding of requirements is\nachieved through further analysis (Lamsweerde, 2009).\nof dataset features that are directly applicable to\nthe task requirements, optimizing the datasetD\nR\nfor the intended application. The schema selection\nprompt is linked in the Appendix A.2.\nPlanning ModuleNow,  the LLM generates a\ncomprehensive plan to adapt each data point from\nthe retrieved datasetD\nR\nto the task requirements.\nThis  plan  is  a  series  of  concrete,  concise  steps.\nThese  steps  may  include  combining  data  fields,\nelaborating on existing fields, or creating new fields\nderived from the existing data or excluding the data\nsample altogether if it is irrelevant.  The LLM is\nprovided with the expanded task description, along\nwith the optimized dataset, complete with its de-\nscription and sample rows. A sample plan is shown\nin Figure 3. We are only required to create a plan\nonce per task, since the same task-level plan will\nget executed for each row of the retrieved dataset.\nThe planning module prompt is linked in the Ap-\npendix A.3.\nExecution ModuleThe execution of the trans-\nformation plan for each dataset sample fromD\nR\nis\ndone using an LLM. The LLM receives three key\npieces of information for each row of the dataset:\nthe row itself, detailed specifications of the input\ntask, and the transformation plan formulated earlier.\nConditioned on these pieces of information, the",
    "Task NameTask CategoryAbbreviationTask Instruction\nTemporal SequencesLogical ReasoningTimeAnswer questions about which times certain events could have occurred.\nCode Line DescriptionsCodingCodeGive an English language description of Python code.\nElementary MathMathMathAnswer a multiple choice mathematical word problem.\nCause and EffectCausal ReasoningC&EAnswer multiple-choice questions distinguishing cause and effect.\nMedical Questions in RussianDomain SpecificRussianAnswer a yes/no question about medical text in Russian.\nImplicaturesContextual QAImpl.Predict whether Speaker 2’s answer to Speaker 1 is affirmative or negative.\nTable 1: We evaluate our method on 6 diverse text-based tasks from BIG-Bench.\nmodel responds by producing an adjusted dataset\nrow which, hopefully, meets the requirements of\nthe input task.  The execution module prompt is\nlinked in Appendix A.4. The adjusted dataset rows\nform our synthetic datasetD\n′\nused for fine-tuning.\n3.3    Using Multiple Datasets\nThe transformation process of data points may re-\nsult in a significant number of them getting filtered\nout. Additionally many datasets may be small to be-\ngin with. Both these factors can result in a reduced\nquantity of transformed data. To balance quantity\nand quality, we adopt a strategy of transforming\nmultiple highly ranked datasets until we reach our\ndesired dataset set. Furthermore, if a considerable\nproportion of data sample transformations fail (e.g.\nnoisy data samples in the dataset) for a specific\ndataset, we opt to exclude it from consideration.\n4    Experimental Setup\nEvaluation  procedureWe  evaluate  DataTune\nand related baselines over 6 tasks from the BIG-\nBench  benchmark  (BIG  Bench  Authors,  2023),\nwhich  we  list  in  Table  1.   BIG-Bench  focuses\non  tasks  believed  to  be  beyond  the  capabilities\nof  current  language  models,  covering  a  variety\nof  task  categories.    We  choose  tasks  spanning\nlogical reasoning, coding, math, causal reasoning,\nmultilinguality,  and  domain  specific  tasks.   For\neach  task,  we  create  3000  data  points  through\nDataTune,   and   apply   the   training   procedure\nhighlighted  earlier.   We  evaluate  all  models  for\ntwo-shot  performance,   as  per  the  BIG-Bench\ntesting suite.\n5\nMethodsWe compare the effectiveness of var-\niousdataset  collectionstrategies,  by  using  the\nresulting  data  to  finetune  the  same  base  model,\nMistral-7B  (Jiang  et  al.,  2023),  a  leading  open-\nsource LLM within its size category.  As shown\n5\nhttps://github.com/google/BIG-bench\nin  Table  2,   this  includes:(1)  retrieving  ex-\nisting  data  using  dense  retrieval  (Viswanathan\net al., 2023a),  (2) generating synthetic data,  (3)\nPrompt2Model (Viswanathan et al., 2023b) (a state-\nof-the-art method that combines retrieving exist-\ning  data  and  synthetic  data  generation),  (4)  our\nDataTune  approach,  and  (5)  a  combination  of\nDataTune and synthetic data generation, which rep-\nresents an integration of all the existing methods.\nWe also include twopromptingbaselines: (6)\nfew-shot prompting on the base model Mistral-7B,\nand (7)GPT-3.5-Turbo, a significantly larger\nmodel, which we include as a robust benchmark\ndue to its extensive capabilities.\nDataset Creation SetupOur initial retrieval step\nusing DataFinder retrieves 25 candidate datasets\nthat are processed for reranking.   We transform\nupto 4 datasets per task until we meet our desired\nset of 3000 data points.   The LLM used for all\ncomponents is GPT-4-Turbo, except for the final\nexecution step, which uses GPT 3.5-Turbo.\n6\nTraining SetupWe used the Mistral-7B model,\nfollowing the approach of Jiang et al. (2023), and\napplied the QLoRA technique from Dettmers et al.\n(2023) for fine-tuning. The process was carried out\nover 3 epochs.  We select parameters by running\n4  runs  across  two  sets  of  hyperparameters  over\ntwo  values(  learning  rate:5e\n−5\nand1e\n−4\n,  and\nQLoRA’sαparameter  between  16  and  24  and\nchoose  the  run  with  the  lowest  validation  loss\nat  any  point.    We  used  the  AdamW  optimizer\n(Loshchilov  and  Hutter,  2017)  and  set  QLoRA\nr= 8.   We conduct our training on 2 NVIDIA\nRTX A6000 GPUs.\nMetricsFollowing BIG Bench Authors (2023),\nwe use anormalized aggregate scoreevaluation\n6\nOur dataset transformation method requires making an\nLLM query for each instance in the dataset. Therefore, the de-\ncision to use GPT-3.5-turbo was made for budgetary reasons.",
    "that normalizes a multiple choice grade score, such\nthat a score of 0 implies random chance perfor-\nmance, and 100 implies human expert performance.\nAdditionally, a score less than 0 indicates perfor-\nmance worse than random chance.\n5    Results and Analysis\n5.1    Performance Comparison\nDataTune    consistently    outperforms    few-shot\nprompting,  as  well  as  existing  individual  data\ncollection  methods.From  Table  2,  we  see  that\nfine-tuning our base model on DataTune outper-\nforms the base Mistral-7B model by 6.4 points on\naverage, improving over it in five out of six tasks.\nWe also show that DataTune provides an average\nimprovement of 11 and 2.9 points over fine-tuning\non existing data and synthetically generated data\nrespectively.\nDataTune’s transformation is complementary to\nsynthetic data generation.It is noteworthy that the\ncombination of DataTune and synthetically gener-\nated data results in a marked performance increase.\nThis synergistic improvement yields an overall av-\nerage score of 44.5 on the BIG-Bench tasks we\nconsider. We provide a more detailed analysis of\nthe synergy between DataTune and existing syn-\nthetic dataset generation methods in Section 5.5.\nOur   system   (DataTune   +   Synthetic   Data\nGeneration)  outperforms  SOTA  baselines  like\nPrompt2Model.In order to make a fair compar-\nison  with  baselines  such  as  Prompt2Model  that\nfine-tune on both existing data and synthetically\ngenerated data, we define our system as the base\nMistral-7B model fine-tuned on DataTune-created\ndata and synthetically generated data. Our analysis\nin Table 2 highlights significant differences in per-\nformance between our system and Prompt2Model,\nwith our system demonstrating a notable advantage\nover Prompt2Model five out of six tasks, with an\naverage improvement of 8.3 points. These findings\nunderscore the effectiveness of our system to create\nquality datasets for fine-tuning across a wide range\nof tasks and domains.\n5.2    DataTune Impact on Dataset Diversity\nSampling diverse yet high-quality examples from a\nlanguage model is a challenging task. Prior work\nhas shown that the correctness of a synthetic dataset\nsampled from a language model is inversely corre-\nlated with its diversity (Ye et al., 2022). Generating\nsynthetic data directly from a language model (us-\nCodeRussianMathTimeImpl.\nTask\n0\n20\n40\n60\n80\n100\n% of unique examples\nTransformed\nSynthetic\nGold\nFigure 4: Synthetic dataset generation often suffers from\nthe problem of generating multiple duplicates of the\nsame example in a given dataset.  On 3 of 5 tasks, we\nfind that data transformation from retrieved datasets\nsignificantly mitigates this issue. The other two datasets,\nRussian and Temporal, represent failure modes of our\nsystem.  Gold represents the BigBench Dataset for a\ngiven task.\ning a method like Prompt2Model) often contains\nnear-duplicate examples.\nDoes transforming existing datasets reduce the\nincidence of duplicate examples? Using ROUGE-\nL  to  determine  lexical  uniqueness  (Lin,  2004;\nWang  et  al.,  2022),  we  determine  a  sentences\nin datasetD\n′\nto beuniqueup to threshold\n7\nT\nif\nmax\ns\n′\n∈D\n′\n\\{s}\nROUGE(s,s\n′\n)< T.  In Figure 4,\nwe observe that over 50% of examples generated\nsynthetically are near-duplicates for 3 of 5 tasks; in\neach of those cases, using DataTune instead effec-\ntively eliminates this problem.\nWe observe similar trends with lexical diversity\nacross all dataset creation methods. In Table 3, we\nmeasure the number of unique bigrams per gen-\nerated input example.  DataTune significantly in-\ncreases the number unique bigrams per example\n(and moderately increases the length of each exam-\nple) on 3 of 5 datasets.\nAccording to both measures, dataset diversity de-\ncreases onTemporal SequencesandMedical Ques-\ntions in Russian, which are also the two tasks where\nDataTune fails to improve over training on fully\nsynthetic data. We discuss these two tasks in Sec-\ntion 5.5 and Limitations, respectively.\n5.3    DataTune Generates Harder Examples\nSynthetic  datasets  sampled  directly  from  a  lan-\nguage  model  tend  to  overrepresent  with  easy-\n7\nWe set the ROUGE-L threshold to 0.8 for Code Line\nDescriptions, where examples are Python snippets, 0.9 for\nTemporal Sequences, where examples are long English texts,\nand 0.7 for the other datasets.",
    "MethodSteps# Train.\nPoints\nTasks\nRetrieval TypeGenerationTimeCodeMathC&ERussn.Impl.Avg.\nFew-Shot Baselines\nGPT-3.5--050.675.630.496.790.664.268.0\nMistral-7B--0-2.562.32.937.239.839.029.8\nMistral-7B+\nSynthetic Finetuning Baselines\nExisting data\nDense-3000-4.762.30.852.90.039.925.2\nSynthetic data-Synthetic30002.060.83.837.254.041.933.3\nDataTune (DT)+ RerankerTransformed3000-2.171.21.356.948.041.936.2\nPrompt2ModelDenseSynthetic6000-2.073.44.733.886.044.040.0\nDT+Synthetic\n+ RerankerBoth600016.984.58.141.268.048.044.5\nTable 2: We compare the performance of different few-shot learning methods across six BIG-Bench tasks. Here,\nwe categorize each method by what base model is used (Mistral-7B or GPT-3.5-Turbo), whether data is retrieved\n(and, if so, whether a dense retriever or dense retriever + reranker is used), how the data points are generated\n(whether transformed from an existing dataset, generated synthetically, or both).  “# Train.  Points” refers to the\nnumber of training examples produced by the method for each task. Normalized aggregate scores below zero imply\nperformance worse than chance.\nDatasetUnique Bigrams  Total Tokens\nPer ExamplePer Example\nCode Line Description\nGold13.232.3\nSynthetic2.535.0\nTransformed14.986.9\nElementary Math\nGold10.848.6\nSynthetic3.334.4\nTransformed11.643.8\nImplicatures\nGold9.924.1\nSynthetic2.727.7\nTransformed17.839.8\nTemporal Sequences\nGold1.099.7\nSynthetic20.854.6\nTransformed0.273.7\nMedical Questions in Russian\nGold62.079.4\nSynthetic20.854.6\nTransformed11.644.8\nTable 3: We observe that dataset transformation yields\ndatasets  with  greater  lexical  diversity  than  synthetic\ndataset generation on 3 of 5 datasets.\nto-solve  examples  (Xu  et  al.,  2023).To  test\nour  ability  to  overcome  this  issue,   we  used\ngpt-3.5-turboto  estimate  the  difficulty  of\neach   example   generated   from   each   method\n(DataTune, synthetic generation, and manual cura-\ntion) for four tasks. We wrote a custom difficulty\nestimation prompt (including in-context examples)\nfor each task; in all prompts, we specify that the\nLLM should rate the example on a scale of 1 to\n5.  We outline an example of this prompt in Ap-\n12345\n0%\n20%\n40%\n60%\n80%\n100%\n% of examples\nElementary Math\n12345\nCode Line Descriptions\n12345\nExample Difficulty\n0%\n20%\n40%\n60%\n80%\n100%\n% of examples\nImplicatures\n12345\nExample Difficulty\nRussian Questions\nTransformed\nSynthetic\nGold\nFigure 5:  Dataset Transformation leads to more diffi-\ncult examples than synthetically generated examples,\nwhich  are  over-represented  by  easy  examples,  rela-\ntive to manually-curated BIG-Bench evaluation datasets\n(Gold).\npendix A.6. As illustrated in Figure 5, we observe\nthat the data from DataTune exhibits a higher level\nof difficulty compared to synthetically generated\ndata for all datasets other thanMedical Questions\nin Russian. We attribute this increase in difficulty\nto the added knowledge introduced by the existing\ndataset selected for transformation, which is often\nmore nuanced and, consequently, more challenging.\nIn contrast, unconditional generation from LLMs\ntends to produce examples that are likely to have a\nhigh prior (prompt-independent) probability under\nthe language model.  The exception to this effect,",
    "Medical Question in Russian, can be attributed to\nthe failure of our approach’s transformation plan to\ngenerate data in the correct language for this task.\nWe provide more details of this failure mode in\nLimitations.\n5.4    Does generating harder examples lead to\nlower label accuracy?\nWe evaluated the label accuracy of generated data\nfor  the  Code  Line  Description  task  by  compar-\ning DataTune against synthetic dataset generation.\nManually annotating a random sample of 300 data\npoints from each generation method for the Code\nLine Description Task, we found the label accuracy\nfrom DataTune is 88%, compared to 86.6% for syn-\nthetic data generation. This comparison suggests\nthat DataTune can produce datasets that are com-\nparable in accuracy to purely-synthetic datasets,\ndespite DataTune generating significantly more di-\nverse and challenging examples on this task.\n5.5    Transformed Data Can Be\nComplementary to Synthetic Data\nIn two cases where DataTune fails to improve over\na synthetic data baseline,Temporal Sequencesand\nElementary Math, our combined DataTune + Syn-\nthetic system still outperforms all other comparable\n(Mistral-based) baselines. We observe these two ap-\nproaches to dataset generation can be complemen-\ntary to each other, yielding additive improvements\nwhen combined.\nFor a concrete example of this, we can visually\nobserve the two-dimensional semantic distribution\nof questions generated for the Elementary Math\ntask via DataTune, Synthetic Generation, and from\nthe gold dataset in Figure 6.   We encoded each\nquestion  using  MiniLM  v2  (Wang  et  al.,  2021)\nviasentence-transformers(Reimers and\nGurevych, 2019), then projected each embedding\ninto a 2D space using t-SNE (van der Maaten and\nHinton, 2008).\nVisually, we observe that the embedding clusters\nof questions generated via DataTune and Synthetic\nGeneration appear to be largely disjoint. This sup-\nports our hypothesis that these two methods system-\natically cover different distributions of the target\ntask space, and therefore combining the examples\nfrom each method can lead to a synergistic effect.\n151050510\n15\n10\n5\n0\n5\n10\nElementary Math\nGold\nTransformed\nSynthetic\nFigure  6:   We  observe  that  examples  generated  via\nDataTune and synthetic dataset generation fall into vi-\nsually well-separated regions of embedding space, after\nprojecting to two dimensions via t-SNE.\n6    Conclusion and Future Work\nWe presented DataTune, an improved approach for\nenhancing automatic dataset generation by trans-\nforming existing labeled datasets. Our method sig-\nnificantly outperforms existing automatic dataset\ngeneration techniques on several challenging tasks\nfrom BIG-Bench. When used in conjunction with\nexisting synthetic dataset generation methods, it\nachieve superior performance compared to other\nfew-shot  learning  methods  on  the  base  model\nwe  used  (Mistral-7B).  Our  analysis  reveals  that\nDataTune not only creates more diverse and ac-\ncurate datasets but also increases their complexity\nfor fine-tuning. An important direction is whether\ntransformation-based dataset creation methods like\nDataTune can still be effective when examples are\nretrieved from the open web rather than from the\ncollections of manually-curated datasets we con-\nsider in our work. Another important direction for\nfuture work will be to generate code to execute a\ntransformation plan (rather than querying an LLM\nfor each instance of data).  Both of these future\ndirections would improve the accessibility and scal-\nability of our suggested approach.\nLimitations\nWe identify four key limitations in our system:\n1.LLM Query Cost:  Our dataset transforma-",
    "tion component requires running the Execu-\ntion Module on each row of each dataset we\nwish  to  transform.   Given  that  our  Execu-\ntion Module prompts a large language model,\nthe number of LLM queries scales linearly\nwith the amount of data to be transformed.\nThis could be cost-prohibitive for transform-\ning very large datasets.  This LLM usage re-\nquirement could also exclude members of the\nresearch community without consistent LLM\naccess from benefiting from our work.\n2.\nDependence on the Planning Module: Our\ndataset transformation system relies heavily\non the Planning Module to produce clear and\ncomprehensive instructions for the Execution\nModule to enact.  Our Planning Module op-\nerates by prompting a large language model.\nGiven that prompted LLMs can behave unpre-\ndictably in changes in prompts (Sclar et al.,\n2023), an important limitation of our method\nis that its success depends on a large language\nmodel following a prompt correctly. We see\nthis failure with Medical Questions in Rus-\nsian,  where  the  agent  is  supposed  to  trans-\nlate a Russian dataset into English to facili-\ntate the generation of question-answer pairs.\nWhen running our benchmarking, the Plan-\nning Agent failed to translate the generated\nquestion-answer pairs back to Russian. This\nresulted in a training dataset that, despite be-\ning  conceptually  close,  was  practically  far\nfrom correct.\n3.Handling Non-English Data: Our transfor-\nmation agent’s ability to process non-English\ndata tasks is substantially compromised, fre-\nquently altering example tasks instead of the\nactual data. This deficiency is primarily due\nto the reliance on models like GPT 3.5, which\nhave been extensively trained on English data,\nthereby  diminishing  their  proficiency  with\nother languages.\n4.DependenceonInstruction-Following\nLLMs:  The system’s execution component\ndepends on Large Language Models (LLMs)\nthat  are  specifically  designed  to  adhere  to\ninstructions. We have identified discrepancies\nin    performance    among    LLMs    tailored\nfor   instruction-based   tasks   versus   those\ndeveloped for conversational purposes. This\nlimitation confines our system to using only a\nnarrow selection of LLMs that demonstrate\nthe ability to follow instructions accurately.\nEthical Considerations\nDataTune could make it easier for the general pub-\nlic to build custom language models. The broaden-\ning of open-ended technology induces ethical con-\ncerns, similar to the issues with open-source deep-\nfake libraries described by Widder et al. (2022).\nWhile DataTune has potential for misuse, this is\nlikely no greater than the potential harms presented\nby the underlying open-source large language mod-\nels. By making it easier to build task-specific lan-\nguage models,  we hope that these risks are bal-\nanced by the benefits of making NLP models acces-\nsible to those outside the NLP community or those\nwithout the resources to manually collect labeled\ndata. We aim to be transparent in our documenta-\ntion about the potential limitations of the system.\nThe platform is particularly valuable for individ-\nuals outside the NLP community who can benefit\nfrom using NLP models in their work but lack the\nspecialized knowledge to develop these tools inde-\npendently. The decision to open-source DataTune\ninvites community contributions,  emphasizing a\ncollaborative approach to improve and expand the\ntool’s capabilities. This strategy not only enhances\nthe system’s utility but also aligns with a broader\ngoal of increasing the accessibility of NLP innova-\ntions and fostering a more inclusive technological\nenvironment.\nReferences\nBIG Bench Authors. 2023. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of lan-\nguage models.Transactions on Machine Learning\nResearch.\nChris Callison-Burch. 2009. Fast, cheap, and creative:\nEvaluating translation quality using amazon’s me-\nchanical turk. InProceedings of the 2009 conference\non empirical methods in natural language processing,\npages 286–295.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken\nChia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.\nIs GPT-3 a good data annotator?   InProceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 11173–11195, Toronto, Canada. Association\nfor Computational Linguistics.",
    "Olivia Huang, Eve Fleisig, and Dan Klein. 2023. Incor-\nporating worker perspectives into MTurk annotation\npractices for NLP. InProceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1010–1028, Singapore. Associa-\ntion for Computational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril,  Thomas Wang,  Timothée Lacroix,\nand William El Sayed. 2023. Mistral 7b.\nA van Lamsweerde. 2009.Requirements engineering:\nfrom system goals to UML models to software speci-\nfications. John Wiley & Sons, Ltd.\nQuentin Lhoest,  Albert Villanova del Moral,  Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil,  Julien  Chaumond,  Mariama  Drame,  Julien\nPlu, Lewis Tunstall, Joe Davison, Mario vSavsko,\nGunjan Chhablani, Bhavitvya Malik, Simon Bran-\ndeis,  Teven  Le  Scao,  Victor  Sanh,  Canwen  Xu,\nNicolas Patry,  Angelina McMillan-Major,  Philipp\nSchmid, Sylvain Gugger, Clement Delangue, Th’eo\nMatussiere, Lysandre Debut, Stas Bekman, Pierric\nCistac, Thibault Goehringer, Victor Mustar, François\nLagunas,  Alexander  M.  Rush,  and  Thomas  Wolf.\n2021.   Datasets:  A community library for natural\nlanguage processing.ArXiv, abs/2109.02846.\nChin-Yew Lin. 2004.   ROUGE: A package for auto-\nmatic evaluation of summaries. InText Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nIlya  Loshchilov  and  Frank  Hutter.  2017.Fixing\nweight   decay   regularization   in   adam.CoRR,\nabs/1711.05101.\nMarius Mosbach, Tiago Pimentel, Shauli Ravfogel, Di-\netrich Klakow, and Yanai Elazar. 2023.   Few-shot\nfine-tuning vs. in-context learning: A fair compari-\nson and evaluation.  InFindings of the Association for\nComputational Linguistics: ACL 2023, pages 12284–\n12314, Toronto, Canada. Association for Computa-\ntional Linguistics.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text.  InProceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nInProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach,  Lintang  Sutawika,  Zaid  Alyafeai,  Antoine\nChaffin,  Arnaud  Stiegler,  Teven  Le  Scao,  Arun\nRaja, et al. 2021.  Multitask prompted training en-\nables zero-shot task generalization.arXiv preprint\narXiv:2110.08207.\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane\nSuhr. 2023. Quantifying language models’ sensitiv-\nity to spurious features in prompt design or: How i\nlearned to start worrying about prompt formatting.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren,\nDawei Yin,  and Zhaochun Ren. 2023.   Is chatgpt\ngood at search? investigating large language models\nas re-ranking agent.ArXiv, abs/2304.09542.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia\nHu. 2023.  Does synthetic data generation of llms\nhelp clinical text mining?\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne.Journal of Machine\nLearning Research, 9(86):2579–2605.\nVijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei\nLiu, and Graham Neubig. 2023a. DataFinder: Scien-\ntific dataset recommendation from natural language\ndescriptions. InProceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 10288–10303,\nToronto, Canada. Association for Computational Lin-\nguistics.\nVijay Viswanathan, Chenyang Zhao, Amanda Bertsch,\nTongshuang   Wu,   and   Graham   Neubig.   2023b.\nPrompt2model: Generating deployable models from\nnatural language instructions.\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-\ndro  Sordoni,  Adam  Trischler,  Andrew  Mattarella-\nMicke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-\nploring and predicting transferability across nlp tasks.\narXiv preprint arXiv:2005.00770.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021.  MiniLMv2:  Multi-head self-\nattention  relation  distillation  for  compressing  pre-\ntrained transformers. InFindings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 2140–2151, Online. Association for Computa-\ntional Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain\nof thought reasoning in language models.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions.arXiv\npreprint arXiv:2212.10560.",
    "David Gray Widder, Dawn Nafus, Laura Dabbish, and\nJames Herbsleb. 2022.  Limits and possibilities for\n“ethical ai” in open source: A study of deepfakes. In\nProceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’22,\npage 2035–2046, New York, NY, USA. Association\nfor Computing Machinery.\nCan  Xu,  Qingfeng  Sun,  Kai  Zheng,  Xiubo  Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions.\nJiacheng Ye, Jiahui Gao, Jiangtao Feng, Zhiyong Wu,\nTao Yu, and Lingpeng Kong. 2022.   Progen:  Pro-\ngressive zero-shot dataset generation via in-context\nfeedback.  InConference on Empirical Methods in\nNatural Language Processing.\nZhisong Zhang, Emma Strubell, and Eduard Hovy. 2022.\nA survey of active learning for natural language pro-\ncessing.arXiv preprint arXiv:2210.10109.",
    "A    Example Appendix\nA.1    Reranking Prompt\nYour objective is to choose the most relevant dataset for a given a task (and few examples of the\ntask). For each dataset, you will be provided with the dataset description, and tags related to the\ndataset. Please return the most relevant dataset, e.g., squad\nThe following is the task\n{{instruction}}\nand these are some examples of the same:\n{{examples}}\nThere are {{num}} datasets available for this task.\n{{datasets}}\nThe name of the most relevant dataset for this task is:\nwhere each dataset in {{datasets}} is defined as:\n{counter}{{dataset_name}}:Description-{{dataset_description}}.\nThis dataset has the following tags:\n{{tags}}\nA.2    Schema Selection Prompt\nYour objective is to carefully analyze the task and the dataset mentioned, and decide whether the\ncolumns are relevant input, relevant output, irrelevant for the given task, or if it is ambiguous.\nThere should be at most one output column. It is possible to have no relevant columns, in which\ncase return the input and output column as empty lists. Answer in a json format, with the following\nkeys: input, output, irrelevant, ambiguous.\n{{INCONTEXT_EXAMPLES}}\nAfter seeing these examples with the required columns, please provide the relevant columns for\nthis context:\nYou  are  tasked  with  the  following  process.{{instruction}}  For  this  task,  you  will  use\nthe {{dataset_name}} dataset from HuggingFace. Dataset Description: {{dataset_description}}\nA sample data instance from this is as follows. {{sample_row}}.\nThis dataset has the following columns: {{dataset_columns}}\nRequired Columns :",
    "A.3    Planning Module Prompt\nYou are a Planning Agent. You create a plan to transform data samples from their existing format\ninto the required format for a given task.\n————————————————-\nHere are some examples for your reference.\n{{in_context_examples}}\n————————————————\nNow do the following task:\nTask Description: {{task_description}}\nTask Examples: example\nHere are samples from a potentially relevant dataset for the task above. Notice how the format\nbelow is not as required by the task above.\nDataset Samples: {{dataset_row}}\nCarefully analyze the ‘Task Description‘ and the ‘Task Examples‘. Propose a higher-level plan to\nconvert data from the Dataset Sample to data in the required format task examples.  Your plan\nshould be a list of sequential steps that can be taken to perform the data transformation. You don’t\nneed to use all columns, as the dataset may not be fully relevant. Keep steps as simple, explicit and\nconcise as possible. Each step in the plan may take any of the following actions: 1. Generate new\ncolumns as required by the task, and save them\n2. Expand on a particular column to make it something more relevant to the task and save it\n3. Combine multiple columns from the dataset\n4. Choose columns that will form \"input\"\n5. After the input field is created, carefully analyze it to choose/generate the output field\n6. Ignore a data sample because it is not all relevant and return null for them.\nReturn only the plan.\nA.4    Execution Module Prompt\nYou  are  a  Data  Transforming  Agent.   Your  job  is  to  transform  data  from  a  given  format  to\nthe required format.  Following are the detailed instructions for the same:  1.  Read the ‘Task\nDescription‘.\n2. An example of the input and output looks like for the task is shown in ‘Task Examples‘\n3. The sample to be transformed is in ‘Data Sample‘.\n4. Read the data transformation plan carefully that will help you convert the ‘Data Sample‘ into\nthe required format. This should be relevant and intune to the ‘Task Description‘\n5. Perform the plan step by step and explain your thinking.\n6.  End your response with the transformed sample as a JSON response with exactly 2 fields:\n\"input\" and \"output\".\n————————————————-\nHere are some examples for your reference. {{incontext_examples}}\n————————————————\nNow do the following task:\nTask Description: {{task_description}}\nTask Examples: {{sample}}\n{{plan}}\nDataset Sample: {{dataset_row}}\nThink step by step through the plan to convert the above ‘Dataset Sample‘ and show your working.\nEnd your response as a JSON with exactly two fields: \"input\", and \"output\" Response:",
    "A.5    Task Expansion Prompt\nCarefully analyse the task description and examples of the task, and explain the task to give a\nclearer description. Do not explain each example, but rather capture the general trends. Also place\nspecial focus on the format of the input/output examples.\n————————————————-\nTask Description: {task description}\nTask Examples: {examples}\nA.6    Difficulty estimation prompt: Code Line Descriptions example\nWe are building a dataset for automatically describing code\n(in words). Evaluate and rate the difficulty and complexity\nof describing the following code lines. You should give an\noverall score on a scale of 1 to 5,\nwhere a higher score indicates higher difficulty.\nYou must just give a score without any other reasons.\nHere’s the grading scale:\n1: Very easy. Anyone who understands the\nprogramming language could describe this almost instantly\n2: Easy. Anyone who understands the programming\nlanguage could describe this with a bit of thought\n3. Neutral. Most non-expert people who understand\nthe programming language would be able to describe this,\nbut it might take time for them to understand the code\n4. Hard. It would require at least a minute\nfor a non-expert person who understand the\nprogramming lanugage to understand and describe this code.\n5. Very hard. Most non-experts would make a mistake\nwhen trying to describe this code in a fixed timeframe.\nProfessional programmers would have an easier time.\nYour answer shoud be a single number, 1 through 5,\nwith nothing else in your response.\n{Incontext examples with code and difficulty}\n{Input Code}"
  ]
}