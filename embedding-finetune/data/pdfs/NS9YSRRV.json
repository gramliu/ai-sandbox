{
  "key": "NS9YSRRV",
  "url": "http://arxiv.org/pdf/2212.10001",
  "metadata": {
    "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of\n  What Matters",
    "abstract": "  Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.\n",
    "published": "2022-12-20T05:20:54Z"
  },
  "text": [
    "Towards Understanding Chain-of-Thought Prompting:\nAn Empirical Study of What Matters\nBoshi Wang\n1\nSewon Min\n2\nXiang Deng\n1\nJiaming Shen\n3\nYou Wu\n3\nLuke Zettlemoyer\n2\nHuan Sun\n1\n1\nThe Ohio State University\n2\nUniversity of Washington\n3\nGoogle Research\n{wang.13930,deng.595,sun.397}@osu.edu\n{sewon,lsz}@cs.washington.edu,{jmshen,wuyou}@google.com\nAbstract\nChain-of-Thought (CoT) prompting can dra-\nmatically improve the multi-step reasoning abil-\nities of large language models (LLMs).  CoT\nexplicitly encourages the LLM to generate in-\ntermediate rationales for solving a problem, by\nproviding  a  series  of  reasoning  steps  in  the\ndemonstrations.  Despite its success, there is\nstill little understanding of what makes CoT\nprompting effective and which aspects of the\ndemonstrated reasoning steps contribute to its\nperformance.    In  this  paper,  we  show  that\nCoT reasoning is possible even with invalid\ndemonstrations—prompting with invalid rea-\nsoning steps can achieve over 80-90% of the\nperformance obtained using CoT under various\nmetrics, while still generating coherent lines\nof reasoning during inference. Further experi-\nments show that other aspects of the rationales,\nsuch as being relevant to the query and correctly\nordering the reasoning steps, are much more\nimportant for effective CoT reasoning. Overall,\nthese findings both deepen our understanding\nof CoT prompting, and open up new questions\nregarding LLMs’ capability to learn to reason\nin context.\n1\n1    Introduction\nLarge language models (LLMs) can perform new\ntasks during inference when prompted with a few\ndemonstrations  (Brown  et  al.,  2020).   Chain-of-\nThought (CoT) prompting (Wei et al., 2022) can\n(Figure 1) improve the ability of sufficiently large\nLLMs to do complex and multi-step reasoning. In\naddition to (query, answer) example-pair demon-\nstrations, CoT prompting includes arationale(col-\nored part in Figure 1) for each example, i.e., a series\nof reasoning steps towards the answer, which en-\ncourages the LLM to explicitly generate its interme-\ndiate reasoning process before predicting the final\nanswer. Despite its successes, there is little under-\nstanding of what makes CoT prompting effective\n1\nOur code and model input/output are available here.\n\nIn-context DemonstrationInference by LLM\nQuery\nLeah had 32 chocolates and her \nsister had 42. If they ate 35, how \nmany pieces do they have left in \ntotal?\nJulie is reading a 120-page book. Yesterday, \nshe was able to read 12 pages and today, she \nread twice as many pages as yesterday. If she \nwants  to  read  half  of  the  remaining  pages \ntomorrow, how many pages should she read?\nStandardThe answer is 39.\nThe answer is 18. ✗\nCoT\nOriginally,   Leah   had   32 \nchocolates and her sister had 42. \nSo in total they had 32 + 42 = \n74. After eating 35, they had 74 \n- 35 = 39 pieces left in total. The \nanswer is 39.\nJulie is reading a 120-page book. Yesterday, \nshe  read  12  pages  and  today,  she  read  24 \npages. So she read a total of 12 + 24 = 36 \npages. Now she has 120 - 36 = 84 pages left. \nSince she wants to read half of the remaining \npages, she should read 84 / 2 = 42 pages. The \nanswer is 42. ✓\nInvalid \nReasoning\nOriginally,   Leah   had   32 \nchocolates and her sister had 42. \nSo  her  sister  had  42  -  32  =  10 \nchocolates more than Leah has. \nAfter eating 35, since 10 + 35 = \n45, they had 45 - 6 = 39 pieces \nleft in total. The answer is 39.\nYesterday,  Julie  read  12  pages. Today,  she \nread 12 * 2 = 24 pages. So she read a total of \n12 + 24 = 36 pages. Now she needs to read \n120 - 36 = 84 more pages. She wants to read \nhalf of the remaining pages tomorrow, so she \nneeds to read 84 / 2 = 42 pages tomorrow. \nThe answer is 42. ✓\nFigure  1:   Results  of  standard  prompting,  Chain-of-\nThought (CoT) prompting, and our ablation setting with\ninvalid reasoning (§4).   We show one demonstration\nexample and one inference example for arithmetic rea-\nsoning, where the rationale is in color (green:  valid,\nyellow:  invalid).  We find that valid reasoning for the\ndemonstrations matters only a small portion to the per-\nformance of CoT—by providing rationales with invalid\nreasoning, LLMs can achieve over 80-90% of the perfor-\nmance of CoT under various metrics while performing\nlogically sound and pertinent reasoning.\nand which aspects of the demonstrated reasoning\nsteps contribute to its performance. Recent findings\nalso reveal that in-context learning could be very\ndifferent from fine-tuning/training;  for example,\nMin et al. (2022) and Webson and Pavlick (2022)\nshow that providing random labels or misleading in-\nstructions in context only marginally harms model\nperformance  for  certain  tasks.   Inspired  by  this\nwork, we take a closer look at CoT prompting to\nstudy how and why it works.\nWe  design  a  series  of  ablation  experiments\narXiv:2212.10001v2  [cs.CL]  1 Jun 2023",
    "where we deliberately change different aspects of\nthe demonstrated rationales and measure how the\nmodel  performance  varies  accordingly  (§4,  §5).\nOn two representative multi-step reasoning tasks—\narithmetic reasoning and multi-hop factual ques-\ntion answering (QA), we find thatthe validity of\nreasoning matters only a small portion to the\nperformance—by providing rationales with com-\npletely invalid reasoning steps, the LLM can still\nachieve over 80-90% of the performance of CoT un-\nder various metrics while generating coherent lines\nof reasoning towards the answer (§4). Through fur-\nther examinations, we identify and formulate other\naspects of a CoT rationale (§5), and find thatbeing\nrelevant to the query and correctly ordering the\nreasoning steps are the keyfor the effectiveness\nof CoT prompting.\nOverall, our findings suggest that what LLMs\nlearnabout how to reason under CoT prompting\ncould be limited. Rather, they have already gained\na lot of such “reasoning abilities” from pretrain-\ning, and the demonstrations may mainly specify an\noutput space/format that regularizes the model gen-\neration to look step-by-step while being in order\nand relevant to the query. Our work suggests a new\nway of interpreting the evaluation scores in view\nof the prior knowledge LLMs possess, and leads\nto reflections on benchmarking few-shot reasoning\nwhich we discuss in §6.\n2    Background & Study Formulation\nChain-of-Thought (CoT) prompting.Different\nfrom the standard way of prompting language mod-\nels where a set of (query, answer) pairs are given as\ndemonstrations (Brown et al., 2020), CoT prompt-\ning (Wei et al., 2022) additionally includes a ra-\ntionale (Figure 1, colored) for each example, en-\ncouraging the model to verbalize the intermedi-\nate reasoning steps for solving the task.  Such a\ntechnique has been shown to improve the perfor-\nmance of LLMs with sufficient scale on complex\nreasoning, sometimes to a large degree especially\non  arithmetic  reasoning,  multi-hop  question  an-\nswering, and symbolic reasoning.\nComponents of a CoT rationale.We identify two\ndistinct components of a CoT rationale (examples\nin Table 1):\n•\nBridging objects: the key and necessary objects\nthat the model needs to traverse in order to make\na successful final prediction. For arithmetic rea-\nsoning, the bridging objects are defined to be the\nArithmetic ReasoningMulti-hop QA\nQ: Leah had 32 chocolates\nand  her  sister  had  42.   If\nthey   ate   35,   how   many\npieces do they have left in\ntotal?\nQ: Who is the grandchild of\nDambar Shah?\nA: Originally, Leah had 32\nchocolates  and  her  sister\nhad 42. So in total they had\n32 + 42 = 74. After eating\n35, they had 74 - 35 = 39\npieces left in total. The an-\nswer is 39.\nA: Dambar Shah (? - 1645)\nwas  the  father  of  Krishna\nShah.Rudra  Shah  was\nthe child of Krishna Shah\n(?   -  1661).   So  the  final\nanswer  (the  name  of  the\ngrandchild) is: Rudra Shah.\nTable 1: Bridging objects and language templates of a\nChain-of-Thought rationale. Here we illustrate with one\nin-context exemplar for each task we experiment with.\nnumeric part (numbers & equations) of the ratio-\nnale, and for factual QA, the bridging objects are\ndefined to be the subject & object entities.\n•Language templates:  the complementary parts\nof bridging objects, which serve as textual hints\nand relations/predicates that guide the model to\nderive the correct bridging objects along the rea-\nsoning process.\nResearch questions.In Chain-of-Thought prompt-\ning,  correct  bridging  objects  and  language  tem-\nplates are provided as demonstrations to show the\nLLM  how  to  reason.   While  CoT  achieves  im-\npressive performance, we are interested in the fol-\nlowing questions:are ground truth bridging ob-\njects/language templates important? If not, what\nwould be the key aspects that are needed for the\nLLM to reason properly?These questions are the\nmain focus of our study, which will be discussed\nin §4 and §5.\n3    Experimental Setup\n3.1    Datasets & In-context Exemplars\nWe experiment on two representative tasks involv-\ning multi-step reasoning: arithmetic reasoning &\nmulti-hop factual question answering (QA). We se-\nlect benchmarks on which CoT prompting brings\nsignificant  improvements  over  standard  prompt-\ning, as shown in previous work (Wei et al., 2022;\nPress et al., 2022); they are more suitable for our\nstudy, since our goal is to understand how different\naspects  of  the  Chain-of-Thought  rationales  con-\ntribute to the performance of CoT prompting. For\narithmetic reasoning, we experiment on GSM8K\n(Cobbe  et  al.,  2021),  one  of  the  most  challeng-",
    "ing mathematical reasoning benchmarks available\nwhich is also repeatedly adopted by prior work as a\nkey benchmark for arithmetic reasoning; for multi-\nhop factual QA, we experiment on Bamboogle, a\ndataset of compositional questions constructed by\nPress et al. (2022). Due to budget considerations,\nwe uniformly sample 800 out of the 1319 test ex-\namples for GSM8K for evaluation. We evaluate on\nall 125 test samples for Bamboogle.\nWe base our experiments on the original prompt\nexemplars,  i.e.,  the  set  of  (query,  rationale,  an-\nswer) pairs released by Wei et al. (2022) and Press\net al. (2022), with slight editing to make the struc-\nture more consistent and reduce redundancy, which\nmakes our ablations more convenient to conduct.\nThese edits only slightly affect the performance of\nCoT; we show our edited demonstration examples\nand include more details in Appendix A.1.\n3.2    Backbone Language Model\nWe use InstructGPT-175B\n2\n(Ouyang et al., 2022;\nBrown  et  al.,  2020)text-davinci-002as  our\nbackbone   LLM,   which   is   one   of   the   most\nperformant  and  widely-used  LLMs  with  pub-\nlic  APIs  and  has  demonstrated  strong  perfor-\nmance under CoT prompting (Wei et al., 2022).\nWe  report  its  results  and  analyze  them  in  the\nmain  content.In  addition,   we  also  test  on\ntext-davinci-003(a very recent improved ver-\nsion oftext-davinci-002), PaLM (Chowdhery\net al., 2022) and Flan-PaLM (Chung et al., 2022),\nwhere the results and discussion could be found in\nAppendix A.3. All generations are done by greedy\ndecoding (i.e., sampling with zero temperature) as\nin the original CoT work (Wei et al., 2022).\n3.3    Evaluation\nPrior work mainly performs evaluation using the\ncorrectness of the final answer,  which could be\nviewed as anextrinsicway of assessing the pre-\ndicted rationales. However, this may not align well\nwith the actual quality of the rationale in many\ncases, as mentioned in Huang and Chang (2022).\nFor example, a rationale that is correct for all but\nthe last step (and hence derives the wrong final an-\nswer) would still be assigned a zero score, while\na rationale that is wrong/incomplete but reaches\nthe correct final answer would be assigned a full\n2\nWe also tried the original GPT-3 175B without instruction-\nfinetuning in our preliminary experiments, but find that CoT\nprompting does not yield much performance gain than stan-\ndard prompting, echoing Fu et al. (2022).\nscore.   Therefore,  in addition to extrinsic evalu-\nation  (Answer  Accuracyfor  GSM8K,Answer\nF1for Bamboogle), we performintrinsicevalua-\ntion where we measure the Recall/F1 (Inter.\n3\nRe-\ncall/F1) of the bridging objects which need to be\nderived by the LLM (i.e., those that do not appear\nin the query). For GSM8K, since annotations for\nground truth reasoning steps are available, we use\nthe derived numbers in the annotated steps as a\nproxy for bridging objects.\n4\nFor Bamboogle, we\nmanually annotate the bridging objects (intermedi-\nate entities) and measure their recall.  While it is\nstill possible for the model to reach correct bridg-\ning objects with the wrong language templates, we\nmanually verify that this rarely happens; details are\nincluded in Appendix A.2.\n4    How Much Does Valid Reasoning\nMatter?\nIntuitively, one of the most important aspects of a\nChain-of-Thought rationale would be its logically\nvalid and sound reasoning. If we provide rationales\nwith invalid reasoning steps in the demonstrated\nexamples instead, we should expect the LLM to fail\nto reason properly and gain little or even negative\nimprovements compared with standard prompting\n(where no rationale is given), since we are teaching\nthe LLM to reason in the wrong way which could\nbe even worse than not doing so at all. To test this\nintuition, we design an ablation study where we\nconstruct invalid reasoning steps for the demon-\nstrated  rationales,  and  measure  its  influence  on\nmodel behavior.\n4.1    Constructing Invalid Chain of Reasoning\nWe manually write rationales with invalid reason-\ning forallthe in-context demonstration examples.\nSince our focus here is to investigate the impor-\ntance of the validity of reasoning, we only ablate\nthe parts in a CoT rationale which are involved with\nderivations that are logically sound and helpful for\nanswering the query.  More specifically, we keep\nthe premise steps which are copies/paraphrases of\nfacts from the query, and change the subsequent\nsteps such that they do not logically derive the final\nanswer.  Importantly, we arenotadopting an ad-\nversarial/counterfactual perturbation setting where\n3\nAbbreviation for “Intermediate”.\n4\nWe do not use whole equations since we observe that\nthe LLM may express the mathematical equation in different\nways, e.g., “5 plus 3 is 8”, “5 + 3 = 8”.",
    "minimal alterations are applied to make the reason-\ning invalid; instead, we apply rather drastic changes\nwhere we change both the bridging objects and lan-\nguage templates and hence little valid reasoning\nexists to help solve the query. The full prompts in\nour setting are included in Appendix A.4.\nFor example, consider an in-context demonstra-\ntion (see1in Table 4) for arithmetic reasoning.\nHere the query is“Leah had 32 chocolates and her\nsister had 42. If they ate 35, how many pieces do\nthey have left in total?”. For the 1st entailment step\nwhich should sum“32”and“42”to get the total\namount“32 + 42 = 74”as in CoT, we instead write\n“So her sister had 42 - 32 = 10 chocolates more than\nLeah has.”which has both the wrong bridging\nobject and language template, and is completely\nunhelpful for solving the problem. The subsequent\nsteps are written based on the previous steps, and in\nthe end, answer the question whereas the rationale\ndoes not in any way lead to the answer logically.\nWhile the step itself still describes something that\ncould be entailed in the example we just gave, this\nis not the case generally and most of the steps we\nwrite are neither helpful nor entailments from ear-\nlier steps. For example, the next step“After eating\n35, since 10 + 35 = 45, they had 45 - 6 = 39 pieces\nleft in total”makes use of unwarranted information\n(“6”) and has no valid entailment anywhere. We\nillustrate our construction using another example\nfor factual QA, where the question is“Who is the\ngrandchild of Dambar Shah?”. Here, we write a\nrationale that finds the kingdom of“Dambar Shah”\nand then a child of the person who established the\nkingdom, which does not lead to“the grandchild\nof Dambar Shah”.\n4.2    Results & Analysis\nQuantitative  results.Table  2  summarizes  the\nquantitative results fortext-davinci-002.   We\ninclude   additional   results   and   discussion   for\ntext-davinci-003, PaLM and Flan-PaLM in Ap-\npendix A.3. LLMs can achieve surprisingly high\nperformance when provided with invalid reasoning\nsteps for the demonstrations (\n1\n). In particular, un-\nderInter. Recall/Inter.F1, i.e., intrinsic evaluation,\nwhich is arguably a more faithful measurement of\nthe rationale quality (§3.3), all LLMs we tested can\nretain over 90% of the performance achieved under\nCoT prompting.\nFor GSM8K where there are large variations in\nthe difficulty levels (here, we use the number of\nreasoning steps required to solve a problem as its\ndifficulty level) of the problem instances, we addi-\ntionally examine the model performance separately\nfor each difficulty level. The results are shown in\nFigure 2.  The performance drop is also uniform\nacross samples with different levels of difficulty.\nAt the instance level, after omitting samples where\nboth settings get the correct/wrong answer, there is\na significant portion for the remaining ones (62/196\nfor GSM8K, 6/20 for Bamboogle) where CoT gets\nthe wrong answer and the invalid reasoning setting\ngets the correct answer.  This further strengthens\nthe finding that there is no strong connection be-\ntween the reasoning validity of the demonstrations\nand the quality of the model predictions.\nFigure 2: Model performance using CoT and demonstra-\ntions with invalid reasoning for examples with different\nreasoning depths on GSM8K. The number of samples\nfor each reasoning depth is shown below (led by“#”).\nThe performance drop is consistent across different lev-\nels of difficulty.\nQualitative analysis.By checking the generated\nrationales for the invalid reasoning setting, we find\nthat overall they look indistinguishable from the\nrationales generated by CoT prompting. In almost\nall cases where the predicted final answer is correct,\nthe rationales do reach the answer with valid and\nsound reasoning steps (as in CoT), drastically dif-\nferent from those in the given demonstrations; for\ncases where the final answer is wrong, the errors\nthe LLM makes are also in the same types with the\nerrors made under CoT prompting. To compare the\ndistribution of errors between CoT and the invalid\nreasoning setting, we examine 20 samples from\nGSM8K where CoT gets the correct final answer\nand the invalid reasoning setting gets the wrong\nanswer, and another 20 examples for the opposite\ncase. We use the same error categorizations as in",
    "GSM8KBamboogle\nInter. RecallInter. F1Answer Acc.Inter. RecallAnswer F1\nSTD (Standard prompting)N/AN/A15.4N/A20.6\nCoT (Chain-of-Thought prompting)43.948.348.545.245.2\n1Invalid Reasoning39.843.939.544.439.4\n2Nocoherencefor bridging objects35.339.235.840.837.4\n3\nNo relevancefor bridging objects21.426.227.539.634.0\n4\nNocoherencefor language templates24.128.325.835.232.1\n5No relevancefor language templates29.534.032.840.429.4\n6Nocoherence25.229.423.139.633.8\n7\nNo relevance9.611.911.036.823.9\nTable 2: Intrinsic and extrinsic evaluation results under InstructGPT (text-davinci-002) for all settings in our\nexperiments. Results fortext-davinci-003, PaLM and Flan-PaLM could be found in Appendix A.3.\nError TypesCoTcorrect\n&IRwrong\nCoTwrong\n&IRcorrect\nCalculation20%20%\nOne step missing35%25%\nSemantic understanding45%55%\nTable  3:  Distribution  of  error  types  of  20  examples\nfrom GSM8K where Chain-of-Thought (CoT) prompt-\ning reaches the correct answer and the Invalid Reasoning\nsetting (IR) reaches a wrong answer, and 20 examples\nfor the opposite case.\nWei et al. (2022) for the qualitative analysis, and\nsummarize the results in Table 3. The distributions\nof errors in both cases are highly similar.\nSummary.Combining the quantitative and quali-\ntative results, we can see that there is a low chance\nfor any systematic difference between CoT and the\ninvalid reasoning setting to exist.  The LLM still\ntries and manages to generate logically sound and\npertinent reasoning decently, and ablating the valid-\nity of reasoning for the demonstrations only brings\na small performance degradation. This opens the\nquestion:If valid reasoning is not required, what\nare the key aspects that determine the effectiveness\nof CoT prompting?\n5    What are the Key Aspects of\nChain-of-Thoughts?\nRe-examining the rationales in our ablation setting\nin §4, we can find that even though the reasoning\nis invalid, they have the following properties:\n•The  rationales  still  use  information  from  the\nquery;  more  specifically,  they  still  start  from\nbridging objects mentioned in the query, and the\nlanguage templates are related to the query. Re-\ncall our running example for arithmetic reason-\ning (Table 4),  even though the reasoning here\nis wrong, the numbers“32”and“42”are kept\nfrom the query, and the language templates are\nstill about“Leah”,“Leah’s sister”and“Choco-\nlates”, and try to seek the answer to the query.\nTherefore,  the rationale is still relevant to the\nquery being asked.\n•\nEach step of a rationale still follows the previous\nsteps. Using again the same example, the bridg-\ning object (equation in this case)“42 - 32 = 10”\nin the first entailment step uses numbers from\nprevious steps; likewise, the language template\n“So her sister had _ chocolates more than Leah\nhas”is something that follows after the earlier\nsteps. Hence, overall, the rationale still appears\nto be coherent.\nWe formulate two notions that capture these two\naspects of a rationale in what follows.\nRelevance.A component of the rationale has rel-\nevance if it is based on the corresponding com-\nponent from the query. For bridging objects, this\ncould be formally defined as using the exact same\nobjects mentioned in the query (numbers for arith-\nmetic reasoning and entities for factual QA); for\nlanguage templates, they have relevance if they are\nstill about the same set of entities/relations as the\nquery, and allude to the question being asked. For\nexample, a template about“Patricia”and“hair”\nwould not have relevance to a query about“Leah”\nand“Chocolates”, and similarly, a template that\nattempts to find the“brother-in-law”of the topic\nentity does not have relevance to a query which\nseeks the“grandchild”(Table 4).",
    "Coherence.A component of the rationale has co-\nherence if it is in the correct order, i.e., later steps\ncould not be pre-conditions for earlier steps and\nreversely, earlier steps could not be based on later\nsteps. For example, a rationale where“32 + 42 =\n74”appears before the introduction of“32”or“42”\nwould not have coherence on bridging objects, and\nsimilarly for language templates.\nIn what follows, we design a set of ablation set-\ntings to examine the impact of these two aspects\nfor different components of a CoT-like rationale.\n5.1    Ablation Settings\nIn order not to introduce mixed effects which could\nmake the results not well-controlled, we base the\nablation settings on top of the CoT prompts instead\nof the setting in §4.\nGiven the two components (bridging objects and\nlanguage templates) and the two aspects (relevance\nand coherence) of the rationale, there are naturally\nfour ablation settings where each could examine\none aspect of a certain component. We also experi-\nment with two other settings:no relevancewhere\nneither bridging objects nor language templates\nhave relevance, andno coherencewhich is defined\nanalogously (6,\n7\nin Table 4).\nDestroying relevance.We perform random sub-\nstitutions to ablate the relevance of a certain com-\nponent. For ablating the relevance of bridging ob-\njects, we randomly sample alternatives (numbers\nfor GSM8K, entities for Bamboogle) for those from\nthe query, and change the bridging objects in the\nsubsequent steps correspondingly to maintain the\ncoherence of the rationale. Using our running ex-\nample, we randomly replace the bridging objects\nfrom the query:“32”→“19”,“42”→“31”and\n“35”→“29”, then change the bridging object from\nthe first entailment step from“32 + 42 = 74”to\n“19 + 31 = 50”, and so on so forth. To ablate the\nrelevance of language templates, for GSM8K, we\nrandomly sample an annotated rationale from the\ntraining set, and use its template in place of the\noriginal template.  For Bamboogle, we manually\nreplace the template with an alternative which is\nirrelevant to the query.\nDestroying coherence.Ablating the coherence is\nrather straightforward, where we randomly shuffle\nthe components and permute their orderings.\n5.2    Results & Analysis\nThe  results  could  be  found  in  Table  2,  and  we\ninclude additional results fortext-davinci-003,\nPaLM and Flan-PaLM in Appendix A.3. We sum-\nmarize the main findings in what follows.\nRelevance and coherence are key for the per-\nformance of CoT prompting.It can be seen that\nmost of the settings for this section (2-\n7\n) have\nrather large performance drops from CoT, where\nthe low-performing ones approach or even under-\nperform standard prompting.  This suggests that\noverall, relevance and coherence are key for the\nperformance of CoT.\nKeeping relevance is crucial.The no relevance\nsetting7where both components of the rationale\nhave no relevance achieves significantly poorer per-\nformance than other ablation settings,  and even\nunderperforms standard prompting (STD) where\nno rationale is given on GSM8K. To see why such\nlow performance happens, we manually examine\nthe generated rationales under this setting for 20\nexamples on GSM8K. We find that the LLM is in-\ndeed generating irrelevant rationales (both bridging\nobjects and language templates) for 15 out of 20 ex-\namples. Many of the rationales have recurring top-\nics (e.g., “cats and dogs”, “passengers and buses”)\nwhich we hypothesize are frequent patterns in the\nportion relevant to mathematics in the pretraining\ncorpora. Overall, this suggests that a certain level\nof relevance is crucial for the LLM to stick to the\nquery being asked.\nRelevance  matters  more  than  coherence  for\nbridging objects.Providing incoherent bridging\nobjects (2) achieves better performance than pro-\nviding irrelevant bridging objects (\n3\n), especially\non the more challenging GSM8K dataset (39.2v.s.\n26.2Inter. F1). which indicates that it is important\nfor the bridging objects to be relevant, but not as\nimportant to have them in the right order to guide\nthe LLM along the reasoning process. We quanti-\ntatively measure the coverage of bridging objects\nfrom the query for the generated rationales, and\nfind that the settings with no relevance for bridging\nobjects (3,7) do have significantly lower cover-\nage (below 60%) than other settings (around 80%).\nCoherence of language templates is important.\nDifferent from the coherence of bridging objects\n2, the coherence of language templates4matters\na lot to the performance of CoT prompting.  By\nexamining the predicted rationales, we find that the\nLLM is indeed generating rationales with incoher-\nent language templates (14 out of 20 examples),\nwhich negatively affects reasoning.",
    "Prompt SettingExample Query (Arithmetic Reasoning)\nLeah had 32 chocolates and her sister had 42. If they\nate 35, how many pieces do they have left in total?\nExample Query (Factual QA)\nWho is the grandchild of Dambar Shah?\nSTD (Standard prompting)39So the final answer is: Rudra Shah.\nCoT (Chain-of-Thought)Originally, Leah had 32 chocolates and her sister had\n42. So in total they had 32 + 42 = 74. After eating 35,\nthey had 74 - 35 = 39 pieces left in total. The answer\nis 39.\nDambar Shah (?  - 1645) was the father of Krishna\nShah. Rudra Shah was the child of Krishna Shah (? -\n1661). So the final answer (the name of the grandchild)\nis: Rudra Shah.\n1\nInvalid ReasoningOriginally, Leah had 32 chocolates and her sister had\n42.  So her sister had 42 - 32 = 10 chocolates more\nthan Leah has.  After eating 35, since 10 + 35 = 45,\nthey had 45 - 6 = 39 pieces left in total. The answer is\n39.\nDambar Shah (? - 1645) was the king of the Gorkha\nKingdom. The Gorkha Kingdom was established by\nPrince Dravya Shah. Dravya Shah has a child named\nRudra Shah.  So the final answer (the name of the\ngrandchild) is: Rudra Shah.\n2Nocoherencefor bridg-\ning objects\nOriginally, Leah had 32 + 42 = 74 chocolates and her\nsister had 32. So in total they had 74 - 35 = 39. After\neating 35, they had 42 pieces left in total. The answer\nis 39.\nKrishna Shah was the father of Rudra Shah. Dambar\nShah (?  - 1645) was the child of Krishna Shah (?  -\n1661). So the final answer (the name of the grandchild)\nis: Rudra Shah.\n3Norelevancefor  bridg-\ning objects\nOriginally, Leah had 19 chocolates and her sister had\n31. So in total they had 19 + 31 = 50. After eating 29,\nthey had 50 - 29 = 21 pieces left in total. The answer\nis 21.\nMetis Amando was the father of David Amando. Ran-\ndall Amando was the child of David Amando. So the\nfinal answer (the name of the grandchild) is: Randall\nAmando.\n4Nocoherencefor  lan-\nguage templates\nAfter eating 32, they had 42 pieces left in total. Origi-\nnally, Leah had 32 + 42 = 74 chocolates and her sister\nhad 35. So in total they had 74 - 35 = 39. The answer\nis 39.\nDambar Shah (? - 1645) was the child of Krishna Shah.\nKrishna Shah (? - 1661) was the father of Rudra Shah.\nSo the final answer (the name of the grandchild) is:\nRudra Shah.\n5Norelevancefor  lan-\nguage templates\nPatricia needs to donate 32 inches, and wants her hair\nto be 42 inches long after the donation. Her hair is 35\ninches long currently. Her hair needs to be 32 + 42 =\n74 inches long when she cuts it. So she needs to grow\n74 - 35 = 39 more inches. The answer is 39.\nThe husband of Dambar Shah (?  - 1645) is Krishna\nShah.  Krishna Shah (?  - 1661) has a brother called\nRudra Shah.  So the final answer (the name of the\nbrother-in-law) is: Rudra Shah.\n6NocoherenceAfter eating 32 + 42 = 74, they had 32 pieces left in\ntotal.  Originally, Leah had 74 - 35 = 39 chocolates\nand her sister had 35.  So in total they had 42.  The\nanswer is 39.\nKrishna Shah was the child of Rudra Shah. Dambar\nShah (?  - 1645) was the father of Krishna Shah (?  -\n1661). So the final answer (the name of the grandchild)\nis: Rudra Shah.\n7No relevancePatricia needs to donate 19 inches, and wants her hair\nto be 31 inches long after the donation. Her hair is 29\ninches long currently. Her hair needs to be 19 + 31 =\n50 inc long when she cuts it. So she needs to grow 50\n- 29 = 21 more inches. The answer is 21.\nThe  husband  of  Metis  Amando  is  David  Amando.\nDavid Amando has a brother called Randall Amando.\nSo the final answer (the name of the brother-in-law)\nis: Randall Amando.\nTable 4: Examples for all settings in our experiments.\n6    Discussion\nThe results from §4 and §5 open up new questions\nregarding learning to reason in context for LLMs,\nwhich we discuss next.\nDoLLMs learn to reason from CoT demonstra-\ntions?Given the surprisingly high performance ob-\ntained by ablating the validity of reasoning for the\nin-context rationales, it can be concluded that what\nthe LLM learns from the demonstrations about how\nto reason properly is limited—rather, the LLM has\nalready gained a lot of such complex reasoning\nability from pretraining (at least for tasks we exper-\niment on), and the provided reasoning steps serve\nmore as the role of an output format/space, that reg-\nularizes the LLM to generate rationales that look\nstep-by-step while being coherent and relevant to\nthe query. Moreover, results obtained from recent\nstronger models includingtext-davinci-003and\nFlan-PaLM (see Appendix A.3) suggest that LLMs\nsuffer further less from the ablations when they\nhave more prior knowledge about the task. In par-\nticular,  for Flan-PaLM which is directly trained\non both arithmetic reasoning and factual QA in\nCoT fashion and hence has immense knowledge on\nthese tasks (Chung et al., 2022), it could be seen\nthat none of the ablations has significant impacts\non its performance. On the positive side, this indi-\ncates that LLMs can effectively utilize their prior\nknowledge to solve new problems. However, from\nanother perspective, if we view the invalid reason-\ning setting as ataskwhere the goal is to generate\ninvalid reasoning steps for the query, then the LLM\nhas basically failed to capture the task as it still\ntries to predict valid reasoning steps. This leads to\nthe concern that LLMs may over-rely on their prior\nknowledge and ignore important information in the\ncontext that are presumably rare in the pretrain-\ning distribution, including those that are crucial for",
    "specifying the task semantics (Jang et al., 2023).\nCanLLMs learn to reason in-context?We note\nthat what we find does not in any way diminish the\npotentialof learning to reason in context for LLMs;\nrecent work has also shown evidence that learning\nin context is possible and could be powerful (Garg\net al., 2022; Akyürek et al., 2023).   Rather,  our\nfindings show that the existing successes of CoT\nare not sufficient for establishing that LLMs are\ngoodfew-shot learnersof reasoning; instead, the\npretraining corpora have already forged them to be\ngood reasoners on the tasks being evaluated, and\nthe main role that the demonstrations play is to\nelicit such reasoning skills.\nReflections on benchmarking few-shot reason-\ning.An important topic on benchmarking in the era\nof large pre-trained language models is to quantify\nthe level of prior knowledge the LLM has gained\nabout the end task being evaluated, which is crucial\nfor assessing how well can the model truly extrapo-\nlate from pretraining and acquire new skills (Chol-\nlet, 2019). One direct way is to look into the pre-\ntraining corpora when it is accessible, e.g., Razeghi\net al. (2022) investigates the correlation between\nthe model performance and the frequency of terms\nfrom the test instances in the pretraining data. How-\never, the pretraining corpora are not always accessi-\nble, and low-level statistics are usually not adequate\nwhen the topics of interest are abstract and high-\nlevel skills such as reasoning. Along this direction,\nour work could be regarded as a way to approxi-\nmately quantify the prior knowledge that the LLM\npossesses on multi-step reasoning.  Our findings\nindicate that evaluations on alternative benchmarks\nwhere LLMs have less prior knowledge are needed\nto more faithfully assess the LLMs’ abilities on\nlearning to reason from few-shot demonstrations.\n7    Related Work\nThere have been several subsequent work of Chain-\nof-Thought prompting since its introduction. Wang\net al. (2023) proposes to sample a diverse set of\nreasoning paths instead of performing greedy de-\ncoding, and marginalize over the sampled paths\nto select the most consistent answer.  Zhang et al.\n(2023) proposes a method for automatically con-\nstructing the in-context exemplars for CoT. Chen\net al. (2022) explores program-based CoT which\ncan better disentangle computation from reason-\ning.   In this paper,  we are primarily focused on\nunderstanding the effectiveness of the original CoT\nprompting method where we use the same exper-\nimental settings (e.g., greedy decoding) and base\nour experiments on the same few-shot exemplars\nused. We believe our findings could also apply to\nsome of the subsequent variants of CoT prompting.\nA  few  recent  work  focuses  on  understand-\ning/analyzing CoT prompting.  Madaan and Yaz-\ndanbakhsh (2022) investigates the importance of\ndifferent components of the demonstrated CoT ra-\ntionales by changing them to becounterfactual.\nThey only experiment with limited ways of chang-\ning the rationales to bewrongincluding using in-\ncorrect calculations (e.g.,“5 + 4 = 7”) or entities.\nFor most of their settings, even though the ratio-\nnales are made counterfactual, they are stillcor-\nrectsince the query is changed accordingly (see,\ne.g., Table 48 of their paper).  Concurrent to our\nwork, Ye et al. (2022) also explores how the model\nperformance could be affected by corrupting the\nCoT rationales.  They experiment with using in-\ncorrect  calculations  anddropping(parts  of)  the\nbridging objects/language templates, which are dif-\nferent from our ablation designs. Saparov and He\n(2023) investigates systematically evaluating CoT\nby creating a synthetic QA dataset based on first-\norder logic, which allows for parsing the generated\nrationales into symbolic proofs for formal analysis.\nOverall, to our knowledge, we are the first to show\nthat it is possible to have CoT rationales that are\nwrong and drastically deviate from the gold ones\nwhile still maintaining high model performance.\nIn general in-context learning (ICL), Min et al.\n(2022) shows that for a wide range of tasks in nat-\nural language understanding with categorical la-\nbel space (classification and multi-choice), ground\ntruth input-label mappings matter very little for\nend-task performance, and other aspects such as\nthe label space, overall format and the distribution\nof text are the key.  Building on this work, Yoo\net al. (2022) finds that the correct input-label corre-\nspondence could have varying impacts based on the\ntask and experimental configurations, and Wei et al.\n(2023) finds that models with larger scale can over-\nride semantic priors and learn input-label mapping\nin context. Webson and Pavlick (2022) finds that\nfor instruction models, the performance on natural\nlanguage inference tasks has small degradations\nunder irrelevant or misleading instructions.  Xie\net al. (2022) provides theoretical analysis of ICL\nby formulating it as Bayesian inference. Our work\ncould be viewed as an attempt to empirically under-",
    "stand ICL in sequence generation tasks requiring\nmulti-step reasoning.\n8    Conclusion\nIn this paper, we aim to better understand Chain-of-\nThought prompting through a series of ablation ex-\nperiments that unveil the impact of different aspects\nof a CoT rationale. We find that 1) the validity of\nreasoning in the prompting examples matters only\na small portion to the performance; 2) relevance to\nthe input query and following the order along the\nreasoning steps are the key to the effectiveness of\nCoT prompting. Overall, our findings deepen the\nunderstanding of CoT prompting, and open up new\nquestions/reflections regarding LLMs’ capability\nof learning to reason in context.\nLimitations\nExperiments on other types of reasoning tasks.\nIn addition to the two representative reasoning tasks\n(arithmetic reasoning and multi-hop question an-\nswering) that we experiment on, there are also other\ntasks where CoT prompting brings significant im-\nprovements over standard prompting shown by pre-\nvious work, many of which are symbolic reasoning\ntasks such as Last letter concatenation, Coin flip\nfrom Wei et al. (2022) and Temporal Sequences,\nTracking Shuffled Objects from BIG-Bench (Sri-\nvastava et al., 2022; Suzgun et al., 2022). However,\nmost (if not all) tasks there are highlytemplate-\nbasedand  hence  the  reasoning  steps  have  little\nvariations, both within each example and across\ndifferent examples. This makes it difficult for us to\nconduct our ablation studies on these tasks. Take\nthe example of Last letter concatenation,  a task\nabout concatenating the last letters of a given se-\nquence of words (e.g.,“Amy Brown”→“yn”).\nHere, every step in the rationale except the last is\nin the form“The last letter of X is Y”whereXis\nsome word in the given sequence andYis the last\nletter ofX. Hence, the language templates are the\nsame and there is no sense of order among the steps\n(the order is completely characterized by the given\nsequence instead), and our ablation settings will\nnot apply well. Extending our ablation designs to\nthese “reduced” cases is one of the items we want\nto explore in the future.\nA more systematic treatment of “invalid reason-\ning”.We manually write rationales with invalid\nreasoning for the experiments in §4 since automat-\nically synthesizing such rationales turns out to be\nchallenging, mostly due to the informal nature of\nthe tasks we experiment on (relatedly, the original\nCoT rationales are also human-written). We intend\nto give a more systematic treatment of the invalid\nreasoning setting in the future, e.g., following the\ncategorizations of informal logical fallacies (Copi\net al., 2016).\nImprovements on intrinsic evaluation.Our in-\ntrinsic  evaluation  of  the  generated  rationales  is\nbased on the correctness of bridging objects, which,\neven though is a good indicator of the quality of\nlanguage templates (Appendix A.2) in our experi-\nments, may not be a good metric in general cases. It\nalso relies on ground truth bridging objects, which\nare usually not available and costly to annotate. To-\nward this end, one direction we want to explore\nfurther is to develop ways to conduct more com-\nprehensive and reference-free intrinsic evaluations.\nRecent papers such as Golovneva et al. (2023) have\nalso done promising work along this line.\nAcknowledgements\nThe authors would like to thank the anonymous re-\nviewers and colleagues from the OSU NLP group\nfor their thoughtful comments. This research was\nsupported in part by Google Faculty Award, Google\nResearch Scholar Award, NSF IIS 1815674, NSF\nCAREER 1942980, NSF OAC-2112606, and Ohio\nSupercomputer Center (Center, 1987). The views\nand conclusions contained herein are those of the\nauthors and should not be interpreted as represent-\ning the official policies, either expressed or implied,\nof the U.S. government. The U.S. Government is\nauthorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copy-\nright notice herein.\nReferences\nEkin  Akyürek,   Dale  Schuurmans,   Jacob  Andreas,\nTengyu Ma,  and Denny Zhou. 2023.   What learn-\ning algorithm is in-context learning? investigations\nwith linear models.  InThe Eleventh International\nConference on Learning Representations.\nTom  Brown,  Benjamin  Mann,  Nick  Ryder,  Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020.  Language models are few-shot\nlearners.Advances in neural information processing\nsystems, 33:1877–1901.\nOhio Supercomputer Center. 1987.  Ohio supercomputer\ncenter.",
    "Wenhu   Chen,   Xueguang   Ma,   Xinyi   Wang,   and\nWilliam  W  Cohen.  2022.Program  of  thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks.arXiv preprint\narXiv:2211.12588.\nFrançois Chollet. 2019. On the measure of intelligence.\narXiv preprint arXiv:1911.01547.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten  Bosma,  Gaurav  Mishra,  Adam  Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann,  et al. 2022.   Palm:  Scaling\nlanguage modeling with pathways.arXiv preprint\narXiv:2204.02311.\nHyung  Won  Chung,  Le  Hou,  Shayne  Longpre,  Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKarl  Cobbe,   Vineet  Kosaraju,   Mohammad  Bavar-\nian,  Jacob  Hilton,  Reiichiro  Nakano,  Christopher\nHesse,  and  John  Schulman.  2021.   Training  veri-\nfiers to solve math word problems.arXiv preprint\narXiv:2110.14168.\nIrving Copi, Carl Cohen, and Victor Rodych. 2016.In-\ntroduction to logic. Routledge.\nYao Fu, Hao Peng, and Tushar Khot. 2022. How does\ngpt obtain its ability?  tracing emergent abilities of\nlanguage models to their sources.Yao Fu’s Notion.\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gre-\ngory Valiant. 2022. What can transformers learn in-\ncontext? a case study of simple function classes. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 30583–30598. Curran Associates,\nInc.\nOlga Golovneva, Moya Peng Chen, Spencer Poff, Mar-\ntin  Corredor,  Luke  Zettlemoyer,  Maryam  Fazel-\nZarandi, and Asli Celikyilmaz. 2023.  ROSCOE: A\nsuite of metrics for scoring step-by-step reasoning.  In\nThe Eleventh International Conference on Learning\nRepresentations.\nJie Huang and Kevin Chen-Chuan Chang. 2022.  To-\nwards reasoning in large language models: A survey.\narXiv preprint arXiv:2212.10403.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2023. Can\nlarge  language  models  truly  understand  prompts?\na  case  study  with  negated  prompts.    InTransfer\nLearning for Natural Language Processing Work-\nshop, pages 52–62. PMLR.\nAman Madaan and Amir Yazdanbakhsh. 2022.  Text\nand patterns: For effective chain of thought, it takes\ntwo to tango.arXiv preprint arXiv:2209.07686.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?  InProceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.Training  language  models  to  follow  in-\nstructions  with  human  feedback.arXiv preprint\narXiv:2203.02155.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022.  Measuring\nand narrowing the compositionality gap in language\nmodels.arXiv preprint arXiv:2210.03350.\nYasaman Razeghi, Robert L Logan IV, Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies  on  few-shot  numerical  reasoning.   In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022, pages 840–854, Abu Dhabi,\nUnited  Arab  Emirates.  Association  for  Computa-\ntional Linguistics.\nAbulhair Saparov and He He. 2023. Language models\nare greedy reasoners: A systematic formal analysis\nof chain-of-thought.  InThe Eleventh International\nConference on Learning Representations.\nAarohi  Srivastava,  Abhinav  Rastogi,  Abhishek  Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam  R  Brown,  Adam  Santoro,  Aditya  Gupta,\nAdrià  Garriga-Alonso,  et  al.  2022.Beyond  the\nimitation game:  Quantifying and extrapolating the\ncapabilities  of  language  models.arXiv preprint\narXiv:2206.04615.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian  Gehrmann,  Yi  Tay,  Hyung  Won  Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether  chain-of-thought  can  solve  them.arXiv\npreprint arXiv:2210.09261.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023.  Self-consistency improves\nchain of thought reasoning in language models.  In\nThe Eleventh International Conference on Learning\nRepresentations.\nAlbert Webson and Ellie Pavlick. 2022.   Do prompt-\nbased models really understand the meaning of their\nprompts?  InProceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022.  Chain-of-thought prompt-\ning elicits reasoning in large language models.  In",
    "Advances in Neural Information Processing Systems,\nvolume 35, pages 24824–24837. Curran Associates,\nInc.\nJerry  Wei,  Jason  Wei,  Yi  Tay,  Dustin  Tran,  Albert\nWebson,  Yifeng  Lu,  Xinyun  Chen,  Hanxiao  Liu,\nDa Huang, Denny Zhou, et al. 2023. Larger language\nmodels  do  in-context  learning  differently.arXiv\npreprint arXiv:2303.03846.\nSang Michael Xie,  Aditi Raghunathan,  Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference.  InInterna-\ntional Conference on Learning Representations.\nXi  Ye,  Srinivasan  Iyer,  Asli  Celikyilmaz,  Ves  Stoy-\nanov, Greg Durrett, and Ramakanth Pasunuru. 2022.\nComplementary explanations for effective in-context\nlearning.arXiv preprint arXiv:2211.13892.\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-\nsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\nand Taeuk Kim. 2022. Ground-truth labels matter: A\ndeeper look into input-label demonstrations. InPro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2422–\n2437, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nZhuosheng  Zhang,  Aston  Zhang,  Mu  Li,  and  Alex\nSmola. 2023. Automatic chain of thought prompting\nin large language models.  InThe Eleventh Interna-\ntional Conference on Learning Representations.",
    "A    Appendix\nA.1    Chain of Thought Exemplars\nWe base our experiments on the original prompt\nexemplars released by Wei et al. (2022); Press et al.\n(2022) with slight editing to make the structure\nmore  consistent  and  reduce  redundancy,  which\nmakes our ablations more convenient to conduct.\nThe edited CoT prompts for arithmetic reasoning\nand multi-hop QA could be found in Table 9 and Ta-\nble 10 respectively. We mainly perform the follow-\ning edits: 1) shift premise steps (copy/paraphrase\nof facts from the query) to the beginning steps of\nthe rationale; 2) add/expand the language templates\nfor steps with no/over-concise language templates;\n3) remove unnecessary steps/information that are\nunhelpful for answering the query.\nOverall, these edits only slightly affect the perfor-\nmance of CoT. A comparison of the performance\nis shown in Table 5.\nA.2    More Details on Intrinsic Evaluation\nWe use Recall/F1 of the bridging objects as the\nmetrics for intrinsic evaluation of the generated ra-\ntionales. While the metrics don’t take into account\nthe quality of the language templates, we exam-\nine the predicted rationales for 20 random exam-\nples under each setting we tested except standard\nprompting (which does not generate any rationale),\nand find that for all the examples, whenever the\nLLM reaches a correct bridging object, the corre-\nsponding language template within the step is also\ncorrect. This suggests that overall, the correctness\nof bridging objects is a very good indicator of the\nquality of the reasoning steps.\nA.3    Additional Results & Discussion\nTable 6 includes results fortext-davinci-003,\ntext-davinci-002’s  very  recent  improved  ver-\nsion.\nComparingwiththeresultsfrom\ntext-davinci-002(Table   2),it   could   be\nseen thattext-davinci-003brings large perfor-\nmance improvements, especially under the ablation\nsettings. In particular, providing invalid reasoning\nfor  the  rationales  (1)  overall  only  marginally\nharms  the  performance,  and  even  outperforms\nCoT on GSM8K under intrinsic evaluation.  This\nsuggests thattext-davinci-003is equipped with\neven stronger multi-step “reasoning” abilities on\nthe evaluated tasks through pre-training, and learns\nlittle about how to reason from the demonstrations.\nFor the remaining settings where we ablate the\nrelevance/coherence (2-7), the same trend can\nbe observed on the challenging GSM8K dataset,\ne.g., the model still suffers a lot when providing\nrationales  that  are  irrelevant  or  have  incoherent\nlanguage templates. For the relatively easier Bam-\nboogle dataset, the high model capacity indicated\nby its impressive performance has basically erased\nsignificant  impacts  from  the  ablations,  with  the\nonly standing observation that the model still needs\nthe rationales to be relevant to maintain its perfor-\nmance.\nOverall,  from  the  performance  achieved  by\ntext-davinci-002andtext-davinci-003, we\ncan observe a general trend where LLMs suffer\nless from the ablations when they have more prior\nknowledge about the task. To further explore this,\nwe test on Flan-PaLM (Chung et al., 2022), the\ninstruction-tuned  version  of  PaLM  (Chowdhery\net al., 2022) that is directly trained on both arith-\nmetic reasoning and factual QA in CoT fashion\nduring instruction tuning, and hence has immense\nknowledge on these tasks. The results are shown in\nTable 7. It could be seen that none of the ablations\nhas significant impacts on the model performance,\nwhich further strengthens this pattern. On the posi-\ntive side, this indicates that LLMs can effectively\nutilize their prior knowledge to solve new prob-\nlems; however, this also leads to the concern that\nLLMs may over-rely on their prior knowledge and\nignore important information in the context, includ-\ning those that are crucial for specifying the task\nsemantics (Jang et al., 2023).\nWe also test PaLM, which is a non-instruction-\nfinetuned LLM that exhibits strong CoT reasoning\nability. The results are included in Table 8. Over-\nall,  similar  observations  could  be  found,  which\nsuggests  that  our  findings  are  not  exclusive  to\ninstruction-tuned models. There are some inconsis-\ntencies between the performance from PaLM and\nInstructGPT on Bamboogle, where the importance\nof coherence and relevance for bridging objects is\nflipped. This could be the consequence of instruc-\ntion tuning, and differences in pretraining corpora\nand model scales.\nA.4    Full List of Prompts\nFull prompts for all settings in our experiments are\nincluded in Table 9-24.",
    "GSM8KBamboogle\nInter. RecallInter. F1Answer Acc.Inter. RecallAnswer F1\nChain-of-Thought (Original)44.548.748.144.843.1\nChain-of-Thought (After Editing)43.948.348.545.245.2\nTable 5: Performance comparison (undertext-davinci-002) of the Chain-of-Thought exemplars before/after our\nediting.\nGSM8KBamboogle\nInter. RecallInter. F1Answer Acc.Inter. RecallAnswer F1\nSTD (Standard prompting)N/AN/A15.2N/A25.1\nCoT (Chain-of-Thought prompting)48.453.154.561.659.5\n1Invalid Reasoning50.253.551.560.856.4\n2\nNocoherencefor bridging objects46.551.550.459.255.2\n3No relevancefor bridging objects32.538.347.260.456.9\n4Nocoherencefor language templates37.843.341.957.251.4\n5No relevancefor language templates44.649.951.862.459.3\n6\nNocoherence34.539.431.057.655.2\n7No relevance15.517.816.250.049.0\nTable 6: Intrinsic and extrinsic evaluation results undertext-davinci-003for all settings. Discussions are included\nin Appendix A.3.\nGSM8KBamboogle\nInter. RecallInter. F1Answer Acc.Inter. RecallAnswer F1\nSTD (Standard prompting)N/AN/A21.8N/A36.5\nCoT (Chain-of-Thought prompting)72.273.063.857.656.9\n1Invalid Reasoning71.872.664.455.652.8\n2Nocoherencefor bridging objects72.172.965.851.649.3\n3\nNo relevancefor bridging objects71.171.964.654.052.8\n4Nocoherencefor language templates71.672.263.954.052.0\n5No relevancefor language templates71.972.764.955.253.5\n6Nocoherence71.772.564.254.454.0\n7No relevance70.771.664.550.051.9\nTable 7:  Intrinsic and extrinsic evaluation results under Flan-PaLM (Chung et al., 2022), the instruction-tuned\nversion of PaLM for all settings. Discussions are included in Appendix A.3.\nGSM8KBamboogle\nInter. RecallInter. F1Answer Acc.Inter. RecallAnswer F1\nSTD (Standard prompting)N/AN/A15.0N/A31.0\nCoT (Chain-of-Thought prompting)36.640.637.054.054.8\n1Invalid Reasoning33.936.931.850.446.1\n2Nocoherencefor bridging objects30.335.033.533.625.7\n3\nNo relevancefor bridging objects15.520.121.247.247.7\n4\nNocoherencefor language templates23.127.321.940.435.5\n5\nNo relevancefor language templates19.522.920.438.430.6\n6Nocoherence23.928.324.139.633.6\n7\nNo relevance12.116.416.428.414.3\nTable 8: Intrinsic and extrinsic evaluation results under PaLM. Discussions are included in Appendix A.3.",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:There are 15 trees originally. Then there were 21 trees after the Grove workers planted some more. So there\nmust have been 21 - 15 = 6 trees that were planted. The answer is 6.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:There are originally 3 cars. Then 2 more cars arrive. Now 3 + 2 = 5 cars are in the parking lot. The answer is\n5.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:Originally, Leah had 32 chocolates and her sister had 42. So in total they had 32 + 42 = 74. After eating 35,\nthey had 74 - 35 = 39 pieces left in total. The answer is 39.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA:Jason had 20 lollipops originally. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8\nlollipops. The answer is 8.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA:Shawn started with 5 toys. He then got 2 toys each from his mom and dad. So he got 2 * 2 = 4 more toys.\nNow he has 5 + 4 = 9 toys. The answer is 9.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA:There were originally 9 computers. For each day from monday to thursday, 5 more computers were installed.\nSo 4 * 5 = 20 computers were added. Now 9 + 20 = 29 computers are now in the server room. The answer is 29.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA:Michael started with 58 golf balls. He lost 23 on Tuesday, and lost 2 more on wednesday. So he had 58 - 23\n= 35 at the end of Tuesday, and 35 - 2 = 33 at the end of wednesday. The answer is 33.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:Olivia had 23 dollars. She bought 5 bagels for 3 dollars each. So she spent 5 * 3 = 15 dollars. Now she has\n23 - 15 = 8 dollars left. The answer is 8.\nTable 9: Full prompt for Chain-of-Thought prompting in our experiments (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:Theodor Haecker was 65 years old when he died. Harry Vaughan Watkins was 69 years old when he\ndied. So the final answer (the name of the person) is: Harry Vaughan Watkins.\nQuestion:Why did the founder of Versus die?\nAnswer:Versus was founded by Gianni Versace. Gianni Versace was shot and killed on July 15, 1997. So the\nfinal answer (reason of death) is: Shot.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:Dambar Shah (? - 1645) was the father of Krishna Shah. Rudra Shah was the child of Krishna Shah (?\n- 1661). So the final answer (the name of the grandchild) is: Rudra Shah.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:The director of the film FAQ: Frequently Asked Questions is Carlos Atanes. The director of the film\nThe Big Money is John Paddy Carstairs. The nationality of Carlos Atanes is Spanish. The nationality of John\nPaddy Carstairs is British.  Spanish is not equal to British.  So the final answer (whether they have the same\nnationality) is: No.\nTable 10: Full prompt for Chain-of-Thought prompting in our experiments (factual QA).",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:There are 15 trees originally. Then there were 21 trees after the Grove workers planted some more. Now 15\n+ 21 = 36. Since there were 6 workers in the grove, so the grove workers planted 36 / 6 = 6 trees today. The\nanswer is 6.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:There are originally 3 cars. Then 2 more cars arrive. Now 3 * 2 = 6 cars come. So 6 - 1 = 5 cars are in the\nparking lot. The answer is 5.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:Originally, Leah had 32 chocolates and her sister had 42. So her sister had 42 - 32 = 10 chocolates more than\nLeah has. After eating 35, since 10 + 35 = 45, they had 45 - 6 = 39 pieces left in total. The answer is 39.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA:Jason had 20 lollipops originally. Then he had 12 after giving some to Denny. Now 20 + 12 = 32. Jason has 4\ntimes what Denny has, so he gave Denny 32 / 4 = 8 lollipops. The answer is 8.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA:Shawn started with 5 toys. He then got 2 toys each from his mom and dad. Now 5 - 2 = 3. So he has 3 * 3 =\n9 toys now for Christmas. The answer is 9.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA:There were originally 9 computers. For each day from monday to thursday, 5 more computers were installed.\nNow 9 * 5 = 45 computers.  Since 4 * 4 = 16, now 45 - 16 = 29 computers are now in the server room.  The\nanswer is 29.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA:Michael started with 58 golf balls. He lost 23 on Tuesday, and lost 2 more on wednesday. So compared with\nwednesday, he lost 23 - 2 = 21 more balls on Tuesday. So he had 58 - 21 = 37 golf balls at the end of wednesday.\nThe answer is 37.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:Olivia had 23 dollars. She bought 5 bagels for 3 dollars each. So she earned 23 - 5 = 18 dollars. Now 18 / 3 =\n6. So she has 6 + 2 = 8 dollars left. The answer is 8.\nTable 11: Full prompt for “invalid reasoning” setting (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:Theodor Haecker wrote an essay, Kierkegaard and the Philosophy of Inwardness in 1913.  Harry\nVaughan Watkins played his final Wales international against England in January 1906. So the final answer (the\nname of the person) is: Theodor Haecker.\nQuestion:Why did the founder of Versus die?\nAnswer:Versus was a diffusion line of the Italian luxury fashion house Versace, which began in 2009. 2009 is\nthe year American singer Michael Jackson died of acute propofol and benzodiazepine intoxication. So the final\nanswer (reason of death) is: Intoxication.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:Dambar Shah (? - 1645) was the king of the Gorkha Kingdom. The Gorkha Kingdom was established\nby Prince Dravya Shah.  Dravya Shah has a child named Rudra Shah.  So the final answer (the name of the\ngrandchild) is: Rudra Shah.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:FAQ: Frequently Asked Questions is a feature-length dystopian movie.  The Big Money is a 1958\ncomedy film. Dystopian stories mostly take place in British. Comedy stories mostly happen in Australia. British\nis not equal to Australia. So the final answer (whether they have the same nationality) is: No.\nTable 12: Full prompt for “invalid reasoning” setting (factual QA).",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:There are 21 - 15 = 6 trees originally. Then there were 15 trees after the Grove workers planted some more.\nSo there must have been 21 trees that were planted. The answer is 6.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:There are originally 3 + 2 = 5 cars. Then 3 more cars arrive. Now 2 cars are in the parking lot. The answer is\n5.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:Originally, Leah had 32 + 42 = 74 chocolates and her sister had 32. So in total they had 74 - 35 = 39. After\neating 35, they had 42 pieces left in total. The answer is 39.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA:Jason had 20 - 12 = 8 lollipops originally. Then he had 20 after giving some to Denny. So he gave Denny 12\nlollipops. The answer is 8.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA:Shawn started with 4 toys. He then got 5 + 4 = 9 toys each from his mom and dad. So he got 5 more toys.\nNow he has 2 * 2 = 4 toys. The answer is 9.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA:There were originally 5 computers. For each day from monday to thursday, 4 * 5 = 20 more computers were\ninstalled. So 9 + 20 = 29 computers were added. Now 9 computers are now in the server room. The answer is 29.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA:Michael started with 2 golf balls. He lost 23 on Tuesday, and lost 35 - 2 = 33 more on wednesday. So he had\n58 at the end of Tuesday, and 58 - 23 = 35 at the end of wednesday. The answer is 33.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:Olivia had 5 * 3 = 15 dollars. She bought 5 bagels for 23 - 15 = 8 dollars each. So she spent 3 dollars. Now\nshe has 23 dollars left. The answer is 8.\nTable 13: Full prompt for “no coherence for bridging objects” setting (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:65 was Harry Vaughan Watkins years old when he died. 65 was 69 years old when he died. Theodor\nHaecker is bigger than 69. So the final answer (the name of the person) is: Harry Vaughan Watkins.\nQuestion:Why did the founder of Versus die?\nAnswer:Versus was shot and founded. Gianni Versace was killed on July 15, 1997 by Gianni Versace. So the\nfinal answer (reason of death) is: Shot.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:Krishna Shah was the father of Rudra Shah. Dambar Shah (? - 1645) was the child of Krishna Shah (?\n- 1661). So the final answer (the name of the grandchild) is: Rudra Shah.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:The director of John Paddy Carstairs is John Paddy Carstairs. The director of British is Spanish. The\nnationality of Carlos Atanes is British. The nationality of John Paddy Carstairs is film FAQ: Frequently Asked\nQuestions. Carlos Atanes is not equal to film The Big Money. So the final answer (whether they have the same\nnationality) is: No.\nTable 14: Full prompt for “no coherence for bridging objects” setting (factual QA).",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:There are 4 trees originally. Then there were 8 trees after the Grove workers planted some more. So there\nmust have been 8 - 4 = 4 trees that were planted. The answer is 4.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:There are originally 18 cars.  Then 9 more cars arrive.  Now 18 + 9 = 27 cars are in the parking lot.  The\nanswer is 27.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:Originally, Leah had 19 chocolates and her sister had 31. So in total they had 19 + 31 = 50. After eating 29,\nthey had 50 - 29 = 21 pieces left in total. The answer is 21.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA: Jason had 37 lollipops originally. Then he had 14 after giving some to Denny. So he gave Denny 37 - 14 = 23\nlollipops. The answer is 23.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA:Shawn started with 8 toys. He then got 6 toys each from his mom and dad. So he got 6 * 2 = 12 more toys.\nNow he has 8 + 12 = 20 toys. The answer is 20.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA:There were originally 23 computers.  For each day from monday to thursday, 10 more computers were\ninstalled. So 4 * 10 = 40 computers were added. Now 23 + 40 = 63 computers are now in the server room. The\nanswer is 63.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA:Michael started with 46 golf balls. He lost 27 on Tuesday, and lost 6 more on wednesday. So he had 46 - 27\n= 19 at the end of Tuesday, and 19 - 6 = 13 at the end of wednesday. The answer is 13.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:Olivia had 48 dollars. She bought 7 bagels for 6 dollars each. So she spent 7 * 6 = 42 dollars. Now she has\n48 - 42 = 6 dollars left. The answer is 6.\nTable 15: Full prompt for “no relevance for bridging objects” setting (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:Albin Barack was 49 years old when he died. Carl Clemens was 55 years old when he died. 55 is\nbigger than 49. So the final answer (the name of the person) is: Carl Clemens.\nQuestion:Why did the founder of Versus die?\nAnswer:The gang was founded by John Vitti. John Vitti drowned and got killed on February 2009. So the final\nanswer (reason of death) is: drowning.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:Metis Amando was the father of David Amando. Randall Amando was the child of David Amando. So\nthe final answer (the name of the grandchild) is: Randall Amando.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:The director of \"The Forgortten Bride\" is Paul Cuevas. The director of \"Grace and the Rose\" is Ronnie\nDixon. The nationality of Paul Cuevas is Australia. The nationality of Ronnie Dixon is France. Australia is not\nequal to France. So the final answer (whether they have the same nationality) is: No.\nTable 16: Full prompt for “no relevance for bridging objects” setting (factual QA).",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:Then there were 15 trees after the Grove workers planted some more. So there must have been 21 trees that\nwere planted. There are 21 - 15 = 6 trees originally. The answer is 6.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:Then 3 more cars arrive. Now 2 cars are in the parking lot. There are originally 3 + 2 = 5 cars. The answer is\n5.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:After eating 32, they had 42 pieces left in total. Originally, Leah had 32 + 42 = 74 chocolates and her sister\nhad 35. So in total they had 74 - 35 = 39. The answer is 39.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA:Then he had 20 after giving some to Denny. So he gave Denny 12 lollipops. Jason had 20 - 12 = 8 lollipops\noriginally. The answer is 8.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA:Now he has 5 toys. So he got 2 more toys. Shawn started with 2 * 2 = 4 toys. He then got 5 + 4 = 9 toys each\nfrom his mom and dad. The answer is 9.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA:So 9 computers were added. Now 5 computers are now in the server room. There were originally 4 * 5 = 20\ncomputers. For each day from monday to thursday, 9 + 20 = 29 more computers were installed. The answer is\n29.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA: So he had 58 at the end of Tuesday, and 23 at the end of wednesday. He lost 2 on Tuesday, and lost 58 - 23 =\n35 more on wednesday. Michael started with 35 - 2 = 33 golf balls. The answer is 33.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:Now she has 23 dollars left. So she spent 5 dollars. Olivia had 3 dollars. She bought 5 * 3 = 15 bagels for 23\n- 15 = 8 dollars each. The answer is 8.\nTable 17: Full prompt for “no coherence for language template” setting (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:Theodor Haecker is bigger than 65. Harry Vaughan Watkins was 69 years old when he died. 69 was 65\nyears old when he died. So the final answer (the name of the person) is: Harry Vaughan Watkins.\nQuestion:Why did the founder of Versus die?\nAnswer:Versus was killed on July 15, 1997. Gianni Versace was founded by Gianni Versace and shot. So the\nfinal answer (reason of death) is: Shot.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:Dambar Shah (? - 1645) was the child of Krishna Shah. Krishna Shah (? - 1661) was the father of\nRudra Shah. So the final answer (the name of the grandchild) is: Rudra Shah.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:The nationality of film FAQ: Frequently Asked Questions is not equal to Carlos Atanes. The nationality\nof film The Big Money is John Paddy Carstairs. The director of Carlos Atanes is Spanish. The director of John\nPaddy Carstairs is British. Spanish is British. So the final answer (whether they have the same nationality) is:\nNo.\nTable 18: Full prompt for “no coherence for language template” setting (factual QA).",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:Then there were 21 - 15 = 6 trees after the Grove workers planted some more. So there must have been 15\ntrees that were planted. There are 21 trees originally. The answer is 6.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:Then 3 + 2 = 5 more cars arrive. Now 3 cars are in the parking lot. There are originally 2 cars. The answer is\n5.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:After eating 32 + 42 = 74, they had 32 pieces left in total. Originally, Leah had 74 - 35 = 39 chocolates and\nher sister had 35. So in total they had 42. The answer is 39.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA:Then he had 20 - 12 = 8 after giving some to Denny. So he gave Denny 20 lollipops. Jason had 12 lollipops\noriginally. The answer is 8.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA:Now he has 4 toys. So he got 5 + 4 = 9 more toys. Shawn started with 5 toys. He then got 2 * 2 = 4 toys each\nfrom his mom and dad. The answer is 9.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA:So 5 computers were added. Now 4 * 5 = 20 computers are now in the server room. There were originally 9 +\n20 = 29 computers. For each day from monday to thursday, 9 more computers were installed. The answer is 29.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA:So he had 2 at the end of Tuesday, and 23 at the end of wednesday. He lost 35 - 2 = 33 on Tuesday, and lost\n58 more on wednesday. Michael started with 58 - 23 = 35 golf balls. The answer is 33.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:Now she has 5 * 3 = 15 dollars left. So she spent 5 dollars. Olivia had 23 - 15 = 8 dollars. She bought 3\nbagels for 23 dollars each. The answer is 8.\nTable 19: Full prompt for “no relevance for language template” setting (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:Theodor Haecker has 65 golf balls. Harry Vaughan Watkins has 69 golf balls. 69 balls are more than\n65 balls. So the final answer (the person who has more golf balls) is: Harry Vaughan Watkins.\nQuestion:Why did the founder of Versus die?\nAnswer:The leader of Versus was Gianni Versace. Gianni Versace shot three people and got into jail. So the\nfinal answer (reason for imprisonment) is: Shot.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:The husband of Dambar Shah (? - 1645) is Krishna Shah. Krishna Shah (? - 1661) has a brother called\nRudra Shah. So the final answer (the name of the brother-in-law) is: Rudra Shah.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:The author of the film FAQ: Frequently Asked Questions is Carlos Atanes. The author of film The Big\nMoney is John Paddy Carstairs. The wife of Carlos Atanes is from Spanish. The wife of John Paddy Carstairs is\nfrom British. Spanish is warmer than British. So the final answer (the country which is warmer) is: Spanish.\nTable 20: Full prompt for “no relevance for language template” setting (factual QA).",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:Then there were 21 - 15 = 6 trees after the Grove workers planted some more. So there must have been 15\ntrees that were planted. There are 21 trees originally. The answer is 6.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:Then 3 + 2 = 5 more cars arrive. Now 3 cars are in the parking lot. There are originally 2 cars. The answer is\n5.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:After eating 32 + 42 = 74, they had 32 pieces left in total. Originally, Leah had 74 - 35 = 39 chocolates and\nher sister had 35. So in total they had 42. The answer is 39.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA:Then he had 20 - 12 = 8 after giving some to Denny. So he gave Denny 20 lollipops. Jason had 12 lollipops\noriginally. The answer is 8.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA:Now he has 4 toys. So he got 5 + 4 = 9 more toys. Shawn started with 5 toys. He then got 2 * 2 = 4 toys each\nfrom his mom and dad. The answer is 9.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA:So 5 computers were added. Now 4 * 5 = 20 computers are now in the server room. There were originally 9 +\n20 = 29 computers. For each day from monday to thursday, 9 more computers were installed. The answer is 29.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA:So he had 2 at the end of Tuesday, and 23 at the end of wednesday. He lost 35 - 2 = 33 on Tuesday, and lost\n58 more on wednesday. Michael started with 58 - 23 = 35 golf balls. The answer is 33.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:Now she has 5 * 3 = 15 dollars left. So she spent 5 dollars. Olivia had 23 - 15 = 8 dollars. She bought 3\nbagels for 23 dollars each. The answer is 8.\nTable 21: Full prompt for “no coherence” setting (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:65 is bigger than Harry Vaughan Watkins. 65 was 69 years old when he died. Theodor Haecker was 69\nyears old when he died. So the final answer (the name of the person) is: Harry Vaughan Watkins.\nQuestion:Why did the founder of Versus die?\nAnswer:Versus was shot and killed on July 15, 1997. Gianni Versace was founded by Gianni Versace. So the\nfinal answer (reason of death) is: Shot.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:Krishna Shah was the child of Rudra Shah. Dambar Shah (? - 1645) was the father of Krishna Shah (?\n- 1661). So the final answer (the name of the grandchild) is: Rudra Shah.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:The nationality of John Paddy Carstairs is not equal to John Paddy Carstairs.  The nationality of\nBritish is Spanish. The director of Carlos Atanes is British. The director of John Paddy Carstairs is film FAQ:\nFrequently Asked Questions. Carlos Atanes is film The Big Money. So the final answer (whether they have the\nsame nationality) is: No.\nTable 22: Full prompt for “no coherence” setting (factual QA).",
    "Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA:Tom started with 4 apples. Then he had 8 after borrowing some from Amy. So he borrowed Amy 8 - 4 = 4.\nThe answer is 4.\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: Benjamin has 18 gloves originally. Then he got 9 more gloves. So he has 18 + 9 = 27 gloves now. The answer\nis 27.\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA: Patricia needs to donate 19 inches, and wants her hair to be 31 inches long after the donation. Her hair is 29\ninches long currently. Her hair needs to be 19 + 31 = 50 inches long when she cuts it. So she needs to grow 50 -\n29 = 21 more inches. The answer is 21.\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA: There were 37 trains originally. Then there were 14 after some were driven away. So there should be 37 - 14\n= 23 that were driven away. The answer is 23.\nQ:Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA: The taxi has a ride fee of 8 dollars. Michelle rode the taxi for 6 miles with 2 dollars per mile. So the taxi\ncharge is 6 * 2 = 12. So the total amount that Michelle paid for the ride was 8 + 12 = 20. The answer is 20.\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA: Haley is currently 23 inches tall. She grows at the rate of 10 inches every year for 4 years. So she will have\ngrown by 10 * 4 = 40 inches. Her height after 4 years will be 23 + 40 = 63 inches. The answer is 63.\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA: Abigail had 46 dollars in her purse originally.  She spent27inthestore, andhas6 left now.  After going\nshopping, she had 46 - 27 = 19 dollars left. So she lost 19 - 6 = 13 dollars. The answer is 13.\nQ:Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA: George earned 48 in total. He sold 7 cars for 6 dollars each. So he earned 7 * 6 = 42 dollars from them. The\nlego set cost was then 48 - 42 = 6. The answer is 6.\nTable 23: Full prompt for “no relevance” setting (arithmetic reasoning).\nQuestion:Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nAnswer:Albin Barack has 49 golf balls. Carl Clemens has 55 golf balls. 55 balls are more than 49 balls. So the\nfinal answer (the person who has more golf balls) is: Carl Clemens.\nQuestion:Why did the founder of Versus die?\nAnswer:The leader of the gang was John Vitti. John Vitti drowned three people and got into jail. So the final\nanswer (reason for imprisonment) is: drowning.\nQuestion:Who is the grandchild of Dambar Shah?\nAnswer:The husband of Metis Amando is David Amando. David Amando has a brother called Randall Amando.\nSo the final answer (the name of the brother-in-law) is: Randall Amando.\nQuestion:Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from\nthe same country?\nAnswer:The author of “The Forgortten Bride” is Paul Cuevas. The author of “Grace and the Rose” is Ronnie\nDixon. The wife of Paul Cuevas is from Spanish. The wife of Ronnie Dixon is from British. Spanish is warmer\nthan British. So the final answer (the country which is warmer) is: Spanish.\nTable 24: Full prompt for “no relevance” setting (factual QA)."
  ]
}