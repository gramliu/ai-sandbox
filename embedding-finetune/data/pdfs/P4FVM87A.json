{
  "key": "P4FVM87A",
  "url": "http://arxiv.org/pdf/2308.12261",
  "metadata": {
    "title": "Prompt2Model: Generating Deployable Models from Natural Language\n  Instructions",
    "abstract": "  Large language models (LLMs) enable system builders today to create competent\nNLP systems through prompting, where they only need to describe the task in\nnatural language and provide a few examples. However, in other ways, LLMs are a\nstep backward from traditional special-purpose NLP models; they require\nextensive computational resources for deployment and can be gated behind APIs.\nIn this paper, we propose Prompt2Model, a general-purpose method that takes a\nnatural language task description like the prompts provided to LLMs, and uses\nit to train a special-purpose model that is conducive to deployment. This is\ndone through a multi-step process of retrieval of existing datasets and\npretrained models, dataset generation using LLMs, and supervised fine-tuning on\nthese retrieved and generated datasets. Over three tasks, we demonstrate that\ngiven the same few-shot prompt as input, Prompt2Model trains models that\noutperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%\nwhile being up to 700 times smaller. We also show that this data can be used to\nobtain reliable performance estimates of model performance, enabling model\ndevelopers to assess model reliability before deployment. Prompt2Model is\navailable open-source at https://github.com/neulab/prompt2model.\n",
    "published": "2023-08-23T17:28:21Z"
  },
  "text": [
    "PROMPT2MODEL:\nGenerating Deployable Models from Natural Language Instructions\nVijay Viswanathan\n1∗\n, Chenyang Zhao\n1,2∗\n,\nAmanda Bertsch\n1\n, Tongshuang Wu\n1\n, Graham Neubig\n1\n1\nCarnegie Mellon University,\n2\nTsinghua University\nAbstract\nLarge language models (LLMs) enable system\nbuilders today to create competent NLP sys-\ntems through prompting, where they only need\nto describe the task in natural language and\nprovide a few examples.   However,  in other\nways, LLMs are a step backward from tradi-\ntional special-purpose NLP models;  they re-\nquire  extensive  computational  resources  for\ndeployment  and  can  be  gated  behind  APIs.\nIn this paper, we proposePrompt2Model, a\ngeneral-purpose  method  that  takes  a  natural\nlanguage task description like the prompts pro-\nvided to LLMs, and uses it to train a special-\npurpose  model  that  is  conducive  to  deploy-\nment.  This is done through a multi-step pro-\ncess of retrieval of existing datasets and pre-\ntrained models, dataset generation using LLMs,\nand supervised fine-tuning on these retrieved\nand  generated  datasets.Over  three  tasks,\nwe demonstrate that given the same few-shot\nprompt as input,Prompt2Modeltrains mod-\nels that outperform the results of a strong LLM,\ngpt-3.5-turbo, by an average of 20% while\nbeing up to 700 times smaller.  We also show\nthat  this  data  can  be  used  to  obtain  reliable\nperformance estimatesof model performance,\nenabling model developers to assess model re-\nliability before deployment.Prompt2Model\nis available open-source athttps://github.\ncom/neulab/prompt2model.\n1\n1  Introduction\nTraditionally, building an NLP model from scratch\nhas been a substantial undertaking. An NLP practi-\ntioner seeking to solve a new problem would need\nto define their task scope, find or create data that\nspecifies the intended system behavior, choose a\nsuitable model architecture, train the model, assess\nits performance through evaluation, and then de-\nploy it for real-world usage (Paleyes et al., 2022).\n∗\nequal contribution.\n1\nOur demo video is posted atyoutu.be/LYYQ_EhGd-Q.\nBERT Score: 94.0, ChrF++: 58.9, EM: 61.5\nRetrieve \nPretrained model\nRetrieve \nData\nGenerate \nData\nPrompt2Model\nInput: Prompt (task description + optional examples)\nOutput: Deployment-ready model\nQuestion: What does LPC stand for? \nContext: The psychoacoustic masking codec was...\nAnswer: linear predictive coding\nAnswer questions given context from a \nrelevant Wikipedia article.\nFigure 1:Prompt2Modelis a framework for generat-\ning a small yet accurate model from a prompt.\nLLMs  like  GPT-3  (Brown  et  al.,  2020;  Liu\net  al.,  2023b)  offer  a  lighter-weight  paradigm\nfor  NLP  system  construction  through  “prompt-\ning” (Reynolds and McDonell, 2021). Practitioners\ncan now write a prompt specifying the intended\nsystem behavior (optionally with a few demonstra-\ntions), and ask an LLM to generate a desired out-\nput via text completion.  This makes it possible\nto prototype NLP systems rapidly for a variety of\napplications without writing a single line of code\n(Floridi and Chiriatti, 2020).\nHowever,  there  is  still  a  gap  between  proof-\nof-concept prototyping — showing LLMs can be\nprompted for a particular task — and practical de-\nployment.  Prompting LLMs can be expensive as\nthey require either a significant amount of com-\nputing or access to commercial APIs, and their re-\nliance on the input prompt quality makes them un-\nstable compared to trained models (Min et al., 2022;\nBubeck et al., 2023). Because practitioners usually\ndo not have enough annotated validation data to\nmeasure their system performance, it is also more\nchallenging for them to debug their systems be-\nfore deployment (Jiang et al., 2022). Additionally,\narXiv:2308.12261v1  [cs.CL]  23 Aug 2023",
    "LLM-prompted systems pose usability challenges.\nPractitioners have expressed concerns about the\nhigh serving cost and slow prediction time asso-\nciated with using LLMs (Park et al., 2022), and\nthose working in high-stakes domains cannot rely\non commercial LLM APIs due to privacy concerns.\nFor instance, sharing user data with LLM service\nproviders is illegal for many applications in the\nUS (Sezgin et al., 2022).\nIn  this  work,  we  presentPrompt2Model,  a\nsystem that retains the ability to specify system\nbehavior in a light-weight way throughprompt-\ning, while still resulting in adeployable special-\npurpose  model,  maintaining  all  the  advantages\nthereof.Prompt2Modelis designed as an auto-\nmated pipeline that extracts essential task informa-\ntion from users’ prompts and then automatically\ncollects and synthesizes task-specific knowledge\nthrough three channels:\n•Dataset retrieval:  Whenever possible, we col-\nlect  training  data  by  retrieving  task-relevant\nannotated  data  (Färber  and  Leisinger,  2021;\nViswanathan et al., 2023).\n•\nDataset generation: We distill knowledge from\nan LLM (“teacher model”) by employing it to\ngenerate a pseudo-labeled dataset.  Prior work\nhas demonstrated that such a dataset can be used\nto train a smaller “student” model to emulate\nthe behavior of the teacher model (Wang et al.,\n2021a; He et al., 2023; Gudibande et al., 2023).\n•\nModel retrieval: Based on the prompt, we iden-\ntify a pretrained language model whose paramet-\nric knowledge is appropriate for the user’s intent.\nThis chosen model serves as the student model\nand is further fine-tuned and evaluated using the\ngenerated and retrieved data.\nPrompt2Modelis  designed  to  support  differ-\nent  instantiations  of  each  of  these  components.\nWe  provide  a  reference  implementation  where\nwe demonstrate its utility with agpt-3.5-turbo-\nbased dataset generator, a dataset retriever based\non  DataFinder  (Viswanathan  et  al.,  2023),  and\na model retriever using BM25.   We evaluate on\nthree tasks covering both traditional NLP bench-\nmarks and novel applications and find that, empiri-\ncally,Prompt2Modelsometimes produces small\nmodels that outperformgpt-3.5-turbowhen us-\ning the same prompt as input.   On 2 of these 3\ntasks, we observe >20 point improvements over the\ngpt-3.5-turbobaseline, despite the final model\nproduced byPrompt2Modelbeing up to 700 times\nsmaller. We also find that we can generate effective\nevaluation  datasets;  performance  improvements\non these synthetic clones of real benchmarks also\nhold on their real counterparts.  We believe that\nPrompt2Modelcan serve the following purposes\nfor the community:\n1.\nA tool for quickly building small and com-\npetent NLP systems:Prompt2Modelcan be\ndirectly used to produce task-specific models\nthat outperform LLMs in a few hours without\nany manual data annotation or architecture de-\nsign. The method bridges the gap between the\nproof-of-concept LLM prototyping and the prac-\ntical deployment of the model.\n2.A testbed for end-to-end, prompt-based\nmodel training:  GivenPrompt2Model’s ex-\ntensible design, it can offer a platform for ex-\nploring  new  techniques  in  model  distillation,\ndataset generation, synthetic evaluation, dataset\nretrieval,  and  model  retrieval.   Our  platform\nallows  studying  these  components  using  ex-\ntrinsic downstream metrics, enabling empirical\nprogress on these research areas.\n2  Prompt2Model Framework\nOur system,Prompt2Model, provides a platform\nto automate the components of a machine learning\npipeline:  data collection, model training, evalua-\ntion, and deployment. We illustrate our automated\npipeline in Figure 2. At the core is our automatic\ndata collection system, which leverages dataset re-\ntrieval and LLM-based dataset generation to obtain\nlabeled data relevant to the user’s needs. We then\nretrieve pretrained models which we finetune on\nthe training splits of the collected datasets. Finally,\nwe evaluate our trained models on the test splits of\nthe same datasets and optionally create a web UI\nthat can be used to interact with the model.\nOur general-purpose method is designed to be\nmodular and extensible; each component can be im-\nplemented differently or disabled by a practitioner.\nWe give an overview of our framework, then in sec-\ntion 3 we describe our reference implementation.\nPrompt ParserAs the primary input to our sys-\ntem, users provide prompts similar to those used\nfor LLMs. These prompts comprise an instruction\nand, optionally, a few demonstrations of the antic-\nipated behavior.  While this open-ended interface\nis convenient for users, end-to-end ML pipelines\nmay benefit from aPrompt Parserthat processes\nthis input, such as segmenting the prompt into an",
    "The Children's \nBook Test \nPrompt +\nFew \nExamples\nDataset \nRetriever\nDataset \nGenerator\nGenerated \nTraining Set\nGenerated Test \nSet\nRetrieved \nDataset\nModel \nTrainer\nEvaluation\nPerformance \nEstimate\nInteractive \nDemo\nInput \nParser\nPrompt \nSpec\n    Model \nRetriever\nRetrieved \nModel\n\"Answer questions given context from a relevant Wikipedia article.\n  Examples: <QA pairs> \"\nflan-t5-base\nInstruction\nAnswer questions [...]\nDemonstrations\n <QA pairs>\nChrF++: 58.9\nEM: 61.5\nBERTScore: 94.0\nTrained \nModel\nFigure 2:  ThePrompt2Modelarchitecture seeks to automate the core machine learning development pipeline,\nallowing us to train a small yet accurate model from just a prompt.\ninstruction and individual demonstrations or trans-\nlating instructions into English.\nDataset RetrieverGiven a prompt, we first try to\ndiscover existing manually-annotated data that can\nsupport a user’s task description. There are several\ndesign decisions for theDataset Retriever:\n1.  What datasets to search against?\n2.  How to index datasets for search?\n3.\nWhich dataset columns are needed for the user’s\ntask, and which columns should be ignored?\nPrior works by Färber and Leisinger (2021) and\nViswanathan et al. (2023) introduced systems for\ndataset search. We use the latter, calledDataFinder,\nin our implementation, as described in §3.2.\nDataset GeneratorNot  all  conceivable  tasks\nhave any existing annotated data, and many tasks\nare only somewhat relevant to an existing dataset.\nTo support a wide range of tasks, we introduce a\nDataset Generatorto produce synthetic training\ndata as per the user-specific requirements parsed by\nthePrompt Parser. This component presents chal-\nlenges related to cost efficiency, generation speed,\nexample diversity, and quality control. We discuss\nour suggested solution to these challenges in §3.3.\nModel RetrieverBesides training data, we must\nidentify an appropriate model to finetune. We cast\nthis as a retrieval problem, where each model is rep-\nresented by user-generated descriptions and meta-\ndata such as popularity or tasks supported.  The\nreference implementation of ourModel Retriever,\ndescribed in §3.4, searches against pretrained mod-\nels on Hugging Face (Wolf et al., 2020), but this\ncould instead cover other model repositories such\nas Model Zoo (Koh, 2020).\nTrainingGiven retrieved and generated datasets\nand a pretrained model, we use aModel Trainer\nto finetune the model on a subset of the data. We\ncurrently train models by treating all tasks as text-\nto-text generation (Raffel et al., 2020), as described\nin §3.5, but emphasize that this component can be\nextended in the future to support new approaches.\nEvaluationAfter training models on a portion of\nthe retrieved and generated datasets, we give the\nremaining data to anModel Evaluatormodule. We\naim to support a variety of tasks, and selecting the\ncorrect task-specific metrics for an arbitrary task\nis a difficult problem. We describe our suggested\nstrategies for task-agnostic evaluation in §3.6.\nWeb App CreationTo enable developers to ex-\npose a model to collaborators or users, we include\nan optional component called theDemo Creator\nto create a graphical interface to interact with the\nmodel. We briefly describe our implementation of\nthis component in §3.7.\n3  Reference Implementation\nPrompt2Model\nis designed modularly to support\ncustomization of each component in our framework\n(described in §2), but we have provided a reference\nimplementation to enable immediate adoption.\n3.1  Prompt Parser\nWe  parse  the  prompt  intoinstructionand\ndemonstrationsfields (shown in Figure 2), where",
    "the instruction represents the primary task or objec-\ntive and the demonstrations exemplify the desired\nbehavior. To achieve this, we utilize an LLM with\nin-context learning to segment user prompts, em-\nploying the OpenAIgpt-3.5-turbo-0613in our\nexperiments.  If the instruction provided is iden-\ntified to be in a language other than English, we\ntranslate it to English using the DeepL API.\n2\n3.2  Dataset Retriever\nTo  retrieve  datasets  for  a  prompt,  we  adapt  the\nDataFindersystem  introduced  by  Viswanathan\net   al.   (2023).By   extracting   user-generated\ndataset descriptions for each dataset in Hugging\nFace  Datasets  (Lhoest  et  al.,  2021),  we  utilize\nDataFinder’s trained bi-encoder retriever to rank\nthe most relevant datasets. Once a relevant dataset\nis identified, the next step is to determine which\ncolumns of the dataset correspond to the input and\nthe desired output specified by the user.  As au-\ntomatically inducing the correct schema for any\ndataset can be challenging, we adopt a human-in-\nthe-loop approach. We present the top-kdatasets,\nwherek= 25by default, to the user and allow\nthem to either select the most relevant dataset or to\nstate that none are a good fit for their task. We then\nask the user to identify the appropriate columns for\ninput and output from the dataset’s schema.\n3.3  Dataset Generator\nWe carefully engineered our dataset generator to\nenable speed-optimized generation at a low-cost\nwhile creating diverse and high-quality examples.\nOur strategy comprises the following components:\nHigh-Diversity Few-Shot PromptingWe use\nautomated prompt engineering to generate a diverse\ndataset. We augment the user-provided demonstra-\ntion examples with a random sample of previously\ngenerated examples to promote diversity and avoid\ngenerating duplicate examples. Without this strat-\negy, 120 out of 200 generated QA examples were\nduplicates; with it, only 25 were duplicates.\nTemperature AnnealingWe adjust the sampling\ntemperature from low (favoring deterministic out-\nputs) to high (encouraging diverse exploration) pro-\nportionally to the number of examples already gen-\nerated. This modulation helps preserve output qual-\nity while gradually encouraging diversity.\n2\nhttps://www.deepl.com/en/docs-api\nSelf-Consistency DecodingGiven  that  LLM\nmay generate non-unique or incorrect outputs for\nthe same inputs, we useself-consistencyfiltering\n(Wang et al., 2022) to select pseudo-labels. Specifi-\ncally, we create a consensus output for each unique\ninput by selecting the most frequent answer;  in\nthe case of ties, we heuristically select the shortest\nanswer. This promotes accuracy of the generated\ndataset while ensuring unique examples.\nAsynchronous BatchingAPI requests are par-\nallelized usingzeno-build(Neubig and He, 2023).\nWe use additional mechanisms, such as dynamic\nbatch size and throttling, to optimize API usage.\n3.4  Model Retriever\nWe need to select an appropriate model to finetune.\nTo  support  many  tasks  with  a  unified  model\ninterface, we presently limit ourselves to encoder-\ndecoder  architectures  on  Hugging  Face  (Wolf\net al., 2020), following recent work that shows that\nencoder-decoder models are more data-efficient for\nmodel distillation (Calderon et al., 2023). This re-\nstriction still leaves a large set of pretrained models\nto choose from, e.g.Salesforce/codet5-base\nfor coding-related tasks (Wang et al., 2021b) or\nMaryaAI/opus-mt-ar-en-finetuned-ar-to-en\nfor Arabic-to-English translation (Tiedemann and\nThottingal, 2020). We frame the problem of select-\ning a pretrained model as a search problem. Using\nthe user’s instruction as a query, we search against\nall textual descriptions of models on Hugging Face.\nThis  search  task  is  challenging  because  Hug-\nging Face model descriptions are sparse and con-\ntain lots of templatic text, often with only a few\nwords that signify the content of the model.  To\naddress this, we follow the HyDE framework (Gao\net al., 2023) and first usegpt-3.5-turboto create\nahypothetical model descriptiongiven the user’s\ninstructions. We show an example of a hypotheti-\ncal document generated for a question-answering\ninstruction in Figure 3.  Using this description as\nan expanded query, we then apply the BM25 al-\ngorithm to compute query-model similarity scores\n(Robertson et al., 1995). To ensure the ease of de-\nployment of the resulting model, we filter out mod-\nels whose size (in bytes) exceeds a user-specified\nthreshold (set to 3GB by default).  Using the in-\ntuition that highly-downloaded models are more\nlikely to be high in quality, we choose the top model\nafter ranking by:\nBM25(query,model)·log(# of Downloads+ 1).",
    "Your task is to generate an answer to a natural \nquestion. In this task, the input is a string that \nconsists of both a question and a context passage.\nHypothetical Document Embedding\n---\nlanguage: en\nlicense: apache-2.0\ntags:\n- question-answering\n- nlp\n- transformers\ndatasets:\n- natural-questions\n- squad\n\n--\n\n## Model Description\nThis model is a fine-tuned version of a BERT model \nfor question-answering tasks. It can generate \nanswers to natural questions given context.\nLLM\nFigure 3: For our model retriever, we first construct a\nhypothetical model description for a query, then com-\npute similarity scores between that hypothetical model\ndescription and the descriptions of real models.\n3.5  Training\nDataset ProcessingWe train the model by lever-\naging  two  datasets-  one  generated  and  one  re-\ntrieved.To  sidestep  the  challenge  of  making\nschema-specific modeling decisions (e.g. construct-\ning specialized architectures for classification or\ngeneration tasks), we treat all datasets as “text-to-\ntext” problems (Raffel et al., 2020). We textualize\nthe input columns of each dataset and prepend the\nuser’s instructions to the input to guide the model.\nFinetuningWe  concatenate  the  retrieved  and\ngenerated datasets and shuffle them before train-\ning the student model.  We use the same default\nhyperparameters for all tasks.\n3\nWe train with the\nAdamW optimizer withlr = 5e-5for 3 epochs,\nwhich takes roughly one hour for all tasks.\n3.6  Evaluation\nOurModel Evaluatorautomatically evaluates mod-\nels for all tasks using three general-purpose met-\nrics:Exact Match,ChrF++(Popovi\n ́\nc, 2015), and\nBERTScore(Zhang  et  al.,  2019).ChrF++bal-\nances precision and recall to assess text genera-\ntion quality.Exact Matchmeasures how often\nthe model output perfectly matches the exact refer-\nence.BERTScorecaptures semantic similarities de-\nspite different wordings or phrasings by comparing\n3\nWe empirically find that these default hyperparameters\nare effective, but we plan on implementing hyperparameter\nselection using generated validation data in the future.\nthe model output and reference in the embedding\nspace.  We use XLM-R (Conneau et al., 2020) as\nthe encoder for BERTScore to support multilingual\nevaluation.\n3.7  Web App Creation\nWefinallyprovideanoptionalstepin\nPrompt2Modeltoautomaticallycreatea\ngraphical user interface that allows downstream\nusers to interact with the trained model. This web\napplication, built using Gradio (Abid et al., 2019),\ncan then be easily deployed publicly on a server.\n4  Experimental Setup\nTasksAs  a  proof-of-concept,  we  test  our  sys-\ntem’s ability to learn a model for three tasks:\n•Machine Reading Question Answering: We first\nconsider a common use case where pretrained\nmodels and training datasets are plentiful.  We\nuse SQuAD (Rajpurkar et al., 2016) as ground\ntruth to evaluate this setting.\n•\nJapanese NL-to-Code:  Code generation from\nJapanese-language queries is a challenging sce-\nnario where prior work exists but no annotated\ndata or pretrained models are available. We use\nMCoNaLa (Wang et al., 2023) for evaluation.\n•Temporal Expression Normalization: We finally\nconsider a task where there are no pretrained\nmodels or training datasets of any kind available.\nHere we use the Temporal dataset of Wu et al.\n(2023) as ground truth for evaluation.\nThoughPrompt2Modeloffers automated model\nevaluation (on generated and retrieved datasests),\nwe use real benchmark datasets here to measure\nour pipeline’s ability to train accurate models.\nLLM BaselineA primary goal of our work is\nto  train  small  models  that  can  match  or  outper-\nform LLMs. To measure success towards this goal,\nwe report the performance ofgpt-3.5-turboon\neach benchmark. We providegpt-3.5-turbo\n4\nthe\nsame instruction and demonstrations provided to\nPrompt2Model, for fair comparison.\n5  Experiment Results\n5.1  Downstream performance\nHow effective isPrompt2Modelat producing a\nhigh-quality model? In Table 1, we evaluated mod-\nels produced byPrompt2Model, as well as our\n4\nWe usedgpt-3.5-turbo-0613, accessed between July\n26 and August 6, 2023.",
    "MethodSQuADMCoNaLaTemporal\n(EM)(ChrF++)(ChrF++)\nPrompt2Model61.513.155.2\nw/o Model Ret.61.515.855.2\nw/o Data Ret.50.216.6N/A\ngpt-3.5-turbo42.137.330.7\nTable   1:We   evaluate   the   model   produced   by\nPrompt2Modelon real benchmarks for each test set,\ncompared togpt-3.5-turbo, which we used to power\nour dataset generator. We also examine the effect of re-\nmoving specific parts of our pipeline — model retrieval\nand dataset retrieval.   There are no relevant datasets\navailable for the Temporal task, so we did not use re-\ntrieved data forPrompt2Modelthere.\nbaseline LLMgpt-3.5-turbo, on real benchmark\ndatasets for each task — SQuAD, MCoNaLa, and\nTemporal.   We further examine the effect of re-\nmoving 2 specific elements of thePrompt2Model\npipeline — model retrieval and dataset retrieval.\nOn 2 of 3 datasets, we find thatPrompt2Model\nproduces models that are considerably more ac-\ncurate thangpt-3.5-turbo.  This is remarkable\nbecause the retrieved model for SQuAD and Tem-\nporal is Flan-T5, which, at 250M parameters, is up\nto 700 times smaller thangpt-3.5-turbo(which\nis believed to contain 175B parameters).\nWe observe thatPrompt2Model’s performance\non MCoNaLa’s Japanese-to-Python task is signif-\nicantly worse thangpt-3.5-turbo. One explana-\ntion for this is the relatively low diversity in the\ngenerated dataset of Japanese queries; 45 of 5000\nexamples are different ways of saying “find the\nmaximum value in a list of numbers“. We do not ob-\nserve this level of redundancy in our other datasets,\nsuggesting thatgpt-3.5-turbomay struggle to\ngenerate diverse text for non-English languages.\nAnother reason is the lack of an appropriate stu-\ndent model — the models found by the model re-\ntriever were trained on either on multiple language\nor code, but not both. The resulting pretrained mod-\nels may lack the parametric knowledge to represent\nthe Japanese inputs, Python outputs, or both.\n5.2  Combining retrieved and generated\ndatasets is powerful\nIdeally, generated and retrieved data should be as\nclose to the target domain as possible.  In our ex-\nperimental setting, where we deliberately choose\nprompts that mimic existing datasets, we can eval-\nuate  how  well  the  model  performs  relative  to  a\nmodel trained on the same amount of data from the\nMethod#TrainPerformanceAnno. Cost\nRetrieval only3,00056.79≈$ 0\nGeneration only3,00044.20≈$ 5\nRetrieval+generation6,00061.46≈$ 5\nCustom annotation3,00061.64≈$ 540\nTable 2: We compare model performance on SQuAD\non an annotation-cost basis, using datasets produced by\ndifferent modules ofPrompt2Model, along with fully-\nmanual annotation. Performance reported for all models\nis the exact match on the test set,\n7\nwhich reflectsthe\ntrue task performance.  Cost of custom annotation is\nestimated from Rajpurkar et al. (2016) using their re-\nported annotator pay rate of $9/hour and keeping 1,000\nvalidation examples.\ntrue dataset.  We use SQuAD as a running exam-\nple.\n5\nAs our prompt is a description of the SQuAD\npassage-level question answering task (Figure 1),\nwe exclude SQuAD from our retrieved datasets list.\nInstead, we evaluate models finetuned on:\n1.3k examples from the closest retrieved dataset\n6\n2.  3k examples generated byPrompt2Model\n3.\nThe union of the above, which is what the full\nPrompt2Modelpipeline uses\n4.3k examples from SQuAD (analogous to the\nuser custom-annotating data for a task).\nTable 2 shows the results across these four set-\ntings.   While  using  retrieved  or  generated  data\ncauses a reduction in performance due to domain\nshift, the combination of the two methods achieves\nsimilar performance to using the true dataset. For\nthis machine reading comprehension task where\nthe user would need to custom-annotate data for\ntheir task,Prompt2Modelallows forsimilar per-\nformance at less than 1% of the cost.\n5.3  Our generated evaluation data can\nidentify real modeling improvements\nHigh-quality generated data should also allow us\ntodiscriminatebetween multiple candidate mod-\nels to select a model that will perform well down-\nstream. We finetune various models on a generated\ndataset and rank their performance according to the\ngenerated test data and the test data from the target\n(real) dataset. We evaluate the Kendall’s rank cor-\n5\nWe focus on only SQuAD here because our other two\ntasks have less real training examples than the datasets we\ngenerate, making comparison impractical.\n6\nThe closest dataset retrieved by the dataset retriever for\nour  SQuAD-inspired  prompt  is  The  Children’s  Book  Test\nDataset (Hill et al., 2016).",
    "DatasetMetricτ   p-value\nSQuADEM64.30.03*\nTemporalChrF++24.20.31\nMCoNaLa (JP)ChrF++70.90.00**\nTable 3: We evaluate 10 different models on real test sets\nand their corresponding generated clones. We compute\nKendall’s Tau on the ranked lists of models and find\nstatistically significant correlations for 2 of 3 datasets.\nrelation (Kendall, 1938) between the two rankings\nto determine if our generated data can effectively\ndetermine which models are likely to perform well\ndownstream. This is closely related to the concept\nof concurrence between benchmarks (Liu et al.,\n2023a); however, we are evaluating whether the\ngenerated and real data rankspecific modelsin the\nsame ordering, rather thanmodeling approaches.\nTable 3 shows the Kendall’sτfor each task, com-\nputed over a set of reasonable models.\n8\nThe gen-\nerated  data  shows  strong  correlation  to  the  true\nperformance on two of the three datasets.\n6  Discussion and Conclusion\nWe  proposePrompt2Model,  a  framework  that\nautomatically constructs task-specific models us-\ning  only  natural  language  prompts.   Our  proof-\nof-concept  experiments  show  that,  despite  us-\ning  a  similar  easy-to-use  interface  as  LLMs,\nPrompt2Modeldelivers small yet accurate mod-\nels and its generated datasets can be used to esti-\nmate real-world performance.  Besides our refer-\nence implementation providing a ready-to-use tool,\nPrompt2Model’s extensible design and modular\nimplementation makes it a platform for advanc-\ning model distillation, dataset generation, synthetic\nevaluation, dataset retrieval, and model retrieval.\nWe believe ourPrompt2Modelframework can\ninspire various novel research questions. We hope\nthat our platform enables future work that looks\nmore deeply into quality assurance on the generated\ndata and the model. Interesting questions include\nhow much data should we generate for downstream\nmodel training and how diverse should it be? How\ndo we effectively mix the retrieved and generated\ndataset such to achieve complementary strengths\n(e.g.  using dataset generation to focus on the ex-\npected inputs to the model that the retrieved dataset\nfails to cover)? Since users often struggle to articu-\n8\nThis set of models consisted of 5 T5-family models, 2\nBART-family models,  and 1-5 additional retrieved models\nfrom theModel Retriever, depending on task.\nlate their needs up front, future extensions should\nalso address the challenge of human-in-the-loop\ncorrection – either by offering potential strategies\nto help humans iteratively refine prompts, or allow-\ning humans to perform post-hoc fixes when the task\nmetadata extraction and generated data do not align\nwith their intentions. We hope to propose explicit\nchallenges and invite the community to contribute\nnovel implementations of various components in\nour framework.\nLimitations\nOne of the primary limitations of our system is that\nour current experiments have all been conducted\nusing thegpt-3.5-turboAPI (used for prompt\nparsing, dataset generation, and model retrieval).\nThis LLM is paid and closed-source, which makes\nthis  problematic  as  a  scientific  artifact  (Rogers\net al., 2023). Furthermore, the service provider of\nthis LLM, OpenAI, prohibits the use of their API\nto create models that may compete with OpenAI,\ncreating potential legal concerns with the use of\nPrompt2Modelin commercial applications.  We\nare exploring the integration of open-source LLMs\nto avoid our reliance on proprietary APIs.\nAnother limitation of our work is the limited abil-\nity ofPrompt2Modelto support tasks that require\nprocessing languages other than English. While we\nhave shown the limitations of our system at sup-\nporting code generation from Japanese natural lan-\nguage queries, our system is likely to struggle more\nwith lower-resource languages. We use the unpub-\nlishedgpt-3.5-turbomodel for our Dataset Gen-\nerator in our reference implementation. This model\nis believed to be similar to GPT-3 (Brown et al.,\n2020), which was trained on 93% English docu-\nments, 1% German documents, 1% French docu-\nments, and <5% documents in any other language.\nOur use of this model may exacerbate existing dis-\nparities  in  language  technologies  between  high-\nresource languages and low-resource languages.\nOne  potential  limitation  is  that  we  have  only\ntested our approach on 3 tasks, each with a single\ndataset and a single evaluation metric. We justify\nthis decision because our focus is on providing an\nextensible software system rather than establishing\nstate-of-the-art results on many datasets, but we be-\nlieve that our results suggest broader applicability.",
    "Ethics Statement\nAny  system  which  makes  powerful  technology\nmore accessible to the public has ethical implica-\ntions.  Widder et al. (2022) discuss ethical issues\nwith open-source packages in relation to software\nlibraries for deepfaking, including the possibility\nof enabling malicious actors to use technology that\nthey would otherwise not have the technical skills\nto leverage. This is also a risk for an AutoML sys-\ntem such asPrompt2Model; however, we believe\nthis risk is outweighed by the benefits of greater\naccessibility, especially given that a low barrier to\nentry for generating harmful data already exists in\nthe form of prompted, web-interface models.\nWhilePrompt2Modelcould,  if  given  harm-\nful  inputs,  generate  toxic,  offensive,  or  inaccu-\nrate synthetic data, this is no more of a risk with\nPrompt2Modelthan  it  is  with  the  underlying\nprompted model (Bender et al., 2021); indeed, the\nuse of models and supplementary datasets retrieved\nfrom Hugging Face may lessen the likelihood of\na downstream model replicating harms from the\nprompted model’s outputs, though more investiga-\ntion is needed.  Like all ML models, the models\nthatPrompt2Modelreturns can make mistakes,\nand we aim to be transparent in our documentation\nabout potential limitations of the system.\nWe hope thatPrompt2Modelwill be broadly\nuseful.  Our work is motivated by a desire to in-\ncrease the accessibility of NLP models to people\nwho are not in the NLP community but would ben-\nefit from the community’s innovations; particularly,\nto people who would use NLP models downstream\nbut may not have the domain-specific knowledge\nto design their own system.Prompt2Modelmay\nalso prove useful for early NLP researchers by pro-\nviding a starting point for intuitions about base-\nlines for various tasks and enabling the discovery\nof similarities between a described task and exist-\ning work.  We open-sourcePrompt2Modeland\nwelcome community contributions.\nAcknowledgements\nThis work was supported in part by a fellowship\nfrom NEC Research Laboratories. We are grateful\nto Alex Cabrera, Will Epperson, Nelson Liu, Ar-\njun Ramani, Zirui Cheng, Zhiyuan Zeng, Tianci\nXue, Yanchen Liu, Yi-Hsin Hung and Zhilin Yang\nfor their feedback and guidance.  We particularly\nappreciate Zirui Cheng’s video production support\nfor our demo.\nReferences\nAbubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,\nAbdulrahman Alfozan, and James Zou. 2019.  Gradio:\nHassle-free sharing and testing of ML models in the\nwild.arXiv preprint arXiv:1906.02569.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021.   On the\ndangers of stochastic parrots:  Can language mod-\nels be too big?   InProceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY,\nUSA. Association for Computing Machinery.\nTom  Brown,  Benjamin  Mann,  Nick  Ryder,  Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,   Sandhini   Agarwal,   Ariel   Herbert-Voss,\nGretchen  Krueger,  Tom  Henighan,  Rewon  Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz  Litwin,  Scott  Gray,  Benjamin  Chess,  Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage  models  are  few-shot  learners.InAd-\nvances in Neural Information Processing Systems,\nvolume  33,  pages  1877–1901.  Curran  Associates,\nInc.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan,  Johannes  Gehrke,  Eric  Horvitz,  Ece  Kamar,\nPeter  Lee,  Yin  Tat  Lee,  Yuanzhi  Li,  Scott  Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4.arXiv preprint\narXiv:2303.12712.\nNitay Calderon, Subhabrata Mukherjee, Roi Reichart,\nand Amir Kantor. 2023. A systematic study of knowl-\nedge distillation for natural language generation with\npseudo-target training.  InProceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 14632–\n14659, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán,  Edouard  Grave,  Myle  Ott,  Luke  Zettle-\nmoyer, and Veselin Stoyanov. 2020.  Unsupervised\ncross-lingual representation learning at scale. InPro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451,  Online. Association for Computational Lin-\nguistics.\nMichael Färber and Ann-Kathrin Leisinger. 2021. Rec-\nommending datasets for scientific problem descrip-\ntions. InCIKM, pages 3014–3018.\nLuciano Floridi and Massimo Chiriatti. 2020.  Gpt-3:\nIts nature, scope, limits, and consequences.Minds\nand Machines, 30:681–694.",
    "Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2023.  Precise zero-shot dense retrieval without rel-\nevance labels.   InProceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1762–1777,\nToronto, Canada. Association for Computational Lin-\nguistics.\nArnav Gudibande, Eric Wallace, Charles Burton Snell,\nXinyang Geng, Hao Liu, P. Abbeel, Sergey Levine,\nand Dawn Song. 2023.  The false promise of imitating\nproprietary llms.ArXiv, abs/2305.15717.\nXingwei He, Zheng-Wen Lin, Yeyun Gong, Alex Jin,\nHang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan\nDuan, and Weizhu Chen. 2023.  Annollm: Making\nlarge  language  models  to  be  better  crowdsourced\nannotators.ArXiv, abs/2303.16854.\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason\nWeston. 2016.   The goldilocks principle:  Reading\nchildren’s books with explicit memory representa-\ntions.\nEllen  Jiang,  Kristen  Olson,  Edwin  Toh,  Alejandra\nMolina, Aaron Donsbach, Michael Terry, and Carrie J\nCai. 2022. Promptmaker: Prompt-based prototyping\nwith large language models. InCHI Conference on\nHuman Factors in Computing Systems Extended Ab-\nstracts, pages 1–8.\nMaurice  G  Kendall.  1938.   A  new  measure  of  rank\ncorrelation.Biometrika, 30(1/2):81–93.\nJing Yu Koh. 2020. Model zoo.URL http://modelzoo.\nco.\nQuentin Lhoest,  Albert Villanova del Moral,  Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis  Tunstall,  Joe  Davison,  Mario  Šaško,  Gun-\njan Chhablani,  Bhavitvya Malik,  Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain  Gugger,  Clément  Delangue,  Théo  Matus-\nsière,  Lysandre  Debut,  Stas  Bekman,  Pierric  Cis-\ntac,  Thibault Goehringer,  Victor Mustar,  François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. InProceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nNelson F. Liu, Tony Lee, Robin Jia, and Percy Liang.\n2023a.  Do question answering modeling improve-\nments  hold  across  benchmarks?InProceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13186–13218, Toronto, Canada. Association\nfor Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023b.  Pre-\ntrain, prompt, and predict:  A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?  InProceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nGraham Neubig and Zhiwei He. 2023. Zeno GPT Ma-\nchine Translation Report.\nAndrei  Paleyes,  Raoul-Gabriel  Urma,  and  Neil  D\nLawrence. 2022. Challenges in deploying machine\nlearning: a survey of case studies.ACM Computing\nSurveys, 55(6):1–29.\nGunho Park, Baeseong Park, Minsub Kim, Sungjae Lee,\nJeonghoon Kim, Beomseok Kwon, Se Jung Kwon,\nByeongwook Kim, Youngjoo Lee, and Dongsoo Lee.\n2022.  Lut-gemm: Quantized matrix multiplication\nbased on luts for efficient inference in large-scale\ngenerative language models.\nMaja Popovi\n ́\nc. 2015.  chrF: character n-gram F-score\nfor automatic MT evaluation. InProceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395,  Lisbon,  Portugal. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.J. Mach. Learn. Res., 21(1).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text.  InProceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models:  Beyond the\nfew-shot paradigm.   InExtended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA ’21, New York, NY, USA.\nAssociation for Computing Machinery.\nStephen   E.   Robertson,   Steve   Walker,   Micheline\nHancock-Beaulieu,  Mike  Gatford,  and  A.  Payne.\n1995. Okapi at trec-4. InText Retrieval Conference.\nAnna Rogers,  Niranjan Balasubramanian,  Leon Der-\nczynski, Jesse Dodge, Alexander Koller, Sasha Luc-\ncioni, Maarten Sap, Roy Schwartz, Noah A Smith,\nand Emma Strubell. 2023.  Closed ai models make\nbad baselines.\nEmre Sezgin, Joseph Sirrianni, and Simon L. Linwood.\n2022. Operationalizing and implementing pretrained",
    "large ai linguistic models in the united states health-\ncare system: An outlook of gpt-3 as a service.JMIR\nMedical Informatics, 10(2).\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-\nMT  —  Building  open  translation  services  for  the\nWorld.  InProceedings of the 22nd Annual Confer-\nenec of the European Association for Machine Trans-\nlation (EAMT), Lisbon, Portugal.\nVijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei\nLiu, and Graham Neubig. 2023. DataFinder: Scien-\ntific dataset recommendation from natural language\ndescriptions. InProceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 10288–10303,\nToronto, Canada. Association for Computational Lin-\nguistics.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021a.  Want to reduce la-\nbeling cost?  GPT-3 can help.   InFindings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4195–4205, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021b.   Codet5:  Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. InEMNLP.\nZhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F.\nXu, and Graham Neubig. 2023. MCoNaLa: A bench-\nmark for code generation from multiple natural lan-\nguages.  InFindings of the Association for Compu-\ntational Linguistics:  EACL 2023,  pages 265–273,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nDavid Gray Widder, Dawn Nafus, Laura Dabbish, and\nJames Herbsleb. 2022.  Limits and possibilities for\n“ethical ai” in open source: A study of deepfakes. In\nProceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’22,\npage 2035–2046, New York, NY, USA. Association\nfor Computing Machinery.\nThomas  Wolf,  Lysandre  Debut,  Victor  Sanh,  Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020.  Trans-\nformers: State-of-the-art natural language processing.\nInProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing:  System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nSherry Wu, Hua Shen, Daniel S Weld, Jeffrey Heer, and\nMarco Tulio Ribeiro. 2023. Scattershot: Interactive\nin-context example curation for text transformation.\nInProceedings of the 28th International Conference\non Intelligent User Interfaces, IUI ’23, page 353–367,\nNew York,  NY, USA. Association for Computing\nMachinery.\nTianyi  Zhang,  Varsha  Kishore,  Felix  Wu,  Kilian  Q.\nWeinberger, and Yoav Artzi. 2019. Bertscore: Evalu-\nating text generation with bert."
  ]
}