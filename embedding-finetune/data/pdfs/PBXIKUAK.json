{
  "key": "PBXIKUAK",
  "url": "http://arxiv.org/pdf/2404.14219",
  "metadata": {
    "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\n  Phone",
    "abstract": "  We introduce phi-3-mini, a 3.8 billion parameter language model trained on\n3.3 trillion tokens, whose overall performance, as measured by both academic\nbenchmarks and internal testing, rivals that of models such as Mixtral 8x7B and\nGPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite\nbeing small enough to be deployed on a phone. The innovation lies entirely in\nour dataset for training, a scaled-up version of the one used for phi-2,\ncomposed of heavily filtered web data and synthetic data. The model is also\nfurther aligned for robustness, safety, and chat format. We also provide some\ninitial parameter-scaling results with a 7B and 14B models trained for 4.8T\ntokens, called phi-3-small and phi-3-medium, both significantly more capable\nthan phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on\nMT-bench).\n",
    "published": "2024-04-22T14:32:33Z"
  },
  "text": [
    "Phi-3 Technical Report:\nA Highly Capable Language Model Locally on Your Phone\nMicrosoft\nAbstract\nWe introducephi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens,\nwhose overall performance,  as measured by both academic benchmarks and internal testing,  rivals\nthat of models such as Mixtral 8x7B and GPT-3.5 (e.g.,phi-3-miniachieves 69% on MMLU and 8.38\non MT-bench), despite being small enough to be deployed on a phone.  The innovation lies entirely in\nour dataset for training, a scaled-up version of the one used forphi-2, composed of heavily filtered\nweb data and synthetic data.  The model is also further aligned for robustness, safety, and chat format.\nWe also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T\ntokens,  calledphi-3-smallandphi-3-medium,  both significantly more capable thanphi-3-mini\n(e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).\n1    Introduction\nThe  striking  progress  of  AI  in  the  last  few  years  can  be  largely  attributed  to  major  efforts  through-\nout the world towardsscaling-upto ever-larger models and datasets.  Large Language Models (LLMs)\nhave steadily increased in size from a mere billion parameters just five years ago (GPT-2 had 1.5 bil-\nlion parameters [RWC\n+\n19]) to trillion parameters today.  The impetus for this effort originates in the\nseemingly  predictable  improvement  one  obtains  by  training  large  models,  the  so-calledscaling laws\n[KMH\n+\n20,  HBM\n+\n22,  MRB\n+\n23].   However  these  laws  assume  a  “fixed”  data  source.   This  assumption\nis now significantly disrupted by the existence of frontier LLMs themselves, which allow us to interact\nwith data in novel ways.  In our previous works on the phi models [GZA\n+\n23, LBE\n+\n23, JBA\n+\n23] it was\nshown that a combination of LLM-based filtering of web data, and LLM-created synthetic data, enable\nperformance in smaller language models that were typically seen only in much larger models.  For exam-\nple our previous model trained on this data recipe,phi-2(2.7B parameters), matched the performance\nof models 25 times larger trained on regular data.  In this report we present a new model,phi-3-mini\n(3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets used in\nphi-2.  With its small size,phi-3-minican easily be inferenced locally on a modern phone (see Figure\n1), yet it achieves a quality that seems on-par with models such as Mixtral 8x7B [JSR\n+\n24] and GPT-3.5.\nUser:Explain why it is surprising that one can build a language model small enough to\nfit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence.\nphi-3-mini:It’s like fitting a supercomputer in a flip phone, but instead of breaking\nthe phone, it just breaks the internet with its tiny, yet mighty, linguistic prowess!\nUser:Okay now more serious answer, and note that this was achieved solely by changing\nthe training data.\nphi-3-mini:The development of a compact language model that rivals the capabilities\nof ChatGPT, while fitting on a phone, is a testament to the power of data-driven\nmachine learning.  By meticulously curating and optimizing the training dataset,\nresearchers can significantly reduce the model’s size without compromising its\nperformance.  [...]\n1\narXiv:2404.14219v2  [cs.CL]  23 Apr 2024",
    "2    Technical Specifications\nThephi-3-minimodel is a transformer decoder architecture [VSP\n+\n17], with default context length 4K.\nWe also introduce a long context version via LongRope [DZZ\n+\n24] that extends the context length to\n128K, calledphi-3-mini-128K.\nTo best benefit the open source community,phi-3-miniis built upon a similar block structure as\nLlama-2  [TLI\n+\n23]  and  uses  the  same  tokenizer  with  vocabulary  size  of  32064\n1\n.   This  means  that  all\npackages developed for Llama-2 family of models can be directly adapted tophi-3-mini.  The model\nuses 3072 hidden dimension, 32 heads and 32 layers.  We trained using bfloat16 for a total of 3.3T tokens.\nThe model is already chat-finetuned, and the chat template is as follows:\n<|user|>/n Question <|end|>/n <|assistant|>\nThephi-3-smallmodel (7B parameters) leverages the tiktoken tokenizer (for better multilingual to-\nkenization) with a vocabulary size of 100352 and has default context length 8K.  It follows the standard\ndecoder architecture of a 7B model class, having 32 layers and a hidden size of 4096.  To minimize KV\ncache footprint, the model also leverages a grouped-query attention, with 4 queries sharing 1 key.  More-\noverphi-3-smalluses alternative layers of dense attention and a novel blocksparse attention to further\noptimize on KV cache savings while maintaining long context retrieval performance.  An additional 10%\nmultilingual data was also used for this model.\nHighly capable language model running locally on a cell-phone.Thanks to its small size,phi-\n3-minican be quantized to 4-bits so that it only occupies≈1.8GB of memory.  We tested the quantized\nmodel by deployingphi-3-minion iPhone 14 with A16 Bionic chip running natively on-device and fully\noffline achieving more than 12 tokens per second.\nTraining Methodology.We  follow  the  sequence  of  works  initiated  in  “Textbooks  Are  All  You\nNeed” [GZA\n+\n23], which utilize high quality training data to improve the performance of small language\nmodels and deviate from the standardscaling-laws.  In this work we show that such method allows to\nreach the level of highly capable models such as GPT-3.5 or Mixtral with only 3.8B total parameters\n(while Mixtral has 45B total parameters for example).  Our training data of consists of heavily filtered\nweb data (according to the “educational level”) from various open internet sources, as well as synthetic\nLLM-generated data.  Pre-training is performed in two disjoint and sequential phases; phase-1 comprises\nmostly  of  web  sources  aimed  at  teaching  the  model  general  knowledge  and  language  understanding.\nPhase-2 merges even more heavily filtered webdata (a subset used in Phase-1) with some synthetic data\nthat teach the model logical reasoning and various niche skills.\nData Optimal Regime.Unlike prior works that train language models in either “compute optimal\nregime”  [HBM\n+\n22]  or  “over-train  regime”,  we  mainly  focus  on  the  quality  of  data  for  agiven scale.\n2\nWe try to calibrate the training data to be closer to the “data optimal” regime for small models.  In\nparticular, we filter the web data to contain the correct level of “knowledge” and keep more web pages\nthat could potentially improve the “reasoning ability” for the model.  As an example,  the result of a\ngame in premier league in a particular day might be good training data for frontier models, but we need\nto remove such information to leave more model capacity for “reasoning” for the mini size models.  We\ncompare our approach with Llama-2 in Figure 2.\n1\nWe remove BoS tokens and add some additional tokens for chat template.\n2\nJust like for “compute optimal regime”, we use the term “optimal” in an aspirational sense for “data optimal regime”.\nWe are not implying that we actually found the provably “optimal” data mixture for a given scale.\n2",
    "Figure 1:  4-bit quantizedphi-3-minirunning natively on an iPhone with A16 Bionic chip, generating over 12\ntokens per second.\nFigure 2:  Scaling law close to the “Data Optimal Regime” (from left to right:  phi-1.5, phi-2, phi-3-mini, phi-3-\nsmall) versus Llama-2 family of models (7B, 13B, 34B, 70B) that were trained on the same fixed data.  We plot\nthe log of MMLU error versus the log of model size.\n3",
    "To test our data on larger size of models,  we also trainedphi-3-medium,  a model with 14B pa-\nrameters using the same tokenizer and architecture ofphi-3-mini, and trained on the same data for\nslightly more epochs (4.8T tokens total as forphi-3-small).  The model has 40 heads and 40 layers,\nwith embedding dimension 5120.  We observe that some benchmarks improve much less from 7B to 14B\nthan they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in\nthe “data optimal regime” for 14B parameters model.  We are still actively investigating some of those\nbenchmarks (including a regression on HumanEval), hence the numbers forphi-3-mediumshould be\nconsidered as a “preview”.\nPost-training.Post-training ofphi-3-miniwent through two stages, including supervised finetuning\n(SFT) and direct preference optimization (DPO). SFT leverages highly curated high-quality data across\ndiverse  domains,  e.g.,  math,  coding,  reasoning,  conversation,  model  identity,  and  safety.   The  SFT\ndata mix starts with using English-only examples.  DPO data covers chat format data, reasoning, and\nresponsible AI (RAI) efforts.  We use DPO to steer the model away from unwanted behavior, by using\nthose outputs as “rejected” responses.  Besides improvement in math, coding, reasoning, robustness, and\nsafety, post-training transforms a language model to an AI assistant that users can efficiently and safely\ninteract with.\nAs part of the post-training process, we developed a long context version ofphi-3-miniwith context\nlength limit enlarged to 128K instead of 4K. Across the board, the 128K model quality is on par with\nthe 4K length version, while being able to handle long context tasks.  Long context extension has been\ndone in two stages, including long context mid-training and long-short mixed post-training with both\nSFT and DPO.\n3    Academic benchmarks\nOn the next page we report the results forphi-3-minion standard open-source benchmarks measuring\nthe model’s reasoning ability (both common sense reasoning and logical reasoning).  We compare to phi-2\n[JBA\n+\n23], Mistral-7b-v0.1 [JSM\n+\n23], Mixtral-8x7b [JSR\n+\n24], Gemma 7B [TMH\n+\n24], Llama-3-instruct-\n8b [AI23], and GPT-3.5.  All the reported numbers are produced with the exact same pipeline to ensure\nthat the numbers are comparable.  These numbers might differ from other published numbers due to\nslightly different choices in the evaluation.  As is now standard,  we use few-shot prompts to evaluate\nthe models, at temperature 0.  The prompts and number of shots are part of a Microsoft internal tool\nto  evaluate  language  models,  and  in  particular  we  did  no  optimization  to  the  pipeline  for  thephi-3\nmodels.\n3\nThe number ofk–shot examples is listed per-benchmark.  An example of a 2-shot prompt is\ndescribed in Appendix A.\n3\nFor example, we found that using ## before the Question can lead to a noticeable improvement tophi-3-mini’s\nresults across many benchmarks, but we did not do such changes in the prompts.\n4",
    "Phi-3-mini\n3.8b\nPhi-3-small\n7b (preview)\nPhi-3-medium\n14b (preview)\nPhi-2\n2.7b\nMistral\n7b\nGemma\n7b\nLlama-3-In\n8b\nMixtral\n8x7b\nGPT-3.5\nversion 1106\nMMLU\n(5-Shot) [HBK\n+\n21]\n68.875.378.256.361.763.666.068.471.4\nHellaSwag\n(5-Shot) [ZHB\n+\n19]\n76.778.783.053.658.549.869.570.478.8\nANLI\n(7-Shot) [NWD\n+\n20]\n52.855.058.742.547.148.754.855.258.1\nGSM-8K\n(0-Shot; CoT) [CKB\n+\n21]\n82.588.990.361.146.459.877.464.778.1\nMedQA\n(2-Shot) [JPO\n+\n20]\n53.858.269.440.949.650.058.962.263.4\nAGIEval\n(0-Shot) [ZCG\n+\n23]\n37.545.048.429.835.142.142.045.248.4\nTriviaQA\n(5-Shot) [JCWZ17]\n64.059.175.645.272.375.273.682.285.8\nArc-C\n(10-Shot) [CCE\n+\n18]\n84.990.791.075.978.678.380.587.387.4\nArc-E\n(10-Shot) [CCE\n+\n18]\n94.697.197.888.590.691.492.395.696.3\nPIQA\n(5-Shot) [BZGC19]\n84.287.887.760.277.778.177.186.086.6\nSociQA\n(5-Shot) [BZGC19]\n76.679.080.268.374.665.573.275.968.3\nBigBench-Hard\n(0-Shot) [SRR\n+\n22, SSS\n+\n22]\n71.775.081.359.457.359.668.969.768.32\nWinoGrande\n(5-Shot) [SLBBC19]\n70.882.581.454.754.255.658.062.068.8\nOpenBookQA\n(10-Shot) [MCKS18]\n83.288.487.273.679.878.681.685.886.0\nBoolQ\n(0-Shot) [CLC\n+\n19]\n77.282.986.6–72.266.078.377.679.1\nCommonSenseQA\n(10-Shot) [THLB19]\n80.280.382.669.372.676.273.678.179.6\nTruthfulQA\n(10-Shot) [LHE22]\n65.068.775.7–52.153.062.060.185.8\nHumanEval\n(0-Shot) [CTJ\n+\n21]\n59.159.155.547.028.034.160.437.862.2\nMBPP\n(3-Shot) [AON\n+\n21]\n70.071.474.560.650.851.565.360.277.8\nAverage71.274.978.2–61.062.068.069.975.3\nGPQA\n(2-Shot; CoT) [RHS\n+\n23]\n32.834.3––––––29.0\nMT Bench\n(2 round ave.) [ZCS\n+\n23]\n8.388.708.91–––––8.35\n4    Safety\nPhi-3-miniwas developed in accordance with Microsoft’s responsible AI principles.  The overall ap-\nproach consisted of safety alignment in post-training, red-teaming, automated testing and evaluations\nacross  dozens  of  RAI  harm  categories.   Helpfulness  and  harmlessness  preference  datasets  [BJN\n+\n22,\nJLD\n+\n23] with modifications inspired by [BSA\n+\n24] and multiple in-house generated datasets were lever-\naged to address the RAI harm categories in safety post-training.  An independent red team at Microsoft\niteratively examinedphi-3-minito further identify areas of improvement during the post-training pro-\ncess.  Based on their feedback, we curated additional datasets tailored to address their insights, thereby\nrefining the post-training dataset.  This process resulted in significant decrease of harmful response rates,\n5",
    "Phi-3-Mini-4k\n3.8b\nPhi-3-Mini-128k\n3.8b\nPhi-2\n2.7b\nMistral\n7b\nGemma\n7b\nLlama-3-In\n8b\nUngroundedness0.6030.6371.4810.9350.6790.328\nIntellectual Property (DR-1)23.95%21.50%24.00%56.20%38.33%37.30%\nHarmful Content Continuation (DR-3)0.75%1.08%2.93%2.58%1.28%1.30%\nHarmful Content Summarization (DR-3)10.00%10.20%14.35%22.33%10.33%8.20%\nJailbreak (DR-1)12.29%12.57%15.00%15.57%11.43%13.00%\nTable 1:  Comparison of Microsoft internal multi-turn conversation RAI benchmark results ofphi-3-miniand\nother models.  Note that a lower value indicates a better performance for all metrics in the table.\nas shown in Figure 3.\nFigure 3:  Comparison of harmful response percentages by Microsoft AI Red Team betweenphi-3-minibefore\nand after the safety alignment.  Note that the harmful response percentages in this chart are inflated numbers as\nthe red team tried to inducephi-3-miniin an adversarial way to generate harmful responses through multi-turn\nconversations.\nTable 1 shows the results of in-house RAI benchmarks forphi-3-mini-4kandphi-3-mini-128k\ncompared to phi-2 [JBA\n+\n23], Mistral-7b-v0.1 [JSM\n+\n23], Gemma 7b [TMH\n+\n24], and Llama-3-instruct-8b\n[AI23].  This benchmark utilized GPT-4 to simulate multi-turn conversations in five different categories\nand to evaluate the model responses.  Ungroundedness between 0 (fully grounded) and 4 (not grounded)\nmeasures if the information in a response is based on a given prompt.  In other categories,  responses\nwere evaluated in terms of the severity of harmfulness from 0 (no harm) to 7 (extreme harm) and the\ndefect rates (DR-x) were computed as the percentage of samples with the severity score being greater\nthan or equal tox.\n6",
    "Figure 4:  Left:phi-3-mini’s completion without search.  Right:phi-3-mini’s completion with search, using the\ndefault HuggingFace Chat-UI search ability.\n5    Weakness\nIn terms of LLM capabilities, whilephi-3-minimodel achieves similar level of language understanding\nand reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks.\nThe model simply does not have the capacity to store too much “factual knowledge”, which can be seen\nfor example with low performance on TriviaQA. However, we believe such weakness can be resolved by\naugmentation with a search engine.  We show an example using the HuggingFace default Chat-UI with\nphi-3-miniin Figure 4.  Another weakness related to model’s capacity is that we mostly restricted the\nlanguage  to  English.   Exploring  multilingual  capabilities  for  Small  Language  Models  is  an  important\nnext step, with some initial promising results onphi-3-smallby including more multilingual data.\nDespite our diligent RAI efforts, as with most LLMs, there remains challenges around factual inaccu-\nracies (or hallucinations), reproduction or amplification of biases, inappropriate content generation, and\nsafety issues.  The use of carefully curated training data, and targeted post-training, and improvements\nfrom red-teaming insights significantly mitigates these issues across all dimensions.  However, there is\nsignificant work ahead to fully address these challenges.\n7",
    "References\n[AI23]Meta AI.  Introducing meta llama 3:  The most capable openly available llm to date, 2023.\n[AON\n+\n21]   Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan,  Ellen  Jiang,  Carrie  Cai,  Michael  Terry,  Quoc  Le,  and  Charles  Sutton.   Program\nsynthesis with large language models.arXiv preprint arXiv:2108.07732, 2021.\n[BJN\n+\n22]    Yuntao  Bai,  Andy  Jones,  Kamal  Ndousse,  Amanda  Askell,  Anna  Chen,  Nova  DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kada-\nvath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds,\nDanny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda,\nCatherine Olsson,  Dario Amodei,  Tom Brown,  Jack Clark,  Sam McCandlish,  Chris Olah,\nBen Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement\nlearning from human feedback, 2022.\n[BSA\n+\n24]    Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R ̈ottger, Dan Jurafsky, Tatsunori\nHashimoto, and James Zou. Safety-tuned llamas:  Lessons from improving the safety of large\nlanguage models that follow instructions, 2024.\n[BZGC19]   Yonatan Bisk, Rowan Zellers, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical\ncommonsense in natural language.arXiv preprint arXiv:1911.11641, 2019.\n[CCE\n+\n18]   Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord.  Think you have solved question answering?  try arc, the ai2 reasoning\nchallenge, 2018.\n[CKB\n+\n21]   Karl  Cobbe,  Vineet  Kosaraju,  Mohammad  Bavarian,  Mark  Chen,  Heewoo  Jun,  Lukasz\nKaiser,  Matthias  Plappert,  Jerry  Tworek,  Jacob  Hilton,  Reiichiro  Nakano,  Christopher\nHesse, and John Schulman.  Training verifiers to solve math word problems.arXiv preprint\narXiv:2110.14168, 2021.\n[CLC\n+\n19]   Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova.  Boolq:  Exploring the surprising difficulty of natural yes/no questions.\nInProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 2924–2936, 2019.\n[CTJ\n+\n21]    Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared  Kaplan,  Harri  Edwards,  Yuri  Burda,  Nicholas  Joseph,  Greg  Brockman,  Alex  Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss,  Alex  Nichol,  Alex  Paino,  Nikolas  Tezak,  Jie  Tang,  Igor  Babuschkin,  Suchir  Balaji,\nShantanu  Jain,  William  Saunders,  Christopher  Hesse,  Andrew  N.  Carr,  Jan  Leike,  Josh\nAchiam,  Vedant Misra,  Evan Morikawa,  Alec Radford,  Matthew Knight,  Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba.  Evaluating large language models trained on code,\n2021.\n8",
    "[DZZ\n+\n24]    Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu,\nFan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens,\n2024.\n[GZA\n+\n23]   Suriya Gunasekar,  Yi Zhang,  Jyoti Aneja,  Caio C ́esar Teodoro Mendes,  Allie Del Giorno,\nSivakanth  Gopi,  Mojan  Javaheripi,  Gustavo  de  Rosa  Piero  Kauffmann,  Olli  Saarikivia,\nAdil  Salim,  Shital  Shah,  Harkirat  Singh  Behl,  Xin  Wang,  S ́ebastien  Bubeck,  Ronen  El-\ndan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need.arXiv\npreprint arXiv:2306.11644, 2023.\n[HBK\n+\n21]   Dan  Hendrycks,  Collin  Burns,  Saurav  Kadavath,  Akul  Arora,  Steven  Basart,  Eric  Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH\ndataset, 2021.\n[HBM\n+\n22]  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Eliza Ruther-\nford Trevor Cai, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Au-\nrelia  Guy,  Simon  Osindero,  Karen  Simonyan,  Erich  Elsen,  Jack  W.  Rae,  Oriol  Vinyals,\nand  Laurent  Sifre.Training  compute-optimal  large  language  models.arXiv preprint\narXiv:2203.15556, 2022.\n[JBA\n+\n23]    Mojan    Javaheripi,S ́ebastien    Bubeck,Marah    Abdin,Jyoti    Aneja,Caio    C ́esar\nTeodoro  Mendes,  Weizhu  Chen,  Allie  Del  Giorno,  Ronen  Eldan,  Sivakanth  Gopi,  Suriya\nGunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli\nSaarikivi,  Adil  Salim,  Shital  Shah,  Michael  Santacroce,  Harkirat  Singh  Behl,  Adam  Tau-\nmann Kalai,  Xin Wang,  Rachel Ward,  Philipp Witte,  Cyril Zhang,  and Yi Zhang.  Phi-2:\nThe surprising power of small language models.Microsoft Research Blog, 2023.\n[JCWZ17]  Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer.  Triviaqa:  A large scale\ndistantly supervised challenge dataset for reading comprehension, 2017.\n[JLD\n+\n23]Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang\nSun, Yizhou Wang, and Yaodong Yang.  Beavertails:  Towards improved safety alignment of\nllm via a human-preference dataset, 2023.\n[JPO\n+\n20]    Di Jin,  Eileen Pan,  Nassim Oufattole,  Wei-Hung Weng,  Hanyi Fang,  and Peter Szolovits.\nWhat disease does this patient have?  a large-scale open domain question answering dataset\nfrom medical exams, 2020.\n[JSM\n+\n23]    Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L ́elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth ́ee Lacroix, and William El Sayed.  Mistral 7b, 2023.\n[JSR\n+\n24]Albert  Q.  Jiang,  Alexandre  Sablayrolles,  Antoine  Roux,  Arthur  Mensch,  Blanche  Savary,\nChris Bamford,  Devendra Singh Chaplot,  Diego de las Casas,  Emma Bou Hanna,  Florian\nBressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L ́elio Renard Lavaud, Lu-\ncile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szy-\nmon Antoniak, Teven Le Scao, Th ́eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth ́ee\nLacroix, and William El Sayed.  Mixtral of experts, 2024.\n9",
    "[KMH\n+\n20]  Jared  Kaplan,  Sam  McCandlish,  Tom  Henighan,  Tom  B  Brown,  Benjamin  Chess,  Rewon\nChild,  Scott Gray,  Alec Radford,  Jeffrey Wu,  and Dario Amodei.  Scaling laws for neural\nlanguage models.arXiv preprint arXiv:2001.08361, 2020.\n[LBE\n+\n23]    Yuanzhi  Li,  S ́ebastien  Bubeck,  Ronen  Eldan,  Allie  Del  Giorno,  Suriya  Gunasekar,  and\nYin  Tat  Lee.    Textbooks  are  all  you  need  ii:   phi-1.5  technical  report.arXiv preprint\narXiv:2309.05463, 2023.\n[LHE22]Stephanie Lin, Jacob Hilton, and Owain Evans.  Truthfulqa:  Measuring how models mimic\nhuman falsehoods, 2022.\n[MCKS18]  Todor  Mihaylov,  Peter  Clark,  Tushar  Khot,  and  Ashish  Sabharwal.   Can  a  suit  of  armor\nconduct electricity?  a new dataset for open book question answering, 2018.\n[MRB\n+\n23]  Niklas  Muennighoff,  Alexander  M  Rush,  Boaz  Barak,  Teven  Le  Scao,  Aleksandra  Piktus,\nNouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.  Scaling data-constrained\nlanguage models.arXiv preprint arXiv:2305.16264, 2023.\n[NWD\n+\n20]  Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\nAdversarial nli:  A new benchmark for natural language understanding, 2020.\n[RHS\n+\n23]    David Rein,  Betty Li Hou,  Asa Cooper Stickland,  Jackson Petty,  Richard Yuanzhe Pang,\nJulien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof\nq&a benchmark, 2023.\n[RWC\n+\n19]  Alec  Radford,  Jeffrey Wu,  Rewon Child,  David  Luan,  Dario  Amodei,  and  Ilya  Sutskever.\nLanguage models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\n[SLBBC19]  Keisuke Sakaguchi,  Ronan Le Bras,  Chandra Bhagavatula,  and Yejin Choi.  Winogrande:\nAn adversarial winograd schema challenge at scale.arXiv preprint arXiv:1907.10641, 2019.\n[SRR\n+\n22]    Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al.\nBeyond  the  imitation  game:   Quantifying  and  extrapolating  the  capabilities  of  language\nmodels.arXiv preprint arXiv:2206.04615, 2022.\n[SSS\n+\n22]Mirac Suzgun, Nathan Scales, Nathanael Sch ̈arli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Chal-\nlenging big-bench tasks and whether chain-of-thought can solve them, 2022.\n[THLB19]   Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.  Commonsenseqa:  A\nquestion answering challenge targeting commonsense knowledge, 2019.\n[TLI\n+\n23]Hugo  Touvron,  Thibaut  Lavril,  Gautier  Izacard,  Xavier  Martinet,  Marie-Anne  Lachaux,\nTimoth ́ee  Lacroix,  Baptiste  Rozi`ere,  Naman  Goyal,  Eric  Hambro,  Faisal  Azhar,  Aurelien\nRodriguez,  Armand  Joulin,  Edouard  Grave,  and  Guillaume  Lample.   Llama:   Open  and\nefficient foundation language models.arXiv preprint arXiv:2302.13971, 2023.\n[TMH\n+\n24]  Gemma  Team,  Thomas  Mesnard,  Cassidy  Hardin,  Robert  Dadashi,  Surya  Bhupatiraju,\nShreya  Pathak,  Laurent  Sifre,  Morgane  Rivi`ere,  Mihir  Sanjay  Kale,  Juliette  Love,  et  al.\nGemma:  Open models based on gemini research and technology, 2024.\n10",
    "[VSP\n+\n17]    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n L  ukasz  Kaiser,  and  Illia  Polosukhin.   Attention  is  all  you  need.   InAdvances in Neural\nInformation Processing Systems, volume 30, 2017.\n[ZCG\n+\n23]   Wanjun  Zhong,  Ruixiang  Cui,  Yiduo  Guo,  Yaobo  Liang,  Shuai  Lu,  Yanlin  Wang,  Amin\nSaied, Weizhu Chen, and Nan Duan.  Agieval:  A human-centric benchmark for evaluating\nfoundation models, 2023.\n[ZCS\n+\n23]Lianmin  Zheng,  Wei-Lin  Chiang,  Ying  Sheng,  Siyuan  Zhuang,  Zhanghao  Wu,  Yonghao\nZhuang,  Zi  Lin,  Zhuohan  Li,  Dacheng  Li,  Eric  Xing,  et  al.   Judging  llm-as-a-judge  with\nmt-bench and chatbot arena.arXiv preprint arXiv:2306.05685, 2023.\n[ZHB\n+\n19]   Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.  Hellaswag:  Can\na  machine  really  finish  your  sentence?    InProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 4791–4800, 2019.\nA    Example prompt for benchmarks\nQuestion:\nSolve forx:(−\n1\n3\n)(−4−3x)=\n1\n2\nOptions:\nA.−\n5\n6\nB.\n7\n6\nC.\n5\n3\nD.\n1\n6\nAnswer:  A\nQuestion:\nWhich of the following is the body cavity that contains the pituitary gland?\nOptions:\nA. Abdominal\nB. Cranial\nC. Pleural\nD. Spinal\nAnswer:  B\nQuestion:\nWhere was the most famous site of the mystery cults in Greece?\nOptions:\nA. Ephesus\nB. Corinth\nC. Athens\nD. Eleusis\nAnswer:\n11",
    "B    Authors\nMarah AbdinRussell J. HewettOlatunji Ruwase\nSam Ade JacobsJamie HuynhOlli Saarikivi\nAmmar Ahmad AwanMojan JavaheripiAmin Saied\nJyoti AnejaXin JinAdil Salim\nAhmed AwadallahPiero KauffmannMichael Santacroce\nHany AwadallaNikos KarampatziakisShital Shah\nNguyen BachDongwoo KimNing Shang\nAmit BahreeMahmoud KhademiHiteshi Sharma\nArash BakhtiariLev KurilenkoXia Song\nHarkirat BehlJames R. LeeMasahiro Tanaka\nAlon BenhaimYin Tat LeeXin Wang\nMisha BilenkoYuanzhi LiRachel Ward\nJohan BjorckChen LiangGuanhua Wang\nS ́ebastien BubeckWeishung LiuPhilipp Witte\nMartin CaiEric LinMichael Wyatt\nCaio C ́esar Teodoro MendesZeqi LinJiahang Xu\nWeizhu ChenPiyush MadanCan Xu\nVishrav ChaudharyArindam MitraSonali Yadav\nParul ChopraHardik ModiFan Yang\nAllie Del GiornoBrandon NorickZiyi Yang\nGustavo de RosaAnh NguyenDonghan Yu\nMatthew DixonBarun PatraChengruidong Zhang\nRonen EldanDaniel Perez-BeckerCyril Zhang\nDan IterHeyang QinJianwen Zhang\nAmit GargThomas PortetLi Lyna Zhang\nAbhishek GoswamiReid PryzantYi Zhang\nSuriya GunasekarSambuddha RoyYue Zhang\nEmman HaiderMarko RadmilacYunan Zhang\nJunheng HaoCorby RossetXiren Zhou\n12"
  ]
}