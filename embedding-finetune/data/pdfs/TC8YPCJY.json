{
  "key": "TC8YPCJY",
  "url": "http://arxiv.org/pdf/2307.13528",
  "metadata": {
    "title": "FacTool: Factuality Detection in Generative AI -- A Tool Augmented\n  Framework for Multi-Task and Multi-Domain Scenarios",
    "abstract": "  The emergence of generative pre-trained models has facilitated the synthesis\nof high-quality text, but it has also posed challenges in identifying factual\nerrors in the generated text. In particular: (1) A wider range of tasks now\nface an increasing risk of containing factual errors when handled by generative\nmodels. (2) Generated texts tend to be lengthy and lack a clearly defined\ngranularity for individual facts. (3) There is a scarcity of explicit evidence\navailable during the process of fact checking. With the above challenges in\nmind, in this paper, we propose FacTool, a task and domain agnostic framework\nfor detecting factual errors of texts generated by large language models (e.g.,\nChatGPT). Experiments on four different tasks (knowledge-based QA, code\ngeneration, mathematical reasoning, and scientific literature review) show the\nefficacy of the proposed method. We release the code of FacTool associated with\nChatGPT plugin interface at https://github.com/GAIR-NLP/factool .\n",
    "published": "2023-07-25T14:20:51Z"
  },
  "text": [
    "FACTOOL: Factuality Detection in Generative AI\nA Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios\nI-Chun Chern\n2\nSteffi Chern\n2\nShiqi Chen\n3\nWeizhe Yuan\n4\nKehua Feng\n1\nChunting Zhou\n5\nJunxian He\n6\nGraham Neubig\n2\nPengfei Liu\n1,7∗\n1\nShanghai Jiao Tong University\n2\nCarnegie Mellon University\n3\nCity University of Hong Kong\n4\nNew York University\n5\nMeta AI\n6\nThe Hong Kong University of Science and Technology\n7\nShanghai Artificial Intelligence Laboratory\nAbstract\nThe emergence of generative pre-trained mod-\nels has facilitated the synthesis of high-quality\ntext, but it has also posed challenges in identi-\nfying factual errors in the generated text.  In\nparticular:   (1)  A  wider  range  of  tasks  now\nface an increasing risk of containing factual\nerrors when handled by generative models. (2)\nGenerated texts tend to be lengthy and lack a\nclearly defined granularity for individual facts.\n(3) There is a scarcity of explicit evidence avail-\nable during the process of fact checking.\nWith  the  above  challenges  in  mind,  in  this\npaper, we proposeFACTOOL, a task and do-\nmain agnostic framework for detecting factual\nerrors  of  texts  generated  by  large  language\nmodels (e.g., ChatGPT). Experiments on four\ndifferent  tasks  (knowledge-based  QA,  code\ngeneration, mathematical reasoning, and sci-\nentific literature review) show the efficacy of\nthe  proposed  method.   We  release  the  code\nofFACTOOLassociated  with  ChatGPT  plu-\ngin  interface  athttps://github.com/\nGAIR-NLP/factool.\n1    Introduction\nGenerative artificial intelligence (AI) technology,\nexemplified  by  GPT-4  (OpenAI,  2023)  consoli-\ndates  various  tasks  in  natural  language  process-\ning  into  a  single  sequence  generation  problem.\nThis unified architecture enables users to complete\nmultiple  tasks  (e.g.,  question  answering  (Thop-\npilan et al., 2022), code generation (Chen et al.,\n2021), math problem solving (Lewkowycz et al.,\n2022), and scientific literature generation (Taylor\net  al.,  2022))  through  anatural  language  inter-\nface(Liu et al., 2023) with both unprecedented\nperformance (Bubeck et al., 2023) and interactiv-\nity.\nHowever, at the same time, such agenerative\nparadigmalso introduces some unique challenges.\n∗\nCorresponding author\nClaim\nExtraction\nQuery\nGeneration\nVerification\nSelf-check \nClaims\n1. \n2. \nEvidence\nCollection\nTool \nQuerying \nTools \n0\n100\nFactuality \nEvidence\nFacTool\n!\nTool Queries\na. \nb. \n1\n2\n3\n4\n5\nChatGPT  \nResponse\nPrompt\n!\nFacTool\nFigure 1: Tool-augmented framework for factuality de-\ntection.\nContent  that  is  automatically  generated  can  of-\nten  exhibit  inaccuracies  or  deviations  from  the\ntruth due to the limited capacity of large language\nmodels (LLMs) (Ji et al., 2023; Schulman, 2023).\nLLMs are susceptible to producing content that\nappears credible but may actually be factually in-\ncorrect or imprecise.  This limitation restricts the\napplication of generative AI in some high-stakes ar-\neas, such as healthcare, finance, and law. Therefore,\nit is crucial to identify these errors systematically\nto improve the usefulness and reliability of the gen-\nerated content.\nCurrent literature on detecting and mitigating\nfactual errors generated by machine learning mod-\nels focuses predominantly on a single specific task,\nfor example, retrieval-augmented verification mod-\nels for QA (Lewis et al., 2020), hallucination detec-\ntion models for text summarization (Fabbri et al.,\n2022),  and  execution-based  evaluation  for  code\n(Shi et al., 2022). While these methods have proven\nsuccessful within their respective areas, given the\nremarkableversatilityof tasks and domains han-\ndled by LLMs, we argue that it is also important\narXiv:2307.13528v2  [cs.CL]  26 Jul 2023",
    "Methods\nResponseClaimEvidenceScenario\nLengthGenerated byGranularityProvidedProvidedDomainTask\nFEVER-based7.30HumanFact✓XWikipediaFact Verification\nFactCC20.83SyntheticSentence✓✓NewswireSumm. Factuality\nQAGS-based16.11ModelSummary✓✓NewswireSumm. Factuality\nWICE-based24.20HumanFact✓✓WikipediaEntailment\nRARR-PaLM/LaMDAFactXXWikipediaQA\nFACTOOL\n41.80ChatGPTFactXXWikipediaQA\n30.37ChatGPTSnippetXXPythonCode generation\n67.13ChatGPTStatementXXMathMath Problems\n76.34ChatGPTTupleXXSci. textSci. Review\nTable 1: A comparison of published approaches for factuality detection in terms of generated responses and claims\nto be verified based on collected evidence. “Scenario” represents which task and domain the corresponding approach\nhas been justified. “Sci.” represents “Scientific”.\nto have a more comprehensive factuality detection\nand verification framework that is similarly versa-\ntile.\nAdditionally, in the current literature, the task\nof factuality detection is usually simplified as ei-\nther (i) given a claim, determining whether it is\nfactually correct, (ii) or given evidence, determin-\ning whether the generated claim is supported. This\ntask definition is not well suited to writing tasks\nthat users commonly engage with when interacting\nwith generative models (e.g., ChatGPT), where we\noften need to validate the factuality of a long-form\ngenerationwithoutexplicit claims and evidence.\nIn this paper,  we propose a task and domain-\nagnostic framework,FACTOOL, which aims to de-\ntect factual errors in LLM-generated texts. We il-\nlustrate our framework in Fig. 1, where we connect\nthe concept of “tool use” (Thoppilan et al., 2022;\nGao et al., 2022b; Schick et al., 2023) with “fac-\ntuality detection” and demonstrate that the ability\nto use tools in LLMs is crucial for factuality de-\ntection. Specifically,FACTOOLleverages various\ntools, including Google Search, Google Scholar,\ncode  interpreters,  Python,  or  even  LLMs  them-\nselves, to gather evidence about the factuality of\nthe generated content.  Moreover, our framework\nemploys the reasoning abilities of LLMs to assess\nthe factuality of the content, given the evidence that\nhas been gathered. We develop a benchmark and\nperform experiments across four tasks: knowledge-\nbased QA, code generation, math problem solving,\nand scientific literature review writing.\nIn summary, our contributions are:\n•We revisit the task of factuality detection and\nextend it in a way that allows for a better audit\nof current generative AI models.\n•We connect the concept of “tool use” with “fac-\ntuality detection”, developing a unified and ver-\nsatile framework for factuality detection across a\nvariety of domains and tasks.\n•We useFACTOOLto evaluate the factuality of\nmodern chatbots, and found that GPT-4 has the\nbest factuality across almost all scenarios.  Su-\npervisely fine-tuned chatbots (Vicuna-13B) have\nreasonably good factuality in KB-based QA but\nperform poorly in more challenging scenarios, in-\ncluding code generation, math problem solving,\nand scientific literature review writing.\n2    Related Work\nFactuality Detection in Natural Language Pro-\ncessingFactuality detection was a topic of rig-\norous study even before the advent of generative\nAI. Existing works can be organized by their dif-\nferences  in  terms  of  the  “response”  to  be  veri-\nfied, the “claim” extracted from the response, and\nsupporting “evidence”.   As illustrated in Tab. 1,\nthe creation of the FEVER dataset (Thorne et al.,\n2018a) spawned models (Zhong et al., 2020; Kr-\nishna et al., 2022) that determine whether a given\nfine-grained claim made based on Wikipedia\n1\narti-\ncles is correct. In this task setting, both the claim\nand related evidence are given. FactCC (Kryscin-\nski et al., 2020) and QAGS-based models  (Wang\net al., 2020) adopted different task formulations\nto  detectfactual  consistency,  i.e.,  given  the  ev-\nidence  text,  and  the  goal  is  to  determine  if  the\ngenerated summaries or summary sentences are\nfactually consistent with the given text.   WICE-\nbased methods (Kamoi et al., 2023) decide if a\nfact from a Wikipedia sentence could be supported\n1\nhttps://www.wikipedia.org/",
    "by provided evidence. RARR (Gao et al., 2022a)\nproposed a new approach by directly prompting\nLLMs to generate queries, retrieve evidence and\ndetermine factuality.\nExisting works typically rely on either a given\nclaim or given evidence and target a specific use\ncase. However, in this paper, we introduce a more\nchallenging yet practical task setting, i.e., factuality\ndetection without explicit claims or evidence, and\npropose a framework capable of addressing this\nchallenge in a variety of scenarios.\nTool use in Large Pretrained Language Models\nLanguage models store limited knowledge within\ntheir parameters. To overcome this limitation, vari-\nous tools have been introduced to assist language\nmodels in order to further expand their capabili-\nties. For example, Press et al. (2022); Komeili et al.\n(2022) gathered information from the Internet to\nenhance question answering and dialog systems,\nrespectively. Schick et al. (2023) trained a model\ncapable of interacting with five tools including a\ncalculator, a translation system, etc. Recently, Shen\net al. (2023) introduced a framework that employs\nLLMs to connect various AI models from the ma-\nchine learning communities to tackle AI tasks. Fur-\nthermore, Liang et al. (2023) proposed a new AI\necosystem that connects LLMs with millions of\nexisting APIs to accomplish tasks. In this work, we\nexplore tool use in LLMs for the task of factuality\ndetection.\n3    Revisiting Factuality in Generative AI\n3.1    Definition\nVersatile FactualityIn most previous works, fac-\ntuality has been defined as whether a claim in a\ntext can be supported by evidence from a separate,\ntrustworthy knowledge base, with applications in\nfact-checking (Thorne et al., 2018b) (where the\nknowledge base is a large source like Wikipedia)\nand summarization (Kryscinski et al., 2020) (where\nthe knowledge base is an input document or doc-\numents).  In this paper, we extend this definition\nto whether the claims made ingenerated signals\n(which could be text,  code,  or mathematical ex-\npressions and so on) can be supported byevidence\nunder specific rules. Specifically, these rules can\nrange from consistency with a knowledge base de-\nrived from Wikipedia, to a verification rule spec-\nified  within  a  Python  library,  or  an  operational\nrule derived from mathematics.  By adopting this\nbroader definition, we are able to establish a uni-\nfied framework for addressing factuality issues in\ngenerative AI beyond just the textual domain.\nFine-grained FactualityOne can usually detect\nthe factuality of a given generated signal (e.g., text)\nat different levels of granularity, such as sentences,\nand documents. A more granular assessment can\nbe  particularly  valuable  because  it  (1)  not  only\nallows  users  to  pinpoint  where  inaccuracies  oc-\ncur (Liu et al., 2021) but also (2) serves as a reward\nmodel for developers to refine their generative sys-\ntems (Lightman et al., 2023).\nHowever, implementing fine-grained factuality\ndetection is challenging due to two reasons:  (1)\nspecifying the desired granularity level without am-\nbiguity, and (2) extracting claims in line with the\npredetermined granularity level. In this paper, we\nargue  that  by  utilizing  the  powerfulinstruction-\nfollowing abilityand thenatural language inter-\nfaceof LLMs, we can effectively address the chal-\nlenge of defining and extracting fine-grained claims\nthrough claim definition-based few-shot prompting.\nMore details can be found in §4.1.\nStructurally speaking, given a prompt (e.g., a\nquery or instruction) and the corresponding model-\ngenerated response, the fine-grained factuality de-\ntection task involves the following concepts:\nPrompt (p)a query or instruction that users pro-\nvide to the generative model.\nResponse (r)a piece of text (usually in long form)\ngenerated by the generative model.\nClaim (c)a statement inferred from the model re-\nsponse, whose granularity is defined by a natural\nlanguage text.\nEvidence  (e)The  available  information  (e.g.,\nknowledge base,  pre-defined rules) that support\nor demonstrate the truth or validity of a claim.\n3.2    Instantiations in Different Scenarios\nUsing the above task definition, we can define fac-\ntuality in different application scenarios (see also\nin Tab.2).\nKnowledge-based QAKnowledge-based (KB)\nQA (Chen et al., 2017) aims to answer questions\nusing a given knowledge base or open-domain data\nsource (e.g., Wikipedia).  In this task, we define\nfactuality as how well each claim in the generated\nanswer is supported by world knowledge. In this\npaper, we consider a more challenging scenario:\nopen-domain QA that requires long-form answers,\nrather than short ones.",
    "TasksPrompt (p)Response (r)Claim (c)Evidence (e)\nKB-based QAQuestionLong-form answerAtomic component unitWeb searched results\nCode GenerationCode QueryExecutable codeCode snippetPython library\nMath ProblemsMath problemsMath solutionMath calculationCalculator\nSci. Lit ReviewScientific questionLong-form reviewTuple (paper title, year, authors)Google scholar\nTable 2: Factuality definition in different tasks. “Sci. Lit Review” represents scientific literature review.\nCode GenerationThe code generation task (Yin\nand Neubig, 2017) involves generating executable\ncode based on a given query.  We define factual-\nity in code generation as how well the generated\ncode, as a whole, can be executed correctly within a\nspecific programming language (e.g., Python) and\nfulfills the provided requirements. This definition\nis grounded in an execution-based approach to code\nevaluation, which measures the correctness of gen-\nerated code by executing it against some test case\ninputs and comparing its output to the expected\noutput.\nMath Problem SolvingThe math problem solv-\ning task involves the use of automated methods\nto address mathematical problems (Cobbe et al.,\n2021). At the claim level, factuality in math prob-\nlem solving is defined as the extent to which the\ngenerated statements adhere to the calculation rules.\nAt the response level, factuality in math problem\nsolving is defined as how effectively the overall\nmathematical solution addresses the given prob-\nlem.\nScientific Literature Review WritingThe scien-\ntific literature review writing task (Jha et al., 2015)\naims to analyze and synthesize existing research on\na specific topic in a field of study. In this task, we\ndefine factuality as whether the generated scientific\nliterature review correctly cites existing scientific\nliterature, including the correct mention of authors\nand publication years.\n2\n4    Approach\nWe propose a tool-augmented framework for detect-\ning factual errors that can apply a unified approach\nacross various tasks. The motivation for using tools\nis twofold.  On one hand, each tool embodies the\ndomain expertise, assisting us in the effective gath-\nering of evidence that verifies the correctness of\n2\nIn this paper, our focus lies in examining the consistency\nof the relationship between the paper title, authors, and publi-\ncation year. However, the task of determining the suitability\nof the cited paper as the most appropriate choice is left for\nfuture investigation.\nthe claim. On the other hand, the ability of LLMs\nto utilize multiple tools paves the way formultiple\ntool-augmented factuality detection. For example,\nby directly using ChatGPT plugins,\n3\nwe can inte-\ngrate multiple tools into a chatbot.\nThe framework is illustrated in Fig. 1,  which\nconsists of five main components:claim extraction,\nquery generation,tool querying,evidence collec-\ntion, andagreement verification. We elaborate each\ncomponent below.\n4.1    Claim Extraction\nExtracting claims from responses under various\ntask settings is challenging due to the inconsistent\ndefinitions of claims across tasks and scenarios.\nThis inconsistency hinders the development of ap-\nplications such as text summarization evaluation\nand factuality detection. To tackle this, we propose\nan approach in this paper that treats claim extrac-\ntion as a process guided by LLM prompts based\non the specific definition of claims. This approach\noffers the following advantages:\n(i) Leveraging the strong instruction-following\ncapabilities of LLMs can significantly reduce the\ncosts associated with data annotation and model\ntraining for claim extraction.\n(ii) When developing a system or constructing\na dataset for an application that relies on the def-\ninition of claims, one simply needs to provide a\ntextual definition of the claim using a large model.\nThis enables future researchers to effectively utilize\nthese definitions as a foundation in their work.\n(iii) Our experiments demonstrate that the claim\nextraction module, implemented by ChatGPT, ex-\nhibits  strong  performance  in  extracting  claims\n(atomic component units). The detailed results of\nthese experiments are discussed in Section 6.1.\nHere, we employ ChatGPT as a base LLM and\napply different textual definitions of claims across\nfour  tasks.   Our  goal  is  to  extract  all  verifiable\nclaims  within  the  generated  textx,  denoted  as\n3\nhttps://openai.com/blog/\nchatgpt-plugins",
    "Who is the CEO of Twitter?\nPrompt\nThe CEO of Twitter at the time \nof  writing  this  answer  is  Jack \nDorsey. He co-founded Twitter \nin 2006 ......\nLLM Response\nClaim\nExtraction\nQuery  \nGeneration\nClaims\n(1) Jack Dorsey is the CEO of Twitter\n(2) Jack Dorsey co-founded Twitter in 2006\n......\nQueries\n(1.1)  Is  Jack  Dorsey  the  CEO \nof Twitter?\n(1.2)  Who  is  the  current  CEO \nof Twitter?\n(2.1) Did Jack Dorsey co-found \nTwitter in 2006?\n......\nEvidence\n(1.2) Former NBC Universal advertising \nchief Linda Yaccarino will become...\n(1.1) Noah Glass, Evan Williams, and Biz \nStone co-founded Odeo......\n......\nClaim-level Factuality: [0, 1, ...] \nResponse-level Factuality: 0\nScores\nReturn a string containing space-delimited \nnumbers starting from 0 up to n inclusive.\nPrompt\nLLM Response\ndef string_sequence(n): \nresult = \"\"   \nfor i in range(n+1):      \nresult += str(i) + \" \"     \nreturn result.strip()\nClaim\nExtraction\nMarie  ordered  5  packs  of  milk  that  costs \n$3  each,  and  some  boxes  of  pizza.  Marie \npaid  a  total  of  $45.  How  many  boxes  of \npizza did Marie order if each box costs $3\nPrompt\nClaims\ndef string_sequence(n): \nresult = \"\"   \nfor i in range(n+1):      \nresult += str(i) + \" \"     \nreturn result.strip()\nTest Cases\n(1) string_sequence(4)\n(2) string_sequence(0)\n(3) string_sequence(3)\nQuery  \nGeneration\nExec Results\n(1) 0 1 2 3 4 expected: 0 1 2 3 4\n(2) 0 expected: 0\n(3) 0 1 2 3 expected: 0 1 2 3\nResponse-level Factuality: 1\nScores\nMath Claims\n(1) 5*3 = $15\n(2) 45 - 15 = $30\n(3) 30 / 3 = 10\nLLM Response\n-\nMarie  ordered  5  packs  of  milk  that  costs  $3 \neach. The total cost of these items is: 5*3 = $15\n-\nTo  find  the  cost  of  the  boxes  of  pizza,  we \nsubtract  the  total  cost  of  the  other  items  from \nthe total amount paid by Marie: 45 - 15 = $30\n-\nTo  find  the  number  of  boxes  of  pizza,  we \ndivide  the  remaining  amount  by  the  cost  of \neach box: 30 / 3 = 10\n-\nTherefore, Marie ordered 10 boxes of pizza.\nClaim \nExtraction\nQuery  \nGeneration\nQueries\n(1) print(round(5*3, 7) == 15)\n(2) print(round(45-15, 7)==30)\n(3) print(round(30/3, 7) == 10)\nExec \nResults\n(1) True\n(2) True\n(3) True\nResponse-level  \nFactuality:1\nScores\nPrompt\nDiscuss the applications and limitations \nof  quantum  computing,  citing  at  least \none relevant paper. When citing papers, \nplease  include  the  title,  the  author(s), \nand the publication year.\nQuantum  computing  has  the  potential  to \nrevolutionize   various   fields   such   as \ncryptography, optimization, and simulation. \nHowever, there are also limitations such as \nthe  need  for  error  correction.  One  papers \nthat   have   contributed   to   this   field   is \n“Quantum Computing in the NISQ era and \nbeyond” by John Preskill (2018).\nLLM Response\nClaim \nExtraction\nClaims\n(1) {title: Quantum Computing in the \nNISQ  era  and  beyond,  authors:  John \nPreskill, publication_year: 2018}\nQueries\n(1) Quantum Computing in the NISQ \nera and beyond\nEvidence\n(1) {title: Quantum Computing in the \nNISQ  era  and  beyond,  authors:  John \nPreskill, publication_year: 2018}\nQuery  \nGeneration\nClaim-level Factuality: [1] \nResponse-level Factuality: 1\nScores\nKnowledge-based QA\nCode Generation\nMath Problem Solving\nFacTool\n!\nScientific Literature Review Writing\nFigure 2: Our proposed framework for factuality detection in four domains: knowledge-based QA, code generation,\nmath problem solving and scientific literature review writing.\n{c\ni\n}\ni=1···n\n.  Detailed prompting instructions can\nbe found in Appendix A.\nKB-based QAThe claim is defined using the\nconcept  of  atomic  content  units  (ACUs)  (Liu\net al., 2022).  Each ACU corresponds to a single\natomic fact within a generated answer. In practice,\nwe leverage ChatGPT\n4\n(specifically, the “gpt-3.5-\nturbo” version) to extract claims based on two cri-\nteria: (i) each claim should not exceed 15 words,\nand (ii) it should clearly describe a fact.  We also\ninclude two in-context examples from the RoSE\ndataset (Liu et al., 2022) in our prompt to obtain\nmore fine-grained claims.   Additionally,  we ask\nChatGPT to resolve any coreferences or ambiguity,\nsuch as unclear pronouns and other related expres-\nsions within the claims.\nCode GenerationWe consider each generated\ncode snippet within the response as a single claim\nto be verified.  We extract all such code snippets\nthat are enclosed with brackets,  in other words,\nwithin a code block.\nMath ProblemsWe define each claim in a step-\nby-step math solution as the arithmetic operation\nperformed between known real numbers. Each of\nthese operations contains two parts: the calculation\n4\nWe have also explored other entailment-based models\nwith BERT, and the result is no better than ChatGPT.\nand the calculated answer. We prompt ChatGPT to\nextract all such claims.\nScientific Literature ReviewEach claim within\nthe generated review is defined as a tuple of “(paper\ntitle, year, authors)” contained from the generated\nreview.  We then prompt ChatGPT to extract all\nsuch tuples within the generated review.\n4.2    Query Generation\nFor each claimc\ni\n, we convert it into a list of queries\n{q\nij\n}\nj=1···m\nthat can be used to query external tools\nsuch as search engines, the Python interpreter, or\nGoogle scholar.\nKB-based QAWe prompt ChatGPT or GPT-4 to\ngenerate two search engine queries from each claim\nc\ni\n.  These queries are intended to help humans in\nverifying the factuality ofc\ni\n. Detailed prompting\ninstructions can be found in Appendix A.\nCode GenerationFor each claimc\ni\nwe gener-\nate two different types of queries:  simulated test\ncase inputs, denoted as{q\nt\nij\n}\nj=1···m\n, and poten-\ntial solutions, denoted as{q\ns\nij\n}\nj=1···m\n. Both types\nof queries are generated by ChatGPT or GPT-4.\nThe simulated test case inputs are function calls\ngenerated for a given code snippet, while potential\nsolutions are repeatedly generated solutions that\nChatGPT generates in response to the user prompt",
    "Write Python code that \ncan square a number.\nPrompt\nUnverified Solution\ndef square_a_num(n): \nreturn n * n\nUnittest \nLibrary\nGPT-4GPT-4\nChatbot\nInputs\n-205\nOutputs\n4025\nGPT-4\nVerify\nWrite Python code \nthat can square a \nnumber.\nPrompt\nUnverified Solution\ndef square(n): \nreturn n * n\nChatbot\nGPT-4\nVerify\nUnittest \nLibrary\nInputs\n-205\nOutputs\n4025\nPotential Solution\nGPT\nGPT\nGPT\nFigure 3: Unit test library generation for detecting fac-\ntual errors in code generation.\np\n.   In our later experiments,  we generate 3 sim-\nulated test case inputs and 3 potential solutions.\nDetailed prompting instructions can be found in\nAppendix A.\nMath ProblemsWe prompt ChatGPT or GPT-4\nto  convert  all  mathematical  operations  into  exe-\ncutable Python code snippets. These snippets are\ndesigned  to  return  “True”  when  the  calculation\nmatches  the  calculated  answer  and  “False”  if  it\ndoesn’t.  Detailed prompting instructions can be\nfound in Appendix A.\nScientific Literature ReviewWe use the paper\ntitle, found within the extracted claim tuple, as the\nquery for Google Scholar. Our assumption here is\nthat if a paper exists, it should appear as the first\nsearch result on Google Scholar when we use the\npaper title as the query.\n4.3    Tool Querying & Evidence Collection\nWe then use the queries to query various tools to\ncollect relevant evidence statements{e\nik\n}\nk=1···l\ni\n.\nKB-based QAThe external tool we use to help\nverify the factuality of the generated text is the\nGoogle Search API, which queries the internet for\nknowledge using the queries generated from the\nclaims extracted from the generated text of LLM.\nWe use the Google Search API provided by Ser-\nper\n5\nto search the top pages and retrieve the most\nrelevant search snippets included in the API’s re-\nsponse. We then parse the response to obtain differ-\nent types of snippets such as answer boxes, knowl-\nedge graphs, and organic search results.\nCode GenerationFor each test case inputt\ni\nand\ngenerated potential solutions\nj\n, we executes\nj\nus-\n5\nhttps://serper.dev/\ningt\ni\nas the input and collect the execution result\n(output) for each(t\ni\n, s\nj\n)pair.   The input-output\npairs are used as test cases for verifying the chat-\nbot generated unverified solution.  The process is\nshown in Fig. 3.\nMath ProblemsWe collect the execution results\nfor code snippets derived from the mathematical\noperations.  As illustrated in Fig. 2, math claims\nlike “30 /3 = 10” are extracted and then con-\nverted into a Python executable code, for instance,\n“print(round(30/3, 7)==10)”.\nScientific Literature ReviewWe use the title of\neach paper, extracted from the text, as the query\nto access relevant information through the Google\nScholar API provided by the Scholarly\n6\nPython\npackage. This allows us to retrieve key information\nabout each paper, including the paper title, author\nlist, and publication year.\n4.4    Agreement Verification\nIn the final step, each claim,c\ni\n, receives a binary\nfactuality label,L\ni\n∈ {TRUE,FALSE}, based on\nthe level of support it receives from the collected\nevidence,{e\nik\n}\nk=1···l\ni\n.   This labeling process is\nperformed for every individual claim.\nKB-based QAWe prompt ChatGPT or GPT-4 to\njudge the factuality of the claim given the retrieved\nlist of evidence snippets.  We follow a zero-shot\nChain-of-Thought (Wei et al., 2023) reasoning pro-\ncess: initially, the model attempts to reason about\nwhether the claim is factual or not.  If an error is\nidentified, we then ask it to explain and attempt to\nrectify the mistake.\nCode GenerationWe conduct a majority vote\nfor each test case across all solutions, establishing\nwhat we refer to as the “pseudo-golden output” for\nthat particular test case. We repeat this process for\nevery test case.  Following this, we compare the\nexecution result of the solution that’s under veri-\nfication against all the test cases with the pseudo\ngolden output. If the results match, we classify the\nsolution under verification as true. Otherwise, it is\ndeemed false.\nMath ProblemsWe compile the results of each\ncode  snippet  execution.   If  any  snippet  returns\n“False”, we classify the associated generated text\nxas false. Conversely, if all snippets yield “True”,\n6\nhttps://github.com/\nscholarly-python-package/scholarly",
    "we classify the corresponding generated textxas\ntrue.\nScientific Literature ReviewWe compare the\nextracted claim: “(paper title, year, authors)” to the\nevidence:  “(paper title, year, authors)” retrieved\nfrom Google Scholar API. For the paper title and\nyear  of  publication,  we  conduct  an  exact,  case-\ninsensitive string match. As for the authors’ match,\nwe prompt ChatGPT or GPT-4 to judge whether\nthe author list in the extracted claim is a subset\nof the retrieved author list.   All the information\nmust be matched in order to be classified as “True”,\notherwise “False”.\n5    Dataset Construction\n5.1    Prompt and Response Collection\nKB-based QAFor KB-based QA, we evaluate\nour framework using RoSE (Liu et al., 2022) and\nFactPrompts. RoSE is a text summarization dataset\nthat provides fine-grained ACUs for each reference\nsummary. FactPrompts is a dataset that comprises\nreal-world prompts sourced from various platforms\nand datasets, such as Quora and TruthfulQA (Lin\net al., 2022), along with corresponding responses\ngenerated by ChatGPT. We construct the dataset\nusing 100 reference summaries from RoSE and 50\nresponses from FactPrompts for our evaluation.\nCode GenerationFor code generation, we eval-\nuate our framework using HumanEval (Chen et al.,\n2021).   HumanEval  is  a  programming  problem\ndataset  that  contains  several  unit  tests  for  each\nproblem. We use ChatGPT to generate responses\nbased on the processed prompts of HumanEval pro-\nvided in (Chen et al., 2022) which solely contain\nthe instruction of the prompt without input-output\ndemonstrations.\nMath ProblemsFor math problems, we evalu-\nate our framework using GSM-Hard (Gao et al.,\n2022b). GSM-Hard is a dataset constructed from\nGSM8K (Cobbe et al., 2021) by replacing the num-\nbers in the questions of GSM8K with larger num-\nbers.  We sampled 100 prompts from GSM-Hard\nthat have a target solution value of positive.\n7\nThen,\n7\nGSM8K involves many application questions, including\ncalculations involving money, measurements of quantities, etc.\nWe found that GSM-Hard examples with negative values often\ncontained illogical situations, such as “negative 5 apples”. A\npositive target solution value helps prevent ChatGPT from\nmaking extra assumptions on top of the description in the\nproblem.\nwe  generate  responses  for  these  prompts  using\nChatGPT.\nScientific Literature ReviewFor the scientific\nliterature  review,  we  follow  self-instruct  (Wang\net al., 2023) to create 100 diverse prompts span-\nning computer science,  business,  law,  medicine,\nand physics.  Each prompt asks for a technical or\nresearch-oriented response that includes at least\none relevant literature citation. Then, we generate\nresponses for these prompts using ChatGPT.\n5.2    Claim Collection\nFor responses from FactPrompts and GSM-Hard,\nwe follow the idea of “claim extraction as prompt-\ning” described in §4.1, This approach allows us to\nreuse claim prompts as listed in Appendix A. We\nuse ChatGPT as the model for claim extraction due\nto its cost efficiency and effectiveness in extract-\ning fine-grained claims.  In terms of HumanEval\nresponses, given that the generated response to a\nHumanEval prompt is already in the form of a code\nsnippet, we consider the “claim” of the response to\nbe identical to the response itself.\n5.3    Claim and Response Annotation\nKB-based QA & Scientific Literature Review\nFor claim annotation, the authors collectively an-\nnotate  the  extracted  claims  as  either  factual  or\nnon-factual. For response annotation, if one claim\nwithin the response is labeled as non-factual, then\nthe response as a whole is considered non-factual;\notherwise, the response is considered factual.\nCode GenerationWe consider the claim label to\nbe identical to the response label since the “claim”\nof the response is the same as the response itself.\nFor response annotation, we annotate ChatGPT’s\nresponses  using  the  execution  code  provided  in\n(Chen  et  al.,  2022)  against  the  HumanEval  test\ncases. This allows us to distinguish between factual\n(those passing all tests) responses and non-factual\nresponses.\nMath ProblemsFor claim annotation,  the au-\nthors collectively annotate the extracted claims as\neither factual or non-factual. For response annota-\ntion, we utilize the target value provided in GSM-\nHard (Gao et al., 2022b) to annotate the generated\nresponses.",
    "TaskDatasetsResponsesClaims\nKB-QARoSE100527\nKB-QAFactPrompts50 (23:27)233 (177:56)\nCodeHumanEval164 (109:55)164 (109:55)\nMathGSM-Hard100 (47:53)284 (246:38)\nSci.LitFactPrompts100 (10:90)186 (33:153)\nTable 3: Detailed statistics of datasets used in this work.\nNote that (p, n) denotes p = count of positive responses\nor claims, and n = count of negative responses or claims.\n“Sci.Lit” represents scientific literature review.\n6    Experiments\nWe evaluateFACTOOLagainst two baselines that\nuse LLMs to check their own inputs: Self-Check\nwith 3-shot CoT and zero-shot CoT, which have\nshown to been effective on various tasks including\ndialogue response, math reasoning, and code gener-\nation (Madaan et al., 2023; Chen et al., 2023). Both\nof these baselines aim to test the ability of LLM to\nidentify its own errors without the use of any exter-\nnal tool. In practice, we prompt ChatGPT (gpt-3.5-\nturbo-0301) and GPT-4 (gpt-4-0314)\n8\nto recognize,\nexplain,  and  attempt  to  rectify  their  own  errors.\nFollowing this reasoning process, the models make\nfinal judgments on the factuality of the given claim.\nThe key difference between Self-Check (zero-shot\nCoT)  and  Self-Check  (3-shot  CoT)  is  that  Self-\nCheck (3-shot CoT) provides three demonstrations\nto models, while Self-Check (zero-shot CoT) does\nnot provide any demonstrations.\n6.1    Exp-I: Claim Extraction Evaluation\nWe evaluate the claim extraction module ofFAC-\nTOOLon  RoSE  (Liu  et  al.,  2022).We  treat\nthe  reference  summary  as  the  generated  textx,\nand the reference ACUs as the golden-extracted\nclaims.We  measure  the  similarity  between\nthe  machine-extracted  (GPT-4,   ChatGPT,  and\nFlan-T5  XXL)  claims{c\nc\ni\n}\ni=1···n\nc\nand  golden-\nextracted  claims{c\ng\ni\n}\ni=1···n\ng\nusing  4  metrics:\nROUGE-1,  ROUGE-2,  ROUGE-L  (Lin,  2004),\nand  BERTScore.    In  Tab.  4,  we  report  the  av-\nerage  of  the  highest  similarity  between  each\n8\nWe anticipate that the recently released models, gpt-3.5-\nturbo-0613 and gpt-4-0613, will lower the inference costs for\nFACTOOL. This expectation arises from their improved ability\nto produce structured responses, such as those in JSON format.\nWhile conducting our experiments on gpt-3.5-turbo-0301 and\ngpt-4-0314, we often ran into problems where the responses\nwere not valid JSON, requiring us to rerun any samples with\ninvalid response formats. The source code ofFACTOOLwill\nbe using the latest versions of ChatGPT and GPT-4.\nModelMetricPrecisionRecallF1-score\nGPT-4ROUGE-10.73940.87580.7860\nROUGE-20.63040.77710.6772\nROUGE-L0.71750.86250.7667\nBERTScore0.66320.78650.7175\nChatGPTROUGE-10.77700.82850.7836\nROUGE-20.65200.71150.6610\nROUGE-L0.75570.81480.7655\nBERTScore0.69580.75210.7174\nFLAN-T5-XXLROUGE-10.65310.89280.7326\nROUGE-20.56090.81570.6413\nROUGE-L0.64280.88850.7237\nBERTScore0.43140.66610.5408\nTable 4: The average similarity between the extracted\nclaims from GPT-4, ChatGPT, and Flan-T5 XXL and\nthe golden ACUs on RoSE.\nChatGPT-extracted claim and the corresponding\ngolden-extracted claim in the same sample.  (i.e.,\n1\nsample_cnt\nP\nsample\n1\nn\nc\nP\nn\nc\ni=1\nmax\nn\ng\nj=1\n(Sim(c\nc\ni\n, c\ng\nj\n))).\nResultsWe demonstrate in Tab. 4 that the claims\nextracted by GPT-4, ChatGPT, and Flan-T5 closely\nmatch the ACUs annotated by humans, as evaluated\nby ROUGE and BERTScore metrics. Note that in\nExp-II, we choose ChatGPT as the claim extractor\nfor two reasons:  (1) The context length of Flan-\nT5 is too short (512 tokens) to effectively extract\nclaims from lengthy responses in our dataset. (2)\nChatGPT is more cost-efficient compared to GPT-\n4, while maintaining similar effectiveness in claim\nextraction.\n6.2    Exp-II: Framework Evaluation\nWe  evaluateFACTOOLand  the  two  Self-Check\nbaselines on the dataset constructed from each sce-\nnario. Depending on the model used for query gen-\neration and agreement verification, we have two\nFACTOOLbaselines:FACTOOLpowered by Chat-\nGPT andFACTOOLpowered by GPT-4. We report\nthe accuracy, recall, precision, and F1-score at both\nthe claim and response levels.\n6.2.1    Result\nTab. 5 shows the claim-level and response-level\nperformance ofFACTOOLand the self-check base-\nlines. We obtain following observations.\nFACTOOLpowered  by  GPT-4  outperforms\nall other baselines across all scenariosFrom\nTab.  5,  we  observe  thatFACTOOLpowered  by\nGPT-4 outperforms all other baselines across all\nscenarios.FACTOOLpowered by GPT-4 achieves\nan89.09claim-level F1 /71.79response-level F1\non KB-based QA, a92.11claim-level F1 /92.11\nresponse-level F1 on code generation (remember",
    "TasksLLMsMethods\nClaim-LevelResponse-Level\nAcc.RPF1Acc.RPF1\nKB-QA\nChatGPT\nSelf-Check (0)75.5490.4080.0084.8854.0060.8750.0054.90\nSelf-Check (3)69.5381.3679.1280.2354.0047.8350.0048.89\nFACTOOL74.2573.4590.9181.2564.0043.4866.6752.63\nGPT-4\nSelf-Check (0)77.2584.7585.2384.9954.0095.6550.0065.67\nSelf-Check (3)79.8385.8887.3686.61\n64.0052.1763.1657.14\nFACTOOL84.1285.3193.2189.0978.0060.8787.5071.79\nCode\nChatGPT\nSelf-Check (0)68.2999.1068.3380.8868.2999.1068.3380.88\nSelf-Check (3)68.90100.0068.5281.3268.90100.0068.5281.32\nFACTOOL78.0589.1980.4984.62\n78.0589.1980.4984.62\nGPT-4\nSelf-Check (0)75.3195.5075.1884.1375.3195.5075.1884.13\nSelf-Check (3)77.4496.4076.4385.2677.4496.4076.4385.26\nFACTOOL89.0294.5989.7492.1189.0294.5989.7492.11\nMath\nChatGPT\nSelf-Check (0)84.1590.2491.3690.8057.0074.4753.0361.95\nSelf-Check (3)87.3294.3191.3492.8061.0089.3655.2668.29\nFACTOOL97.5497.5699.5998.5678.0093.6269.8480.00\nGPT-4\nSelf-Check (0)83.1086.9993.0489.9249.0085.1147.6261.07\nSelf-Check (3)92.6196.7594.8295.77\n65.0089.3658.3370.59\nFACTOOL98.2497.97100.0098.9778.0095.7469.2380.36\nScientific\nChatGPT\nSelf-Check (0)28.6996.0021.8235.56\n18.00100.0010.8719.61\nSelf-Check (3)24.1996.9718.6031.22\n22.0090.0010.4718.75\nFACTOOL97.3184.85100.0091.8099.0090.00100.0094.74\nGPT-4\nSelf-Check (0)35.7584.8520.2932.7519.00100.0010.9919.80\nSelf-Check (3)44.7587.8823.2036.71\n49.0070.0012.7321.54\nFACTOOL98.3990.91100.0095.2499.0090.00100.0094.74\nTable 5: Experimental results ofFACTOOLpowered by ChatGPT andFACTOOLpowered by GPT-4 on KB-based\nQA, Code Generation, Math Problems, and Scientific Literature Review.\nthat claim-level factuality is considered equivalent\nto response-level factuality in our experiment for\ncode generation), a98.97claim-level F1 /80.36\nresponse-level F1 on math problems, and a95.24\nclaim-level F1 /94.74response-level F1 on scien-\ntific literature review. Each of these figures is the\nhighest for their respective tasks.\nFACTOOLpowered by GPT-4 outperforms all\nself-check baselines across all scenariosFrom\nTab. 5, we show thatFACTOOLwith GPT-4 outper-\nforms all self-check baselines across all scenarios.\nOnFACTOOLpowered by GPT-4 v.s. Self-Check\n(3)  powered  by  GPT-4,  we  observe:71.79v.s.\n57.14response-level F1 on KB-based QA,92.11\nv.s.85.26response-level F1 on code generation,\n80.36v.s.70.59response-level F1 on math prob-\nlems, and94.74v.s.21.54response-level F1 on\nscientific literature review.\nFACTOOLpowered by GPT-4 significantly out-\nperforms all self-check baselines in scientific lit-\nerature reviewFrom Tab. 5, we show thatFAC-\nTOOLpowered by GPT-4 significantly outperforms\nthe self-check baselines in scientific literature re-\nview. OnFACTOOLpowered by GPT-4 v.s. Self-\nCheck (3) powered by GPT-4, we observe:95.24\nv.s.36.71claim-level  F1  and94.74v.s.21.54\nresponse-level F1. Here, Google Scholar shown to\nbe highly robust in performing its specified task of\nfinding citations when compared to LLM itself.\nFACTOOLpowered by GPT-4 outperformsFAC-\nTOOLpowered by ChatGPTFACTOOLpow-\nered by GPT-4 outperformsFACTOOLpowered\nby ChatGPT across all scenarios.   This trend is\nespecially significant in KB-QA, where query gen-\neration and agreement verification are harder for\nChatGPT but relatively easier for GPT-4 (89.09v.s\n81.25claim-level F1 and71.79v.s52.63response-\nlevel F1).  On the other hand, in scenarios where\nquery generation and agreement verification are\nrelatively easy for both ChatGPT and GPT-4, the\nperformance is similarly good.\nSelf-check  models  are  prone  to  false  positive\nand thus less sensitive in detecting errorsFrom\nTab. 5,  we observe that self-check models have\nlower precision compared toFACTOOL. On Self-\nCheck (3) powered by GPT-4 v.s.FACTOOLpow-",
    "ered  by  GPT-4,  we  observe:63.16v.s.87.50\nresponse-level precision on KB-based QA,76.43\nv.s.89.74response-level precision on code gen-\neration,58.33v.s.69.23response-level precision\non math problems, and12.73v.s.100.00response-\nlevel precision on scientific literature review. These\nfigures show that self-check models tend to clas-\nsify claims as “True” considerably more frequently\nthanFACTOOL, suggesting a lower sensitivity for\nerror detection.\nSelf-check   models   powered   by   ChatGPT\noutperformFACTOOLpowered by ChatGPT\non  KB-QATab.  5  shows  that  Self-Check  (0)\npowered  by  ChatGPT  outperformsFACTOOL\npowered by ChatGPT. Through examining specific\ncases, we found that reasoning errors are the main\nreason  whyFACTOOLpowered  by  ChatGPT\nperforms  worse  than  the  self-check  baselines.\nEven  when  provided  with  sufficient  evidence\nto  determine  whether  the  claim  is  factual  or\nnot,  the  agreement  verification  implemented  by\nChatGPT  can  become  confused.   For  example,\nfor  the  claim  “The modern-day version\nof fortune cookies was invented\nin the United States.\n”,   the   reasoning\nofFACTOOLpowered   by   ChatGPT   is   self-\ncontradictory:“The given text is not\nentirely factual.  The modern-day\nversion of fortune cookies was\nnot invented in the United States.\nMost people nowadays believe that\nfortune cookies were created\nby a Japanese man named Makoto\nHagiwara in 1914 in San Francisco.\nHagiwara owned what is now called\nthe Golden Gate Park Japanese\nTea Garden, where he served\ntea and fortune cookies.  This\nis supported by the provided\nevidences.\n”Detailed   examples   can   be\nfound in Fig. 9 of Appendix B.\n6.2.2    Performance Analysis\nWe take a closer look at performance in different\nscenarios by examining evaluated cases.\nKB-based QAThe fact-checking capability of\nFACTOOLon KB-based QA is determined by sev-\neral factors, including whether the search engine\ncan return the most relevant snippets that could\nassist in determining the factuality of the given\nclaim, the quality of the generated search engine\nqueries, and the LLM’s ability to reason about the\nvalidity of the claim given the retrieved evidence.\nWe  found  thatFACTOOLpowered  by  GPT-4  is\nespecially capable under the following situations:\n(1)  Fact-checking  recent  events,  discoveries,  or\nnews:FACTOOLpowered by GPT-4 successfully\nidentify   false   claims   such   as   “Argentina\nhas not won the World Cup since\n1986”and“The most valuable NFT\never sold is a digital artwork\ncalled ‘Everydays:  The First\n5000 Days’\n”. (2) Fact-checking high-precision\nstatistics:FACTOOLpowered by GPT-4 success-\nfully  identify  false  claims  such  as  “Ireland\nhas an obesity rate of 26.9%”and\n“Everydays:  The First 5000 Days’\nsold for69million”.  Detailed examples\ncan be found in Fig. 10 of Appendix B.\nCode GenerationThe fact-checking capability\nofFACTOOLon code generation is determined by\nthe LLM’s capability to generate high-quality test\ncases and potential solutions. We demonstrate that\ndue to GPT-4’s exceptional ability to generate such\nhigh-quality test cases and potential solutions,FAC-\nTOOLpowered by GPT-4 outperforms other base-\nlines. For example, in “HumanEval/36”, GPT-4\nis consistently generating high quality solutions,\nleading to its correctly identifies the mistakes in\nthe response, while ChatGPT fails to identify the\nmistake. Detailed examples can be found in Fig. 11\nand Fig. 12 of Appendix B.\nMath ProblemsThe fact-checking capability of\nFACTOOLon math problems is determined by the\nLLM’s capability to generate accurate Python snip-\npets that verify the correctness of given extracted\nmathematical calculations.  BothFACTOOLpow-\nered by GPT-4 andFACTOOLpowered by Chat-\nGPT excel in this regard. For example, bothFAC-\nTOOLpowered  by  GPT-4  andFACTOOLpow-\nered by ChatGPT correctly identify23×4319216\ndoesn’t equal to99305768. Detailed examples can\nbe found in Fig. 13 of Appendix B.\nScientific Literature ReviewThe fact-checking\ncapability  ofFACTOOLon  Scientific  Literature\nReview  is  determined  by  the  LLM’s  capability\nto  identifying  whether  the  author  list  generated\nis a subset of the actual author list.   BothFAC-\nTOOLpowered  by  GPT-4  andFACTOOLpow-\nered by ChatGPT excel in this regard.  For exam-\nple, bothFACTOOLpowered by GPT-4 andFAC-",
    "TOOLpowered by ChatGPT correctly identify that\nthe   paper   “The Impact of Artificial\nIntelligence on Employment”  was  not\nwritten by “Acemoglu and Restrepo”.  De-\ntailed  examples  can  be  found  in  Fig.  14  of  Ap-\npendix B.\n6.2.3    Failure Analysis\nTo gain a comprehensive understanding ofFAC-\nTOOL’s performance, we conduct analysis on cases\nwhere FACTOOLwill fail.\nKB-based QAWe summarize following sources\nof  errors:   (1)  Reasoning  error:   Although  the\nevidence  provided  is  sufficient  and  the  LLM\naccurately  finds  the  most  relevant  information,\nthe  model  fails  to  reason  about  the  relationship\nbetween  the  claim  and  the  provided  evidence.\nFor  example,  for  claim  “Jupiter is less\ndense than Saturn\n\", FACTOOLpowered by\nGPT-4 fails to reason the relative relationship even\nthough the evidences provided are sufficient.  (2)\nConflicting  evidence:   Conflict  in  evidence  can\ncause  confusion  for  LLM,  leading  to  incorrect\ndecisions.    For  example,  for  claim  “Jupiter\nhas a density of 1.33 grams per\ncubic centimeter\n\",   there   are   conflicting\nevidences  claiming  that  the  density  is  1.326  or\n1.33g/cm\n3\n.   (3)  Ambiguity  in  claim:   Ambigu-\nous  descriptions  and  subjective  adjectives  can\nlead  to  incorrect  decisions.    For  example,  the\nclaim“Fortune cookies are enjoyed\nby people all over the world.\"is\nambiguous and can have different answers based\non different interpretations. Detailed examples can\nbe found in Fig. 15 of Appendix B.\nCode  GenerationErrors  in  code  generation\nmainly comes from:  (1) Limited variety in syn-\nthetic  test  cases:   The  synthetic  test  cases  gen-\nerated  by  LLMs  may  not  be  fully  representa-\ntive  or  sufficiently  diverse.For  example,  in\nthe  “HumanEval/64”  sample,  all  the  inputs\nof  the  generated  synthetic  test  cases  are  com-\nposed of strings that only include lowercase let-\nters (without uppercase letters).  (2) Potential er-\nrors  in  code  generation:   The  generated  poten-\ntial solutions  could contain  errors or  bugs.   De-\nspite implementing a  majority voting system  to\nlessen this issue, it cannot completely eliminate\nthe  chance  of  bugs  in  the  code  generation  pro-\ncess. For example, in the “HumanEval/79” sam-\nple, all the generated solutions failed to correctly\n“decimal_to_binary(0)” as “db0db”. De-\ntailed  examples  can  be  found  in  Fig.  16  of  Ap-\npendix B.\nMath ProblemsThere are two major types of\nerrors in factuality detection for math problems:\n(1) Round-off error:  Round-off errors can occur\nduring  numerical  calculations  in  Python.For\nexample,FACTOOLpowered by GPT-4 incorrectly\nclassify the math calculation “60444034 / 12\n= 5037002.83\n” as “False”.   (2) Reasoning\nerror:  Since  the  claims  extracted  byFACTOOL\nonly  involve  mathematical  calculations,FAC-\nTOOLwill   not   verify   the   reasoning   process\nof  the  mathematical  solution.For  example,\nforthequestion“Kylar went to the\nstore to buy glasses for his new\napartment.  One glass costs $5,\nbut every second glass costs only\n60% of the price.  Kylar wants\nto buy 5364765 glasses.  How much\ndoes he need to pay for them?\n”,   the\nChatGPT generated response contains reasoning\nerror that incorrectly substitute the total cost as\n“5,364,765\n*\n5”.  However, sinceFACTOOL\nonly  checks  math  calculation  errors,FACTOOL\npowered by GPT-4 did not identify the reasoning\nerror.  Detailed examples can be found in Fig. 17\nof Appendix B.\nScientific  Literature  ReviewThere  are  two\nmajor  types  of  errors  in  factuality  detection  for\nscientific literature review: (1) Errors in title match-\ning: Title matching can sometimes be problematic\ndue to abbreviations in the generated citations or\nthe retrieved title. For example, although the paper\n“MDMA-assisted psychotherapy for\ntreatment of PTSD: study design\nand rationale for phase 3 trials\nbased on pooled analysis of six\nphase 2 randomized controlled\ntrials\nexists,FACTOOLpowered  by  GPT-4\nidentify the paper title as incorrect. (2) Errors in au-\nthor matching: the author matching process might\nsometimes not be robust.  For example, although\nthe    authors    of    “Language Models are\nUnsupervised Multitask Learners\"\nare  indeed  “Alec Radford, Jeffrey Wu,\nRewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever,FAC-\nTOOLpowered by GPT-4 identify the author list\nas incorrect.  Detailed examples can be found in",
    "Fig. 18 of Appendix B.\n6.3    Exp-III: Using FACTOOLto Evaluate the\nFactuality of Modern Chatbots\nThe purpose of developing a factuality detector is\nto audit the actual generative chatbots to assess the\nreliability of the responses generated by chatbots.\nTo this end, we evaluate the factuality of modern\nchatbots, including GPT-4, ChatGPT, Claude-v1,\nBard, and Vicuna-13B, usingFACTOOLpowered\nby GPT-4.  It is important to note that in Exp-III,\nwe considerFACTOOLas a golden evaluator, re-\nsponsible  for  evaluating  the  factual  accuracy  of\nthe responses generated by different chatbots. For\nprompts selection, we follow the same intuition\nas (Zhou et al., 2023):  KB-QA is the most com-\nmon scenario. Thus, we select 30 KB-QA prompts,\n10 code prompts, 10 math prompts. and 10 scien-\ntific prompts (i.e., 3 times more KB-QA prompts\ncompare to prompts from other scenarios) to carry\nout this factuality evaluation on chatbots. The KB-\nQA prompts are collected from (Zhou et al., 2023),\ncode prompts from HumanEval (Chen et al., 2021),\nmath prompts from (Gao et al., 2022b), while the\nscientific prompts are generated by us. Responses\nfor these prompts are generated by each of the eval-\nuated chatbots.\nWe  report  both  the  claim-level  and  response-\nlevel accuracies for each chatbot, as evaluated by\nFACTOOLpowered by GPT-4. Given that KB-QA\nresponses contain significantly more claims than\nresponses  from  other  scenarios,  we  report  the\nweighted  claim-level  accuracy.    This  weight  is\ndetermined by the ratio of the number of prompts\nin each scenario. In other words,\nweighted_claim_level_accuracy\n=\n3\n6\n×claim_level_accuracy in KB−QA\n+\n1\n6\n×claim_level_accuracy in Code\n+\n1\n6\n×claim_level_accuracy in Math\n+\n1\n6\n×claim_level_accuracy in Scientific\nAdopting  the  weighted-claim  level  accuracy\nevaluation helps us provide a more holistic and\nfair assessment of each chatbot’s factual accuracy.\nResultsTab. 6 shows that GPT-4 has the best\nweighted claim-level factual accuracy and response-\nlevel   accuracy   compared   to   ChatGPT,   Bard,\nClaude-v1,  and  Vicuna.    Fig.  4  and  5  demon-\nstrate fine-grained performance w.r.t each scenario\n(KB-QA, code, math, scientific). We observe that\n(a) GPT-4 has the best claim-level accuracy and\nresponse-level accuracy in most of the scenarios.\n(b)  Supervised  fine-tuned  Chatbots  like  Vicuna-\n13B perform reasonably well in more common sce-\nnarios like KB-QA but less so in more challenging\nscenarios such as math, code, and scientific.\nLLMsWCL Acc.RL Acc.Avg. Resp. Len.\nGPT-475.6043.33196.83\nChatGPT68.6336.67144.05\nClaude-v163.9526.67208.70\nBard61.1533.33263.77\nVicuna-13B50.3521.67207.13\nTable 6: Factual accuracy of different chatbots evaluated\nbyFACTOOL.  WCL Acc.  stands for weighted claim-\nlevel accuracy of each chatbot.   RL Acc.   stands for\nresponse-level accuracy of each chatbot.  Avg.  Resp.\nLen. stands for average response length of each chatbot.\nNote that we considerFACTOOLas the golden evaluator\nthat evaluates the factuality of the responses generated\nby each chatbot.\n7    Conclusion\nWe  introduceFACTOOL,  a  task-  and  domain-\nagnostic framework designed to tackle the esca-\nlating challenge of factual error detection in gen-\nerative AI. We expand the conventional definition\nof factuality, particularly focusing on auditing the\ncapabilities of generative AI models. Realizing that\n(1) the generated texts of LLMs tend to be lengthy\nand lack a clearly defined granularity for individ-\nual facts, and that (2) there is a scarcity of explicit\nevidence available during the process of fact check-\ning, we buildFACTOOLas a 5-step tool-augmented\nframework that consists of claim extraction, query\ngeneration, tool querying, evidence collection, and\nverification.\nWe demonstrate the potential of incorporating\ntools like Google Search,  Google Scholar,  code\ninterpreters, Python, and even LLMs themselves\nin factual error detection through experimentation\nacross diverse tasks such as knowledge-based QA,\ncode generation, math problem solving, and sci-\nentific literature review writing.  We believe that\nour holistic and adaptable framework can be easily\nextended to more scenarios.",
    "KB-QACodeMathScientific\n0\n0.5\n1\nAccuracy\nGPT-4ChatGPTBardClaude-v1Vicuna-13B\nFigure 4:  Claim-Level Accuracy across scenarios for\nGPT-4, ChatGPT, Bard, Claude-v1, and Vicuna-13B\nKB-QACodeMathScientific\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy\nGPT-4ChatGPTBardClaude-v1Vicuna-13B\nFigure 5:  Response-Level Accuracy across scenarios\nfor GPT-4, ChatGPT, Bard, Claude-v1, and Vicuna-13B\nAcknowledgements\nWe thank Yixin Liu, Zhengbao Jiang, Zhiruo Wang\nfor the useful discussion and suggestions.\nReferences\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan,  Johannes  Gehrke,  Eric  Horvitz,  Ece  Kamar,\nPeter  Lee,  Yin  Tat  Lee,  Yuanzhi  Li,  Scott  Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4.arXiv preprint\narXiv:2303.12712.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022.\nCodet: Code generation with generated tests.arXiv\npreprint arXiv:2207.10397.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017.  Reading Wikipedia to answer open-\ndomain questions. InProceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nMark  Chen,   Jerry  Tworek,   Heewoo  Jun,   Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg  Brockman,  Alex  Ray,  Raul  Puri,  Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry,  Pamela  Mishkin,  Brooke  Chan,  Scott  Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser,   Mohammad  Bavarian,   Clemens  Winter,\nPhilippe  Tillet,  Felipe  Petroski  Such,  Dave  Cum-\nmings,  Matthias  Plappert,  Fotios  Chantzis,  Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam  Saunders,  Christopher  Hesse,  Andrew  N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nXinyun  Chen,  Maxwell  Lin,  Nathanael  Schärli,  and\nDenny Zhou. 2023. Teaching large language models\nto self-debug.arXiv preprint arXiv:2304.05128.\nKarl Cobbe,  Vineet Kosaraju,  Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert,  Jerry  Tworek,  Jacob  Hilton,  Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems.arXiv preprint arXiv:2110.14168.\nAlexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. QAFactEval: Improved QA-\nbased factual consistency evaluation for summariza-\ntion.  InProceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 2587–2601, Seattle, United States. Asso-\nciation for Computational Linguistics.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen,  Arun  Tejasvi  Chaganty,  Yicheng  Fan,  Vin-\ncent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\nand Kelvin Guu. 2022a.  Rarr: Researching and re-\nvising what language models say,  using language\nmodels.\nLuyu Gao,  Aman  Madaan,  Shuyan Zhou,  Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022b.  Pal: Program-aided language\nmodels.arXiv preprint arXiv:2211.10435.\nRahul Jha, Reed Coke, and Dragomir Radev. 2015. Sur-\nveyor: A system for generating coherent survey arti-\ncles for scientific topics. InProceedings of the AAAI\nConference on Artificial Intelligence, volume 29.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu,  Yan  Xu,  Etsuko  Ishii,  Ye  Jin  Bang,  Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation.ACM Comput-\ning Surveys, 55(12):1–38.\nRyo  Kamoi,  Tanya  Goyal,  Juan  Diego  Rodriguez,\nand  Greg  Durrett.  2023.Wice:   Real-world  en-\ntailment  for  claims  in  wikipedia.arXiv  preprint\narXiv:2303.01432.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. InProceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\nAmrith Krishna, Sebastian Riedel, and Andreas Vlachos.\n2022. ProoFVer: Natural logic theorem proving for\nfact verification.Transactions of the Association for\nComputational Linguistics, 10:1013–1030.",
    "Wojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020.   Evaluating the factual\nconsistency of abstractive text summarization.   In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks.Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose  Slone,  Cem  Anil,  Imanol  Schlag,  Theo\nGutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. 2022. Solving quantita-\ntive reasoning problems with language models.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan  Xia,  Yu  Liu,  Yang  Ou,  Shuai  Lu,  Lei  Ji,\nShaoguang  Mao,  Yun  Wang,  Linjun  Shou,  Ming\nGong, and Nan Duan. 2023.  Taskmatrix.ai:  Com-\npleting tasks by connecting foundation models with\nmillions of apis.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman,  Ilya  Sutskever,  and  Karl  Cobbe.  2023.\nLet’s verify step by step.\nChin-Yew Lin. 2004.   ROUGE: A package for auto-\nmatic evaluation of summaries. InText Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. InProceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nPengfei  Liu,  Jinlan  Fu,  Yang  Xiao,  Weizhe  Yuan,\nShuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye,\nand Graham Neubig. 2021. ExplainaBoard: An ex-\nplainable leaderboard for NLP. InProceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing: System\nDemonstrations, pages 280–289, Online. Association\nfor Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023.  Pre-\ntrain, prompt, and predict:  A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nYixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao,\nLinyong Nan, Ruilin Han, Simeng Han, Shafiq Joty,\nChien-Sheng Wu, Caiming Xiong, et al. 2022.  Re-\nvisiting the gold standard:  Grounding summariza-\ntion evaluation with robust human evaluation.arXiv\npreprint arXiv:2212.07981.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan,  Luyu  Gao,  Sarah  Wiegreffe,  Uri  Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank  Gupta,  Bodhisattwa  Prasad  Majumder,\nKatherine Hermann,  Sean Welleck,  Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Iterative\nrefinement with self-feedback.\nOpenAI. 2023. Gpt-4 technical report.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022.  Measuring\nand narrowing the compositionality gap in language\nmodels.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nJohn Schulman. 2023.  Reinforcement learning from\nhuman feedback: Progress and challenges.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023.  Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface.\nFreda Shi, Daniel Fried, Marjan Ghazvininejad, Luke\nZettlemoyer, and Sida I. Wang. 2022.  Natural lan-\nguage to code translation with execution. InProceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3533–3546,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer,  Apoorv  Kulshreshtha,  Heng-Tze  Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions.arXiv preprint arXiv:2201.08239.\nJamesThorne,AndreasVlachos,Christos\nChristodoulopoulos,    and   Arpit   Mittal.   2018a.\nFEVER:  a  large-scale  dataset  for  fact  extraction\nand  VERification.InProceedings  of  the  2018\nConference  of  the  North  American  Chapter  of\nthe   Association   for   Computational   Linguistics:\nHuman  Language  Technologies,  Volume  1  (Long\nPapers),  pages 809–819,  New Orleans,  Louisiana.\nAssociation for Computational Linguistics.\nJamesThorne,AndreasVlachos,Christos\nChristodoulopoulos,    and   Arpit   Mittal.   2018b.\nFEVER: a large-scale dataset for fact extraction and\nVERification. InNAACL-HLT.",
    "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. InProceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008–5020, Online. Asso-\nciation for Computational Linguistics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023.  Self-instruct:  Aligning language\nmodels with self-generated instructions.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nInProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 440–450, Vancouver, Canada.\nAssociation for Computational Linguistics.\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan\nDuan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020.\nReasoning over semantic-level graph for fact check-\ning.  InProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6170–6180, Online. Association for Computational\nLinguistics.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023.  Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206.",
    "A    Prompts\nWe list the claim extraction, query generation, and\nagreement verification prompts used in this paper.\nAll the prompts listed are user prompts. We use the\nsame system prompt “You are a brilliant assistant.”",
    "[KB-Based QA]\nYou are given a piece of text that includes knowledge\nclaims. A claim is a statement that asserts something\nas true or false, which can be verified by humans.\n[Task]\nYour task is to accurately identify and extract every\nclaim stated in the provided text.  Then, resolve any\ncoreference (pronouns or other referring expressions)\nin the claim for clarity. Each claim should be concise\n(less than 15 words) and self-contained.\nYour response MUST be a list of dictionaries.  Each\ndictionary  should  contains  the  key  \"claim\",  which\ncorrespond  to  the  extracted  claim  (with  all  corefer-\nences resolved). You MUST only respond in the for-\nmat as described below. DO NOT RESPOND WITH\nANYTHING ELSE. ADDING ANY OTHER EXTRA\nNOTES THAT VIOLATE THE RESPONSE FORMAT\nIS BANNED. START YOUR RESPONSE WITH ’[’.\n[Response Format]\n[{\"claim\":  \"Ensure that the claim is fewer than 15\nwords and conveys a complete idea. Resolve any coref-\nerence (pronouns or other referring expressions) in the\nclaim for clarity.\" },... ]\nHere are two examples:\n[text]:\nTomas Berdych defeated Gael Monfis 6-1, 6-4 on Sat-\nurday.  The sixth-seed reaches Monte Carlo Masters\nfinal for the first time . Berdych will face either Rafael\nNadal or Novak Djokovic in the final.\n[response]:\n[{\"claim\":    \"Tomas  Berdych  defeated  Gael  Mon-\nfis 6-1,  6-4\"},  {\"claim\":  \"Tomas Berdych defeated\nGael   Monfis   6-1,   6-4   on   Saturday\"},   {\"claim\":\n\"Tomas  Berdych  reaches  Monte  Carlo  Masters  fi-\nnal\"}, {\"claim\": \"Tomas Berdych is the sixth-seed\"},\n{\"claim\": \"Tomas Berdych reaches Monte Carlo Mas-\nters  final  for  the  first  time\"},  {\"claim\":   \"Berdych\nwill face either Rafael Nadal or Novak Djokovic\"},\n{\"claim\":  \"Berdych will face either Rafael Nadal or\nNovak Djokovic in the final\"}]\n[text]:\nTinder only displays the last 34 photos - but users can\neasily see more.  Firm also said it had improved its\nmutual friends feature.\n[response]:\n[{\"claim\":  \"Tinder  only  displays  the  last  photos\"},\n{\"claim\": \"Tinder only displays the last 34 photos\"},\n{\"claim\": \"Tinder users can easily see more photos\"},\n{\"claim\":  \"Tinder said it had improved its feature\"},\n{\"claim\":   \"Tinder  said  it  had  improved  its  mutual\nfriends feature\"}]\nNow complete the following:\n[text]:\n{input_text}\n[response]:\n[Math Problems]\nYou are given a math problem and a potential solution\nto the math problem.\n[Task]\nYour task is to identify all the math calculations that\ninvolve arithmetic operations between known real num-\nbers within the potential solution.  However, do not\ninclude math calculations that contain variable(s).\nYour response MUST be a list of dictionaries.  Each\ndictionary should contains 2 key - \"math_calculation\"\nand \"calculated_answer\", which correspond to the ex-\ntracted math calculation,  and the calculated answer\nwithin the potential solution. You MUST only respond\nin the format as described below. DO NOT RESPOND\nWITH ANYTHING ELSE. ADDING ANY OTHER\nEXTRA NOTES THAT VIOLATE THE RESPONSE\nFORMAT IS BANNED. START YOUR RESPONSE\nWITH ’[’.\n[Response format]:\n[{\"math_calculation\": \"Extracted math calculation in-\nvolving  real  numbers  within  the  potential  solution.\nDo not include math calculations that contains vari-\nable(s). Do not include units such as $, %, etc.\", \"cal-\nculated_answer\":  \"The calculated answer for the ex-\ntracted math calculation.\"},...]\nHere are two examples:\n[math problem]:\nWhat  is  the  area  of  a  circle  with  a  diameter  of  10\ninches?\n[potential solution]:\nTo find the area, we first calculate the radius as the\ndiameter divided by 2, so the radius is 10/2 = 5 inches.\nThen, we use the formula for the area of a circle, which\nisπr\n2\n. Plugging in the radius we get, Area =π5\n2\n=\n78.54 square inches.\n[response]:\n[{\"math_calculation\": \"10/2\", \"calculated_answer\":\n\"5\"},   {\"math_calculation\":\"π∗5\n2\n\",   \"calcu-\nlated_answer\": \"78.54\"}]\n[math problem]:\nA store originally sold a shirt for $45. They are offering\na 20% discount on the shirt. How much will the shirt\ncost now?\n[potential solution]:\nThe discount on the shirt is calculated as 20% of $45,\nwhich is 0.20 * 45 = $9.  The new price of the shirt\nafter the discount is $45 - $9 = $36.\n[response]:\n[{\"math_calculation\":\"0.20*45\",\"calcu-\nlated_answer\":\"9\"},   {\"math_calculation\":\"45\n- 9\",\"calculated_answer\": \"36\"}]\nNow complete the following:\n[math problem]:\n{input_question}\n[potential solution]:\n{input_solution}\n[response]:\n[Scientific Literature Review]\nYou are given a piece of text that mentions some scien-\ntific literature.\n[Task]\nYour task is to accurately find all papers mentioned in\nthe text and identify the title, author(s), and publica-\ntion year for each paper. The response should be a list\nof dictionaries, with each dictionary having keys \"pa-\nper_title\", \"paper_author(s)\", and \"paper_pub_year\",\nwhich correspond to the title of the paper, the authors\nof the paper, and the publication year of the paper.\nThe following is the given text:\n[text]:\n{input_text}\nYou MUST only respond in the format as described be-\nlow. DO NOT RESPOND WITH ANYTHING ELSE.\nADDING ANY OTHER EXTRA NOTES THAT VI-\nOLATE THE RESPONSE FORMAT IS BANNED.\nSTART YOUR RESPONSE WITH ’[’.\n[Response Format]:\n[   {   \"paper_title\":\"Title   of   the   paper.\",    \"pa-\nper_author(s)\":\"Author(s)   of   the   paper.\",   \"pa-\nper_pub_year\":  \"Year of the paper published.\" }, ...\n]\nFigure 6: Prompts for Claim Extraction",
    "[KB-based QA]\nYou are a query generator designed to help users verify a given claim using\nsearch engines. Your primary task is to generate a Python list of two effective\nand skeptical search engine queries. These queries should assist users in criti-\ncally evaluating the factuality of a provided claim using search engines. You\nshould only respond in format as described below (a Python list of queries).\nPLEASE STRICTLY FOLLOW THE FORMAT. DO NOT RETURN ANY-\nTHING ELSE. START YOUR RESPONSE WITH ’[’.   [response format]:\n[’query1’, ’query2’]\nHere are 3 examples:  [claim]:  The CEO of twitter is Bill Gates.  [response]:\n[\"Who is the CEO of twitter?\", \"CEO Twitter\"]\n[claim]: Michael Phelps is the most decorated Olympian of all time. [response]:\n[\"Who is the most decorated Olympian of all time?\", \"Michael Phelps\"]\n[claim]: ChatGPT is created by Google. [response]: [\"Who created ChatGPT?\",\n\"ChatGPT\"]\nNow complete the following: [claim]: input [response]:\n[Math Problems]\nYou are given a math calculation and its corresponding calculated answer.\n[Task]\nYour task is to write an executable Python snippet that validate the accuracy of\nthe math calculation against the calculated answer. The Python snippet should\nprint ’True’ if the calculated answer is correct, and ’False’ otherwise.\nYour response MUST be a dictionary with key \"python_snippet\", which corre-\nspond to the executable python snippet.\n[math calculation]: {math_calculation}\n[calculated answer]: {calculated_answer}\nYou MUST only respond in the format as described below.  DO NOT RE-\nSPOND WITH ANYTHING ELSE. ADDING ANY OTHER EXTRA NOTES\nTHAT VIOLATE THE RESPONSE FORMAT IS BANNED. START YOUR\nRESPONSE WITH ’{’.\n[Response format]:\n{ \"python_snippet\": \"An executable Python snippet that validates the accuracy\nof the math calculation against the calculated answer.  The Python snippet\nshould print ’True’ if the calculated answer is correct, and ’False’ otherwise.\" }\n[Code Potential Solution Generation]\nPlease solve the given coding question.  Make sure that the solution is opti-\nmized and correct. You MUST use Python to solve the coding question. Your\nresponse MUST be a dictionary with keys \"reasoning\" and \"python_solution\",\nwhich correspond to the reasoning and Python implementations of the function\n{entry_point}. The following is the given coding question - [coding question]:\n{input_question} You MUST only respond in the format as described below.\nDO NOT RESPOND WITH ANYTHING ELSE. ADDING ANY OTHER\nEXTRA NOTES THAT VIOLATE THE RESPONSE FORMAT IS BANNED.\nSTART YOUR RESPONSE WITH ’{’.   [response format]:  { \"reasoning\":\n\"Reasoning for solution.\", \"python_solution\": \"Python implementation of the\nfunction {entry_point}. Include only the implementation of the function itself.\nEnsure the output of the function aligns with its specified return type.\" }\n[Code Unit test Generation]\nPlease generate 3 distinct function calls for the given coding question to test the\nfunctionality of the function {entry_point} that attempts to solve the provided\ncoding question.\nYour response must be a dictionary with 3 keys - \"function_call_1\", \"func-\ntion_call_2\", \"function_call_3\", which correspond to the 3 distinct function\ncalls for function {entry_point}. The following is the given coding question -\n[coding question]: {input_question}\nYou MUST only respond in the format as described below.  DO NOT RE-\nSPOND WITH ANYTHING ELSE. ADDING ANY OTHER EXTRA NOTES\nTHAT VIOLATE THE RESPONSE FORMAT IS BANNED. START YOUR\nRESPONSE WITH ’{’.\n[response format]: { \"function_call_1\": \"First function call for function {en-\ntry_point}. Do not include anything else.\", \"function_call_2\": \"Second func-\ntion call for function {entry_point}.  Do not include anything else.\", \"func-\ntion_call_3\": \"Third function call for function {entry_point}. Do not include\nanything else.\" }\nFigure 7: Prompts for Query Generation\n[KB-based QA]\nYou are given a piece of text. Your task is to identify whether there are any fac-\ntual errors within the text. When you are judging the factuality of the given text,\nyou could reference the provided evidences if needed. The provided evidences\nmay be helpful.  Some evidences may contradict to each other.  You must be\ncareful when using the evidences to judge the factuality of the given text. When\nThe response should be a dictionary with four keys - \"reasoning\", \"factuality\",\n\"error\", and \"correction\", which correspond to the reasoning, whether the given\ntext is factual or not (Boolean - True or False), the factual error present in the\ntext, and the corrected text. The following is the given text [text]: claim The\nfollowing is the provided evidences [evidences]:  evidence You should only\nrespond in format as described below. DO NOT RETURN ANYTHING ELSE.\nSTART YOUR RESPONSE WITH ’{{’.  [response format]: {{ \"reasoning\":\n\"Why is the given text factual or non-factual? Be careful when you said some-\nthing is non-factual. When you said something is non-factual, you must provide\nmulitple evidences to support your decision.\", \"error\":  \"None if the text is\nfactual; otherwise, describe the error.\", \"correction\": \"The corrected text if there\nis an error.\", \"factuality\": True if the given text is factual, False otherwise. }}\n[Scientific Literature Review]\nPlease generate 3 distinct function calls for the given coding question to test the\nYou are provided with two inputs, a string (string1) containing several names,\nand a list (list1) also containing names. Your task is to assess whether all the\nlast names mentioned in string1 are included in list1.\nYou should only respond in format as described below.  DO NOT RETURN\nANYTHING ELSE. START YOUR RESPONSE WITH ’{{’. [response format]:\n{{ \"reasoning\": \"Explanation on whether all the last names in string1 are found\nwithin list1\", \"factuality\": This will be True if all last names from string1 are\npresent in list1, and False otherwise. }}\nExample 1: [string1]: \"J. Devlin and M. Chang\" [list1]: [\"Devlin\", \"M Chang\",\n\"Kristina Toutanova\"] [response]: \"reasoning\": \"string1 contains 2 last names\n’Devlin’ and ’Chang’. Both of these last names are present in list1.\", \"factuality\":\nTrue\nExample 2: [string1]: \"Tom Brown et. al\" [list1]: [\"Y. Lecun\", \"G. Hinton\"]\n[response]: \"reasoning\": \"string 1 contains 1 last name ’Brown’. Brown is not\npresent in list1.\", \"factuality\": False\nComplete the following: [string1]: string1 [list1]: list2 [response]:\nFigure 8: Prompts for Agreement Verification",
    "B    Example cases of FACTOOL\nWe list the example cases ofFACTOOLin each\nscenario.",
    "{\n\"claim\": \"The modern-day version of fortune cookies was invented in the United States.\",\n\"label\": true, \"with_tool_classification\": false,\n\"with_tool_reasoning\": \"The given text is not entirely factual.  The modern-day version of fortune cookies was not invented in the United States.  Most people\nnowadays believe that fortune cookies were created by a Japanese man named Makoto Hagiwara in 1914 in San Francisco. Hagiwara owned what is now called the\nGolden Gate Park Japanese Tea Garden, where he served tea and fortune cookies. This is supported by the provided evidences.\",\n\"queries\": [\"Where were fortune cookies invented?\", \"History of fortune cookies in the United States\"],\n\"evidences\": [\"Most people nowadays believe that fortune cookies were created by a Japanese man named Makoto Hagiwara in 1914 in San Francisco. Hagiwara\nowned what is now called the Golden Gate Park Japanese Tea Garden, where he served tea and fortune cookies. Nov 19, 2019\", \"Most people nowadays believe that\nfortune cookies were created by a Japanese man named Makoto Hagiwara in 1914 in San Francisco. Hagiwara owned what is now called the Golden Gate Park\nJapanese Tea Garden, where he served tea and fortune cookies. Nov 19, 2019\"]\n}\n{\n\"claim\": \"Sonia Sotomayor is a female US Supreme Court justice\",\n\"label\": true,\n\"with_tool_classification\": false,\n\"with_tool_reasoning\": \"The given text is partially factual. While Sonia Sotomayor is indeed a female US Supreme Court justice, the text fails to mention that she\nwas also the first Latina to serve on the Supreme Court. This information is provided in both of the provided evidences.\",\n\"queries\": [\"Is Sonia Sotomayor a US Supreme Court justice?\", \"Sonia Sotomayor gender US Supreme Court\"],\n\"evidences\": [\"In 2009, Sonia Sotomayor broke boundaries by becoming the first Latina, and the third woman, to serve on the Supreme Court of the United States,\nthe highest court in the country. Sotomayor was born in New York City to Puerto Rican parents.\", \"Supreme Court of the United States. Sonia Sotomayor\n ̆\n2013 the\nfearless federal trial court judge who saved Major League Baseball from a ruinous 1995 strike\n ̆\n2013 entered the record book as the first Hispanic and the third woman\nto serve on the High Court.\"]\n}\nFigure 9: Reasoning errors of FACTOOLpowered by ChatGPT on KB-based QA\n[Fact-checking recent events, discoveries, or news]\n{\n\"claim\": \"Argentina has not won the World Cup since 1986\",\n\"label\": false,\n\"with_tool_classification\": false,\n\"with_tool_reasoning\": \"The given text states that Argentina has not won the\nWorld Cup since 1986.  However, multiple pieces of evidence suggest that\nArgentina won the World Cup in 2022.\",\n\"queries\": [\"Argentina World Cup wins since 1986\", \"Last time Argentina won\nWorld Cup\"],\n\"evidences\": [\"Argentina is one of the most successful teams in the tournament’s\nhistory, having won three World Cups:  in 1978, 1986, 2022.  Argentina has\nalso been runner up three times:  in 1930, 1990 and 2014.  In 18 World Cup\ntournaments, Argentina has 47 victories in 88 matches.\", \"Argentina is one of\nthe most successful teams in the tournament’s history, having won three World\nCups: in 1978, 1986, 2022. Argentina has also been runner up ...\", \"Previously,\nthe last time Argentina won the World Cup was 1986, when it defeated Germany\nto win its second title in three tournaments.\", \"Argentina have won the FIFA\nWorld Cup three times\n ̆\n2013 in 1978, 1986 and 2022. They have reached the\nfinal six times, including a second-place ...\", \"The Argentines have won two\nFIFA World Cups, in 1978 and 1986.  They have placed second three times\nbefore, in 1930, 1990, and 2014.\", \"Argentina last won the FIFA World Cup\nin 1986, defeating West Germany in the championship round, 3-2. Argentina\nWorld Cup Wins. Argentina’s ...\"]\n}\n{\n\"claim\": \"The most valuable NFT ever sold is a digital artwork called ’Every-\ndays: The First 5000 Days’ \",\n\"label\": false,\n\"with_tool_classification\": false,\n\"with_tool_reasoning\": \"The given text states that the most valuable NFT ever\nsold is ’Everydays: The First 5000 Days’. According to the provided evidence,\n’The First 5000 Days’ is the second most expensive NFT ever sold, with ’Merge’\nbeing the most expensive one.\",\n\"queries\": [\"Most valuable NFT ever sold\", \"Everydays: The First 5000 Days\"],\n\"evidences\":  [\"The Most Expensive NFTs Ever Sold 1.  Merge - 2014 US\n$91.8M. Pak - Merge. . . . 2. The First 5000 Days - 2014 US$69.3M. Beeple\n- 2014 The First 5000 Days.  . . . 3.  Clock - 2014 US$52.7M. Julian Assange\n& Pak - 2014 Clock.  . . . 4.  HUMAN ONE - 2014  US$29M. Beeple - 2014\nHUMAN  ONE.  . . . 5.   CryptoPunk  #5822  -  2014  US$23.7M.  CryptoPunk\n#5822.\", \"Everydays:  the First 5000 Days:  Work of art.\", \"Everydays:  the\nFirst 5000 Days is a digital work of art created by Mike Winkelmann, known\nprofessionally as Beeple. The work is a collage of 5000 digital images created\nby Winkelmann for his Everydays series.\", \"Everydays: the First 5000 Days\nArtist: Mike Winkelmann.\", \"Everydays: the First 5000 Days Owner: Vignesh\nSundaresan (MetaKovan).\", \"Everydays: the First 5000 Days Medium: Digital\n(JPEG).\"]\n}\n[Fact-checking high-precision statistics]\n{\n\"claim\": \"Ireland has an obesity rate of 26.9%\",\n\"label\": false,\n\"with_tool_classification\": false,\n\"with_tool_reasoning\": \"The given text states that Ireland has an obesity rate of\n26.9%, but the provided evidences show different numbers. The first evidence\ndoesn’t provide a clear percentage for obesity, but the second evidence states\nthat the prevalence of obesity in Irish adults is currently 18%, with men at 20%\nand women at 16%. This contradicts the given text.\",\n\"queries\": [\"Ireland obesity rate statistics\", \"Current obesity rate in Ireland\"],\n\"evidences\": [\"Just under four in ten (37%) of people have a normal weight, six\nout of ten (37% overweight and a further 23% obese) overweight or obese. A\nsmaller difference exists between men and women aged under 25 than those\nolder than this.\", \"The prevalence of obesity in Irish adults is currently 18%,\nwith men at 20% and women at 16%. A further 47% of men and 33% of women\nare overweight (BMI 25.0-29.9 kg/m2).  Since 1990, obesity has more than\ndoubled in men from 8% to 20%, and increased from 13% to 16% in women.\"]\n}\n{\n\"claim\": \" ’Everydays: The First 5000 Days’ sold for $69 million\",\n\"label\": false,\n\"with_tool_classification\": false,\n\"with_tool_reasoning\": \"The given text is almost factual, but the exact amount\nfor which the artwork was sold is slightly off. Multiple evidences mention the\ncorrect amount.\",\n\"queries\": [\"Everydays: The First 5000 Days sale price\", \" $69 million Every-\ndays artwork\"],\n\"evidences\": [\"The work is a collage of 5000 digital images created by Winkel-\nmann for his Everydays series. Its associated non-fungible token (NFT) was\nsold for $69.3 million at Christie’s in 2021,  making it the most expensive\nnon-fungible token ever.\", \"\n ̆\n201cNow there is a way to collect digital art.\n ̆\n201d\nThe piece that was sold,  Everydays:  The First 5000 Days,  is a collage of\nWinkelmann’s work starting ...\", \"That day, Winkelmann, a digital artist more\ncommonly known by the handle Beeple, sold\n ̆\n201cEverydays: The First 5000\nDays,\n ̆\n201d an NFT-linked ...\", \"\n ̆\n201cEverydays\n ̆\n2014 The First 5000 Days,\n ̆\n201d\nby the artist known as Beeple, set a record for a digital artwork in a sale at\nChristie’s.\", \"Artist Beeple, a North Fond du Lac native, sold a digital artwork\nfor $69 million. Now you can see the same art in a book for a lot less.\", \"Beeple,\naka artist Mike Winkelmann, appeared on\n ̈\nThe ...\"]\n}\nFigure 10: Examples of KB-QA claims verified byFACTOOLpowered by GPT-4. The queries generated, evidences\ncollected, the reasoning process, and the final decision of FACTOOLpowered by GPT-4 is reported.",
    "{\n\"task_id\": \"HumanEval/2\",\n\"prompt\":\nd e ft r u n c a t e _ n u m b e r ( n u m b e r :f l o a t)   −>f l o a t:\n\" \" \" G i v e n   a   p o s i t i v e    f l o a t i n g    p o i n t   number ,\ni t    c a n   b e   d e c o m p o s e d   i n t o   a n d   i n t e g e r    p a r t\n( l a r g e s t    i n t e g e r    s m a l l e r   t h a n   g i v e n   n u m b e r )\na n d   d e c i m a l s\n( l e f t o v e r    p a r t   a l w a y s   s m a l l e r   t h a n    1 ) .\nR e t u r n   t h e   d e c i m a l    p a r t    o f    t h e   n u m b e r . \" \" \"\n\"entry_point\": \"truncate_number\",\n\"completion\":\nd e ft r u n c a t e _ n u m b e r ( n u m b e r :f l o a t)   −>f l o a t:\ni n t e g e r _ p a r t   =   n u m b e r    / /   1\nd e c i m a l _ p a r t   =   n u m b e r   −   i n t e g e r _ p a r t\nr e t u r nd e c i m a l _ p a r t\n\"result\": \"passed\",\n\"passed\": true,\n\"with_tool_classification\": true,\n\"testcases_input\": [\"truncate_number(4.56)\", \"truncate_number(0.123)\", \"trun-\ncate_number(19.999)\"],\n\"multi_solutions\": [\nd e ft r u n c a t e _ n u m b e r ( n u m b e r :f l o a t)   −>f l o a t:\nr e t u r nn u m b e r   −i n t( n u m b e r )\nd e ft r u n c a t e _ n u m b e r ( n u m b e r :f l o a t)   −>f l o a t:\nr e t u r nn u m b e r   −i n t( n u m b e r )\nd e ft r u n c a t e _ n u m b e r ( n u m b e r :f l o a t)   −>f l o a t:\nr e t u r nn u m b e r   −i n t( n u m b e r )\nd e ft r u n c a t e _ n u m b e r ( n u m b e r :f l o a t)   −>f l o a t:\ni n t e g e r _ p a r t   =   n u m b e r    / /   1\nd e c i m a l _ p a r t   =   n u m b e r   −   i n t e g e r _ p a r t\nr e t u r nd e c i m a l _ p a r t\n]\n\"exec_result\":[[0.5599999999999996,0.5599999999999996,\n0.5599999999999996,0.5599999999999996],[0.123,0.123,0.123,\n0.123], [0.9989999999999988, 0.9989999999999988, 0.9989999999999988,\n0.9989999999999988]]\n}\n{\n\"task_id\": \"HumanEval/57\",\n\"prompt\":\nd e fm o n o t o n i c ( l :l i s t) :\n\" \" \" R e t u r n   T r u e    i fl i s t    e l e m e n t s   a r e   m o n o t o n i c a l l y\ni n c r e a s i n g   o r   d e c r e a s i n g . \" \" \"\n\"entry_point\": \"monotonic\",\n\"completion\":\nd e fm o n o t o n i c ( l :l i s t) :\ni f    l e n( l )   <=   1 :\nr e t u r nT r u e\ni n c r e a s i n g   =a l l( l [ i ]   <=   l [ i + 1 ]f o rii n   r a n g e(l e n( l ) − 1 ) )\nd e c r e a s i n g   =a l l( l [ i ]   >=   l [ i + 1 ]f o rii n   r a n g e(l e n( l ) − 1 ) )\nr e t u r ni n c r e a s i n go rd e c r e a s i n g\n\"result\": \"passed\",\n\"passed\": true,\n\"with_tool_classification\": true,\n\"testcases_input\": [\"monotonic([1, 2, 3, 4, 5])\", \"monotonic([5, 4, 3, 2, 1])\",\n\"monotonic([1, 3, 5, 4, 7])\"],\n\"multi_solutions\": [\nd e fm o n o t o n i c ( l :l i s t) :\ni f    l e n( l )   <   2 :\nr e t u r nT r u e\ni n c r e a s i n g   =   d e c r e a s i n g   =   T r u e\nf o rii n   r a n g e(l e n( l )   −   1 ) :\ni fl [ i ]   <   l [ i   +   1 ] :\nd e c r e a s i n g   =   F a l s e\ni fl [ i ]   >   l [ i   +   1 ] :\ni n c r e a s i n g   =   F a l s e\ni f    n o ti n c r e a s i n gand   n o td e c r e a s i n g :\nr e t u r nF a l s e\nr e t u r nT r u e\nd e fm o n o t o n i c ( l :l i s t) :\ni f    l e n( l )   <   2 :\nr e t u r nT r u e\nd i r e c t i o n   =   l [ 1 ]   −   l [ 0 ]\nf o rii n   r a n g e( 2 ,l e n( l  ) ) :\ni fd i r e c t i o n   ==   0 :\nd i r e c t i o n   =   l [ i ]   −   l [ i − 1 ]\nc o n t i n u e\ni f( l [ i ]   −   l [ i − 1 ] )\n*\nd i r e c t i o n   <   0 :\nr e t u r nF a l s e\nr e t u r nT r u e\nd e fm o n o t o n i c ( l :l i s t) :\ni f    l e n( l )   <   2 :\nr e t u r nT r u e\nd i r e c t i o n   =   l [ 1 ]   −   l [ 0 ]\nf o rii n   r a n g e( 2 ,l e n( l  ) ) :\ni fd i r e c t i o n   ==   0 :\nd i r e c t i o n   =   l [ i ]   −   l [ i   −   1 ]\nc o n t i n u e\ni f( l [ i ]   −   l [ i   −   1 ] )\n*\nd i r e c t i o n   <   0 :\nr e t u r nF a l s e\nr e t u r nT r u e\nd e fm o n o t o n i c ( l :l i s t) :\ni f    l e n( l )   <=   1 :\nr e t u r nT r u e\ni n c r e a s i n g   =a l l( l [ i ]   <=   l [ i + 1 ]f o rii n   r a n g e(l e n( l ) − 1 ) )\nd e c r e a s i n g   =a l l( l [ i ]   >=   l [ i + 1 ]f o rii n   r a n g e(l e n( l ) − 1 ) )\nr e t u r ni n c r e a s i n go rd e c r e a s i n g\n]\n\"exec_result\": [[true, true, true, true], [true, true, true, true], [false, false, false,\nfalse]]\n}\nFigure 11: Examples of code claims verified byFACTOOLpowered by GPT-4. The queries generated, evidences\ncollected, the reasoning process, and the final decision of FACTOOLpowered by GPT-4 is reported.",
    "[ChatGPT]\n{\n\"task_id\": \"HumanEval/36\",\n\"prompt\":\nd e ff i z z _ b u z z ( n :i n t) :\n\" \" \" R e t u r n   t h e   n u m b e r   o f    t i m e s    t h e    d i g i t   7\na p p e a r s   i n    i n t e g e r s    l e s s    t h a n   n\nw h i c h   a r e    d i v i s i b l e    b y   11   o r   1 3 . \" \" \"\n,\n\"entry_point\": \"fizz_buzz\",\n\"completion\":\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\ni f    s t r( i ) . c o u n t ( ’ 7 ’ )   >   0 :\nc o u n t   +=   1\nr e t u r nc o u n t\n\"result\": \"failed: AssertionError: \",\n\"passed\": false,\n\"testcases_input\": [\"fizz_buzz(50)\", \"fizz_buzz(100)\", \"fizz_buzz(200)\"],\n\"multi_solutions\": [\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\ni f’ 7 ’i n    s t r( i ) :\nc o u n t   +=   1\nr e t u r nc o u n t\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\ni f’ 7 ’i n    s t r( i ) :\nc o u n t   +=   1\nr e t u r nc o u n t\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\nc o u n t   +=s t r( i ) . c o u n t ( ’ 7 ’ )\nr e t u r nc o u n t\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\ni f    s t r( i ) . c o u n t ( ’ 7 ’ )   >   0 :\nc o u n t   +=   1\nr e t u r nc o u n t\n]\n\"exec_result\": [[0, 0, 0, 0], [2, 2, 3, 2], [5, 5, 6, 5]],\n\"with_tool_classification\": true\n}\n[GPT-4]\n{\n\"task_id\": \"HumanEval/36\",\n\"prompt\":\nd e ff i z z _ b u z z ( n :i n t) :\n\" \" \" R e t u r n   t h e   n u m b e r   o f    t i m e s    t h e    d i g i t   7\na p p e a r s   i n    i n t e g e r s    l e s s    t h a n   n\nw h i c h   a r e    d i v i s i b l e    b y   11   o r   1 3 . \" \" \"\n,\n\"entry_point\": \"fizz_buzz\",\n\"completion\":\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\ni f    s t r( i ) . c o u n t ( ’ 7 ’ )   >   0 :\nc o u n t   +=   1\nr e t u r nc o u n t\n\"result\": \"failed: AssertionError: \",\n\"passed\": false,\n\"with_tool_classification\": false,\n\"testcases_input\": [\"fizz_buzz(50)\", \"fizz_buzz(100)\", \"fizz_buzz(150)\"],\n\"multi_solutions\": [\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( 1 ,   n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\nc o u n t   +=s t r( i ) . c o u n t ( ’ 7 ’ )\nr e t u r nc o u n t\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( 1 ,   n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\nc o u n t   +=s t r( i ) . c o u n t ( ’ 7 ’ )\nr e t u r nc o u n t\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( 1 ,   n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\nc o u n t   +=s t r( i ) . c o u n t ( ’ 7 ’ )\nr e t u r nc o u n t\nd e ff i z z _ b u z z ( n :i n t) :\nc o u n t   =   0\nf o rii n   r a n g e( n ) :\ni fi   %  11   ==   0o ri   %  13   ==   0 :\ni f    s t r( i ) . c o u n t ( ’ 7 ’ )   >   0 :\nc o u n t   +=   1\nr e t u r nc o u n t\n]\n\"exec_result\": [[0, 0, 0, 0], [3, 3, 3, 2], [4, 4, 4, 3]]\n}\nFigure 12: Examples of code claims verified byFACTOOLpowered by ChatGPT andFACTOOLpowered by GPT-4.\nThe queries generated, evidences collected, the reasoning process, and the final decisions ofFACTOOLpowered by\nChatGPT and FACTOOLpowered by GPT-4 are reported.",
    "[ChatGPT]\n{\n\"math_calculation\": \"23 * 4319216\",\n\"calculated_answer\": \"99305768\",\n\"label\": false,\n\"query\": \"python_snippet\": \"print(23 * 4319216 == 99305768)\",\n\"exec_result\": \"False\",\n\"with_tool_classification\": false\n}\n{\n\"math_calculation\": \"20/100 x $10884297.00\",\n\"calculated_answer\": \"2176859.40\",\n\"label\": true,\n\"query\": {\"python_snippet\": \"print(abs((20/100) * 10884297.00 - 2176859.40)\n< 0.01)\"},\n\"exec_result\": \"True\",\n\"with_tool_classification\": true\n}\n[GPT-4]\n{\n\"math_calculation\": \"23 * 4319216\",\n\"calculated_answer\": \"99305768\",\n\"label\": false,\n\"query\": {\"python_snippet\": \"calculation = 23 * 4319216; answer = 99305768;\nprint(calculation == answer)\"},\n\"exec_result\": \"False\",\n\"with_tool_classification\": false\n}\n{\n\"math_calculation\": \"20/100 x $10884297.00\",\n\"calculated_answer\": \"2176859.40\",\n\"label\": true,\n\"query\": {\"python_snippet\": \"calculation = (20/100) * 10884297.00; answer =\n2176859.40; print(abs(calculation - answer) < 0.01)\"},\n\"exec_result\": \"True\",\n\"with_tool_classification\": true\n}\nFigure 13: Examples of math claims verified byFACTOOLpowered by ChatGPT and GPT-4. The full response\nofFACTOOLpowered by GPT-4 is reported, including the queries generated, evidences collected, the reasoning\nprocess, and the final decision.\n[ChatGPT]\n{\n\"paper_title\":  \"BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\",\n\"paper_author(s)\":  \"Jacob Devlin,  Ming-Wei Chang,  Kenton Lee,  Kristina\nToutanova\",\n\"paper_pub_year\": \"2018\",\n\"label\": true,\n\"with_tool_classification\": true,\n\"error\": []\n}\n{\n\"paper_title\": \"The Impact of Artificial Intelligence on Employment\",\n\"paper_author(s)\": \"Acemoglu and Restrepo\",\n\"paper_pub_year\": \"2019\",\n\"label\": false,\n\"with_tool_classification\": false,\n\"error\": [\"wrong_paper_author(s)\", \"wrong_paper_pub_year\"]\n}\n[GPT-4]\n{\n\"paper_title\":  \"BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\",\n\"paper_author(s)\":  \"Jacob Devlin,  Ming-Wei Chang,  Kenton Lee,  Kristina\nToutanova\",\n\"paper_pub_year\": \"2018\",\n\"label\": true,\n\"with_tool_classification\": true,\n\"error\": []\n}\n{\n\"paper_title\": \"The Impact of Artificial Intelligence on Employment\",\n\"paper_author(s)\": \"Acemoglu and Restrepo\",\n\"paper_pub_year\": \"2019\",\n\"label\": false,\n\"with_tool_classification\": false,\n\"error\": [\"wrong_paper_author(s)\", \"wrong_paper_pub_year\"]\n}\nFigure 14: Examples of scientific claims verified byFACTOOLpowered by ChatGPT and GPT-4. The full response\nofFACTOOLpowered by ChatGPT and GPT-4 is reported, including the queries generated, evidences collected, the\nreasoning process, and the final decision.\n[Reasoning Error]\n{\n\"claim\": \"Jupiter is less dense than Saturn\",\n\"label\": false,\n\"with_tool_classification\": true,\n\"with_tool_reasoning\": \"The given text is factual be-\ncause both provided evidences support the claim that\nJupiter is less dense than Saturn.  The first evidence\nprovides numerical data on the densities of Jupiter and\nSaturn, and the second evidence confirms that Saturn\nis the least dense planet in the solar system.\",\n\"queries\": [\"Jupiter vs Saturn density\", \"Which planet\nis less dense Jupiter or Saturn?\"],\n\"evidences\": [\"The density of Jupiter is 1,33 g/cm\n ̆\n00b3\nwhereas the density of Saturn is only 0,687 g/cm\n ̆\n00b3.\nIf Saturn were the same density as Jupiter, it would\nhave a radius of 30.079 km.\", \"Jupiter is the largest\nplanet in the solar system, but it’s Saturn\n ̆\n2014the solar\nsystem’s second largest planet\n ̆\n2014that takes the prize\nfor least dense. Aug 1, 2019\"]\n}\n[Conflicting Evidences]\n{\n\"claim\": \"Jupiter has a density of 1.33 grams per cubic\ncentimeter\",\n\"label\": true,\n\"with_tool_classification\": false,\n\"with_tool_reasoning\":  \"The given text is mostly in\nline with the provided evidences. There is a minor dis-\ncrepancy between the two evidences, with one stating\nthe density of Jupiter as 1.326 g/cm\n ̆\n00b3 and the other\nas 1.33 g/cm\n ̆\n00b3. However, the difference is minimal\nand could be due to rounding.\",\n\"queries\": [\"Jupiter density grams per cubic centime-\nter\", \"Density of Jupiter\"],\n\"evidences\": [\"Density of Jupiter: However, at 1.326\ng/cm3,  it  is  also  the  second-most  dense  of  the  gas\ngiants. Feb 17, 2016\", \"1.33 g/cm\n ̆\n00b3\"]\n}\n[Ambiguity in descriptions]\n{\n\"claim\":  \"Fortune cookies are enjoyed by people all\nover the world.\",\n\"label\": false,\n\"with_tool_classification\": true,\n\"with_tool_reasoning\": \"The given text states that for-\ntune cookies are enjoyed by people all over the world.\nWhile the evidence provided does not explicitly men-\ntion the global enjoyment of fortune cookies, it does\nmention that 3 billion fortune cookies are made each\nyear globally, with the majority consumed in the US.\nThis suggests that fortune cookies are indeed enjoyed\nby people in various parts of the world.\",\n\"queries\": [\"Origin of fortune cookies\", \"Popularity of\nfortune cookies worldwide\"],\n\"evidences\":  [\"Japan\",  \"There  are  approximately  3\nbillion fortune cookies made each year globally, the\nmajority of them consumed in the US. The largest man-\nufacturer of the cookies is Wonton Food, Inc., head-\nquartered in Brooklyn, New York. They make over 4.5\nmillion fortune cookies per day.\"]\n}\nFigure 15: Some error cases ofFACTOOLon KB-QA. The full response ofFACTOOLpowered by GPT-4 is reported,\nincluding the queries generated, evidences collected, the reasoning process, and the final decision is also reported.",
    "[Limited variety in synthetic testcases]\n{\n\"task_id\": \"HumanEval/64\",\n\"prompt\": \"Write a functionvowels_count(s)which takes a string repre-\nsenting a word as input and returns the number of vowels in the string. Vowels\nin this case are ’a’, ’e’, ’i’, ’o’, ’u’. Here, ’y’ is also a vowel, but only when it is\nat the end of the given word.\",\n\"entry_point\": \"vowels_count\",\n\"completion\":\nd e fv o w e l s _ c o u n t ( s ) :\nv o w e l s   =   [ ’ a ’ ,    ’ e ’ ,    ’ i ’ ,    ’ o ’ ,    ’ u ’ ]\nc o u n t   =   0\nf o rii n   r a n g e(l e n( s ) ) :\ni fs [ i ]i nv o w e l s :\nc o u n t   +=   1\ni fi   ==l e n( s )   −   1ands [ i ]   ==   ’ y ’ :\nc o u n t   +=   1\nr e t u r nc o u n t\n\"result\": \"failed: AssertionError: Test 2\",\n\"passed\": false,\n\"with_tool_classification\": true,\n\"testcases_input\": [\"vowels_count(’encyclopedia’)\", \"vowels_count(’rhythm’)\",\n\"vowels_count(’beautiful’)\"],\n\"multi_solutions\": [\nd e fv o w e l s _ c o u n t ( s ) :\nv o w e l s   =   ’ a e i o u ’\nc o u n t   =   0\nf o ri  ,   ci n   e n u m e r a t e( s ) :\ni fc . l o w e r ( )i nv o w e l s :\nc o u n t   +=   1\ne l i fc . l o w e r ( )   ==   ’ y ’andi   ==l e n( s )   −   1 :\nc o u n t   +=   1\nr e t u r nc o u n t\nd e fv o w e l s _ c o u n t ( s ) :\nv o w e l s   =   ’ a e i o u ’\nc o u n t   =   0\nf o rc h a ri ns :\ni fc h a r . l o w e r ( )i nv o w e l s :\nc o u n t   +=   1\ni fs [ − 1 ] . l o w e r ( )   ==   ’ y ’ :\nc o u n t   +=   1\nr e t u r nc o u n t\nd e fv o w e l s _ c o u n t ( s ) :\nv o w e l s   =   ’ a e i o u ’\nc o u n t   =   0\nf o rc h a ri ns :\ni fc h a r . l o w e r ( )i nv o w e l s :\nc o u n t   +=   1\ni fs [ − 1 ] . l o w e r ( )   ==   ’ y ’ :\nc o u n t   +=   1\nr e t u r nc o u n t\nd e fv o w e l s _ c o u n t ( s ) :\nv o w e l s   =   [ ’ a ’ ,    ’ e ’ ,    ’ i ’ ,    ’ o ’ ,    ’ u ’ ]\nc o u n t   =   0\nf o rii n   r a n g e(l e n( s ) ) :\ni fs [ i ]i nv o w e l s :\nc o u n t   +=   1\ni fi   ==l e n( s )   −   1ands [ i ]   ==   ’ y ’ :\nc o u n t   +=   1\nr e t u r nc o u n t\n]\n\"exec_result\": [[5, 5, 5, 5], [0, 0, 0, 0], [5, 5, 5, 5]]\n}\n[Potential errors in code generation]\n{\n\"task_id\": \"HumanEval/79\",\n\"prompt\":\nd e fd e c i m a l _ t o _ b i n a r y ( d e c i m a l ) :\n\" \" \" You   w i l l   b e   g i v e n   a   n u m b e r   i n   d e c i m a l   f o r m\na n d   y o u r   t a s k    i s    t o    c o n v e r t    i t    t o    b i n a r y    f o r m a t .\nT h e   f u n c t i o n    s h o u l d    r e t u r n   a   s t r i n g  ,\nw i t h   e a c h   c h a r a c t e r    r e p r e s e n t i n g   a   b i n a r y   n u m b e r .\nEach   c h a r a c t e r    i n    t h e    s t r i n g    w i l l   b e    ’ 0 ’   o r    ’ 1 ’ .\nT h e r e    w i l l   b e   an   e x t r a    c o u p l e   o f\nc h a r a c t e r s    ’ db ’   a t    t h e    b e g i n n i n g\na n d   a t    t h e   e n d   o f    t h e    s t r i n g .\nT h e   e x t r a    c h a r a c t e r s   a r e   t h e r e\nt o    h e l p   w i t h    t h e    f o r m a t .    \" \" \"\n,\n\"entry_point\": \"decimal_to_binary\",\n\"completion\":\nd e fd e c i m a l _ t o _ b i n a r y ( d e c i m a l ) :\nb i n a r y _ l i s t   =   [ ]\nw h i l ed e c i m a l   >   0 :\nr e m a i n d e r   =   d e c i m a l  %  2\nb i n a r y _ l i s t . a p p e n d (s t r( r e m a i n d e r ) )\nd e c i m a l   =   d e c i m a l    / /   2\nb i n a r y _ l i s t . r e v e r s e ( )\nb i n a r y _ s t r i n g   =   ’ ’ . j o i n ( b i n a r y _ l i s t )\nr e t u r n’ db ’   +   b i n a r y _ s t r i n g   +   ’ db ’\n\"result\": \"failed: AssertionError: \",\n\"passed\": false,\n\"with_tool_classification\": true,\n\"testcases_input\": [\"decimal_to_binary(10)\", \"decimal_to_binary(255)\", \"deci-\nmal_to_binary(0)\"],\n\"multi_solutions\": [\nd e fd e c i m a l _ t o _ b i n a r y ( d e c i m a l ) :\nb i n a r y _ n u m   =   ’ ’\nw h i l ed e c i m a l   >   0 :\nr e m a i n d e r   =   d e c i m a l  %  2\nd e c i m a l   =   d e c i m a l    / /   2\nb i n a r y _ n u m   =s t r( r e m a i n d e r )   +   b i n a r y _ n u m\nr e t u r n’ db ’   +   b i n a r y _ n u m   +   ’ db ’\nd e fd e c i m a l _ t o _ b i n a r y ( d e c i m a l ) :\nb i n a r y   =   ’ ’\nw h i l ed e c i m a l   >   0 :\nr e m a i n d e r   =   d e c i m a l  %  2\nd e c i m a l   =   d e c i m a l    / /   2\nb i n a r y   =s t r( r e m a i n d e r )   +   b i n a r y\nr e t u r n’ db ’   +   b i n a r y   +   ’ db ’\nd e fd e c i m a l _ t o _ b i n a r y ( d e c i m a l ) :\nb i n a r y   =   ’ ’\nw h i l ed e c i m a l   >   0 :\nr e m a i n d e r   =   d e c i m a l  %  2\nd e c i m a l   =   d e c i m a l    / /   2\nb i n a r y   =s t r( r e m a i n d e r )   +   b i n a r y\nr e t u r n’ db ’   +   b i n a r y   +   ’ db ’\nd e fd e c i m a l _ t o _ b i n a r y ( d e c i m a l ) :\nb i n a r y _ l i s t   =   [ ]\nw h i l ed e c i m a l   >   0 :\nr e m a i n d e r   =   d e c i m a l  %  2\nb i n a r y _ l i s t . a p p e n d (s t r( r e m a i n d e r ) )\nd e c i m a l   =   d e c i m a l    / /   2\nb i n a r y _ l i s t . r e v e r s e ( )\nb i n a r y _ s t r i n g   =   ’ ’ . j o i n ( b i n a r y _ l i s t )\nr e t u r n’ db ’   +   b i n a r y _ s t r i n g   +   ’ db ’\n],\n\"exec_result\":[[\"db1010db\",   \"db1010db\",   \"db1010db\",   \"db1010db\"],\n[\"db11111111db\",  \"db11111111db\",  \"db11111111db\",  \"db11111111db\"],\n[\"dbdb\", \"dbdb\", \"dbdb\", \"dbdb\"]]\n}\nFigure 16: Some error cases ofFACTOOLon code. The full response ofFACTOOLpowered by GPT-4 is reported,\nincluding the queries generated, evidences collected, the reasoning process, and the final decision is also reported.",
    "[Round-off error]\n{\n\"math_calculation\": \"60444034 / 12\",\n\"calculated_answer\": \"5037002.83\",\n\"label\": true,\n\"query\": \"python_snippet\": \"calculation = 60444034 / 12; answer = 5037002.83; print(abs(calculation - answer) < 1e-9)\",\n\"exec_result\": \"False\",\n\"with_tool_classification\": false\n}\n[Reasoning error]\n{\n\"input\": \"Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy\n5364765 glasses. How much does he need to pay for them?\",\n\"target\": 21459061.0,\n\"response\": \"Let’s assume that Kylar buys x glasses at the full price of $5 each. Therefore, he will buy (5364765 - x) glasses at a discounted price of 60% of $5,\nwhich is $3. Now, we can set up the equation:\n5x + 3(5364765 - x) = total cost\nSimplifying the equation, we get:\n5x + 16094295 - 3x = total cost\n2x + 16094295 = total cost\nTherefore, the total cost of all the glasses will be 2x + 16094295 dollars. Now, we just need to find the value of x. We know that Kylar wants to buy 5364765 glasses,\nso we can substitute this value in the equation:\n2x + 16094295 = total cost\n2x + 16094295 = 5,364,765 * 5\n2x + 16094295 = 26823825\n2x = 10729530\nx = 5364765\nTherefore, Kylar needs to pay 2x + 16094295 dollars, which is: 2(5364765) + 16094295 = 26823825.\nHence, Kylar needs to pay $26823825 for all the glasses.\",\n\"answer\": \"26823825\",\n\"annotation\": false,\n\"claims\": [{\"math_calculation\": \"5364765 - x\", \"calculated_answer\": \"5364765 - x\", \"label\": true}, {\"math_calculation\": \"0.60 * 5\", \"calculated_answer\": \"3\",\n\"label\": true}, {\"math_calculation\": \"5x\", \"calculated_answer\": \"5x\", \"label\": true}, {\"math_calculation\": \"3(5364765 - x)\", \"calculated_answer\": \"16094295 -\n3x\", \"label\": true}, {\"math_calculation\": \"5x + 16094295 - 3x\", \"calculated_answer\": \"16094295 + 2x\", \"label\": true}, {\"math_calculation\": \"2x + 16094295\",\n\"calculated_answer\": \"total cost\", \"label\": \"null\"}, {\"math_calculation\": \"5364765 * 5\", \"calculated_answer\": \"26823825\", \"label\": true}, {\"math_calculation\":\n\"2(5364765)\", \"calculated_answer\": \"10729530\", \"label\": true}, {\"math_calculation\": \"2x\", \"calculated_answer\": \"10729530\", \"label\": \"null\"}]\n\"math_calculation\":  \"5364765 - x\", \"calculated_answer\":  \"5364765 - x\", \"label\":  true, \"query\":  \"python_snippet\":  \"x = 0; math_calculation = 5364765 - x;\ncalculated_answer = 5364765 - x; print(math_calculation == calculated_answer)\", \"exec_result\": \"True\", \"with_tool_classification\": true \"math_calculation\": \"0.60 *\n5\", \"calculated_answer\": \"3\", \"label\": true, \"query\": \"python_snippet\": \"calculation = 0.60 * 5; answer = 3; print(calculation == answer)\", \"exec_result\": \"True\",\n\"with_tool_classification\": true \"math_calculation\": \"5x\", \"calculated_answer\": \"5x\", \"label\": true, \"query\": \"python_snippet\": \"def validate_calculation(calculation,\nanswer): return calculation == answer; print(validate_calculation(’5x’, ’5x’))\", \"exec_result\": \"True\", \"with_tool_classification\": true \"math_calculation\": \"3(5364765\n- x)\", \"calculated_answer\": \"16094295 - 3x\", \"label\": true, \"query\": \"python_snippet\": \"x = 1; result1 = 3 * (5364765 - x); result2 = 16094295 - 3 * x; print(result1\n== result2)\", \"exec_result\": \"True\", \"with_tool_classification\": true \"math_calculation\": \"5x + 16094295 - 3x\", \"calculated_answer\": \"16094295 + 2x\", \"label\":\ntrue, \"query\":  \"python_snippet\":  \"x = 1;  print((5 * x + 16094295 - 3 * x) == (16094295 + 2 * x))\", \"exec_result\":  \"True\", \"with_tool_classification\":  true\n\"math_calculation\":  \"2x  +  16094295\",  \"calculated_answer\":  \"total  cost\",  \"label\":  \"null\",  \"query\":  \"python_snippet\":  \"x  =  5;  math_calculation  =  2  *  x  +\n16094295; calculated_answer = 16094305; print(math_calculation == calculated_answer)\", \"exec_result\": \"True\", \"with_tool_classification\": true \"math_calculation\":\n\"5364765 * 5\", \"calculated_answer\": \"26823825\", \"label\": true, \"query\": \"python_snippet\": \"calculation = 5364765 * 5; answer = 26823825; print(calculation\n== answer)\", \"exec_result\": \"True\", \"with_tool_classification\": true \"math_calculation\": \"2(5364765)\", \"calculated_answer\": \"10729530\", \"label\": true, \"query\":\n\"python_snippet\":  \"calculation  =  2  *  5364765;  answer  =  10729530;  print(calculation  ==  answer)\",  \"exec_result\":  \"True\",  \"with_tool_classification\":  true\n\"math_calculation\": \"2x\", \"calculated_answer\": \"10729530\", \"label\": \"null\", \"query\": \"python_snippet\": \"x = 5364765; print(2 * x == 10729530)\", \"exec_result\":\n\"True\", \"with_tool_classification\": true\n}\nFigure 17: Some error cases ofFACTOOLon math. The full response ofFACTOOLpowered by GPT-4 is reported,\nincluding the queries generated, evidences collected, the reasoning process, and the final decision is also reported.\n[Errors in title matching]\n{\n\"paper_title\": \"MDMA-assisted psychotherapy for treatment of PTSD: study design and rationale for phase 3 trials based on pooled analysis of six phase 2 randomized\ncontrolled trials\",\n\"paper_author(s)\": \"Mithoefer et al.\",\n\"paper_pub_year\": \"2019\",\n\"label\": true,\n\"with_tool_classification\": false,\n\"error\": [\"wrong_paper_title\"]\n}\n[Errors in author matching]\n{\n\"paper_title\": \"Language Models are Unsupervised Multitask Learners\",\n\"paper_author(s)\": \"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever\",\n\"paper_pub_year\": \"2019\",\n\"label\": true,\n\"with_tool_classification\": false,\n\"error\": [\"wrong_paper_author(s)\"]\n}\nFigure 18:  Some error cases ofFACTOOLon scientific.  The full response ofFACTOOLpowered by GPT-4 is\nreported, including the queries generated, evidences collected, the reasoning process, and the final decision is also\nreported."
  ]
}