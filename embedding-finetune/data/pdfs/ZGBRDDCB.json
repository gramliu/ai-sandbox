{
  "key": "ZGBRDDCB",
  "url": "http://arxiv.org/pdf/2112.01488",
  "metadata": {
    "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late\n  Interaction",
    "abstract": "  Neural information retrieval (IR) has greatly advanced search and other\nknowledge-intensive language tasks. While many neural IR methods encode queries\nand documents into single-vector representations, late interaction models\nproduce multi-vector representations at the granularity of each token and\ndecompose relevance modeling into scalable token-level computations. This\ndecomposition has been shown to make late interaction more effective, but it\ninflates the space footprint of these models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\ncompression mechanism with a denoised supervision strategy to simultaneously\nimprove the quality and space footprint of late interaction. We evaluate\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\nquality within and outside the training domain while reducing the space\nfootprint of late interaction models by 6--10$\\times$.\n",
    "published": "2021-12-02T18:38:50Z"
  },
  "text": [
    "ColBERTv2:\nEffective and Efficient Retrieval via Lightweight Late Interaction\nKeshav Santhanam\n∗\nStanford University\nOmar Khattab\n∗\nStanford University\nJon Saad-Falcon\nGeorgia Institute of Technology\nChristopher Potts\nStanford University\nMatei Zaharia\nStanford University\nAbstract\nNeural  information  retrieval  (IR)  has  greatly\nadvancedsearchandotherknowledge-\nintensive language tasks.  While many neural\nIR  methods  encode  queries  and  documents\nintosingle-vectorrepresentations,late\ninteraction models produce multi-vector repre-\nsentations at the granularity of each token and\ndecompose  relevance  modeling  into  scalable\ntoken-level computations. This decomposition\nhas been shown to make late interaction more\neffective, but it inflates the space footprint of\nthese models by an order of magnitude. In this\nwork,  we  introduce  ColBERTv2,  a  retriever\nthat  couples  an  aggressive  residual  compres-\nsion  mechanism  with  a  denoised  supervision\nstrategy to simultaneously improve the quality\nand  space  footprint  of  late  interaction.    We\nevaluate   ColBERTv2   across   a   wide   range\nof  benchmarks,   establishing  state-of-the-art\nquality within and outside the training domain\nwhile  reducing  the  space  footprint  of  late\ninteraction models by 6–10×.\n1    Introduction\nNeural information retrieval (IR) has quickly domi-\nnated the search landscape over the past 2–3 years,\ndramatically advancing not only passage and doc-\nument search (Nogueira and Cho, 2019) but also\nmany knowledge-intensive NLP tasks like open-\ndomain  question  answering  (Guu  et  al.,  2020),\nmulti-hop claim verification (Khattab et al., 2021a),\nand open-ended generation (Paranjape et al., 2022).\nMany neural IR methods follow asingle-vector\nsimilarityparadigm: a pretrained language model\nis used to encode each query and each document\ninto  a  single  high-dimensional  vector,  and  rele-\nvance is modeled as a simple dot product between\nboth vectors. An alternative islate interaction, in-\ntroduced in ColBERT (Khattab and Zaharia, 2020),\nwhere queries and documents are encoded at a finer-\ngranularity into multi-vector representations, and\n∗\nEqual contribution.\nrelevance is estimated using rich yet scalable in-\nteractions between these two sets of vectors. Col-\nBERT produces an embedding for every token in\nthe query (and document) and models relevance\nas the sum of maximum similarities between each\nquery vector and all vectors in the document.\nBy decomposing relevance modeling into token-\nlevel computations, late interaction aims to reduce\nthe burden on the encoder: whereas single-vector\nmodels must capture complex query–document re-\nlationships within one dot product, late interaction\nencodes meaning at the level of tokens and del-\negates query–document matching to the interac-\ntion mechanism.  This added expressivity comes\nat a cost: existing late interaction systems impose\nan order-of-magnitude largerspace footprintthan\nsingle-vector models, as they must store billions\nof small vectors for Web-scale collections.  Con-\nsidering this challenge, it might seem more fruit-\nful to focus instead on addressing the fragility of\nsingle-vector models (Menon et al., 2022) by in-\ntroducing new supervision paradigms for negative\nmining (Xiong et al., 2020), pretraining (Gao and\nCallan, 2021),  and distillation (Qu et al., 2021).\nIndeed, recent single-vector models with highly-\ntuned supervision strategies (Ren et al., 2021b; For-\nmal et al., 2021a) sometimes perform on-par or\neven better than “vanilla” late interaction models,\nand it is not necessarily clear whether late inter-\naction architectures—with their fixed token-level\ninductive biases—admit similarly large gains from\nimproved supervision.\nIn this work, we show that late interaction re-\ntrievers naturally produce lightweight token rep-\nresentations that are amenable to efficient storage\noff-the-shelf and that they can benefit drastically\nfrom denoised supervision.   We couple those in\nColBERTv2,\n1\na new late-interaction retriever that\nemploys a simple combination of distillation from\n1\nCode, models, and LoTTE data are maintained athttps:\n//github.com/stanford-futuredata/ColBERT\narXiv:2112.01488v3  [cs.IR]  10 Jul 2022",
    "a cross-encoder and hard-negative mining (§3.2)\nto boost quality beyond any existing method, and\nthen uses aresidual compressionmechanism (§3.3)\nto reduce the space footprint of late interaction by\n6–10×while preserving quality. As a result, Col-\nBERTv2 establishes state-of-the-art retrieval qual-\nity bothwithinandoutsideits training domain with\na competitive space footprint with typical single-\nvector models.\nWhen trained on MS MARCO Passage Rank-\ning, ColBERTv2 achieves the highest MRR@10 of\nany standalone retriever. In addition to in-domain\nquality, we seek a retriever that generalizes “zero-\nshot” to domain-specific corpora and long-tail top-\nics, ones that are often under-represented in large\npublic training sets. To this end, we evaluate Col-\nBERTv2 on a wide array ofout-of-domainbench-\nmarks.  These include three Wikipedia Open-QA\nretrieval tests and 13 diverse retrieval and semantic-\nsimilarity tasks from BEIR (Thakur et al., 2021). In\naddition, we introduce a new benchmark, dubbed\nLoTTE, forLong-TailTopic-stratifiedEvaluation\nfor  IR  that  features  12  domain-specific  search\ntests, spanning StackExchange communities and\nusing queries from GooAQ (Khashabi et al., 2021).\nLoTTE  focuses  on  relatively  long-tail  topics  in\nits passages, unlike the Open-QA tests and many\nof the BEIR tasks, and evaluates models on their\ncapacity to answer natural search queries with a\npractical intent, unlike many of BEIR’s semantic-\nsimilarity tasks. On 22 of 28 out-of-domain tests,\nColBERTv2 achieves the highest quality, outper-\nforming the next best retriever by up to 8% relative\ngain, while using its compressed representations.\nThis work makes the following contributions:\n1.We propose ColBERTv2, a retriever that com-\nbines denoised supervision and residual com-\npression, leveraging the token-level decom-\nposition  of  late  interaction  to  achieve  high\nrobustness with a reduced space footprint.\n2.\nWe introduce LoTTE, a new resource for out-\nof-domain evaluation of retrievers. LoTTE fo-\ncuses on natural information-seeking queries\nover long-tail topics, an important yet under-\nstudied application space.\n3.We evaluate ColBERTv2 across a wide range\nof settings, establishing state-of-the-art qual-\nity within and outside the training domain.\n2    Background & Related Work\n2.1    Token-Decomposed Scoring in Neural IR\nMany neural IR approaches encode passages as\na single high-dimensional vector, trading off the\nhigher quality of cross-encoders for improved ef-\nficiency and scalability (Karpukhin et al., 2020;\nXiong  et  al.,  2020;  Qu  et  al.,  2021).Col-\nBERT’s  (Khattab  and  Zaharia,  2020)  late  inter-\naction paradigm addresses this tradeoff by com-\nputing multi-vector embeddings and using a scal-\nable  “MaxSim”  operator  for  retrieval.    Several\nother  systems  leverage  multi-vector  representa-\ntions,  including  Poly-encoders  (Humeau  et  al.,\n2020),  PreTTR  (MacAvaney  et  al.,  2020),  and\nMORES  (Gao  et  al.,  2020),   but  these  target\nattention-based  re-ranking  as  opposed  to  Col-\nBERT’s scalable MaxSim end-to-end retrieval.\nME-BERT (Luan et al., 2021) generates token-\nlevel document embeddings similar to ColBERT,\nbut retains a single embedding vector for queries.\nCOIL (Gao et al., 2021) also generates token-level\ndocument embeddings, but the token interactions\nare restricted to lexical matching between query\nand document terms. uniCOIL (Lin and Ma, 2021)\nlimits the token embedding vectors of COIL to a\nsingle dimension, reducing them to scalar weights\nthat extend models like DeepCT (Dai and Callan,\n2020) and DeepImpact (Mallia et al., 2021).  To\nproduce scalar weights, SPLADE (Formal et al.,\n2021b) and SPLADEv2 (Formal et al., 2021a) pro-\nduce a sparse vocabulary-level vector that retains\nthe  term-level  decomposition  of  late  interaction\nwhile simplifying the storage into one dimension\nper token. The SPLADE family also piggybacks on\nthe language modeling capacity acquired by BERT\nduring pretraining.  SPLADEv2 has been shown\nto be highly effective, within and across domains,\nand it is a central point of comparison in the exper-\niments we report on in this paper.\n2.2    Vector Compression for Neural IR\nThere has been a surge of recent interest in com-\npressing representations for IR. Izacard et al. (2020)\nexplore dimension reduction, product quantization\n(PQ), and passage filtering for single-vector retriev-\ners. BPR (Yamada et al., 2021a) learns to directly\nhash embeddings to binary codes using a differen-\ntiabletanhfunction. JPQ (Zhan et al., 2021a) and\nits extension, RepCONC (Zhan et al., 2022), use\nPQ to compress embeddings, and jointly train the\nquery encoder along with the centroids produced",
    "by PQ via a ranking-oriented loss.\nSDR (Cohen et al., 2021) uses an autoencoder to\nreduce the dimensionality of the contextual embed-\ndings used for attention-based re-ranking and then\napplies a quantization scheme for further compres-\nsion. DensePhrases (Lee et al., 2021a) is a system\nfor Open-QA that relies on a multi-vector encod-\ning  of  passages,  though  its  search  is  conducted\nat the level of individual vectors and not aggre-\ngated with late interaction. Very recently, Lee et al.\n(2021b) propose a quantization-aware finetuning\nmethod based on PQ to reduce the space footprint\nof DensePhrases. While DensePhrases is effective\nat Open-QA, its retrieval quality—as measured by\ntop-20 retrieval accuracy on NaturalQuestions and\nTriviaQA—is competitive with DPR (Karpukhin\net al., 2020) and considerably less effective than\nColBERT (Khattab et al., 2021b).\nIn  this  work,  we  focus  on  late-interaction  re-\ntrieval and investigate compression using a residual\ncompression approach that can be applied off-the-\nshelf to late interaction models,  without special\ntraining. We show in Appendix A that ColBERT’s\nrepresentations naturally lend themselves to resid-\nual compression. Techniques in the family of resid-\nual compression are well-studied (Barnes et al.,\n1996) and have previously been applied across sev-\neral domains, including approximate nearest neigh-\nbor search (Wei et al., 2014; Ai et al., 2017), neural\nnetwork parameter and activation quantization (Li\net al., 2021b,a), and distributed deep learning (Chen\net al., 2018; Liu et al., 2020).  To the best of our\nknowledge, ColBERTv2 is the first approach to use\nresidual compression for scalable neural IR.\n2.3    Improving the Quality of Single-Vector\nRepresentations\nInstead  of  compressing  multi-vector  representa-\ntions  as  we  do,  much  recent  work  has  focused\non  improving  the  quality  of  single-vector  mod-\nels, which are often very sensitive to the specifics\nof supervision.  This line of work can be decom-\nposed into three directions: (1) distillation of more\nexpressive architectures (Hofstätter et al., 2020;\nLin et al., 2020) including explicit denoising (Qu\net al., 2021; Ren et al., 2021b), (2) hard negative\nsampling (Xiong et al., 2020; Zhan et al., 2020a,\n2021b),  and  (3)  improved  pretraining  (Gao  and\nCallan, 2021; O\n ̆\nguz et al., 2021). We adopt similar\ntechniques to (1) and (2) for ColBERTv2’s multi-\nvector representations (see §3.2).\nQuestion\nPassage\nQuestion EncoderPassage Encoder\nMaxSimMaxSimMaxSim\nscore\nOffline Indexing\nFigure  1:   The  late  interaction  architecture,  given  a\nquery  and  a  passage.    Diagram  from  Khattab  et  al.\n(2021b) with permission.\n2.4    Out-of-Domain Evaluation in IR\nRecent progress in retrieval has mostly focused on\nlarge-data evaluation,  where many tens of thou-\nsands of annotated training queries are associated\nwith the test domain, as in MS MARCO or Natu-\nral Questions (Kwiatkowski et al., 2019). In these\nbenchmarks, queries tend to reflect high-popularity\ntopics like movies and athletes in Wikipedia.  In\npractice, user-facing IR and QA applications often\npertain to domain-specific corpora, for which little\nto no training data is available and whose topics\nare under-represented in large public collections.\nThis out-of-domain regime has received recent\nattention with the BEIR (Thakur et al., 2021) bench-\nmark.   BEIR combines several existing datasets\ninto a heterogeneous suite for “zero-shot IR” tasks,\nspanning bio-medical, financial, and scientific do-\nmains.   While the BEIR datasets provide a use-\nful testbed, many capture broad semantic related-\nness tasks—like citations, counter arguments, or\nduplicate questions–instead of natural search tasks,\nor else they focus on high-popularity entities like\nthose in Wikipedia. In §4, we introduce LoTTE, a\nnew dataset for out-of-domain retrieval, exhibiting\nnatural search queries over long-tail topics.\n3    ColBERTv2\nWe now introduce ColBERTv2, which improves\nthe quality of multi-vector retrieval models (§3.2)\nwhile reducing their space footprint (§3.3).\n3.1    Modeling\nColBERTv2 adopts the late interaction architecture\nof ColBERT, depicted in Figure 1. Queries and pas-\nsages are independently encoded with BERT (De-\nvlin et al., 2019), and the output embeddings encod-\ning each token are projected to a lower dimension.\nDuring  offline  indexing,  every  passagedin  the\ncorpus is encoded into a set of vectors, and these",
    "vectors are stored.  At search time, the queryqis\nencoded into a multi-vector representation, and its\nsimilarity to a passagedis computed as the summa-\ntion of query-side “MaxSim” operations, namely,\nthe largest cosine similarity between each query to-\nken embedding and all passage token embeddings:\nS\nq,d\n=\nN\n∑\ni=1\nM\nmax\nj=1\nQ\ni\n·D\nT\nj\n(1)\nwhereQis an matrix encoding the query withN\nvectors andDencodes the passage withMvectors.\nThe intuition of this architecture is to align each\nquery token with the most contextually relevant\npassage token, quantify these matches, and com-\nbine the partial scores across the query. We refer\nto Khattab and Zaharia (2020) for a more detailed\ntreatment of late interaction.\n3.2    Supervision\nTraining a neural retriever typically requiresposi-\ntiveandnegativepassages for each query in the\ntraining  set.   Khattab  and  Zaharia  (2020)  train\nColBERT  using  the  official〈q,  d\n+\n,  d\n−\n〉triples\nof MS MARCO. For each query, a positived\n+\nis\nhuman-annotated, and each negatived\n−\nis sampled\nfrom unannotated BM25-retrieved passages.\nSubsequent work has identified several weak-\nnesses   in   this   standard   supervision   approach\n(see §2.3). Our goal is to adopt a simple, uniform\nsupervision scheme that selects challenging neg-\natives and avoids rewarding false positives or pe-\nnalizing false negatives. To this end, we start with\na ColBERT model trained with triples as in Khat-\ntab et al. (2021b), using this to index the training\npassages with ColBERTv2 compression.\nFor each training query, we retrieve the top-k\npassages.  We feed each of those query–passage\npairs  into  a  cross-encoder  reranker.    We  use  a\n22M-parameter MiniLM (Wang et al., 2020) cross-\nencoder trained with distillation by Thakur et al.\n(2021).\n2\nThis small model has been shown to ex-\nhibit  very  strong  performance  while  being  rela-\ntively  efficient  for  inference,  making  it  suitable\nfor distillation.\nWe then collectw-way tuples consisting of a\nquery, a highly-ranked passage (or labeled posi-\ntive), and one or more lower-ranked passages. In\nthis work, we usew= 64passages per example.\nLike RocketQAv2 (Ren et al., 2021b), we use a\n2\nhttps://huggingface.co/cross-encoder/\nms-marco-MiniLM-L-6-v2\nKL-Divergence loss to distill the cross-encoder’s\nscores into the ColBERT architecture. We use KL-\nDivergence as ColBERT produces scores (i.e., the\nsum of cosine similarities) with a restricted scale,\nwhich may not align directly with the output scores\nof  the  cross-encoder.   We  also  employ  in-batch\nnegatives per GPU, where a cross-entropy loss is\napplied to the positive score of each query against\nall passages corresponding to other queries in the\nsame batch.  We repeat this procedure once to re-\nfresh the index and thus the sampled negatives.\nDenoised training with hard negatives has been\npositioned in recent work as ways to bridge the\ngap between single-vector and interaction-based\nmodels, including late interaction architectures like\nColBERT. Our results in §5 reveal that such super-\nvision can improve multi-vector models dramati-\ncally, resulting in state-of-the-art retrieval quality.\n3.3    Representation\nWe hypothesize that the ColBERT vectors cluster\ninto regions that capture highly-specific token se-\nmantics.  We test this hypothesis in Appendix A,\nwhere evidence suggests that vectors correspond-\ning to each sense of a word cluster closely, with\nonly minor variation due to context.  We exploit\nthis regularity with aresidualrepresentation that\ndramatically reduces the space footprint of late in-\nteraction models, completelyoff-the-shelfwithout\narchitectural or training changes.  Given a set of\ncentroidsC, ColBERTv2 encodes each vectorvas\nthe index of its closest centroidC\nt\nand aquantized\nvector ̃rthat approximates the residualr=v−C\nt\n.\nAt search time, we use the centroid indextand\nresidual ̃rrecover an approximate ̃v=C\nt\n+  ̃r.\nTo encode ̃r, we quantize every dimension ofr\ninto one or two bits. In principle, ourb-bit encod-\ning ofn-dimensional vectors needsdlog|C|e+bn\nbits per vector. In practice, withn= 128, we use\nfour bytes to capture up to2\n32\ncentroids and 16 or\n32 bytes (forb= 1orb= 2) to encode the resid-\nual. This total of 20 or 36 bytes per vector contrasts\nwith ColBERT’s use of 256-byte vector encodings\nat 16-bit precision. While many alternatives can be\nexplored for compression, we find that this simple\nencoding largely preserves model quality, while\nconsiderably lowering storage costs against typi-\ncal 32- or 16-bit precision used by existing late\ninteraction systems.\nThis centroid-based encoding can be considered\na natural extension of product quantization tomulti-",
    "vectorrepresentations. Product quantization (Gray,\n1984; Jegou et al., 2010) compresses a single vector\nby splitting it into small sub-vectors and encoding\neach of them using an ID within a codebook.  In\nour approach, each representation is already a ma-\ntrix that is naturally divided into a number of small\nvectors (one per token).  We encode each vector\nusing its nearest centroid plus a residual.  Refer\nto Appendix B for tests of the impact of compres-\nsion on retrieval quality and a comparison with a\nbaseline compression method for ColBERT akin to\nBPR (Yamada et al., 2021b).\n3.4    Indexing\nGiven  a  corpus  of  passages,  the  indexing  stage\nprecomputes  all  passage  embeddings  and  orga-\nnizes their representations to support fast nearest-\nneighbor search. ColBERTv2 divides indexing into\nthree stages, described below.\nCentroid  Selection.\nIn  the  first  stage,  Col-\nBERTv2 selects a set of cluster centroidsC. These\nare  embeddings  that  ColBERTv2  uses  to  sup-\nport residual encoding (§3.3) and also for nearest-\nneighbor search (§3.5).  Standardly, we find that\nsetting|C|proportionally  to  the  square  root  of\nn\nembeddings\nin the corpus works well empirically.\n3\nKhattab and Zaharia (2020) only clustered the vec-\ntors after computing the representations of all pas-\nsages, but doing so requires storing them uncom-\npressed. To reduce memory consumption, we apply\nk-means clustering to the embeddings produced by\ninvoking our BERT encoder over only a sample of\nall passages, proportional to the square root of the\ncollection size, an approach we found to perform\nwell in practice.\nPassage  Encoding.Having  selected  the  cen-\ntroids, we encode every passage in the corpus. This\nentails invoking the BERT encoder and compress-\ning the output embeddings as described in §3.3,\nassigning each embedding to the nearest centroid\nand computing a quantized residual. Once a chunk\nof passages is encoded, thecompressedrepresenta-\ntions are saved to disk.\nIndex  Inversion.\nTo  support  fast  nearest-\nneighbor search, we group the embedding IDs that\ncorrespond to each centroid together, and save this\ninverted listto disk. At search time, this allows us\nto quickly find token-level embeddings similar to\nthose in a query.\n3\nWe round down to the nearest power of two larger than\n16×\n√\nn\nembeddings\n, inspired by FAISS (Johnson et al., 2019).\n3.5    Retrieval\nGiven a query representationQ, retrieval starts with\ncandidate generation.  For every vectorQ\ni\nin the\nquery, the nearestn\nprobe\n≥1centroids are found.\nUsing the inverted list, ColBERTv2 identifies the\npassage embeddings close to these centroids, de-\ncompresses them, and computes their cosine simi-\nlarity with every query vector. The scores are then\ngrouped by passage ID for each query vector, and\nscores corresponding to the same passage aremax-\nreduced.  This allows ColBERTv2 to conduct an\napproximate “MaxSim” operation per query vector.\nThis computes a lower-bound on the true MaxSim\n(§3.1) using the embeddings identified via the in-\nverted list, which resembles the approximation ex-\nplored for scoring by Macdonald and Tonellotto\n(2021) but is applied for candidate generation.\nThese  lower  bounds  are  summed  across  the\nquery  tokens,  and  the  top-scoringn\ncandidate\ncan-\ndidate passages based on these approximate scores\nare selected for ranking, which loads the complete\nset of embeddings of each passage, and conducts\nthe same scoring function using all embeddings\nper document following Equation 1.   The result\npassages are then sorted by score and returned.\n4    LoTTE: Long-Tail, Cross-Domain\nRetrieval Evaluation\nWe introduceLoTTE(pronounced latte), a new\ndataset forLong-TailTopic-stratifiedEvaluation\nfor IR. To complement the out-of-domain tests of\nBEIR (Thakur et al., 2021), as motivated in §2.4,\nLoTTE focuses onnatural user queriesthat pertain\ntolong-tail topics, ones that might not be covered\nby an entity-centric knowledge base like Wikipedia.\nLoTTE consists of 12 test sets, each with 500–2000\nqueries and 100k–2M passages.\nThe test sets are explicitly divided by topic, and\neach test set is accompanied by a validation set of\nrelated but disjointqueriesandpassages. We elect\nto make the passage texts disjoint to encourage\nmore realistic out-of-domain transfer tests, allow-\ning for minimal development on related but distinct\ntopics. The test (and dev) sets include a “pooled”\nsetting.   In the pooled setting,  the passages and\nqueries are aggregated across all test (or dev) topics\nto evaluate out-of-domain retrieval across a larger\nand more diverse corpus.\nTable 1 outlines the composition of LoTTE. We\nderive  the  topics  and  passage  corpora  from  the\nanswer  postsacross  various  StackExchange  fo-",
    "TopicQuestion Set\nDevTest\n# Questions# PassagesSubtopics# Questions# PassagesSubtopics\nWriting\nSearch497\n277k\nESL, Linguistics,\nWorldbuilding\n1071\n200kEnglish\nForum20032000\nRecreation\nSearch563\n263k\nSci-Fi, RPGs,\nPhotography\n924\n167k\nGaming,\nAnime, MoviesForum20022002\nScience\nSearch538\n344k\nChemistry,\nStatistics, Academia\n617\n1.694M\nMath,\nPhysics, BiologyForum20132017\nTechnology\nSearch916\n1.276M\nWeb Apps,\nUbuntu, SysAdmin\n596\n639k\nApple, Android,\nUNIX, SecurityForum20032004\nLifestyle\nSearch417\n269k\nDIY, Music, Bicycles,\nCar Maintenance\n661\n119k\nCooking,\nSports, TravelForum20762002\nPooled\nSearch2931\n2.4M\nAll of the above\n3869\n2.8M\nAll of the above\nForum1009710025\nTable 1:  Composition of LoTTE showing topics, question sets, and a sample of corresponding subtopics.  Search\nQueries are taken from GooAQ, while Forum Queries are taken directly from the StackExchange archive.  The\npooled datasets combine the questions and passages from each of the subtopics.\nrums.   StackExchange  is  a  set  of  question-and-\nanswer communities that target individual topics\n(e.g., “physics” or “bicycling”). We gather forums\nfrom five overarching domains: writing, recreation,\nscience, technology, and lifestyle. To evaluate re-\ntrievers, we collectSearchandForumqueries, each\nof which is associated with one or more target an-\nswer posts in its corpus.   Example queries,  and\nshort snippets from posts that answer them in the\ncorpora, are shown in Table 2.\nSearch Queries.\nWe collect search queries from\nGooAQ (Khashabi et al., 2021), a recent dataset\nof Google search-autocomplete queries and their\nanswer boxes, which we filter for queries whose\nanswers link to a specific StackExchange post. As\nKhashabi et al. (2021) hypothesize, Google Search\nlikely maps these natural queries to their answers\nby relying on a wide variety of signals for rele-\nvance,  including expert annotations,  user clicks,\nand hyperlinks as well as specialized QA compo-\nnents for various question typeswith access to the\npost title and question body. Using those annota-\ntions as ground truth, we evaluate the models on\ntheir capacity for retrieval usingonlyfree text of\nthe answer posts (i.e., no hyperlinks or user clicks,\nquestion title or body, etc.), posing a significant\nchallenge for IR and NLP systems trained only on\npublic datasets.\nForum Queries.We collect the forum queries\nby extracting post titles from the StackExchange\ncommunities  to  use  as  queries  and  collect  their\ncorresponding answer posts as targets. We select\nquestions in order of their popularity and sample\nquestions according to the proportional contribu-\ntion of individual communities within each topic.\nQ:what is the difference between root and stem in lin-\nguistics?A:A root isthe form to which derivational\naffixes are added\nto form a stem.  A stem isthe form\nto which inflectional affixes are added\nto form a word.\nQ:are there any airbenders left?A:the Fire Nation\nhad wiped out all Airbenders while Aang was frozen.\nTenzin and his 3 children are the only Airbenders left\nin Korra’s time.\nQ:Why are there two Hydrogen atoms on some peri-\nodic tables?A:some periodic tables show hydrogen in\nboth placesto emphasize that hydrogen isn’t really a\nmember of the first group or the seventh group.\nQ:How can cache be that fast?A:the cache memory\nsits right next to the CPU on the same die (chip),it is\nmade using SRAM which is much, much faster than\nthe DRAM.\nTable 2: Examples of queries and shortened snippets of\nanswer passages from LoTTE. The first two examples\nshow  “search”  queries,  whereas  the  last  two  are  “fo-\nrum” queries. Snippets are shortened for presentation.\nThese queries tend to have a wider variety than\nthe “search” queries, while the search queries may\nexhibit more natural patterns. Table 3 compares a\nrandom samples of search and forum queries.  It\ncan be seen that search queries tend to be brief,\nknowledge-based questions with direct answers,\nwhereas forum queries tend to reflect more open-\nended questions. Both query sets target topics that\nexceed the scope of a general-purpose knowledge\nrepository such as Wikipedia.\nFor search as well as forum queries, the result-\ning evaluation set consists of a query and a target\nset of StackExchange answer posts (in particular,\nthe answer posts from the target StackExchange\npage).  Similar to evaluation in the Open-QA lit-\nerature  (Karpukhin  et  al.,  2020;  Khattab  et  al.,",
    "Q:what is xerror in rpart?Q:is sub question one word?\nQ:how to open a garage door without making noise?Q:\nis docx and dotx the same?Q:are upvotes and downvotes\nanonymous?Q:what is the difference between descriptive\nessay and narrative essay?Q:how to change default\nuser profile in chrome?Q:does autohotkey need to be\ninstalled?Q:how do you tag someone on facebook with\na youtube video?Q:has mjolnir ever been broken?\nQ:Snoopy can balance on an edge atop his doghouse. Is any\nreason given for this?Q:How many Ents were at the\nEntmoot?Q:What does a hexagonal sun tell us about\nthe camera lens/sensor?Q:Should I simply ignore it if\nauthors assume that Im male in their response to my review of\ntheir article?Q:Why is the 2s orbital lower in energy than\nthe 2p orbital when the electrons in 2s are usually farther from\nthe nucleus?Q:Are there reasons to use colour filters\nwith digital cameras?Q:How does the current know how\nmuch to flow, before having seen the resistor?Q:What\nis the difference between Fact and Truth?Q:hAs a DM,\nhow can I handle my Druid spying on everything with Wild\nshape as a spider?Q:What does 1x1 convolution mean\nin a neural network?\nTable  3:   Comparison  of  a  random  sample  of  search\nqueries (top) vs. forum queries (bottom).\n2021b), we evaluate retrieval quality by comput-\ning thesuccess@5(S@5) metric. Specifically, we\naward a point to the system for each query where\nit finds an accepted or upvoted (score≥1) answer\nfrom the target page in the top-5 hits.\nAppendix D reports on the breakdown of con-\nstituent communities per topic,  the construction\nprocedure of LoTTE as well as licensing considera-\ntions, and relevant statistics. Figures 5 and 6 quan-\ntitatively compare the search and forum queries.\n5    Evaluation\nWe now evaluate ColBERTv2 on passage retrieval\ntasks, testing its quality within the training domain\n(§5.1) as well as outside the training domain in\nzero-shot settings (§5.2). Unless otherwise stated,\nwe compress ColBERTv2 embeddings tob= 2\nbits per dimension in our evaluation.\n5.1    In-Domain Retrieval Quality\nSimilar to related work, we train for IR tasks on MS\nMARCO Passage Ranking (Nguyen et al., 2016).\nWithin the training domain, our development-set re-\nsults are shown in Table 4, comparing ColBERTv2\nwith vanilla ColBERT as well as state-of-the-art\nsingle-vector systems.\nWhile ColBERT outperforms single-vector sys-\ntems like RepBERT, ANCE, and even TAS-B, im-\nprovements in supervision such as distillation from\ncross-encoders  enable  systems  like  SPLADEv2,\nMethod\nOfficial Dev (7k)Local Eval (5k)\nMRR@10  R@50  R@1k  MRR@10  R@50  R@1k\nModels without Distillation or Special Pretraining\nRepBERT30.4-94.3---\nDPR31.1-95.2---\nANCE33.0-95.9---\nLTRe34.1-96.2---\nColBERT36.082.996.836.7--\nModels with Distillation or Special Pretraining\nTAS-B34.7-97.8---\nSPLADEv236.8-97.937.984.998.0\nPAIR37.986.498.2---\ncoCondenser38.2-98.4---\nRocketQAv238.886.298.139.885.897.9\nColBERTv239.786.898.440.886.398.3\nTable 4:  In-domain performance on the development\nset of MS MARCO Passage Ranking as well the “Local\nEval” test set described by Khattab and Zaharia (2020).\nDev-set results for baseline systems are from their re-\nspective papers: Zhan et al. (2020b), Xiong et al. (2020)\nfor DPR and ANCE, Zhan et al. (2020a), Khattab and\nZaharia (2020), Hofstätter et al. (2021), Gao and Callan\n(2021), Ren et al. (2021a), Formal et al. (2021a), and\nRen et al. (2021b).\nPAIR,  and  RocketQAv2  to  achieve  higher  qual-\nity than vanilla ColBERT. These supervision gains\nchallenge the value of fine-grained late interaction,\nand it is not inherently clear whether the stronger\ninductive biases of ColBERT-like models permit it\nto accept similar gains under distillation, especially\nwhen using compressed representations.  Despite\nthis, we find that with denoised supervision and\nresidual  compression,  ColBERTv2  achieves  the\nhighest quality across all systems. As we discuss\nin §5.3, it exhibits space footprint competitive with\nthese single-vector models and much lower than\nvanilla ColBERT.\nBesides the official dev set, we evaluated Col-\nBERTv2,  SPLADEv2,  and  RocketQAv2  on  the\n“Local Eval” test set described by Khattab and Za-\nharia (2020) for MS MARCO, which consists of\n5000 queries disjoint with the training and the of-\nficial dev sets.  These queries are obtained from\nlabeled 50k queries that are provided in the official\nMS MARCO Passage Ranking task as additional\nvalidation data.\n4\nOn this test set, ColBERTv2 ob-\ntains 40.8% MRR@10, considerably outperform-\ning the baselines,  including RocketQAv2 which\nmakes use of document titles in addition to the\npassage text unlike the other systems.\n4\nThese are sampled from delta betweenqrels.dev.tsv\nandqrels.dev.small.tsvonhttps://microsoft.\ngithub.io/msmarco/Datasets\n.   We  refer  to  Khattab  and\nZaharia (2020) for details.  All our query IDs will be made\npublic to aid reproducibility.",
    "Corpus\nModels without Distillation    Models with Distillation\nColBERT\nDPR-M\nANCE\nMoDIR\nTAS-B\nRocketQAv2\nSPLADEv2\nColBERTv2\nBEIR Search Tasks (nDCG@10)\nDBPedia39.2    23.6    28.128.438.4    35.6    43.544.6\nFiQA31.7    27.5    29.529.630.0    30.2    33.635.6\nNQ52.4    39.8    44.644.246.3    50.5    52.156.2\nHotpotQA59.3    37.1    45.646.258.4    53.368.466.7\nNFCorpus30.5    20.8    23.724.431.9    29.3    33.433.8\nT-COVID67.7    56.1    65.467.648.1    67.5    71.073.8\nTouché (v2)-----24.727.226.3\nBEIR Semantic Relatedness Tasks (nDCG@10)\nArguAna23.3    41.4    41.541.842.7    45.147.946.3\nC-FEVER18.4    17.6    19.820.622.8    18.023.517.6\nFEVER77.1    58.9    66.968.070.0    67.678.678.5\nQuora85.4    84.2    85.285.683.5    74.9    83.885.2\nSCIDOCS14.5    10.8    12.212.414.9    13.115.815.4\nSciFact67.1    47.8    50.750.264.3    56.869.369.3\n(a)\nCorpus\nColBERT\nBM25\nANCE\nRocketQAv2\nSPLADEv2\nColBERTv2\nOOD Wikipedia Open QA (Success@5)\nNQ-dev65.7    44.6--65.668.9\nTQ-dev72.6    67.6--74.776.7\nSQuAD-dev60.0    50.6--60.465.0\nLoTTE Search Test Queries (Success@5)\nWriting74.7    60.3    74.4    78.0    77.180.1\nRecreation68.5    56.5    64.7    72.1    69.072.3\nScience53.6    32.7    53.6    55.3    55.456.7\nTechnology61.9    41.8    59.6    63.4    62.466.1\nLifestyle80.2    63.8    82.3    82.1    82.384.7\nPooled67.3    48.3    66.4    69.8    68.971.6\nLoTTE Forum Test Queries (Success@5)\nWriting71.0    64.0    68.8    71.5    73.076.3\nRecreation65.6    55.4    63.8    65.7    67.170.8\nScience41.8    37.1    36.5    38.0    43.746.1\nTechnology48.5    39.4    46.8    47.3    50.853.6\nLifestyle73.0    60.6    73.1    73.7    74.076.9\nPooled58.2    47.2    55.7    57.7    60.163.4\n(b)\nTable 5:  Zero-shot evaluation results.  Sub-table (a) reports results on BEIR and sub-table (b) reports results on\nthe Wikipedia Open QA and the test sets of the LoTTE benchmark.  On BEIR, we test ColBERTv2 and Rock-\netQAv2 and copy the results for ANCE, TAS-B, and ColBERT from Thakur et al. (2021), for MoDIR and DPR-\nMSMARCO (DPR-M) from Xin et al. (2021), and for SPLADEv2 from Formal et al. (2021a).\n5.2    Out-of-Domain Retrieval Quality\nNext, we evaluate ColBERTv2 outside the train-\ning  domain  using  BEIR  (Thakur  et  al.,  2021),\nWikipedia Open QA retrieval as in Khattab et al.\n(2021b), and LoTTE. We compare against a wide\nrange of recent and state-of-the-art retrieval sys-\ntems from the literature.\nBEIR.We start with BEIR, reporting the quality\nof models that do not incorporate distillation from\ncross-encoders, namely, ColBERT (Khattab and\nZaharia, 2020), DPR-MARCO (Xin et al., 2021),\nANCE (Xiong et al., 2020), and MoDIR (Xin et al.,\n2021),  as  well  as  models  that  do  utilize  distil-\nlation,  namely,  TAS-B  (Hofstätter  et  al.,  2021),\nSPLADEv2 (Formal et al., 2021a), and also Rock-\netQAv2, which we test ourselves using the official\ncheckpoint  trained  on  MS  MARCO.  We  divide\nthe table into “search” (i.e.,  natural queries and\nquestions) and “semantic relatednes” (e.g., citation-\nrelatedness and claim verification) tasks to reflect\nthe nature of queries in each dataset.\n5\nTable   5a   reports   results   with   the   official\nnDCG@10  metric.Among  the  models  with-\n5\nFollowing Formal et al. (2021a), we conduct our evalu-\nationg using the publicly-available datasets in BEIR. Refer\nto §E for details.\nout distillation, we see that the vanilla ColBERT\nmodel outperforms the single-vector systems DPR,\nANCE, and MoDIR across all but three tasks. Col-\nBERT often outpaces all three systems by large\nmargins and, in fact, outperforms the TAS-B model,\nwhich utilizes distillation, on most datasets. Shift-\ning our attention to models with distillation, we see\na similar pattern: while distillation-based models\nare generally stronger than their vanilla counter-\nparts, the models that decompose scoring into term-\nlevel interactions,  ColBERTv2 and SPLADEv2,\nare almost always the strongest.\nLooking more closely into the comparison be-\ntween SPLADEv2 and ColBERTv2, we see that\nColBERTv2 has an advantage on six benchmarks\nand ties SPLADEv2 on two, with the largest im-\nprovements attained on NQ, TREC-COVID, and\nFiQA-2018,  all  of  which  feature  natural  search\nqueries.  On the other hand, SPLADEv2 has the\nlead  on  five  benchmarks,  displaying  the  largest\ngains  on  Climate-FEVER  (C-FEVER)  and  Hot-\nPotQA. In C-FEVER, the input queries are sen-\ntences making climate-related claims and, as a re-\nsult,  do not reflect the typical characteristics of\nsearch queries. In HotPotQA, queries are written\nby crowdworkers who have access to the target pas-",
    "sages.  This is known to lead to artificial lexical\nbias (Lee et al., 2019), where crowdworkers copy\nterms from the passages into their questions as in\nthe Open-SQuAD benchmark.\nWikipedia Open QA.As a further test of out-\nof-domain  generalization,  we  evaluate  the  MS\nMARCO-trained ColBERTv2,  SPLADEv2,  and\nvanilla  ColBERT  on  retrieval  for  open-domain\nquestion answering, similar to the out-of-domain\nsetting  of  Khattab  et  al.  (2021b).We  report\nSuccess@5 (sometimes referred to as Recall@5),\nwhich is the percentage of questions whose short\nanswer string overlaps with one or more of the\ntop-5 passages.   For the queries,  we use the de-\nvelopment set questions of the open-domain ver-\nsions (Lee et al., 2019; Karpukhin et al., 2020) of\nNatural Questions (NQ; Kwiatkowski et al. 2019),\nTriviaQA (TQ; Joshi et al. 2017), and SQuAD (Ra-\njpurkar et al., 2016) datasets in Table 5b.   As a\nbaseline, we include the BM25 (Robertson et al.,\n1995) results using the Anserini (Yang et al., 2018a)\ntoolkit. We observe that ColBERTv2 outperforms\nBM25, vanilla ColBERT, and SPLADEv2 across\nthe three query sets, with improvements of up to\n4.6 points over SPLADEv2.\nLoTTE.Next, we analyze performance on the\nLoTTE test benchmark, which focuses on natural\nqueries over long-tail topics and exhibits a different\nannotation pattern to the datasets in the previous\nOOD evaluations. In particular, LoTTE uses auto-\nmatic Google rankings (for the “search” queries)\nand organic StackExchange question–answer pairs\n(for “forum” queries), complimenting the pooling-\nbased annotation of datasets like TREC-COVID (in\nBEIR) and the answer overlap metrics of Open-QA\nretrieval. We report Success@5 for each corpus on\nboth search queries and forum queries.\nOverall,  we  see  that  ANCE  and  vanilla  Col-\nBERT outperform BM25 on all topics,  and that\nthe three methods using distillation are generally\nthe strongest.  Similar to the Wikipedia-OpenQA\nresults, we find that ColBERTv2 outperforms the\nbaselines across all topics for both query types, im-\nproving upon SPLADEv2 and RocketQAv2 by up\nto 3.7 and 8.1 points, respectively.  Considering\nthe baselines, we observe that while RocketQAv2\ntends to have a slight advantage over SPLADEv2\non the “search” queries, SPLADEv2 is consider-\nably more effective on the “forum” tests.  We hy-\npothesize that the search queries, obtained from\nGoogle (through GooAQ) are more similar to MS\nMARCO than the forum queries and, as a result,\nthe latter stresses generalization more heavily, re-\nwarding term-decomposed models like SPLADEv2\nand ColBERTv2.\n5.3    Efficiency\nColBERTv2’s residual compression approach sig-\nnificantly reduces index sizes compared to vanilla\nColBERT. Whereas ColBERT requires 154 GiB\nto store the index for MS MARCO, ColBERTv2\nonly requires 16 GiB or 25 GiB when compressing\nembeddings to 1 or 2 bit(s) per dimension, respec-\ntively, resulting in compression ratios of 6–10×.\nThis storage figure includes 4.5 GiB for storing the\ninverted list.\nThis matches the storage for a typical single-\nvector model on MS MARCO, with 4-byte lossless\nfloating-point storage for one 768-dimensional vec-\ntor for each of the 9M passages amounting to a little\nover 25 GiBs. In practice, the storage for a single-\nvector model could be even larger when using a\nnearest-neighbor index like HNSW for fast search.\nConversely, single-vector representations could be\nthemselves compressed very aggressively (Zhan\net al., 2021a, 2022), though often exacerbating the\nloss in quality relative to late interaction methods\nlike ColBERTv2.\nWe  discuss  the  impact  of  our  compression\nmethod  on  search  quality  in  Appendix  B  and\npresent query latency results on the order of 50–\n250 milliseconds per query in Appendix C.\n6    Conclusion\nWe  introduced  ColBERTv2,  a  retriever  that  ad-\nvances the quality and space efficiency of multi-\nvector representations. We hypothesized that clus-\nter centroids capture context-aware semantics of\nthe  token-level  representations  and  proposed  a\nresidual representation that leverages these patterns\nto dramatically reduce the footprint of multi-vector\nsystemsoff-the-shelf. We then explored improved\nsupervision for multi-vector retrieval and found\nthat their quality improves considerably upon distil-\nlation from a cross-encoder system. The proposed\nColBERTv2 considerably outperforms existing re-\ntrievers in within-domain and out-of-domain evalu-\nations, which we conducted extensively across 28\ndatasets, establishing state-of-the-art quality while\nexhibiting competitive space footprint.",
    "Acknowledgements\nThis  research  was  supported  in  part  by  affiliate\nmembers  and  other  supporters  of  the  Stanford\nDAWN project—Ant Financial, Facebook, Google,\nand VMware—as well as Cisco, SAP, Virtusa, and\nthe NSF under CAREER grant CNS-1651570. Any\nopinions, findings, and conclusions or recommen-\ndations expressed in this material are those of the\nauthors and do not necessarily reflect the views of\nthe National Science Foundation.\nBroader Impact & Ethical Considerations\nThis work is primarily an effort toward retrieval\nmodels  that  generalize  better  while  performing\nreasonably efficiently in terms of space consump-\ntion. Strong out-of-the-box generalization to small\ndomain-specific applications can serve many users\nin practice, particularly where training data is not\navailable.   Moreover,  retrieval  holds  significant\npromise for many downstream NLP tasks,  as it\ncan help make language models smaller and thus\nmore efficient (i.e., by decoupling knowledge from\ncomputation), more transparent (i.e., by allowing\nusers to check the sources the model relied on when\nmaking a claim or prediction), and easier to update\n(i.e., by allowing developers to replace or add doc-\numents to the corpus without retraining the model)\n(Guu et al., 2020; Borgeaud et al., 2021; Khattab\net al., 2021a). Nonetheless, such work poses risks\nin terms of misuse, particularly toward misinforma-\ntion, as retrieval can surface results that are relevant\nyet inaccurate, depending on the contents of a cor-\npus.   Moreover,  generalization from training on\na large-scale dataset can propagate the biases of\nthat dataset well beyond its typical reach to new\ndomains and applications.\nWhile our contributions have made ColBERT’s\nlate interaction more efficient at storage costs, large-\nscale distillation with hard negatives increases sys-\ntem complexity and accordingly increases train-\ning cost, when compared with the straightforward\ntraining paradigm of the original ColBERT model.\nWhile ColBERTv2 is efficient in terms of latency\nand storage at inference time, we suspect that un-\nder extreme resource constraints, simpler model de-\nsigns like SPLADEv2 or RocketQAv2 could lend\nthemselves to easier-to-optimize environments. We\nleave low-level systems optimizations of all sys-\ntems  to  future  work.Another  worthwhile  di-\nmension for future exploration of tradeoffs is re-\nranking architectures over various systems with\ncross-encoders, which are known to be expensive\nyet precise due to their highly expressive capacity.\nResearch Limitations\nWhile we evaluate ColBERTv2 on a wide range of\ntests, all of our benchmarks are in English and, in\nline with related work, our out-of-domain tests eval-\nuate models that are trained on MS MARCO. We\nexpect our approach to work effectively for other\nlanguages and when all models are trained using\nother, smaller training set (e.g., NaturalQuestions),\nbut we leave such tests to future work.\nWe  have  observed  consistent  gains  for  Col-\nBERTv2 against existing state-of-the-art systems\nacross many diverse settings. Despite this, almost\nall IR datasets contain false negatives (i.e., rele-\nvant but unlabeled passages) and thus some cau-\ntion is needed in interpreting any individual result.\nNonetheless, we intentionally sought out bench-\nmarks with dissimilar annotation biases:  for in-\nstance,  TREC-COVID  (in  BEIR)  annotates  the\npool of documents retrieved by the systems submit-\nted at the time of the competition, LoTTE uses au-\ntomatic Google rankings (for “search” queries) and\nStackExchange question–answer pairs (for “forum”\nqueries), and the Open-QA tests rely on passage-\nanswer overlap for factoid questions. ColBERTv2\nperformed well in all of these settings. We discuss\nother issues pertinent to LoTTE in Appendix §D.\nWe have compared with a wide range of strong\nbaselines—including sparse retrieval and single-\nvector models—and found reliable patterns across\ntests.  However, we caution that empirical trends\ncan change as innovations are introduced to each of\nthese families of models and that it can be difficult\nto ensure exact apple-to-apple comparisons across\nfamilies of models, since each of them calls for\ndifferent sophisticated tuning strategies. We thus\nprimarily used results and models from the rich\nrecent literature on these problems, with models\nlike RocketQAv2 and SPLADEv2.\nOn the representational side, we focus on reduc-\ning the storage cost using residual compression,\nachieving strong gains in reducing footprint while\nlargely preserving quality.  Nonetheless, we have\nnot exhausted the space of more sophisticated opti-\nmizations possible, and we would expect more so-\nphisticated forms of residual compression and com-\nposing our approach with dropping tokens (Zhou\nand Devlin, 2021) to open up possibilities for fur-\nther reductions in space footprint.",
    "References\nStack Exchange Data Dump.\nLiefu  Ai,  Junqing  Yu,  Zebin  Wu,  Yunfeng  He,  and\nTao Guan. 2017.  Optimized Residual Vector Quan-\ntization for Efficient Approximate Nearest Neighbor\nSearch.Multimedia Systems, 23(2):169–181.\nSören  Auer,  Christian  Bizer,  Georgi  Kobilarov,  Jens\nLehmann,   Richard  Cyganiak,   and  Zachary  Ives.\n2007. DBpedia: A Nucleus for a Web of Open Data.\nInThe semantic web, pages 722–735. Springer.\nChristopher  F  Barnes,  Syed  A  Rizvi,  and  Nasser  M\nNasrabadi.  1996.Advances  in  Residual  Vector\nQuantization:  A Review.IEEE transactions on im-\nage processing, 5(2):226–262.\nAlexander   Bondarenko,   Maik   Fröbe,   Meriem   Be-\nloucif,  Lukas  Gienapp,  Yamen  Ajjour,  Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, et al. 2020.  Overview\nof  touché  2020:   Argument  Retrieval.    InInterna-\ntional  Conference  of  the  Cross-Language  Evalua-\ntion  Forum  for  European  Languages,  pages  384–\n395. Springer.\nSebastian  Borgeaud,   Arthur  Mensch,   Jordan  Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge  van  den  Driessche,  Jean-Baptiste  Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021.  Improv-\ning language models by retrieving from trillions of\ntokens.arXiv preprint arXiv:2112.04426.\nVera Boteva, Demian Gholipour, Artem Sokolov, and\nStefan Riezler. 2016.  A Full-text Learning to Rank\nDataset for Medical Information Retrieval.   InEu-\nropean Conference on Information Retrieval, pages\n716–722. Springer.\nChia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur\nAgrawal,  Wei Zhang,  and Kailash Gopalakrishnan.\n2018.   Adacomp :  Adaptive residual gradient com-\npression  for  data-parallel  distributed  training.In\nProceedings of the Thirty-Second AAAI Conference\non Artificial Intelligence, (AAAI-18), the 30th inno-\nvative  Applications  of  Artificial  Intelligence  (IAAI-\n18),  and  the  8th  AAAI  Symposium  on  Educational\nAdvances in Artificial Intelligence (EAAI-18),  New\nOrleans, Louisiana, USA, February 2-7, 2018, pages\n2827–2835. AAAI Press.\nArman  Cohan,  Sergey  Feldman,  Iz  Beltagy,  Doug\nDowney,   and   Daniel   Weld.   2020.SPECTER:\nDocument-levelrepresentationlearningusing\ncitation-informed  transformers.InProceedings\nof  the  58th  Annual  Meeting  of  the  Association\nfor  Computational  Linguistics,  pages  2270–2282,\nOnline. Association for Computational Linguistics.\nNachshon  Cohen,  Amit  Portnoy,  Besnik  Fetahu,  and\nAmir  Ingber.  2021.SDR:  Efficient  Neural  Re-\nranking  using  Succinct  Document  Representation.\narXiv preprint arXiv:2110.02065.\nZhuyun Dai and Jamie Callan. 2020.   Context-aware\nterm weighting for first stage passage retrieval.   In\nProceedings of the 43rd International ACM SIGIR\nconference  on  research  and  development  in  Infor-\nmation Retrieval, SIGIR 2020, Virtual Event, China,\nJuly 25-30, 2020, pages 1533–1536. ACM.\nJacob  Devlin,   Ming-Wei  Chang,   Kenton  Lee,   and\nKristina  Toutanova.  2019.    BERT:  Pre-training  of\ndeep bidirectional transformers for language under-\nstanding.   InProceedings  of  the  2019  Conference\nof  the  North  American  Chapter  of  the  Association\nfor  Computational  Linguistics:   Human  Language\nTechnologies,  Volume  1  (Long  and  Short  Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\nlian, Massimiliano Ciaramita, and Markus Leippold.\n2020.   CLIMATE-FEVER:  A  Dataset  for  Verifica-\ntion of Real-World Climate Claims.arXiv preprint\narXiv:2012.00614.\nThibault   Formal,   Carlos   Lassance,   Benjamin   Pi-\nwowarski,andStéphaneClinchant.2021a.\nSPLADE   v2:Sparse   Lexical   and   Expansion\nModel  for  Information  Retrieval.arXiv  preprint\narXiv:2109.10086.\nThibault Formal, Benjamin Piwowarski, and Stéphane\nClinchant. 2021b. SPLADE: Sparse Lexical and Ex-\npansion Model for First Stage Ranking.  InProceed-\nings of the 44th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 2288–2292.\nLuyu Gao and Jamie Callan. 2021.  Unsupervised cor-\npus  aware  language  model  pre-training  for  dense\npassage retrieval.arXiv preprint arXiv:2108.05540.\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Mod-\nularized  transfomer-based  ranking  framework.    In\nProceedings  of  the  2020  Conference  on  Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4180–4190, Online. Association for Computa-\ntional Linguistics.\nLuyu  Gao,   Zhuyun  Dai,   and  Jamie  Callan.  2021.\nCOIL:  Revisit  exact  lexical  match  in  information\nretrieval  with  contextualized  inverted  list.   InPro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics:  Human Language Technologies, pages\n3030–3042, Online. Association for Computational\nLinguistics.\nRobert Gray. 1984.   Vector quantization.IEEE Assp\nMagazine, 1(2):4–29.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020.  Realm:  Retrieval-\naugmented  language  model  pre-training.arXiv\npreprint arXiv:2002.08909.",
    "Sebastian   Hofstätter,   Sophia   Althammer,   Michael\nSchröder,  Mete Sertkan,  and Allan Hanbury. 2020.\nImproving  Efficient  Neural  Ranking  Models  with\nCross-Architecture  Knowledge  Distillation.arXiv\npreprint arXiv:2010.02666.\nSebastian  Hofstätter,  Sheng-Chieh  Lin,  Jheng-Hong\nYang,  Jimmy Lin,  and Allan Hanbury. 2021.   Effi-\nciently Teaching an Effective Dense Retriever with\nBalanced  Topic  Aware  Sampling.arXiv  preprint\narXiv:2104.06967.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020.  Poly-encoders:  Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring.  In8th International Confer-\nence on Learning Representations, ICLR 2020, Ad-\ndis  Ababa,  Ethiopia,  April  26-30,  2020.  OpenRe-\nview.net.\nGautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola\nDe Cao, Sebastian Riedel, and Edouard Grave. 2020.\nA memory efficient baseline for open domain ques-\ntion answering.arXiv preprint arXiv:2012.15156.\nHerve  Jegou,  Matthijs  Douze,  and  Cordelia  Schmid.\n2010.Product  quantization  for  nearest  neighbor\nsearch.IEEE transactions on pattern analysis and\nmachine intelligence, 33(1):117–128.\nYichen  Jiang,  Shikha  Bordia,  Zheng  Zhong,  Charles\nDognin,  Maneesh  Singh,  and  Mohit  Bansal.  2020.\nHoVer:  A dataset for many-hop fact extraction and\nclaim  verification.   InFindings  of  the  Association\nfor Computational Linguistics: EMNLP 2020, pages\n3441–3460, Online. Association for Computational\nLinguistics.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale  similarity  search  with  gpus.IEEE\nTransactions on Big Data.\nMandar  Joshi,  Eunsol  Choi,  Daniel  Weld,  and  Luke\nZettlemoyer.  2017.TriviaQA:  A  large  scale  dis-\ntantly supervised challenge dataset for reading com-\nprehension. InProceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume  1:  Long  Papers),  pages  1601–1611,  Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau  Yih.  2020.Dense  passage  retrieval  for\nopen-domain question answering. InProceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural  Language  Processing  (EMNLP),  pages  6769–\n6781,  Online.  Association  for  Computational  Lin-\nguistics.\nDaniel Khashabi, Amos Ng, Tushar Khot, Ashish Sab-\nharwal,  Hannaneh  Hajishirzi,  and  Chris  Callison-\nBurch.  2021.GooAQ:  Open  Question  Answer-\ning  with  Diverse  Answer  Types.arXiv  preprint\narXiv:2104.08727.\nOmar  Khattab,  Christopher  Potts,  and  Matei  Zaharia.\n2021a.    Baleen:   Robust  Multi-Hop  Reasoning  at\nScale via Condensed Retrieval.  InThirty-Fifth Con-\nference on Neural Information Processing Systems.\nOmar  Khattab,  Christopher  Potts,  and  Matei  Zaharia.\n2021b.    Relevance-guided  supervision  for  openqa\nwith ColBERT.Transactions of the Association for\nComputational Linguistics, 9:929–944.\nOmar Khattab and Matei Zaharia. 2020.  Colbert:  Ef-\nficient and effective passage search via contextual-\nized late interaction over BERT.  InProceedings of\nthe 43rd International ACM SIGIR conference on re-\nsearch and development in Information Retrieval, SI-\nGIR 2020,  Virtual Event,  China,  July 25-30,  2020,\npages 39–48. ACM.\nTom Kwiatkowski,  Jennimaria Palomaki,  Olivia Red-\nfield,   Michael  Collins,   Ankur  Parikh,   Chris  Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin,  Kenton Lee,  Kristina Toutanova,  Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob  Uszkoreit,  Quoc  Le,  and  Slav  Petrov.  2019.\nNatural  questions:   A  benchmark  for  question  an-\nswering research.Transactions of the Association\nfor Computational Linguistics, 7:452–466.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen.  2021a.Learning  dense  representations  of\nphrases at scale.  InProceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1:  Long\nPapers), pages 6634–6647, Online. Association for\nComputational Linguistics.\nJinhyuk  Lee,   Alexander  Wettig,   and  Danqi  Chen.\n2021b.  Phrase retrieval learns passage retrieval, too.\narXiv preprint arXiv:2109.08133.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019.   Latent retrieval for weakly supervised open\ndomain question answering.  InProceedings of the\n57th  Annual  Meeting  of  the  Association  for  Com-\nputational Linguistics, pages 6086–6096, Florence,\nItaly. Association for Computational Linguistics.\nYue Li,  Wenrui Ding,  Chunlei Liu,  Baochang Zhang,\nand  Guodong  Guo.  2021a.   TRQ:  Ternary  Neural\nNetworks With Residual Quantization.  InProceed-\nings  of  the  AAAI  Conference  on  Artificial  Intelli-\ngence, volume 35, pages 8538–8546.\nZefan Li, Bingbing Ni, Teng Li, Xiaokang Yang, Wen-\njun Zhang, and Wen Gao. 2021b.  Residual Quanti-\nzation for Low Bit-width Neural Networks.IEEE\nTransactions on Multimedia.\nJimmy  Lin  and  Xueguang  Ma.  2021.    A  Few  Brief\nNotes  on  DeepImpact,  COIL,  and  a  Conceptual\nFramework  for  Information  Retrieval  Techniques.\narXiv preprint arXiv:2106.14807.",
    "Sheng-Chieh Lin,  Jheng-Hong Yang,  and Jimmy Lin.\n2020.   Distilling  Dense  Representations  for  Rank-\ning using Tightly-Coupled Teachers.arXiv preprint\narXiv:2010.11386.\nXiaorui  Liu,  Yao  Li,  Jiliang  Tang,  and  Ming  Yan.\n2020.A  double  residual  compression  algorithm\nfor  efficient  distributed  learning.    InThe  23rd  In-\nternational Conference on Artificial Intelligence and\nStatistics,  AISTATS  2020,  26-28  August  2020,  On-\nline [Palermo, Sicily, Italy], volume 108 ofProceed-\nings of Machine Learning Research, pages 133–143.\nPMLR.\nYi  Luan,  Jacob  Eisenstein,  Kristina  Toutanova,  and\nMichael Collins. 2021.   Sparse,  Dense,  and Atten-\ntional Representations for Text Retrieval.Transac-\ntions of the Association for Computational Linguis-\ntics, 9:329–345.\nSean  MacAvaney,   Franco  Maria  Nardini,   Raffaele\nPerego,   Nicola  Tonellotto,   Nazli  Goharian,   and\nOphir Frieder. 2020.  Efficient document re-ranking\nfor  transformers  by  precomputing  term  representa-\ntions. InProceedings of the 43rd International ACM\nSIGIR  conference  on  research  and  development  in\nInformation  Retrieval,  SIGIR  2020,  Virtual  Event,\nChina, July 25-30, 2020, pages 49–58. ACM.\nCraig  Macdonald  and  Nicola  Tonellotto.  2021.On\napproximate  nearest  neighbour  selection  for  multi-\nstage  dense  retrieval.    InProceedings  of  the  30th\nACM  International  Conference  on  Information  &\nKnowledge Management, pages 3318–3322.\nMacedo  Maia,  Siegfried  Handschuh,  André  Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra  Balahur.  2018.    WWW’18  Open  Chal-\nlenge:  Financial Opinion Mining and Question An-\nswering. InCompanion Proceedings of the The Web\nConference 2018, pages 1941–1942.\nAntonio  Mallia,   Omar  Khattab,   Torsten  Suel,   and\nNicola Tonellotto. 2021.  Learning passage impacts\nfor  inverted  indexes.    InProceedings  of  the  44th\nInternational ACM SIGIR Conference on Research\nand  Development  in  Information  Retrieval,  pages\n1723–1727.\nAditya   Krishna   Menon,   Sadeep   Jayasumana,   Se-\nungyeon Kim, Ankit Singh Rawat, Sashank J. Reddi,\nand  Sanjiv  Kumar.  2022.In  defense  of  dual-\nencoders for neural ranking.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh  Tiwary,  Rangan  Majumder,  and  Li  Deng.\n2016.  MS MARCO: A human-generated MAchine\nreading  COmprehension  dataset.arXiv  preprint\narXiv:1611.09268.\nRodrigo  Nogueira  and  Kyunghyun  Cho.  2019.    Pas-\nsage   Re-ranking   with   BERT.arXiv   preprint\narXiv:1901.04085.\nBarlas O\n ̆\nguz, Kushal Lakhotia, Anchit Gupta, Patrick\nLewis,   Vladimir   Karpukhin,   Aleksandra   Piktus,\nXilun Chen,  Sebastian Riedel,  Wen-tau Yih,  Sonal\nGupta,  et  al.  2021.   Domain-matched  Pre-training\nTasks   for   Dense   Retrieval.arXiv   preprint\narXiv:2107.13602.\nAshwin  Paranjape,  Omar  Khattab,  Christopher  Potts,\nMatei  Zaharia,  and  Christopher  D  Manning.  2022.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation.   InInternational\nConference on Learning Representations.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen,  Wayne  Xin  Zhao,  Daxiang  Dong,  Hua  Wu,\nand  Haifeng  Wang.  2021.RocketQA:  An  opti-\nmized training approach to dense passage retrieval\nfor  open-domain  question  answering.   InProceed-\nings  of  the  2021  Conference  of  the  North  Ameri-\ncan  Chapter  of  the  Association  for  Computational\nLinguistics:  Human Language Technologies, pages\n5835–5847, Online. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text.  InProceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nRuiyang  Ren,  Shangwen  Lv,  Yingqi  Qu,  Jing  Liu,\nWayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng\nWang,  and  Ji-Rong  Wen.  2021a.    PAIR:  Leverag-\ning passage-centric similarity relation for improving\ndense passage retrieval.   InFindings of the Associ-\nation for Computational Linguistics:  ACL-IJCNLP\n2021,  pages  2173–2183,  Online.  Association  for\nComputational Linguistics.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoqiao  She,  Hua  Wu,  Haifeng  Wang,  and  Ji-\nRong Wen. 2021b.  RocketQAv2:  A Joint Training\nMethod for Dense Passage Retrieval and Passage Re-\nranking.arXiv preprint arXiv:2110.07367.\nStephen  E  Robertson,   Steve  Walker,   Susan  Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at TREC-3.NIST Special Publication.\nNandan  Thakur,  Nils  Reimers,  Andreas  Rücklé,  Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA  Heterogenous  Benchmark  for  Zero-shot  Eval-\nuation  of  Information  Retrieval  Models.arXiv\npreprint arXiv:2104.08663.\nJamesThorne,AndreasVlachos,Christos\nChristodoulopoulos,andArpitMittal.2018.\nFEVER:  a  large-scale  dataset  for  fact  extraction\nand  VERification.InProceedings  of  the  2018\nConference   of   the   North   American   Chapter   of\nthe   Association   for   Computational   Linguistics:\nHuman  Language  Technologies,  Volume  1  (Long\nPapers),  pages  809–819,  New  Orleans,  Louisiana.\nAssociation for Computational Linguistics.",
    "Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R Hersh, Kyle Lo, Kirk\nRoberts,  Ian  Soboroff,  and  Lucy  Lu  Wang.  2021.\nTREC-COVID:  Constructing  a  Pandemic  Informa-\ntion Retrieval Test Collection.   InACM SIGIR Fo-\nrum, volume 54, pages 1–12. ACM New York, NY,\nUSA.\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein.\n2018.   Retrieval of the best counterargument with-\nout  prior  topic  knowledge.   InProceedings  of  the\n56th  Annual  Meeting  of  the  Association  for  Com-\nputational  Linguistics  (Volume  1:   Long  Papers),\npages 241–251,  Melbourne,  Australia. Association\nfor Computational Linguistics.\nDavid  Wadden,  Shanchuan  Lin,  Kyle  Lo,  Lucy  Lu\nWang,  Madeleine  van  Zuylen,  Arman  Cohan,  and\nHannaneh Hajishirzi. 2020.  Fact or fiction:  Verify-\ning  scientific  claims.   InProceedings  of  the  2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 7534–7550, On-\nline. Association for Computational Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020.   MiniLM: Deep Self-\nAttention  Distillation  for  Task-Agnostic  Compres-\nsion  of  Pre-Trained  Transformers.arXiv  preprint\narXiv:2002.10957.\nBenchang  Wei,   Tao  Guan,   and  Junqing  Yu.  2014.\nProjected  Residual  Vector  Quantization  for  ANN\nSearch.IEEE multimedia, 21(3):41–51.\nThomas  Wolf,  Lysandre  Debut,  Victor  Sanh,  Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara  Ma,  Yacine  Jernite,  Julien  Plu,  Canwen  Xu,\nTeven  Le  Scao,  Sylvain  Gugger,  Mariama  Drame,\nQuentin Lhoest, and Alexander Rush. 2020.  Trans-\nformers:  State-of-the-art  natural  language  process-\ning.  InProceedings of the 2020 Conference on Em-\npirical  Methods  in  Natural  Language  Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nJi  Xin,  Chenyan  Xiong,  Ashwin  Srinivasan,  Ankita\nSharma,  Damien  Jose,  and  Paul  N  Bennett.  2021.\nZero-Shot Dense Retrieval with Momentum Adver-\nsarial  Domain  Invariant  Representations.arXiv\npreprint arXiv:2110.07581.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin  Liu,  Paul  N  Bennett,  Junaid  Ahmed,  and\nArnold  Overwijk.  2020.Approximate  Nearest\nNeighbor Negative Contrastive Learning for Dense\nText  Retrieval.InInternational  Conference  on\nLearning Representations.\nIkuya Yamada,  Akari Asai,  and Hannaneh Hajishirzi.\n2021a.  Efficient passage retrieval with hashing for\nopen-domain question answering. InProceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational  Linguistics  and  the  11th  International\nJoint Conference on Natural Language Processing\n(Volume 2:  Short Papers),  pages 979–986,  Online.\nAssociation for Computational Linguistics.\nIkuya Yamada,  Akari Asai,  and Hannaneh Hajishirzi.\n2021b.  Efficient passage retrieval with hashing for\nopen-domain question answering. InProceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational  Linguistics  and  the  11th  International\nJoint Conference on Natural Language Processing\n(Volume 2:  Short Papers),  pages 979–986,  Online.\nAssociation for Computational Linguistics.\nPeilin   Yang,   Hui   Fang,   and   Jimmy   Lin.   2018a.\nAnserini:Reproducible  ranking  baselines  using\nlucene.Journal  of  Data  and  Information  Quality\n(JDIQ), 10(4):1–20.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher  D.  Manning.  2018b.HotpotQA:  A  dataset\nfor diverse, explainable multi-hop question answer-\ning.  InProceedings of the 2018 Conference on Em-\npirical  Methods  in  Natural  Language  Processing,\npages  2369–2380,  Brussels,  Belgium.  Association\nfor Computational Linguistics.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2021a.   Jointly Optimiz-\ning Query Encoder and Product Quantization to Im-\nprove Retrieval Performance.  InProceedings of the\n30th ACM International Conference on Information\n& Knowledge Management, pages 2487–2496.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2021b. Optimizing Dense\nRetrieval Model Training with Hard Negatives.   In\nProceedings  of  the  44th  International  ACM  SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 1503–1512.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2022.  Learning discrete\nrepresentations via constrained clustering for effec-\ntive  and  efficient  dense  retrieval.    InProceedings\nof  the  Fifteenth  ACM  International  Conference  on\nWeb  Search  and  Data  Mining,  WSDM  ’22,  page\n1328–1336. Association for Computing Machinery.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and\nShaoping  Ma.  2020a.    Learning  to  retrieve:   How\nto  train  a  dense  retrieval  model  effectively  and  ef-\nficiently.arXiv preprint arXiv:2010.10469.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and\nShaoping Ma. 2020b.  Repbert:  Contextualized text\nembeddings for first-stage retrieval.arXiv preprint\narXiv:2006.15498.\nGiulio  Zhou  and  Jacob  Devlin.  2021.Multi-vector\nattention  models  for  deep  re-ranking.   InProceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 5452–5456,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.",
    "ColBERTRandom\n1162564096\n# Distinct Tokens per Cluster\n0\n25\n50\n75\n100\nProportion\n(a) Number of distinct tokens\nappearing in each cluster.\n4641024\n# Distinct Clusters per Token\n0\n25\n50\n75\n100\nProportion\n(b) Number of distinct clus-\nters each token appears in.\nFigure 2:  Empirical CDFs analyzing semantic proper-\nties of MS MARCO token-level embeddings both en-\ncoded by ColBERT and randomly generated.  The em-\nbeddings  are  partitioned  into2\n18\nclusters  and  corre-\nspond to roughly 27,000 distinct tokens.\nA    Analysis of ColBERT’s Semantic\nSpace\nColBERT (Khattab and Zaharia, 2020) decomposes\nrepresentations and similarity computation at the\ntoken level.  Because of this compositional archi-\ntecture, we hypothesize that ColBERT exhibits a\n“lightweight” semantic space: without any special\nre-training, vectors corresponding to each sense of\na word would cluster very closely, with only minor\nvariation due to context.\nIf this hypothesis is true, we would expect the\nembeddings corresponding to each token in the\nvocabulary to localize in only a small number of\nregions  in  the  embedding  space,  corresponding\nto the contextual “senses” of the token.   To val-\nidate  this  hypothesis,  we  analyze  the  ColBERT\nembeddings  corresponding  to  the  tokens  in  the\nMS  MARCO  Passage  Ranking  (Nguyen  et  al.,\n2016) collection: we performk-means clustering\non the nearly 600M embeddings—corresponding\nto 27,000 unique tokens—intok= 2\n18\nclusters.\nAs a baseline, we repeat this clustering with ran-\ndom embeddings but keep the true distribution of\ntokens. Figure 2 presents empirical cumulative dis-\ntribution function (eCDF) plots representing the\nnumber of distinct non-stopword tokens appear-\ning in each cluster (2a) and the number of distinct\nclusters in which each token appears (2b).\n6\nMost\ntokens appear in a very small fraction of the num-\nber of centroids: in particular, we see that roughly\n90% of clusters have≤16 distinct tokens with\n6\nWe rank tokens by number of clusters they appear in and\ndesignate the top-1% (under 300) as stopwords.\nthe ColBERT embeddings, whereas less than 50%\nof clusters have≤16 distinct tokens with the ran-\ndom embeddings. This suggests that the centroids\neffectively map the ColBERT semantic space.\nTable 6 presents examples to highlight the se-\nmantic space captured by the centroids. The most\nfrequently appearing tokens in cluster #917 relate\nto photography; these include, for example, ‘pho-\ntos’ and ‘photographs’.  If we then examine the\nadditional clusters in which these tokens appear,\nwe find that there is substantial semantic overlap\nbetween  these  new  clusters  (e.g.,  Photos-Photo,\nPhoto-Image-Picture) and cluster #917.   We ob-\nserve a similar effect with tokens appearing in clus-\nter #216932, comprising tornado-related terms.\nThis analysis indicates that cluster centroids can\nsummarize the ColBERT representations with high\nprecision. In §3.3, we propose a residual compres-\nsion mechanism that uses these centroids along\nwith minor refinements at the dimension level to\nefficiently encode late-interaction vectors.\nB    Impact of Compression\nOur  residual  compression  approach  (§3.3)  pre-\nserves approximately the same quality as the un-\ncompressed embeddings.  In particular, when ap-\nplied to a vanilla ColBERT model on MS MARCO\nwhose  MRR@10  is  36.2%  and  Recall@50  is\n82.1%, the quality of the model with 2-bit compres-\nsion is 36.2% MRR@10 and 82.3% Recall@50.\nWith 1-bit compression, the model achieves 35.5%\nMRR@10 and 81.6% Recall@50.\n7\nWe  also  tested  the  residual  compression  ap-\nproach on late-interaction retrievers that conduct\ndownstream tasks, namely, ColBERT-QA (Khat-\ntab et al., 2021b) for the NaturalQuestions open-\ndomain QA task, and Baleen (Khattab et al., 2021a)\nfor multi-hop reasoning on HoVer for claim verifi-\ncation.  On the NQ dev set, ColBERT-QA’s suc-\ncess@5  (success@20)  dropped  only  marginally\nfrom   75.3%   (84.3%)   to   74.3%   (84.2%)   and\nits  downstream  Open-QA  answer  exact  match\ndropped from 47.9% to 47.7%, when using 2-bit\ncompression for retrieval and using the same check-\npoints of ColBERT-QA otherwise.\n7\nWe contrast this with an early implementation of com-\npression for ColBERT, which used binary representations as\nin BPR (Yamada et al., 2021a) without residual centroids,\nand achieves 34.8% (35.7%) MRR@10 and 80.5% (81.8%)\nRecall@50 with 1-bit (2-bit) binarization. Like the original\nColBERT, this form of compression relied on a separate FAISS\nindex for candidate generation.",
    "Cluster IDMost Common Tokens\nMost Common Clusters Per Token\nTokenClusters\n917\n‘photos’, ‘photo’, ‘pictures’,\n‘photographs’, ‘images’,\n‘photography’, ‘photograph’\n‘photos’\nPhotos-Photo, Photos-Pictures-Photo\n‘photo’\nPhoto-Image-Picture, Photo-Picture-Photograph, Photo-Picture-Photography\n‘pictures’\nPictures-Picture-Images, Picture-Pictures-Artists, Pictures-Photo-Picture\n216932\n‘tornado’, ‘tornadoes’, ‘storm’\n‘hurricane’, ‘storms’\n‘tornado’\nTornado-Hurricane-Storm, Tornadoes-Tornado-Blizzard\n‘tornadoes’\nTornadoes-Tornado-Storms, Tornadoes-Tornado-Blizzard, Tornado-Hurricane-Storm\n‘storm’\nStorm-Storms, Storm-Storms-Weather, Storm-Storms-Tempest\nTable 6:  Examples of clusters taken from all MS MARCO passages.   We present the tokens that appear most\nfrequently in the selected clusters as well as additional clusters the top tokens appear in.\n38.5038.7539.0039.2539.5039.75\nMRR@10\n50\n100\n150\n200\n250\nQuery Latency (ms)\nMS MARCO\n68.068.569.069.5\nSuccess@5\nLoTTE Pooled (dev)\n74.074.575.075.576.0\nSuccess@5\nLoTTE Lifestyle (dev)\nprobe\n1\n2\n4\nbits\n2\n1\ncandidates\nprobe x 2^14\nprobe x 2^12\nFigure 3: Latency vs. retrieval quality with varying parameter configurations for three datasets of different collec-\ntion sizes.  We sweep a range of values for the number of centroids per vector (probe), the number of bits used\nfor residual compression, and the number of candidates.  Note that retrieval quality is measured in MRR@10 for\nMS MARCO and Success@5 for LoTTE datasets.  Results toward the bottom right corner (higher quality, lower\nlatency) are best.",
    "Similarly, on the HoVer (Jiang et al., 2020) dev\nset, Baleen’s retrieval R@100 dropped from 92.2%\nto only 90.6% but its sentence-level exact match\nremained roughly the same, going from 39.2% to\n39.4%. We hypothesize that the supervision meth-\nods applied in ColBERTv2 (§3.2) can also be ap-\nplied to lift quality in downstream tasks by improv-\ning the recall of retrieval for these tasks. We leave\nsuch exploration for future work.\nC    Retrieval Latency\nFigure  3  evaluates  the  latency  of  ColBERTv2\nacross three collections of varying sizes, namely,\nMS MARCO, LoTTE Pooled (dev), and LoTTE\nLifestyle (dev), which contain approximately 9M\npassages,  2.4M  answer  posts,  and  270k  answer\nposts, respectively. We average latency across three\nruns of the MS MARCO dev set and the LoTTE\n“search” queries. Search is executed using a Titan\nV GPU on a server with two Intel Xeon Gold 6132\nCPUs, each with 28 hardware execution contexts.\nThe figure varies three settings of ColBERTv2.\nIn particular, we evaluate indexing with 1-bit and\n2-bit encoding (§3.4) and searching by probing the\nnearest 1, 2, or 4 centroids to each query vector\n(§3.5). When probingprobecentroids per vector,\nwe score eitherprobe×2\n12\norprobe×2\n14\ncandi-\ndates per query.\n8\nTo begin with, we notice that the quality reported\non thex-axis varies only within a relatively narrow\nrange.  For instance, the axis ranges from 38.50\nthrough 39.75 for MS MARCO, and all but two of\nthe cheapest settings score above 39.00. Similarly,\nthey-axis varies between approximately 50 mil-\nliseconds per query up to 250 milliseconds (mostly\nunder 150 milliseconds) using our relatively simple\nPython-based implementation.\nDigging  deeper,  we  see  that  the  best  quality\nin these metrics can be achieved or approached\nclosely with around 100 milliseconds of latency\nacross all three datasets, despite their various sizes\nand characteristics, and that 2-bit indexing reliably\noutperforms 1-bit indexing but the loss from more\naggressive compression is small.\nD    LoTTE\nDomain coverageTable 9 presents the full dis-\ntribution of communities in the LoTTE dev dataset.\n8\nThese settings are selected based on preliminary explo-\nration of these parameters, which indicated that performance\nfor largerprobevalues tends to require scoring a larger num-\nber of candidates.\n0200400600\nWords per passage\nPooled\nLifestyle\nTechnology\nScience\nRecreation\nWriting\nFigure 4: LoTTE words per passage\n5101520\nWords per query\n[Forum] Pooled\n[Forum] Lifestyle\n[Forum] Technology\n[Forum] Science\n[Forum] Recreation\n[Forum] Writing\n[Search] Pooled\n[Search] Lifestyle\n[Search] Technology\n[Search] Science\n[Search] Recreation\n[Search] Writing\nFigure 5: LoTTE words per query\nThe topics covered by LoTTE cover a wide range\nof linguistic phenomena given the diversity in top-\nics and communities represented. However, since\nall posts are submitted by anonymous users we do\nnot have demographic information regarding the\nidentify of the contributors.  All posts are written\nin English.\nPassages\nAs  mentioned  in  §4,  we  construct\nLoTTE collections by selecting passages from the\nStackExchange archive with positive scores.  We\nremove HTML tags from passages and filter out\nempty passages.  For each passage we record its\ncorresponding query and save the query-to-passage\nmapping to keep track of the posted answers corre-\nsponding to each query.\nSearch queriesWe construct the list of LoTTE\nsearch queries by drawing from GooAQ queries\nthat appear in the StackExchange post archive. We\nfirst shuffle the list of GooAQ queries so that in\ncases where multiple queries exist for the same\nanswer passage we randomly select the query to\ninclude in LoTTE rather than always selecting the\nfirst appearing query.  We verify that every query\nhas at least one corresponding answer passage.\nForum queriesFor each LoTTE topic and its\nconstituent communities we first compute the frac-\ntion of the total queries attributed to each individ-\nual community.  We then use this distribution to\nconstruct a truncated  query set by selecting  the",
    "5101520\nAnswers per query\n[Forum] Pooled\n[Forum] Lifestyle\n[Forum] Technology\n[Forum] Science\n[Forum] Recreation\n[Forum] Writing\n[Search] Pooled\n[Search] Lifestyle\n[Search] Technology\n[Search] Science\n[Search] Recreation\n[Search] Writing\nFigure 6: LoTTE answers per query\nCorpus\nColBERT\nBM25\nANCE\nRocketQAv2\nSPLADEv2\nColBERTv2\nLoTTE Search Dev Queries (Success@5)\nWriting76.3    47.3    75.779.578.981.7\nRecreation71.8    56.3    66.173.070.776.0\nScience71.7    52.2    66.967.773.474.2\nTechnology52.8    35.8    55.754.356.359.3\nLifestyle73.1    54.4    69.872.471.275.8\nPooled65.4    45.6    63.766.467.069.3\nLoTTE Forum Dev Queries (Success@5)\nWriting75.5    66.2    74.475.578.180.8\nRecreation69.1    56.6    65.969.068.971.8\nScience58.2    51.3    56.356.759.962.6\nTechnology39.6    30.7    38.839.942.145.0\nLifestyle61.1    48.2    61.862.061.865.8\nPooled59.1    47.8    57.458.960.663.7\nTable 7: Zero-shot evaluation results on the dev sets of\nthe LoTTE benchmark.\nhighest ranked queries from each community as\ndetermined by 1) the query scores and 2) the query\nview counts.  We only use queries which have an\naccepted answer. We ensure that each community\ncontributes at least 50 queries to the truncated set\nwhenever possible.  We set the overall size of the\ntruncated set to be 2000 queries, though note that\nthe total can exceed this due to rounding and/or the\nminimum per-community query count. We remove\nall quotation marks and HTML tags.\nStatisticsFigure  4  plots  the  number  of  words\nper passage in each LoTTE dev corpus. Figures 5\nand 6 plot the number of words and number of\ncorresponding  answer  passages  respectively  per\nquery, split across search and forum queries.\nDev Results\nTable 7 presents out-of-domain eval-\nuation results on the LoTTE dev queries. Continu-\ning the trend we observed in 5, ColBERTv2 consis-\ntently outperforms all other models we tested.\nLicensing and Anonymity\nThe original Stack-\nExchange  post  archive  is  licensed  under  a  Cre-\native Commons BY-SA 4.0 license (sta). Personal\ndata is removed from the archive before being up-\nloaded, though all posts are public; when we re-\nlease LoTTE publicly we will include URLs to the\noriginal posts for proper attribution as required by\nthe license. The GooAQ dataset is licensed under\nan Apache license,  version 2.0 (Khashabi et al.,\n2021). We will also release LoTTE with a CC BY-\nSA 4.0 license. The search queries can be used for\nnon-commercial research purposes only as per the\nGooAQ license.\nE    Datasets in BEIR\nTable 8 lists the BEIR datasets we used in our evalu-\nation, including their respective license information\nas well as the numbers of documents as well as the\nnumber of test set queries. We refer to Thakur et al.\n(2021) for a more detailed description of each of\nthe datasets.\nOur Touché evaluation uses an updated version\nof the data in BEIR, which we use for evaluating the\nmodels we run (i.e., ColBERTv2 and RocketQAv2)\nas well as SPLADEv2.\nDatasetLicense# Passages# Test Queries\nArguAna (Wachsmuth et al., 2018)CC BY 4.086741406\nClimate-Fever (Diggelmann et al., 2020)Not reported54165931535\nDBPedia (Auer et al., 2007)CC BY-SA 3.04635922400\nFEVER (Thorne et al., 2018)CC BY-SA 3.0\nFiQA-2018 (Maia et al., 2018)Not reported57638648\nHotpotQA (Yang et al., 2018b)CC BY-SA 4.052333297405\nNFCorpus (Boteva et al., 2016)Not reported3633323\nNQ (Kwiatkowski et al., 2019)CC BY-SA 3.026814683452\nSCIDOCS (Cohan et al., 2020)\nGNU General Public\nLicense v3.0\n256571000\nSciFact (Wadden et al., 2020)CC BY-NC 2.05183300\nQuoraNot reported52293110000\nTouché-2020 (Bondarenko et al., 2020)CC BY 4.038254549\nTREC-COVID (Voorhees et al., 2021)\nDataset License\nAgreement\n17133250\nTable 8: BEIR dataset information.\nWe also tested on the Open-QA benchmarks NQ,\nTQ, and SQuAD, each of which has approximately\n9k dev-set questions and muli-hop HoVer, whose\ndevelopment set has 4k claims. In the compression\nevaluation §B, we used models trained in-domain\non NQ and HoVer, whose training sets contain 79k\nand 18k queries, respectively.\nF    Implementation & Hyperparameters\nWe  implement  ColBERTv2  using  Python  3.7,\nPyTorch  1.9,   and  HuggingFace  Transformers\n4.10 (Wolf et al., 2020), extending the original im-\nplementation of ColBERT by Khattab and Zaharia\n(2020). We use FAISS 1.7 (Johnson et al., 2019) for",
    "k-means clustering,\n9\nthough unlike ColBERT we\ndo not use it for nearest-neighbor search. Instead,\nwe implement our candidate generation mechanism\n(§3.5) using PyTorch primitives in Python.\nWe conducted our experiments on an internal\ncluster, typically using up to four 12GB Titan V\nGPUs for each of the inference tasks (e.g., index-\ning, computing distillation scores, and retrieval)\nand four 80GB A100 GPUs for training, though\nGPUs with smaller RAM can be used via gradient\naccumulation. Using this infrastructure, computing\nthe distillation scores takes under a day, training a\n64-way model on MS MARCO for 400,000 steps\ntakes around five days, and indexing takes approx-\nimately two hours.  We very roughly estimate an\nupper bound total of 20 GPU-months for all experi-\nmentation, development, and evaluation performed\nfor this work over a period of several months.\nLikeColBERT,ourencoderisa\nbert-base-uncasedmodelthatisshared\nbetween the query and passage encoders and which\nhas 110M parameters. We retain the default vector\ndimension  suggested  by  Khattab  and  Zaharia\n(2020)  and  used  in  subsequent  work,  namely,\nd=128. For the experiments reported in this paper,\nwe  train  on  MS  MARCO  training  set.   We  use\nsimple defaults with limited manual exploration on\nthe official development set for the learning rate\n(10\n−5\n),  batch size (32 examples),  and warm up\n(for 20,000 steps) with linear decay.\nHyperparameters corresponding to retrieval are\nexplored  in  §C.  We  default  toprobe= 2,  but\nuseprobe= 4on the largest datasets,  namely,\nMS MARCO and Wikipedia.  By default we set\ncandidates=probe∗2\n12\n,  but  for  Wikipedia\nwe setcandidates=probe∗2\n13\nand for MS\nMARCO we setcandidates=probe∗2\n14\n. We\nleave extensive tuning of hyperparameters to future\nwork.\nWe train on MS MARCO using 64-way tuples\nfor distillation, sampling them from the top-500\nretrieved passages per query.  The training set of\nMS MARCO contains approximately 800k queries,\nthough only about 500k have associated labels. We\napply  distillation  using  all  800k  queries,  where\neach training example contains exactly one “posi-\ntive”, defined as a passage labeled as positive or the\ntop-ranked passage by the cross-encoder teacher,\nirrespective of its label.\nWe train for 400k steps, initializing from a pre-\n9\nhttps://github.com/facebookresearch/faiss\nfinetuned checkpoint using 32-way training exam-\nples and 150k steps.  To generate the top-kpas-\nsages per training query, we apply two rounds, fol-\nlowing Khattab et al. (2021b).   We start from a\nmodel trained with hard triples (akin to Khattab\net al. (2021b)), train with distillation, and then use\nthe distilled model to retrieve for the second round\nof training. Preliminary experiments indicate that\nquality has low sensitivity to this initialization and\ntwo-round training, suggesting that both of them\ncould be avoided to reduce the cost of training.\nUnless otherwise stated, the results shown rep-\nresent a single run.  The latency results in §3 are\naverages of three runs. To evaluate for Open-QA re-\ntrieval, we use evaluation scripts from Khattab et al.\n(2021b), which checks if the short answer string\nappears  in  the  (titled)  Wikipedia  passage.   This\nadapts the DPR (Karpukhin et al., 2020) evaluation\ncode.\n10\nWe use the preprocessed Wikipedia Dec\n2018 dump released by Karpukhin et al. (2020).\nFor out-of-domain evaluation, we elected to fol-\nlow  Thakur  et  al.  (2021)  and  set  the  maximum\ndocument length of ColBERT, RocketQAv2, and\nColBERTv2 to 300 tokens on BEIR and LoTTE.\nFormal et al. (2021a) selected maximum sequence\nlength 256 for SPLADEv2 both on MS MARCO\nand on BEIR for both queries and documents, and\nwe retained this default when testing their system\non LoTTE. Unless otherwise stated, we keep the\ndefault query maximum sequence length for Col-\nBERTv2 and RocketQAv2, which is 32 tokens. For\nthe ArguAna test in BEIR, as the queries are them-\nselves long documents, we set the maximum query\nlength used by ColBERTv2 and RocketQAv2 to\n300. For Climate-FEVER, as the queries are rela-\ntively long sentence claims, we set the maximum\nquery length used by ColBERTv2 to 64.\nWe use the open source BEIR implementation\n11\nand SPLADEv2 evaluation\n12\ncode as the basis for\nour evaluations of SPLADEv2 and ANCE as well\nas for BM25 on LoTTE. We use the Anserini (Yang\net al., 2018a) toolkit for BM25 on the Wikipedia\nOpen-QA retrieval tests as in Khattab et al. (2021b).\nWe use the implementation developed by the Rock-\netQAv2 authors for evaluating RocketQAv2.\n13\n10\nhttps://github.com/facebookresearch/DPR/blob/\nmain/dpr/data/qa_validation.py\n11\nhttps://github.com/UKPLab/beir\n12\nhttps://github.com/naver/splade\n13\nhttps://github.com/PaddlePaddle/RocketQA",
    "TopicCommunities# Passages# Search queries# Forum queries\nWriting\nell.stackexchange.com1081434331196\nliterature.stackexchange.com4778758\nwriting.stackexchange.com2933023163\nlinguistics.stackexchange.com1230222116\nworldbuilding.stackexchange.com12251912470\nRecreation\nrpg.stackexchange.com8906691621\nboardgames.stackexchange.com2034067179\nscifi.stackexchange.com102561343852\nphoto.stackexchange.com5105862350\nScience\nchemistry.stackexchange.com39435245267\nstats.stackexchange.com144084137949\nacademia.stackexchange.com7645066302\nastronomy.stackexchange.com145801588\nearthscience.stackexchange.com67341050\nengineering.stackexchange.com120641677\ndatascience.stackexchange.com2323415156\nphilosophy.stackexchange.com2706134124\nTechnology\nsuperuser.com418266441648\nelectronics.stackexchange.com205891118314\naskubuntu.com296291132480\nserverfault.com323943148506\nwebapps.stackexchange.com318317755\nLifestyle\npets.stackexchange.com100702087\nlifehacks.stackexchange.com7893250\ngardening.stackexchange.com2060116182\nparenting.stackexchange.com183571087\ncrafts.stackexchange.com3094450\noutdoors.stackexchange.com133241676\ncoffee.stackexchange.com22491150\nmusic.stackexchange.com4739965287\ndiy.stackexchange.com82659135732\nbicycles.stackexchange.com3556740229\nmechanics.stackexchange.com2768098246\nTable 9: Per-community distribution of LoTTE dev dataset passages and questions."
  ]
}