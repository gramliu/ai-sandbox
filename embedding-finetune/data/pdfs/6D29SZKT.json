{
  "key": "6D29SZKT",
  "url": "http://arxiv.org/pdf/2403.20329",
  "metadata": {
    "title": "ReALM: Reference Resolution As Language Modeling",
    "abstract": "  Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.\n",
    "published": "2024-03-29T17:59:06Z"
  },
  "text": [
    "ReALM: Reference Resolution As Language Modeling\nJoel Ruben Antony Moniz\n*\n1\n, Soundarya Krishnan\n*\n2\n, Melis Ozyildirim\n3\n,\nPrathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu\n4\n, Nidhi Rajshree\n{\n1\njoelmoniz,\n2\nskrishnan22,\n3\nmelisozyildirim,\n4\nhong_yu}@apple.com\nApple\nAbstract\nReference resolution is an important problem,\none that is essential to understand and success-\nfully handle context of different kinds.  This\ncontext includes both previous turns and con-\ntext that pertains to non-conversational entities,\nsuch as entities on the user’s screen or those\nrunning in the background. While LLMs have\nbeen shown to be extremely powerful for a va-\nriety of tasks, their use in reference resolution,\nparticularly for non-conversational entities, re-\nmains underutilized. This paper demonstrates\nhow LLMs can be used to create an extremely\neffective system to resolve references of var-\nious types, by showing how reference resolu-\ntion can be converted into a language model-\ning problem, despite involving forms of enti-\nties like those on screen that are not tradition-\nally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements\nover an existing system with similar functional-\nity across different types of references, with our\nsmallest model obtaining absolute gains of over\n5% for on-screen references.  We also bench-\nmark  against  GPT-3.5  and  GPT-4,  with  our\nsmallest  model  achieving  performance  com-\nparable to that of GPT-4, and our larger models\nsubstantially outperforming it.\n1    Introduction\nHuman speech typically contains ambiguous refer-\nences such as \"they\" or \"that\", whose meaning is\nobvious (to other humans) given the context. Being\nable to understand context, including references\nlike these, is essential for a conversational assis-\ntant that aims to allow a user to naturally commu-\nnicate their requirements to an agent, or to have\na  conversation  with  it  (Luger  and  Sellen,  2016;\nLjungholm, 2021). In addition, enabling the user\nto issue queries about what they see on their screen\nis a crucial step in ensuring a true hands-free ex-\nperience in voice assistants. For instance, consider\n*\nEqual contribution\nthe following interactions between a user and an\nagent shown in Table 1.\nTable 1:  Sample Interactions between a user and an\nagent.\nSpeakerDialogue\nUserShow me pharmacies near me\nAgentHere is a list I found.\nAgent... (list presented)\nUser (eg 1)Call the one on Rainbow Rd.\nUser (eg 2)Call the bottom one.\nUser (eg 3)Call this number (present onscreen)\nHere, it is immediately apparent that it would\nnot be possible for the Agent to understand or com-\nplete the user’s query without the ability to use and\ncomprehend context. It also stands to reason that\nthere are multiple types of context that are neces-\nsary to handle user queries: conversational context\nand on-screen context being two prime examples.\nRecent   Large    Language   Models   (LLMs)\n(Stammbach  et  al.,  2022;  Touvron  et  al.,  2023;\nSanthanam  et  al.,  2022;  Dettmers  et  al.,  2023)\nhave often enabled end-to-end experiences,  per-\nhaps even obviating the need of a traditional multi-\nstage pipeline that includes reference resolution.\nThere are, however, still several real-world cases\nwhere a pipeline is valuable, perhaps even essen-\ntial, and an end-to-end approach falls short. First,\nwhen a framework runs completely on-device (for\nexample, for privacy and efficiency reasons) on a\nsystem such as a smartphone that has relatively\nlimited computing power, due to the low-power\nnature of the system and latency constraints, us-\ning a single, large, end-to-end model is infeasible:\nusing a single LLM for this task would usually re-\nquire the use of a large model with long prompts\nfor true end-to-end experiences (Wei et al., 2022).\nSecond, consider the case when the model has to\nintegrate with APIs, has to consume information\nfrom components upstream, or has to provide in-\nformation to be consumed downstream: while in\narXiv:2403.20329v1  [cs.CL]  29 Mar 2024",
    "these  cases  it  is  possible  to  have  an  end-to-end\napproach having the LLM write API calls (Patil\net al., 2023; Qin et al., 2023), this often requires a\nlarge language model and a complete overhaul of\nexisting pipelines, which might be cumbersome or\ncompletely infeasible. Third, the use of a focused\nmodel would allow for an existing reference resolu-\ntion module to be swapped with improved versions\nin a transparent way,  while providing improved\nability to hill-climb and improved interpretability,\nby virtue of the system being modular. Finally, for\nthe task under consideration in this paper, reference\nresolution does not include solely conversational\nreferences, but also includes the ability to refer-\nence an on-screen and/or a background entity that\nis part of what the user currently perceives in their\ninteraction with a device, but has not been a part\nof the conversational history that results from their\ndirect interaction with the virtual agent in question.\nThere thus continues to be utility in exploring \"tra-\nditional\" NLP tasks such as reference resolution,\ndespite some of the larger language models being\nable to handle them implicitly.  In this work, we\nthus advocate the use of (relatively) smaller lan-\nguage models, but fine-tuned for specifically and\nexplicitly for the task of reference resolution.\nAlong similar lines, relying on language model-\ning alone (Bajaj et al., 2022; Patra et al., 2022;\nZheng  et  al.,  2023)  has  recently  shown  great\npromise in being able to handle a variety of tasks\n(Wang et al., 2018, 2019; Hendrycks et al., 2020;\nWei et al., 2021; Chung et al., 2022), such as causal\nreasoning, linguistic acceptability, question answer-\ning, textual entailment and even coreference res-\nolution: Using Language Models (LMs) does ex-\nceedingly well on tasks that can be modeled in\na  sequence-to-sequence  fashion.   However,  the\nbiggest challenge with adopting this technique for\nthe general reference resolution task in the context\nof a voice assistant lies in resolving references to\nentities on the screen and using their properties, in\nother words, getting the LM to, informally speak-\ning,  “see”.   In particular,  it is non-obvious how\nto encode entities on a screen in a manner that is\nconducive to being resolved by an LM, while also\nbeing consistent enough with how conversational\nentities are encoded to enable the LM to success-\nfully perform reference resolution on both types of\nentities.\nIn  this  work,  we  propose  reconstructing  the\nscreen  using  parsed  entities  and  their  locations\nto generate a purely textual representation of the\nscreen that is visually representative of the screen\ncontent.  The parts of the screen that are entities\nare then tagged, so that the LM has context around\nwhere entities appear, and what the text surround-\ning them is (Eg: call the business number). To the\nbest of our knowledge, this is the first work using a\nLarge Language Model that aims to encode context\nfrom a screen.\n2    Related Work and Motivation\nWhile traditional reference resolution systems have\nexplored  conversational  and  visual/deictic  refer-\nences in great depth (Kottur et al., 2018; Schwartz\net al., 2019; Kang et al., 2019), resolving on-screen\nreferences  is  a  domain  that  has  been  relatively\nunder-explored.  However, as shown above, con-\nversational agents on a mobile device need to un-\nderstand references to the screen, and to support\nsuch experiences, to be truly natural.  On screen\nreferences differ from visual and deictic references\nfor several reasons:  they tend to be more struc-\ntured and highly textual, which enables the use of\na lighter model to treat this as a text-only problem\nwithout a visual component; further, user queries\naround on-screen elements often tend to be more\naction-oriented rather than QA based; finally, they\nuse synthetic screens rather than natural real-world\nimages, which are much easier to parse, but whose\ndistribution completely differs from that on which\nlarger pre-trained image-based systems (such as\nCLIP (Radford et al., 2021)) tend to be trained. Fur-\nther, jointly being able to perform conversational\nand on-screen has been even less explored.\nVision transformers (Dosovitskiy et al., 2020;\nTouvron et al., 2021; Liu et al., 2021; Yu et al.,\n2021) and other pre-trained models have recently\ngained prominence as a popular first step in tasks\nthat require visual understanding. However, these\ntend to be trained on natural, real-world images\nrather than screenshots of on-screen layouts, which\nhave a very different distribution. In addition, these\ncan be extremely expensive to (pre-)train, requiring\na very large number of images and several hun-\ndred  GPU  hours  (or  more).   Further,  they  tend\nto not perform as well on images heavily embed-\nded with text, and dedicated textual understanding\napproaches (Xu et al., 2020, 2021; Hwang et al.,\n2021a,b; Hong et al., 2022) tend to heavily rely on\nmultiple modules such as bounding box detection\nand OCR while also relying on good image quality.\nJoint vision+text models are also substantially more",
    "expensive with respect to parameters and compu-\ntational cost. Finally, these models would need to\nparse text to be able to perform function (Eg: “call\nthe business number” needs to extract the number\nassociated with the business landline from the raw\nimage), a process which can be complex and com-\npute intensive when bearing in mind that the under-\nlying text and its location on the screen has been\nreferred by the system, and as a consequence can be\nrelatively easily extracted without large, complex\nmodels.\nThe  most  closely  related  work  which  we  are\naware of, and which we consequently use as our\nbaseline,  is that of Ates et al. (2023),  an exten-\nsion of Bhargava et al. (2023) which deals purely\nwith on-screen references; however, it suffers from\nseveral drawbacks, which we address in this work.\nFirst, this approaches rely on a dedicated “Category\nmodule” to deal with type-based references. This\nmodule often requires manually on-boarding enti-\nties every time a new type is created (a common\noccurrence in  voice assistants,  as the  supported\nfunctionality of assistant is expanded over time). In\naddition, such modules often treat each type as dis-\ntinct, with the similarity of different types ignored.\nThis, in turn, leaves on the table the potential pos-\nitive transfer that could have happened between\nsemantically related classes (such as “phone num-\nber” and “contact”) when data is added for one\nof those classes.  This approach is thus difficult\nto scale to new entity types and use cases.  Sec-\nond, these systems rely on the use of hand-crafted\nrule-based textual overlap features, which require\nheavy feature engineering and tend not to be robust.\nIn addition, these heuristics often do not account\nfor semantic similarity, and are not able to encode\nreal-world understanding or commonsense reason-\ning. Finally, these methods effectively classify how\nrelated each entity is to the query in question in-\ndependently of all other entities, and neither take\ninto consideration the whole screen nor the other\nentities.\n3    Task\nWe formulate our task as follows: Given relevant\nentities and a task the user wants to perform, we\nwish to extract the entity (or entities) that are perti-\nnent to the current user query. The relevant entities\nare of 3 different types:\n1.On-screen Entities: These are entities that are\ncurrently displayed on a user’s screen\n2.Conversational Entities: These are entities rel-\nevant to the conversation. These entities might\ncome from a previous turn for the user (for ex-\nample,  when  the  user  says  “Call  Mom”,  the\ncontact for Mom would be the relevant entity\nin question), or from the virtual assistant (for\nexample, when the agent provides a user a list\nof places or alarms to choose from).\n3.Background Entities: These are relevant entities\nthat come from background processes that might\nnot necessarily be a direct part of what the user\nsees  on  their  screen  or  their  interaction  with\nthe virtual agent;  for example,  an alarm that\nstarts  ringing  or  music  that  is  playing  in  the\nbackground.\nWe pose the task of reference resolution as a mul-\ntiple choice task for the LLM, where the intended\noutput is a single option (or multiple options) from\nthe entities shown on the user’s screen.  In some\ncases, the answer could also be \"None of these\".\nTo  evaluate  this  task,  we  allow  the  model  to\noutput the relevant entities in any order, i.e. if the\nGround Truth is entities 8, 7, and 4, then we accept\nany permutation of these 3 correct entities while\nevaluating the performance of the model.\n4    Datasets\nOur datasets comprise data that was either syntheti-\ncally created, or created with the help of annotators.\nEach data point contains the user query and a list of\nentities, along with the ground-truth entity (or set of\nentities) that are relevant to the corresponding user\nquery.  Each entity, in turn, contains information\nabout its type and other properties such as the name\nand other textual details associated with the entity\n(the label and time of an alarm, for example). For\ndata points where relevant on-screen context exists,\nthis context is available in the form of the bounding\nbox of the entity, and the list of objects surround-\ning it along with properties of these surrounding\nobjects such as their types, textual contents and\nlocations.\nTable 2: Dataset Sizes (Train Set and Test Set)\nDatasetTrainTest\nConversational2.3k1.2k\nSynthetic3.9k1.1k\nOn-screen10.1k1.9k",
    "4.1    Conversational Data\nIn this case, data is collected for entities that are\nrelevant to the user interaction with the agent. To\ndo this, graders are shown screenshots with syn-\nthetic lists of entities provided, and asked to pro-\nvide queries that unambiguously references an ar-\nbitrarily picked entity in the synthetic list provided.\nFor example, graders might be provided a synthe-\nsized list of businesses or alarms and asked to refer\nto a particular entity within that list.\nFor example, the grader might be shown a list of\nbusinesses that are synthetically constructed, and\nthey then refer to a specific one in the list provided.\nFor example, they might say “Take me to the one\nthat’s second from the bottom” or “Call the one on\nMain Street”.\n4.2    Synthetic Data\nAnother approach to obtain data is to rely on syn-\nthetic data from templates. This approach is partic-\nularly useful for type-based references, when the\nuser query and the entity type are sufficient to re-\nsolve the reference, and descriptions are not relied\nupon. Note that the synthetic nature of this dataset\ndoes not preclude it from containing datapoints in\nwhich multiple entities can be resolved to a given\nreference: for example, for the query “play it”, “it”\ncan be resolved to all entities of both the types\n“music” and “video”.\nThere  are  two  templates  to  generate  the  syn-\nthetic data. The first “base” template includes men-\ntions, entities, and possible slot values if necessary.\nThe second “language” template template imports\nthe base template and adds different variations of\nqueries that can be used for targeted cases with\nreferences defined in the base template.\nThe data generation script takes the base and lan-\nguage templates and generates the possible queries\ngiven in language template by substituting refer-\nences with mention and slot values defined in base\ntemplate.  It loops through all supported entities.\nFor the ones matching with the entity in the tem-\nplate, it connects mention and the entity, otherwise\nit only adds entity types with no reference to men-\ntion.\n4.3    On-screen Data\nScreen data were collected from various web pages\nwhere phone number, e-mail and/or physical ad-\ndress information exist. We had two phased anno-\ntation processes on screen data. First phase was for\nextracting queries based on the screens and second\none was for identifying the entities and mention\nfor the given query.  In the first grading project,\ngraders were given a screenshot (Figure 1a) with\ngreen and red boxes, the information in the green\nbox and asked to classify the green boxed data into\none of the entities such as phone number, email\naddress, etc. Then, graders were asked to provide\nthree unique queries for the green boxed data.\n(a) Screenshot example used\nin first annotation project\n(b) Screenshot example used\nin second annotation project\nFigure 1: Sample screenshots used in annotation\nIn second annotation project (Figure 1b), queries\ncollected in first step were shown to graders one by\none with corresponding screenshots without bound-\ning  boxes  and  all  screen  entities  as  a  list.   The\ngraders were asked if the query contains mention\nto the one of the given visual entities, if the query\nsound natural.  Also, they were asked to provide\nthe entities from the list referred in the given query\nand tag the part of the query referring that entity.\n5    Models\nWe  compare  our  proposed  model  ReALM,  de-\nscribed  in  detail  in  Section  5.3  below,   with\ntwo  baseline  approaches:   one  based  on  a  re-\nimplementation of the reference resolver proposed\nin MARRS (Section 5.1), which is non-LLM based,\nand one based on ChatGPT (both GPT-3.5 and GPT-\n4; Section 5.2).\n5.1    MARRS\nAs  a  baseline,  we  compare  against  the  system\nproposed  in  Ates  et  al.  (2023),  in  turn  a  varia-",
    "tion of Bhargava et al. (2023), both of which are\nnon-LLM based approaches.  While the latter ap-\nproach focuses on on-screen entities, MARRS ex-\ntends it to conversational and background entities\nas well. For our baseline comparison, we trained a\nre-implementation of this system with the datasets\ndescribed in Section 4, which includes conversa-\ntion, on-screen and synthetic data.  Note that in\ncontrast to our approach, which uses a generic off-\nthe-shelf LLM, this baseline we compare against\nwas specifically designed for the task of reference\nresolution.\n5.2    ChatGPT\nAs another baseline, we run the GPT-3.5 (Brown\net  al.,  2020;  Ouyang  et  al.,  2022)  and  GPT-4\n(Achiam et al., 2023) variants of ChatGPT, as avail-\nable on January 24, 2024, with in-context learning.\nAs in our setup, we aim to get both variants to pre-\ndict a list of entities from a set that is available. In\nthe case of GPT-3.5, which only accepts text, our\ninput consists of the prompt alone; however, in the\ncase of GPT-4, which also has the ability to con-\ntextualize on images, we provide the system with\na screenshot for the task of on-screen reference\nresolution, which we find helps substantially im-\nprove performance. Note that our ChatGPT prompt\nand prompt+image formulation are, to the best of\nour knowledge, in and of themselves novel. While\nwe believe it might be possible to further improve\nresults, for example, by sampling semantically sim-\nilar utterances up until we hit the prompt length,\nthis more complex approach deserves further, dedi-\ncated exploration, and we leave this to future work.\n5.3    Our Approach\nWe use the following pipeline for fine-tuning an\nLLM (a FLAN-T5 model (Chung et al., 2022)) in\nour case. We provide the parsed input to our model,\nand finetune it.  Note that unlike for the baseline,\nwe do not run an extensive hyperparameter search\non  the FLAN-T5  model,  sticking to  the default\nfine-tuning parameters.\nFor each data point consisting of a user query\nand the corresponding entities, we convert it to a\nsentence-wise format that we can feed to an LLM\nfor  training.   Examples  of  the  input  before  and\nafter processing are shown in Appendix Sections\nA and C, with examples of how we convert entities\nof different types into text shown in Appendix B.\nNote that the entities are shuffled before being sent\nto the model so that the model does not overfit to\nparticular entity positions.\n5.3.1    Conversational References\nFor the sake of this work,  we assume conversa-\ntional references to be of two types:  type-based\nand descriptive. Type-based references are heavily\nreliant on using the user query in conjunction with\nthe types of the entities to identify which entity\n(of a set of entities) are most relevant to the user\nquery in question:  for example, if the user says\n“play this”, we know that they are referring to an\nentity like a song or a movie, as opposed to a phone\nnumber or an address; “call him” likewise refers to\nthe first among a set of phone numbers or contacts,\nas opposed to an alarm. Descriptive references, in\ncontrast, tend to use an property of the entity to\nuniquely identify it: “The one in Times Square” for\nexample might help uniquely refer to one among\na set of addresses or business. Note that it is often\nthe case that references might rely on both types\nand descriptions to unambiguously refer to a sin-\ngle object:  consider the examples “play the one\nfrom Abbey Road” vs “directions to the one on\nAbbey Road”, both of which rely on both the entity\ntype and description to identify a song in the first\ncase and address in the second.  In our proposed\napproach, we simply encode the type and various\nproperties of the entity. We show our detailed en-\ncoding scheme in Appendix B.\n5.3.2    Onscreen References\nFor  onscreen  references,  as  in  Bhargava  et  al.\n(2023), we assume the presence of upstream data\ndetectors that are able to parse screen text to extract\nentities. These entities are then available along with\ntheir types, bounding boxes and a list of non-entity\ntext elements surrounding the entity in question.\nTo encode these entities (and thereby, the rele-\nvant parts of the screen) into the LM in a manner\nthat involves text alone, we use the novel algorithm\ngiven in Algorithm 2.  Intuitively, we assume the\nlocation of all entities and their surrounding objects\nto be representible by the center of their respective\nbounding boxes.  We then sort these centers (and\nthereby, the associated objects) from top-to-bottom\n(i.e., vertically, along the y-axis), and the use a sta-\nble sort to sort from left-to-right (i.e., horizontally,\nalong the x-axis). Next, all objects that are within\namarginare treated as being on the same line, and\nare separated from each other by a tab; objects fur-\nther down outside the margin are placed on the next\nline, and this is repeatedly, effectively encoding the",
    "screen in a left-to-right, top-to-bottom fashion in\nplain text.\n6    Results\nTable 3:  Model Accuracy for Different Datasets.   A\nprediction is correct is the model correctly predicts all\nrelevant entities, and incorrect otherwise. Conv refers to\nthe Conversational Dataset, Synth to the Synthetic one,\nScreen to the Onscreen one and Unseen to a conversa-\ntional dataset pertaining to a held-out domain.\nModelConv   Synth   Screen   Unseen\nMARRS92.199.483.584.5\nGPT-3.584.134.274.167.5\nGPT-497.058.790.198.4\nReALM-80M96.799.588.999.3\nReALM-250M97.899.890.697.2\nReALM-1B97.999.791.494.8\nReALM-3B97.999.893.097.8\nWe present our results in Table 3. Overall, we find\nthat our approach outperforms the MARRS model\nin all types of datasets.  We also find that our ap-\nproach is able to outperform GPT-3.5, which has a\nsignificantly larger number of parameters than our\nmodel by several orders of magnitude. We also find\nthat our approach performs in the same ballpark\nas the latest GPT-4 despite being a much lighter\n(and faster) model. We especially wish to highlight\nthe gains on onscreen datasets, and find that our\nmodel with the textual encoding approach is able to\nperform almost as well as GPT-4 despite the latter\nbeing provided with screenshots.\nAdditionally, we also experiment with models of\ndifferent sizes. We see that while performance im-\nproves in all datasets with an increase in model\nsize, the difference is most pronounced for the on-\nscreen datasets,  which alludes to the task being\nmore complex in nature.\n6.1    Analysis\nGPT-4≈ReaLM≫MARRS for new use-cases:\nAs a case study, we explore zero-shot performance\nof this model on an unseen domain:  Alarms (we\nshow a sample data point in Appendix Table 11).\nThe last row in Table 3 compares the performance\nof all approaches and baselines on this unseen test\nset. We find that all of the LLM-based approaches\noutperform the FT model for this test set. Among\nthe two, we find that the performance of ReaLM\nand GPT-4 are very similar for the unseen domain.\nAdditionally,  table  4  shows  completely  new  ex-\nperiences enabled by ReaLM due to the LLM’s\nsuperior ability to perform complex understanding\nof natural language.\nReaLM>GPT-4 for domain-specific queries\nWe find that due to finetuning on user requests,\nReaLM is able to understand more domain-specific\nquestions. For instance, consider Table 4. GPT-4\nincorrectly assumes the reference to be about only\na setting, whereas the ground truth consists of a\nhome automation device in the background as well,\nand GPT-4 lacks the domain knowledge to be able\nto recognise that the device would also be relevant\nto this reference. ReaLM, in contrast, doesn’t suffer\nfrom this due to being trained on domain-specific\ndata.\nTable 4: User Request for Setting or Home Device\nUser Request:Can you make it brighter?\nEntities Shown to User:\n1. Type: Settings\n2. Type: UserEntity | homeAutomationAccessoryName\nGPT-4 Prediction: 1 Ground Truth:1, 2\n7    Conclusion and Future Work\nIn this work, we demonstrate how large language\nmodels can be used to perform reference resolu-\ntion. We do this by encoding entity candidates as\nnatural text; critically, we demonstrate how enti-\nties that are present on the screen can be passed\ninto an LLM using a novel textual representation\nthat effectively summarizes the user’s screen while\nretaining relative spatial positions of these entities.\nWe show that ReaLM outperforms previous ap-\nproaches, and performs roughly as well as the state-\nof-the-art LLM today, GPT-4, despite consisting of\nfar fewer parameters, even for onscreen references\ndespite being purely in the textual domain. It also\noutperforms GPT-4 for domain-specific user utter-\nances, thus making ReaLM an ideal choice for a\npractical reference resolution system that can exist\non-device without compromising on performance.\nWhile our approach is effective in encoding the\nposition of entities on the screen, we find that it re-\nsults in loss of information that may not be able to\nresolve complex user queries that rely on nuanced\npositional understanding. We thus believe that ex-\nploring more complex approaches such as splitting\nthe screen into a grid and encoding these relative\nspatial positions into text, while challenging, is a\npromising avenue of future exploration.",
    "Ethics Statement\nWhile LLMs can generate unexpected output, our\nsystem offers the ability to constrain decoding or\nuse simple post-processing to ensure this does not\nhappen.   Note however,  that practically we find\nvery little hallucination, and do not constrain the\ndecoding of the LLM.\nAcknowledgements\nThe authors would like to thank Stephen Pulman,\nLeon Liyang Zhang, Jiarui Lu, Jeff Nichols, Shruti\nBhargava, Dhivya Piraviperumal and Junhan Chen\nfor their help and feedback.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad,   Ilge  Akkaya,   Florencia  Leoni  Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nHalim Cagri Ates, Shruti Bhargava, Site Li, Jiarui Lu,\nSiddhardha  Maddula,  Joel  Ruben  Antony  Moniz,\nAnil  Kumar  Nalamalapu,  Roman  Hoang  Nguyen,\nMelis Ozyildirim, Alkesh Patel, et al. 2023.  Marrs:\nMultimodal reference resolution system. InProceed-\nings of The Sixth Workshop on Computational Mod-\nels of Reference, Anaphora and Coreference (CRAC\n2023), pages 51–58.\nPayal Bajaj, Chenyan Xiong, Guolin Ke, Xiaodong Liu,\nDi He, Saurabh Tiwary, Tie-Yan Liu, Paul Bennett,\nXia Song, and Jianfeng Gao. 2022. Metro: Efficient\ndenoising pretraining of large scale autoencoding lan-\nguage models with model generated signals.arXiv\npreprint arXiv:2204.06644.\nShruti  Bhargava,  Anand  Dhoot,  Ing-Marie  Jonsson,\nHoang Long Nguyen, Alkesh Patel, Hong Yu, and\nVincent Renkens. 2023.   Referring to screen texts\nwith voice assistants. InACL.\nTom  Brown,  Benjamin  Mann,  Nick  Ryder,  Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020.  Language models are few-shot\nlearners.Advances in neural information processing\nsystems, 33:1877–1901.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms.arXiv preprint arXiv:2305.14314.\nAlexeyDosovitskiy,LucasBeyer,Alexander\nKolesnikov,   Dirk   Weissenborn,   Xiaohua   Zhai,\nThomas Unterthiner,  Mostafa Dehghani,  Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn  image  is  worth  16x16  words:   Transformers\nfor  image  recognition  at  scale.arXiv  preprint\narXiv:2010.11929.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding.  InInternational Conference on Learning\nRepresentations.\nTeakgyu Hong,  Donghyun Kim,  Mingi Ji,  Wonseok\nHwang,  Daehyun  Nam,  and  Sungrae  Park.  2022.\nBros: A pre-trained language model focusing on text\nand layout for better key information extraction from\ndocuments. InProceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 10767–\n10775.\nWonseok Hwang, Hyunji Lee, Jinyeong Yim, Geewook\nKim, and Minjoon Seo. 2021a. Cost-effective end-to-\nend information extraction for semi-structured docu-\nment images. InProceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3375–3383, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, So-\nhee Yang, and Minjoon Seo. 2021b.  Spatial depen-\ndency parsing for semi-structured document infor-\nmation extraction.   InFindings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 330–343, Online. Association for Computa-\ntional Linguistics.\nGi-Cheon Kang, Jaeseo Lim, and Byoung-Tak Zhang.\n2019.  Dual attention networks for visual reference\nresolution in visual dialog.   InProceedings of the\n2019  Conference  on  Empirical  Methods  in  Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2024–2033.\nSatwik Kottur, José MF Moura, Devi Parikh, Dhruv Ba-\ntra, and Marcus Rohrbach. 2018. Visual coreference\nresolution in visual dialog using neural module net-\nworks. InProceedings of the European Conference\non Computer Vision (ECCV), pages 153–169.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. 2021.\nSwin transformer:  Hierarchical vision transformer\nusing  shifted  windows.InProceedings  of  the\nIEEE/CVF international conference on computer vi-\nsion, pages 10012–10022.\nAlice Ljungholm. 2021.   Voice interaction vs screen\ninteraction when controlling your music-system. In\nProceedings of the 21st Student Conference in Inter-\naction Technology and Design, pages 103–108.",
    "Ewa Luger and Abigail Sellen. 2016.  \" like having a\nreally bad pa\" the gulf between user expectation and\nexperience of conversational agents. InProceedings\nof  the  2016  CHI  conference  on  human  factors  in\ncomputing systems, pages 5286–5297.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.  Training language models to follow instruc-\ntions  with  human  feedback.Advances  in  Neural\nInformation Processing Systems, 35:27730–27744.\nShishir  G  Patil,   Tianjun  Zhang,   Xin  Wang,   and\nJoseph E Gonzalez. 2023.  Gorilla: Large language\nmodel connected with massive apis.arXiv preprint\narXiv:2305.15334.\nBarun Patra, Saksham Singhal, Shaohan Huang, Zewen\nChi, Li Dong, Furu Wei, Vishrav Chaudhary, and Xia\nSong. 2022. Beyond english-centric bitexts for better\nmultilingual language representation learning.arXiv\npreprint arXiv:2210.14867.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023.   Toolllm:  Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry,  Amanda Askell,  Pamela Mishkin,  Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts,  and Matei Zaharia. 2022.   Col-\nBERTv2:Effective   and   efficient   retrieval   via\nlightweight late interaction.  InProceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle,  United  States.  Association  for  Computational\nLinguistics.\nIdan Schwartz, Seunghak Yu, Tamir Hazan, and Alexan-\nder G Schwing. 2019. Factor graph attention. InPro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 2039–2048.\nDominik Stammbach, Maria Antoniak, and Elliott Ash.\n2022. Heroes, villains, and victims, and GPT-3: Au-\ntomated extraction of character roles without train-\ning  data.   InProceedings  of  the  4th  Workshop  of\nNarrative Understanding (WNU2022), pages 47–56,\nSeattle, United States. Association for Computational\nLinguistics.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Hervé Jé-\ngou. 2021. Training data-efficient image transform-\ners & distillation through attention. InInternational\nconference on machine learning, pages 10347–10357.\nPMLR.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert,  Amjad Almahairi,  Yasmine Babaei,  Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale,  et  al.  2023.Llama  2:   Open  founda-\ntion  and  fine-tuned  chat  models.arXiv  preprint\narXiv:2307.09288.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019.   Superglue:  A stick-\nier benchmark for general-purpose language under-\nstanding systems.Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill,  Omer  Levy,  and  Samuel  R  Bowman.  2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. InInternational\nConference on Learning Representations.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021. Finetuned language mod-\nels are zero-shot learners.  InInternational Confer-\nence on Learning Representations.\nJason  Wei,  Yi  Tay,  Rishi  Bommasani,  Colin  Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022.  Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nYang  Xu,  Yiheng  Xu,  Tengchao  Lv,  Lei  Cui,  Furu\nWei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\nZhang, Wanxiang Che, Min Zhang, and Lidong Zhou.\n2021.  LayoutLMv2:  Multi-modal pre-training for\nvisually-rich document understanding.  InProceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 2579–2591, Online.\nAssociation for Computational Linguistics.\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu\nWei, and Ming Zhou. 2020. Layoutlm: Pre-training\nof text and layout for document image understanding.\nInProceedings of the 26th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data\nMining, pages 1192–1200.\nJiahui Yu,  Xin Li,  Jing Yu Koh,  Han Zhang,  Ruom-\ning Pang, James Qin, Alexander Ku, Yuanzhong Xu,\nJason Baldridge,  and Yonghui Wu. 2021.   Vector-\nquantized  image  modeling  with  improved  vqgan.\narXiv preprint arXiv:2110.04627.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang,  Zhanghao Wu,  Yonghao Zhuang,  Zi Lin,\nZhuohan  Li,  Dacheng  Li,  Eric  Xing,  et  al.  2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena.arXiv preprint arXiv:2306.05685.",
    "Queen Anne\n5520 Roy St, Seattle,\n98109\n(206) 380 4699\nBelltown\n2209 1st Ave S, Seattle,\n98121\n(206) 380 4898\n(a) Onscreen Capture 1\nYour New Home\nSteven Realtors Inc.\nTrusted by over 5 million\nproud homeowners\nContact Us\nMon - Fri\n(206) 198-8699\nSat - Sun\n(206) 198-8672\n(b) Onscreen Capture 2\nFigure 2: Technical diagrams representing user screens. Shaded rectangles represent various elements shown on the\nscreen detectable by screen parser-extractors.\nA    Encoding onscreen entities\nFirst, we show sample representations of what a\nscreen grab might look like, as parsed and exposed\nto the system.  We show these representations in\nFigure 2\nWe now describe some other strategies of encod-\ning on-screen elements that we explored.\n•Clustering:We explored a clustering-based\napproach  wherein  we  perforedm  a  spatial\nclustering of the various surrounding objects\npresent in the screen.  We did this to estab-\nlish semantic clusters wherein a user could\nrefer to nearby bounding boxes (such as the\ncontact information) by a particular title. The\ndetailed approach is given in Algorithm 1, and\na sample encoding is shown in Table 5. The\nbiggest drawback of the approach was that the\nprompt length often explodes as the number\nof entities in a cluster increases, as each of the\nobjects in the cluster would have every other\nobject in its surrounding objects.\n•Onscreen Grab:To mitigate this issue, we\nemployed a second approach (similar to our\nfinal approach), wherein we parsed the screen\nas in our final approach, the only difference\nbeing that we didn’t annotate the turn objects\nwithin the parse itself, but provided the turn\nobjects as a list instead (see Table 6).\n•Onscreen Grab with Injected Turn Objects:\nFinally, the exact algorithm employed in our\nfinal  approach  is  given  in  2,  and  a  sample\nencoding is shown in Table 7.\nWe show an ablation in Figure 3, in which we\nTable 5: Clustering-based encoding\nUser Request:Get me directions to the branch in\nQueen Anne\nEntities Shown to User:\n1. Type: Postal Address | Value: 5520 Roy St,\nSeattle 98109 | surr_objects: Queen Anne, (206) 380 4699\n2. Type: Phone Number | Value: (206) 380 4699\nsurr_objects: Queen Anne, 5520 Roy St, Seattle 98109\n3. Type: Phone Number | Value: (206) 380 4898\nsurr_objects: Belltown, 2209 1st Ave S, Seattle 98121\n4. Type: Postal Address | Value: 2209 1st Ave,\nSeattle 98121 | surr_objects: Belltown, (206) 380 4898\nGround Truth:1",
    "Table 6: Onscreen Grab encoding\nUser Request:Save the phone number at the bottom-right\nScreen:\nYour New home!\nSteven Realtors Inc.\nTrusted by over 5 million\nProud homeowners\nContact Us\nMonday -Saturday -\nFridaySunday\n(206) 198 1699(206) 198 1999\nEntities Shown to User:\n1. Type: Phone Number | Value: (206) 198 1999\n2. Type: Phone Number | Value: (206) 198 1699\nGround Truth:1, 2\nshow the performance of the various encoding ap-\nproaches  described  above  (and  some  other  hill-\nclimbing efforts).\nFigure 3: Performance improvements with each exper-\niment –  (a) Baseline Finetuned LLM,  (b) Obtaining\nscreen elements through OCR, (c) Obtaining screen ele-\nments through UI elements and Clustering (d) Adding\nan extra newline between the instruction and user re-\nquest, (e) Onscreen Grab, (f) Onscreen Grab with in-\njected turn objects, (g) Onscreen Grab with injected turn\nobject + needing lines to be separated by at least Margin,\n(h) Separating elements in the same line by a tab\nWe show the algorithm used to encode onscreen\nTable 7: Injected Onscreen Encoding (Final Approach)\nUser Request:Save the phone number at the bottom-right\nScreen:\nYour New home!\nSteven Realtors Inc.\nTrusted by over 5 million\nProud homeowners\nContact Us\nMonday -Saturday -\nFridaySunday\n{{1. (206) 198 1999}}{{2. (206) 198 1699}}\nGround Truth:1, 2\nAlgorithm 1:Surrounding Object Cluster-\ning and Prompt Generation\nData:List of MDF turn objects\nResult:Updated turn objects with\nsurrounding object prompts\n1foreach MDF turn objecttdo\n// Step 1: Get unique\nsurrounding objects\n2surrounding_objects←Set of\nunique surrounding objects fort;\n// Step 2: Spatially cluster\nsurrounding object bounding\nboxes\n3clusters←\nDBScan(surrounding_objects, rect_distance)\n;\n// Step 3: Predict the cluster\nfor turn object\n4t_cluster←Predicted cluster fort;\n5foreach surrounding objectsin\nsurrounding_objectsdo\n6ifsbelongs to clustert_cluster\nthen\n// Step 4: Process\nnon-overlapping\nsurrounding objects\n7ifno string overlap betweent\nandsthen\n8Addsto the prompt under\nkey ‘surrounding_object‘;\n// Step 5: Provide global\npositioning information\n9t.distance_f rom_top←Compute\ndistance from the top fort;\n10t.distance_f rom_lef t←Compute\ndistance from the left fort;\n11returnprompt;\nentities, described in Section 5.3.2, in Algorithm 2.",
    "Algorithm 2:Onscreen Parse Construction\nwith Turn Object Injection\nData:List of turn objects\nResult:Onscreen parse constructed with\nturn object injection\n1onscreen_parse←Empty list of onscreen\nparse elements;\n// Step 0: Get all text boxes\npresent in the screen\n2foreach turn objectt, indexido\n// Step 1: Get unique\nsurrounding objects\n3surrounding_objects←Set of\nsurrounding objects fort;\n// Step 2: Insert turn objects\ninto the set (as turn objects\nare also text)\n4surrounding_objects←\nsurrounding_objects∪{[[i.t]]};\n// Step 3: Sorting the centers of\nall surrounding objects\n5sorted_objects←Sort objects in\nsurrounding_objectsby center (Top→\nBottom, Left→Right);\n// Step 4: Determine vertical\nlevels\n6margin←Margin for considering objects\nat the same level;\n7levels←List of vertical levels;\n8foreach objectoinsorted_objectsdo\n9same_level←List of objects at the\nsame level aso;\n10foreach objectotherin\nsorted_objectsdo\n11ifois not the same asotherand\n|o.center_top−\nother.center_top|≤margin\nthen\n12same_level←\nsame_level∪{other};\n13levels←levels∪{same_level};\n// Step 5: Construct onscreen parse\n14foreach levellinlevelsdo\n15level_parse←Empty string;\n16foreach objectobjinldo\n17level_parse←\nlevel_parse+\"\\t\"+obj;\n18onscreen_parse←\nonscreen_parse+\"\\n\"+level_parse;\n19returnonscreen_parse;",
    "B    Entity Representations\nIn Table 8,  we show some examples of various\ndomains and their representations, as fed into the\nLLM.\nTable 8: Entity Domains and their Representations\nEntity TypeAfter\nalarmType: Alarm | time: 08:06 PM; label: brush hair; status: Off\nappType: App | clock\nbookType: Book\ndate timeType: DateTime | 1 | 1 | 2021\nemail addressType: EmailAddress | membership@ipsa.org\nflight numberType: FlightNumber\ngeneral textType: GeneralText\nhome deviceType: UserEntity | heater\nhome roomType: UserEntity | Db Bedroom\nlocal businessType: LocalBusiness | PostalAddress: 15 Broad St, Albany 31701 | Ameris Bank | list_position: 13\nmedia albumType: MediaItem | MediaItemType: MediaItemType_Album | Mellon Collie\npackageType: Package\npaintingType: Painting\npersonType: Person | Sebastian\nphone numberType: PhoneNumber | 955 545 060\nphotoType: Photo\nphysical addressType: PostalAddress | GeographicArea: 814 Elmwood Ave, NY, 14222\nplant animalType: PlantAnimal\nsettingType: Setting | dark mode\ntracking numberType: TrackingNumber\nurlType: Uri | NY.gov",
    "C    Sample Inputs\nIn this section, we show examples of how inputs\ninto the model have been encoded, in the form of a\nvisual representation.",
    "Table 9: Sample input with single ground truth\nUser Request:Call the one on Rainbow St.\nEntities Shown to User:\n1. Type: Local Business | Name: Walgreens | Address: 225 Rainbow St, San Jose CA 94088\n2. Type: Local Business | Name: CVS | Address: 105 E El Camino Real, Sunnyvale, CA 94087\n3. Type: Local Business | Name: Qwark | Address: 1287 Hammerwood Ave, Sunnyvale, CA 94089\nGround Truth:1\nTable 10: Sample input with multiple ground truths\nUser Request:Save the address.\nEntities Shown to User:\n1. Type: Postal Address | Value: 225 Rainbow St, San Jose CA 94088\n2. Type: Email Address | Value:contactus@cvs.com\n3. Type: URL | Value:cvspharmacies.com/usa\nGround Truth:1, 2, 3\nTable 11: User Request for Alarms\nUser Request:Switch off the one reminding me to\npick up didi.\nEntities Shown to User:\n1. Type: Alarm | open laptop\n2. Type: Alarm | text Lauren to shower\n3. Type: Alarm | pick up didi\n4. Type: Alarm | forget this\nGround Truth:3",
    "(a) Semantic Understanding\nUser Request:Call the evening Number\nScreen:\n{{1. 9 AM - 5 PM}}\n{{2. 901.969.3120}}\n{{3. 5 PM - 9 PM}}\n{{4. 901.969.3391}}\nModel Output:4\n(b) Summarisation\nUser Request:Remind me to get printouts before the tax deadline\nScreen:\nTax Deadlines 2023\n{{1. Feb 15}}\nReclaim your tax exemption from withholding\n{{2. April 18}}\nFirst-quarter estimated tax payment due\nModel Output:2\n(c) World Understanding\nUser Request:Take me to the one in Washington\nScreen:\nIndian Embassy\n{{1. 1701 El Camino Real, Mountain View 94040}}\n{{2. 333 Dexter Ave N, Seattle 98109}}\n{{3. 8295 Tournament Drive, Memphis, TN 38125}}\nModel Output:2\n(d) Commonsense Reasoning\nUser Request:Save the link to the breakfast Recipe\nScreen:\nIMAGE\nStrawberry Granola\n{{1. Recipe link}}\nIMAGE\nLavender boba tea\n{{2. Recipe link}}\nModel Output:1\nFigure 4: Qualitative Examples of LLM-based finetuned model able to adapt to complex use-cases"
  ]
}