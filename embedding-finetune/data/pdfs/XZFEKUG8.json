{
  "key": "XZFEKUG8",
  "url": "http://arxiv.org/pdf/2401.12187",
  "metadata": {
    "title": "WARM: On the Benefits of Weight Averaged Reward Models",
    "abstract": "  Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.\n",
    "published": "2024-01-22T18:27:08Z"
  },
  "text": [
    "WARM: On the Benefits of Weight Averaged\nReward Models\nAlexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret\nGoogle DeepMind\nAligning large language models (LLMs) with human preferences through reinforcement learning (RLHF)\ncan lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly\nhigh rewards without meeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies\nin human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-\ntuning multiple RMs, then averaging them in the weight space. This strategy follows the observation\nthat fine-tuned weights remain linearly mode connected when sharing the same pre-training. By\naveraging weights,WARMimproves efficiency compared to the traditional ensembling of predictions,\nwhile improving reliability under distribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-푁and RL methods, shows thatWARMimproves the\noverall quality and alignment of LLM predictions; for example, a policy RL fine-tuned withWARMhas a\n79.4% win rate against a policy RL fine-tuned with a single RM.\nKeywords: Alignment, RLHF, Reward Modeling, Model Merging\n1. Introduction\nReward modeling.Conversational assistants such as Gemini [1] or GPT-4 [2] have revolutionized the\nAI community and beyond. These LLMs are capable of completing novel and intricate tasks, including\nmathematics, coding, and tool use [3]. These advancements are underpinned by a systematic three\nstage training procedure: pre-training by next token prediction [4,5,6], supervised fine-tuning (SFT)\nto learn to follow instructions [7,8,9], and ultimately, reinforcement learning (RL) to maximize a\nreward encapsulating the desired behaviors [10]. However, defining such rewards for real-world tasks\nis non-trivial [11]. In reinforcement learning from human feedback (RLHF) [12,13,14,15], rewards\nare reward models (RMs), trained on binary preference datasets to emulate human judgment. The\nenhancement of LLM capabilities from RL is strongly tied to the quality of the RMs [16].\nReward hacking.Particularly insidious in RLHF [17,18] is thereward hackingissue [19,20,21,22]\n(a.k.a. reward overoptimization), arising fromreward misspecification[23,24] between the proxy\nRM and actual human preferences. While optimizing for the RM initially provides improvements, in\nlater stages the policy (i.e., the LLM being trained) usually learns to exploit loopholes in the RM and\nachieves high rewards without truly fulfilling the intended objectives, as illustrated in Figure 1(b).\nThis reward hacking phenomenon poses numerous issues. First, it degrades performances, manifesting\nas linguistically flawed [25] or unnecessarily verbose [26] outputs, which do not reflect true human\npreferences. Second, it complicates checkpoint selection due to the unreliability of the proxy RM,\nechoing Goodhart’s Law [27]: “when a measure becomes a target, it ceases to be a good measure”.\nThird, it can engender sycophancy [28,29] or amplify social biases, reflecting the limited and skewed\ndemographics of feedback providers [30,31]. Lastly and most critically, misalignment [32,33] due\nto reward hacking can escalate into safety risks [19,34,35], in particular given the rapid integration\nof LLMs in everyday life and critical decision-making. Such concerns underscore the need to mitigate\nreward hacking to ensure the beneficial and safe deployment of LLMs.\nCorresponding author: alexandrerame@google.com\narXiv:2401.12187v1  [cs.LG]  22 Jan 2024",
    "WARM: On the Benefits of Weight Averaged Reward Models\nUse RL to maximize reward \nby updating weights.\n\nRL fine-tune step\nGenerate output by feeding \nan unlabeled input data point\nCompute reward\nAssign a reward to the \nmodel’s output.\nSFT \nSample from policy\nMultiple RM \nfine-tunings with \ndifferent \nhyperparams\nHuman or AI pairwise feedback.\nCollect preference dataset\nWARM:\nWeight Averaged \nReward Model\nReward function\nWeight averaging\nRL fine-tuning\nAligned LLM \n(a)WARMprocedure with푀=3.\n02000400060008000\n# steps\n4\n5\n6\n7\n8\n9\n10\n11\nControl reward\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b)WARMmitigates reward hacking.\nFigure 1|Figure 1(a) illustrates the alignment process withWARM. From a SFT-ed LLM, we apply RL fine-tuning\nto optimize a proxy reward model (RM), in line with RLHF [12]. The innovation ofWARMlies in the design of\nthe proxy RM, which is the weight average (WA) of푀individual RMs, each fine-tuned from a shared pre-trained\nLLM on the same preference dataset, but with slight differences such as diverse hyperparameters. This WA\napproach is efficient, while enhancing the reliability under distribution shifts and robustness under inconsistent\npreferences. Figure 1(b) showcases the impact during RL alignment. The control reward (detailed in Section 5)\ninitially increases but eventually deteriorates, a phenomenon called reward hacking [19]. However, when\nWARMserves as the proxy RM, increasing푀(the number of averaged RMs) significantly improves absolute\nresults while delaying the collapse, as indicated by the control rewards maintaining higher values for longer\nduring training. Same plot withKLas the푥-axis in Figure 8(a) and with label corruption in Figure 18.\nChallenges.Two primary challenges underlie reward hacking. The first major issue arethe distribu-\ntion shiftsencountered by the RM [36,37]. Indeed, the generations from the policy might deviate\nsubstantially from those in the offline preference dataset, posing an out-of-distribution (OOD) chal-\nlenge. Moreover, those distribution shifts are accentuated by the policy drift during the RL procedure:\nthe policy moves away from its SFT initialization, continually altering the distribution of predictions\nthe RM needs to interpret reliably. Second,preferences are inconsistent: the binary labels in the\npreference dataset are noisy. Indeed, human labelers often rely on simpler criteria (length, bullet\npoints, politeness) over more nuanced indicators. Moreover, errors can be exacerbated for complex\ntasks requiring specific expertise [38], and because of the multi-objective nature of alignment [39]\nrequiring handling the heterogeneity of human opinions. Overall, this results in a low inter-labeler\nagreement (72.6% for InstructGPT [40]), altering the robustness of the RM.\nGoal and ensembling baseline.Designing good RMs must meet a tripartite objective: guiding RL\nefficiently, reliably scoring generations despite the distribution shifts, and providing robust signals\namidst label noise. To address these challenges, the seminal work on RLHF from Christianoet al.[12]\nand more recent works [41,42] leveragedprediction ensembling(ENS) [43], averaging the rewards\nfrom multiple RMs. ENS improves the reliability of the reward and mitigates hacking risks [41,42].\nYet, ENS suffers from memory and inference overhead causing efficiency challenges; we will also\nshow that ENS fails to improve robustness to label noise in the preference datasets.\nWARM.In this paper, we propose weight averaged reward models (WARM), a simple, efficient and\nscalable strategy to obtain a reliable and robust RM by combining multiple RMs. Starting from a\nshared pre-trained LLM, we launch multiple RM fine-tunings: in practice, the different runs have\ndifferent hyperparameters (as in grid search), and see the preference data in different orders, thus\n2",
    "WARM: On the Benefits of Weight Averaged Reward Models\nleading to diverse RMs. A key contribution is how the different RMs are merged: bylinear interpolation\nin the weight space. This follows the findings from the linear mode connectivity (LMC) [44, 45] and\nweight averaging (WA) literature [46, 47, 48]: under shared pre-training, the different weights can\nbe linearly interpolated despite the non-linearities in the architecture.\nOn the benefits ofWARM.Firstly,WARMstands out for its efficiency and practicality. By requiring\na single model at inference time, it provides a scalable approximation to the traditional, costlier\nensembling of predictions, without its memory and inference burdens. Secondly,WARMimproves\nreliability by inheriting from the generalization abilities of WA under distribution shifts, a quality\nwell-documented in the OOD literature for supervised learning [47,48,49]. Lastly,WARMimproves\nrobustness to label corruption. We show that WA selects the invariant predictive mechanisms [50,51]\nacross different runs [52,53], thus naturally diminishing the memorization of corrupted samples,\noccurring in each run in different ways. In contrast, ENS simply memorizes the corrupted samples.\nWe also explain why reducing memorization when modeling noisy preferences enhances stability in\nthe RL process. These multifaceted benefits ofWARMare further explored in Section 4.\nWe summarize our contributions as follows.\n1.Innovation in reward modeling. We introduceWARM, the first instance of weight averaging for\nreward modeling. This novel strategy efficiently mitigates reward hacking, improves reliability\nunder distribution shifts and robustness to label corruption.\n2.Theoretical and empirical insights into weight averaging. We validate linear mode connectivity\nfor reward models trained on binary preference datasets. Moreover, we reveal a key difference\nbetween weight and prediction averaging, that appears clearly under label corruption; weight\naveraging only maintains the invariant predictive mechanisms across runs, thereby diminishing\nmemorization and enhancing the focus on generalizable features.\nOur experiments on summarization tasks in Section 5 confirm thatWARMimproves performance\nwithout any memory or inference overhead, either when used as the reward selector in best-of-푁,\nor as the proxy RM in RL.WARMmitigates reward hacking, and thus provides better downstream\npolicies; specifically, it leads to a win rate of 79.4% (according to the preference oracle metric) against\na policy trained with a standard RM.\n2. Context and challenges\n2.1. Context\nLLMs.We consider an LLM푓\n휃\nof a fixed non-linear architecture parameterized by휃, usually a\nTransformer with attention layers [54]. It defines a policy by mapping prompt inputs푥to푓\n휃\n(푥).\nFollowing the foundation model paradigm [55] and the success of transfer learning [56], the weights\n휃are first pre-trained [4] on the vast amount of web data into휃\n푝푡\n, before supervised fine-tuning\n(SFT) [7] to learn to follow instructions into휃\n푠푓푡\n. However, the high cost and limited scope of\ninstruction data (i.e., prompts and responses) can create a misalignment [19,32,33] between the\nLLM and its intended application. Reinforcement learning (RL) as a third step in the training process\nof LLMs was shown to help alignment of LLMs with the intended usage [40].\nRMs.A notable aspect of RL is the absence of supervised samples to be imitated by the policy; instead,\nthe focus shifts to maximizing the reward of generated samples, that should measure their quality.\nThe challenge is that the oracle reward, perfectly encapsulating the desired behaviors, is not given by\nthe environment. The key innovation from RLHF [12] is that this reward is the output of a reward\nmodel (RM), trained in a supervised way to predict and thus reflect human preferences. Specifically,\n3",
    "WARM: On the Benefits of Weight Averaged Reward Models\nan RM is an LLM푟\n휙\nparameterized by휙, predicting a single scalar as the reward푟\n휙\n(푥, 푦)for a prompt\n푥and generation푦. The weights휙are usually initialized from\n\u0000\n휃\n푠푓푡\n, 휔\n\u0001\n, where the final linear layer휔\nis added on top of the extracted features from the SFT model휃\n푠푓푡\n. Then휙is trained on a preference\ndatasetD\n푡푟푎푖푛\n={푥\n푑\n, 푦\n+\n푑\n, 푦\n−\n푑\n}\n퐷\n푑=1\nwhere the generation푦\n+\n푑\nhas been preferred over푦\n−\n푑\nto continue푥\n푑\n.\nUsually human labelers evaluate those generations, but recent works on RLAIF [57,58] showed\nthat similar performances can be obtained by prompting an LLM for AI feedback. Following the\nBradley-Terry [59] assumption about the distribution of preferences, and by framing the problem\nas binary classification, the maximum likelihood principle motivates learning휙by minimizing the\nfollowing negative log-likelihood loss (where휎is the logistic function):\nL\n푅\n\u0000\n푟\n휙\n,D\n푡푟푎푖푛\n\u0001\n=−피\n(\n푥,푦\n+\n,푦\n−\n)\n∈D\n푡푟푎푖푛\n\u0002\nlog휎\n\u0000\n푟\n휙\n\u0000\n푥, 푦\n+\n\u0001\n−푟\n휙\n(\n푥, 푦\n−\n)\n\u0001\n\u0003\n.(1)\n.\nReward inference.With this RM, the literature suggests applying any kind of RL algorithm (usually\nREINFORCE [60] or PPO [61]) to fine-tuned휃\n푠푓푡\ninto휃\n푟푙\n, as analyzed in Section 5.2. A training-free\nalternative is best-of-푁(BoN) sampling, analyzed in Section 5.1, which returns the generation that\nhas the highest reward among푁generations from휃\n푠푓푡\n. Both methods aim to align the policy with\nhuman preferences. Yet, thereward misspecification[23] between the proxy RM and the true human\npreferences can lead toreward hacking[19,20,21,22], where the policy exploits loopholes in the\nproxy RM to artificially increase the score without matching human preferences.\n2.2. Challenges in reward modeling\nWhen handling rich inputs such as text, or when assessing complex behaviours, designing rewards\naligned with human preferences is a complex challenge for two main reasons, described below.\nDistribution shifts.The primary challenge is the distribution shifts resulting from the offline nature\nof preference data. Indeed, the generations in the preference dataset and those from the policy휃\n푠푓푡\ndo\nnot necessarily follow the same distributions, and the shifts can become even more pronounced due to\nmodel drift during RL. The OOD generalization literature has extensively analyzed the repercussions of\nthese shifts. Firstly, they often lead to a reduction in performance [62,63]. RMs (of limited capacity)\ntrained on narrow data distributions may rely on spurious correlations [51] or a limited number of\nfeatures [64], thus failing when encountering OOD examples [65,66]. Secondly, they complicate the\nselection of RMs, as ID validation metrics may poorly correlate with real-world OOD performances\n[67,68] and the ability to guide the RL [41]. Lastly, RMs can become poorly calibrated [69] in OOD\nscenarios [70,71], and predict more extreme values as rewards. Such miscalibration exacerbates\nthe problem in a negative feedback loop, further intensifying model drift and distribution shifts. In\nconclusion, limited data coverage during reward modeling reduces the reliability of the RM and\nfacilitates reward hacking [36] in regions where the RM is badly specified.\nInconsistent preferences.The second major challenge is the label noise in preference datasets.\nHuman labelers, often grappling with fatigue, misunderstandings [72,73] and imperfect incentives\n[74], might default to simpler criteria such as length, bullet points, or politeness rather than more\ncausal indicators. This tendency is exacerbated for complex tasks [38] or when considering multiple\nobjectives, ranging from harmlessness [75] to engagement [76] and representing the heterogeneity\nof human opinions. Consequently, these factors lead to low inter-rater agreement, where human\ndata appears as an imperfect representation of the underlying ground truth [77,78]. To mitigate\nthese issues, there has been a shift towards AI-generated preferences [57,58], which, while reducing\nhuman labor costs, introduces its own set of noise and failure cases, such as sensitivity to prompting\nstrategies [79,80]. These layers of noise and inconsistency challenge the robustness of the RM, and\nits ability to provide stable signals.\n4",
    "WARM: On the Benefits of Weight Averaged Reward Models\nWith this in mind, a good RM should ideally satisfy the three following properties.\nProperty1:efficiency.The RM should incur no memory or inference overhead. Then the policy\ncan be optimized efficiently.\nProperty2:reliability.The RM should reliably reward predictions despite the distribution\nshifts. Then the policy can explore away from its initialization while relying on the RM.\nProperty3:robustness.The RM should be robust to the label inconsistencies in binary\npreferences. Then the policy can learn from robust signals given by the RM.\n2.3. Existing approaches\nTo tackle those issues, previous works have explored a few research directions, further detailed in\nour related work from Appendix A.2. During RL, the standard strategy is to encourage the policy to\nremain close to its SFT initialization with Kullback-Leibler (KL) regularization [81,82];KLreduces\nmodel drift [83,84] but can cause underfitting and adds an extra hyperparameter (the regularization\nstrength훼). Collecting, labelling and then training on new data (reflecting the evolving policy)\ncan improve the reliability of the RM [16]. Yet it poses significant efficiency challenges due to the\ncontinuous requirement for human annotation and computational resources. In contrast,active\nlearningstrategies [85,86] proactively enrich the preference dataset by seeking out a diverse set of\ngenerations and potential failure cases. Concurrent work [87] suggests applying label smoothing\nand flipping. Finally, and most similar toWARM,prediction ensembling(ENS) [43] strategies average\nthe logits from푀RMs. From a bias-variance perspective [88], ENS reduces the variance term when\nmembers are sufficiently diverse [89], and thus favors reliability under distribution shifts where\nvariance is the key issue [47]. From a RL perspective, ENS was shown to mitigate hacking risks\n[12,41,42]. Despite its advantages, ENS faces efficiency challenges; the memory and inference\ncosts grow linearly with푀, making ENS incompatible with the scaling trend in RMs, where larger\narchitectures consistently perform better [90]. Moreover, we will also show in Section 4.2 that ENS\nfails to improve robustness to preference inconsistencies.\n3.WARM\n3.1. Weight averaging of reward models\nFacing those challenges in reward modeling and the limitations from existing approaches, we propose\nWeight Averaged Reward Models (WARM).WARMis a simple and efficient strategy that combines\nmultiple models without the memory and inference overheads of prediction ensembling, enhancing\nreward reliability (under distribution shifts) and robustness (amidst noisy preference dataset).WARM\nis illustrated in Figure 1(a) and described below.\n1.\nShared pre-trained initialization. For a given pre-trained LLM, each RM is initialized from\n\u0000\n휃\n푠푓푡\n, 휔\n\u0001\ncombining SFT weights and a linear probed [91] classifier.\n2.Diverse fine-tunings. We run푀RM fine-tunings, optimizing Equation (1) with diverse hyperpa-\nrameters (as in a grid search), yielding푀weights{휙\n푖\n}\n푀\n푖=1\n.\n3.Weight averaging. We average those푀weights together to form휙\nWARM\n=\n1\n푀\nÍ\n푀\n푖=1\n휙\n푖\n.\n5",
    "WARM: On the Benefits of Weight Averaged Reward Models\nThen푟\n휙\nWARM\nserves as the proxy RM to guide the RL procedure, as efficiently as an individual RM, but\nwith the enhanced reliability and robustness provided by the WA strategy, that leverages the strengths\nand mitigates the weaknesses of the individual RMs.\n3.2. Linear mode connectivity\nCompared to ENS, the main difference lies in howWARMcombines the different RMs: we do so\nthroughlinear interpolation in the weight space. It relies on the linear mode connectivity (LMC) [44,45]\nproperty across fine-tuned weights, i.e., the fact that the accuracy of the interpolated model is at\nleast as good as the interpolation of the individual accuracies. Precisely, by defining the pairwise\naccuracy of an RM푟\n휙\nw.r.t. a datasetDasAcc\n\u0000\n푟\n휙\n,D\n\u0001\n=피\n(\n푥,푦\n+\n,푦\n−\n)\n∈D\n\u0002\n1\n푟\n휙\n(푥,푦\n+\n)≥푟\n휙\n(푥,푦\n−\n)\n\u0003\n, the following\nObservation 1 underpins the success ofWARM.\nObservation 1(LMC).Given two fine-tuned weights휙\n1\nand휙\n2\nwith a shared pre-training and a test\ndatasetD\n푡푒푠푡\n, then for all휆∈ [0,1],\nAcc\n\u0000\n푟\n(\n1−휆\n)\n·휙\n1\n+휆·휙\n2\n,D\n푡푒푠푡\n\u0001\n≥\n(\n1−휆\n)\n×Acc\n\u0000\n푟\n휙\n1\n,D\n푡푒푠푡\n\u0001\n+휆×Acc\n\u0000\n푟\n휙\n2\n,D\n푡푒푠푡\n\u0001\n.(2)\nWe empirically validate this LMC in Figure 3, by evaluating interpolated RMs on OOD test samples.\nThis follows similar observations for multi-class classification in the context of computer vision [44,45],\nwhich led to a plethora of weight averaging (WA) works such as the model soups [46,47,48] variants\n(detailed in our related work in Appendix A.1).\nRemark 1(Importance of pre-training and linear probing).The efficacy of WA can be surprising\ngiven the non-linearities [54] and permutation symmetries [92] in deep neural network architectures.\nWA is actually possible only because of the shared pre-training which constrains the divergence during\nfine-tunings [45], such as the weights remain in convex regions of the loss valley [93]. In contrast, the\nLMC does not hold when training weights from scratch [45], even if the random initialization is shared.\nFor these reasons and to facilitate the LMC, we follow [47,48] and use linear probing to initialize the\nclassifier휔; compared to random initialization, such linear probing prevents feature distortion [91].\n3.3. Sources of diversity\nOn one hand,WARMrequires shared pre-training so that the fine-tuned weights remain linearly\nconnected. On the other hand, weights must not be identical: actually, the diversity across those\nfine-tuned weights significantly contributes to the accuracy gains observed in WA [47]. Overall, an\neffectiveWARMrequires a delicate trade-off between ensuring LMC and diversity across weights.\nIn practice, we use the following sources of diversity [94], leading the RM fine-tunings todiverse\nyet linearly connectedmodels. First, the different fine-tunings see the data samples indifferent\norders. Second, we sample slightlydifferent hyperparameters, notably different learning rates and\ndropout probabilities, as detailed in Appendix B.3. Third, we investigate a new source ofdiversity in\ninitializationnamedBaklava, illustrated in Figure 2. Specifically, we initialize the RMs’ featurizers\nfrom different checkpoints{휃\n푠푓푡\n푖\n}\n푀\n푖=1\ncollected along a given SFT trajectory.Baklavarelaxes the shared\ninitialization constraint from model soups [46] to simply sharing the same pre-training:Baklavais\nactually an efficient alternative to model ratatouille [48] but without the need of multiple auxiliary\ntasks. Overall,Baklavaincreases diversity compared to only initializing from the last SFT checkpoint,\nwhile adhering to the shared pre-training requisite for LMC, without incurring any overhead.\n6",
    "WARM: On the Benefits of Weight Averaged Reward Models\nSFT\nReward\nmodelings\nWeight\naveraging\n휃\n푝푡\n휃\n푠푓푡\n1\n휃\n푠푓푡\n2\n휃\n푠푓푡\n푀\n휙\n1\n휙\n2\n휙\n푀\n휙\nWARM\n=\n1\n푀\nÍ\n푀\n푖=1\n휙\n푖\nFigure 2|Baklavadiversity procedure. Starting from a pre-trained LLM휃\n푝푡\n, we consider different checkpoints\n{휃\n푠푓푡\n푖\n}\n푀\n푖=1\nalong a single SFT run (dashed arrow) collected at different number of SFT training steps. Those\ncheckpoints serve as initializations for푀RM fine-tunings on the preference dataset (thick solid arrows)\nto learn the{휙\n푖\n}\n푀\n푖=1\n. Finally, those RMs are weight averaged (dotted arrows) into the final model휙\nWARM\n.\nFollowing the culinary analogy from model soups [46] and model ratatouille [48], we named this method\nBaklavabecause of its diamond geometric shape.\nRemark 2(Moving average).Following stochastic weight average [95] or moving average [96], we\nalso tried to average checkpoints collected along a single RM fine-tuning. Though interesting because less\ncostly for training, the lower results in Figure 3(a) suggest that the accuracy-diversity trade-off was not\nfavorable: incorporating early checkpoints would compromise individual accuracies, and considering only\nlater checkpoints would not bring the necessary diversity. As a result, we opted to use inWARMonly the\nlast checkpoint from each RM fine-tuning.\n4. On the benefits ofWARM\nWe now explore the properties and benefits from theWARMstrategy, previously described in Section 3.\nWe ground our analysis on the empirical comparison between WA and ENS for reward modeling, and\na novel general theoretical comparison in Section 4.3.\nExperimental setup.We leverage the TL;DR summarization benchmark [97], a standard in reward\nmodeling for LLMs, that we briefly describe below and further detail in Appendix B. The goal of the\nRMs is to score summaries such as they are ranked properly. In training, we use the datasetD\n푡푟푎푖푛\nfrom Stiennonet al.[14] where the candidate summaries are generated by GPT-3 [6] variants. To\nobtain the labels, we follow the RLAIF procedure from [58], where a PaLM-L [98] is prompted with\nchain-of-thought [99] to generate feedback mimicking human preferences. This strategy performs\nsimilarly to human labelers with similar inter-agreement, and will be useful in Section 5 as an oracle\nmetric. The RMs are PaLM-XXS models, pre-trained and SFT-ed on the preferred summaries from\nD\n푡푟푎푖푛\n, on which we plug a linear probed [91] classification layer. We train the RMs for 10k steps on\nD\n푡푟푎푖푛\n, with hyperparameters and procedure detailed in Appendix B.3. We report accuracies of those\nRMs on a novel out-of-distribution (OOD) test datasetD\n표표푑\nwith 92k pairwise comparisons where the\nsummaries are generated by multiple PaLM-XS policies with high temperature, some of which are\npre-trained only, others SFT-ed and others RLHF-ed.\n4.1. 1\nst\norder analysis: weight averaging for reliable and more efficient ensembling\nPrevious works [46,47,95] have argued that the best way to understand WA is as an efficient\napproximation of ENS, as clarified in Observation 2.\nObservation 2(WA and ENS: 1\nst\norder analysis).Weight averaging and prediction ensembling perform\nsimilarly: i.e., for all휆∈ [0,1]and a test datasetD\n푡푒푠푡\n,\nAcc\n\u0000\n푟\n(\n1−휆\n)\n·휙\n1\n+휆·휙\n2\n,D\n푡푒푠푡\n\u0001\n≈Acc\n\u0000\n(\n1−휆\n)\n×푟\n휙\n1\n+휆×푟\n휙\n2\n,D\n푡푒푠푡\n\u0001\n.(3)\n7",
    "WARM: On the Benefits of Weight Averaged Reward Models\n0.00.20.40.60.81.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(a) 1 RM fine-tuning at 2\ndifferent training steps.\n0.00.20.40.60.81.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(b) 2 RM fine-tunings with\nshared config.\n0.00.20.40.60.81.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(c) 2 RM fine-tunings with\ndifferent learning rates.\n0.00.20.40.60.81.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(d) 2 RM fine-tunings with\ndifferent inits:Baklava.\nFigure 3|Experiments under distribution shifts validating Observations 1 and 2on the TL;DR summa-\nrization benchmark [97]. We report the accuracies onD\n표표푑\nwhen interpolating between two RM weights휙\n1\nand휙\n2\nwith the coefficient휆sliding between0and1.WAstands for weight averaging푟\n(\n1−휆\n)\n·휙\n1\n+휆·휙\n2\nwhileENS\ncombines the predictions(1−휆)×푟\n휙\n1\n+휆×푟\n휙\n2\n;Diagis the interpolated accuracy\n(\n1−휆\n)\n×Acc\n\u0000\n푟\n휙\n1\n\u0001\n+휆×Acc\n\u0000\n푟\n휙\n2\n\u0001\n.\nWe consider sources of increasing diversity [94] between휙\n1\nand휙\n2\n: in Figure 3(a), they are collected at\ndifferent number of training steps (8k and 10k) along a single RM fine-tuning; in Figure 3(b), they are from\ntwo independant RM fine-tunings, with the exact same config, but seeing the data in different orders; in\nFigure 3(c), they have different learning rates (1e-4 and 4e-5); in Figure 3(d), they are initalized from different\nSFT checkpoints collected at different number of SFT steps (8k and 12k), perBaklavaintroduced in Figure 2.\n0.00.20.40.60.81.0\n0.45\n0.46\n0.47\n0.48\n0.49\nAcc.\nWA\nENS\nDiag\n(a) Train (corrupt).\n0.00.20.40.60.81.0\n0.894\n0.896\n0.898\n0.900\n0.902\nAcc.\nWA\nENS\nDiag\n(b) Train (clean).\n0.00.20.40.60.81.0\n0.766\n0.768\n0.770\n0.772\n0.774\n0.776\n0.778\nAcc.\nWA\nENS\nDiag\n(c) Validation (ID).\n0.00.20.40.60.81.0\n0.708\n0.710\n0.712\n0.714\n0.716\nAcc.\nWA\nENS\nDiag\n(d) Test (OOD).\nFigure 4|Corruption experiment validating Observation 3. We consider휙\n1\nand휙\n2\n, two RMs fine-tuned\nindependently with the same config as in Figure 3(b), but this time with 25% of the training labels corrupted.\nWe then report the performances of their WA and ENS on the different data subsets. We observe that WA\nreduces memorization of the corrupted labels in Figure 4(a), and still performs slightly worse than ENS on the\nclean training samples in Figure 4(b); yet, the performances of WA w.r.t. ENS improves as we move away from\nthe training distribution, in particular onD\n표표푑\nin Figure 4(d) where WA generalizes better.\nTheoretically, a simple Taylor expansion can justify this similarity when∥휙\n1\n−휙\n2\n∥ ≪1. Empirically,\nthis is validated in Figure 3 where the accuracy curves onD\n표표푑\nfor WA and ENS closely match. This\nsimilarity justifies that WA is a variance reduction method; then, because variance is the dominant\nissue under distribution shifts [47], this explains the significant gains in Figure 3 over the individual\nRMs휙\n1\nand휙\n2\n(validating Observation 1), in particular when weights are sufficiently diverse. This\nsuggests improved reliability inWARM, with efficiency benefits over ENS: indeed, WA maintains a\nsingle set of weights, removing the memory and inference overheads from ENS.\n8",
    "WARM: On the Benefits of Weight Averaged Reward Models\n4.2. 2\nnd\norder analysis: weight averaging for more robust ensembling\nA surprising fact remains unexplained.WA is slightly superior to ENS under distribution shifts,\nwhich one can see on the plots from Figure 3, and more consistently in Figure B.1 from model soups\n[46] or in Figure 1 from DiWA [47]. More generally, WA is the state-of-the-art strategy for OOD\ngeneralization, consistently outperforming ENS; yet, this was not explained in previous works, thus\nurging for new insights about the difference between WA and ENS.\nCorruption setup.To refine our understanding on the difference between WA and ENS, we propose\na new setup where 25% of the binary labels are swapped in training. We then report the per-subset\naccuracies on Figure 4, enriched in Appendix C.1 and aggregated in Figure 5. On the corrupted\nsubset of training data, the accuracy curve for WA is below the expected accuracies, while it is above\non all other subsets. More precisely, we make the following Observation 3.\nObservation 3(WA and ENS: 2\nnd\norder analysis).The accuracy gains of WA over ENS grow as data\nmoves away from the training distribution.\n•WA≪ENS on train corrupt: WA is far worse than ENS on train samples with swapped labels,\nshowing reduced memorization and improved robustness to label corruption.\n•WA≤ENS on train clean: WA is worse than ENS on train samples with correct labels.\n•WA⪆ENS on ID val: WA is better or similar to ENS on samples without distribution shifts.\n•WA≥ENS on OOD test: WA is far better than ENS on test samples from new distributions, showing\nbetter reliability under distribution shifts.\n0.060.050.040.030.020.010.000.010.02\nAccuracy gain of WA over ENS\n0\n50\n100\n150\n200\nDensity\nTrain corrupted\nTrain clean\nVal ID\nTest OOD\nTrain corrupted\nTrain clean\nVal ID\nTest OOD\nFigure 5|Histograms of the differ-\nences in accuracy between WA and\nENS on different data subsets.\nOverall, this suggests that weight averaging memorizes less and\ngeneralizes better than ensembling predictions.\n4.3. Weight averaging enforces invariance across runs\nWe now provide theoretical support to this Observation 3. In brief,\nour simplifying assumptions suggest that WA acts as a regulariza-\ntion towards the predictive mechanisms that areinvariantacross\nruns, i.e., learned simultaneously in each independent run. Then,\nin contrast with ENS, WA would improve robustness to corruption\nbecause it would underweight the run-specific features (with low\nprobability of being learned) inducing memorization.\nSetup.We follow Linet al.[53], and consider a simplified binary classification setup with labels\n푦∈ {−1,1}, related to퐹features{푧\n푗\n}\n퐹\n푗=1\nsuch as푧\n푗\n∈ℝ\n푑\n. From inputs푥, we train a binary classifier\n푟(푥)=휔\n⊺\n푓(푥). Following [53], we make three key assumptions. First,features orthogonality: we\nassume that{푧\n푗\n}\n퐹\n푗=1\nare orthogonal, i.e.,(푧\n푗\n)\n⊺\n푧\n푗\n′\n=0when푗≠푗\n′\n. Second,input as bag of features: we\nassume that the input푥=\n\u0002\n푥\n푗\n\u0003\n퐹\n푗=1\n∈ℝ\n퐹×푑\ncan be represented as the concatenation of푥\n푗\ngenerated\nby푥\n푗\n∼ N\n\u0000\n푦·푧\n푗\n, 휎·I\n푑\n\u0001\nwith휎≪1. Finally, thebinary featurizerassumption: we consider that the\nfeaturizer푓=\n\u0002\n푓\n푗\n\u0003\n퐹\n푗=1\n∈ {0,1}\n퐹\nis a binary selector of the features that make the input. For example,\nif푦=1,퐹=3,푥≈ [푧\n1\n, 푧\n2\n, 푧\n3\n], and푓=\n[\n1,0,1\n]\nlearns to extract the first and third features, then\n푓(푥) ≈푧\n1\n+푧\n3\n. We denote푝\n푗\nthe probability that the featurizer푓learns to use the푗-th feature\ndimension (associated with푧\n푗\n); this means푓\n푗\nis1with probability푝\n푗\nand0otherwise. Moreover,\nfor infinite training samples and under some constraint on휎, Lemma 5 in [53] proved that, to learn\n푟=휔\n⊺\n푓, the optimal linear fit휔on the features selected from푓would be휔=\nÍ\n퐹\n푗=1\n푓\n푗\n·푧\n푗\n.\n9",
    "WARM: On the Benefits of Weight Averaged Reward Models\nResults.We consider푀RMs{푟\n푖\n=휔\n⊺\n푖\n푓\n푖\n}\n푀\n푖=1\n, and compare the limit behaviours of their predic-\ntion ensembling푟\n퐸푁푆\n푀\nand weight averaging푟\n푊퐴\n푀\nwhen푀→ ∞. In this limit case, the averaged\nprediction푟\n퐸푁푆\n푀\n=\n1\n푀\nÍ\n푀\n푖=1\n휔\n⊺\n푖\n푓\n푖\nfor an input푥from label푦tends towards the expected prediction\n피\n[\n푟(푥)\n]\n=피\n[\n휔\n⊺\n푓(푥)\n]\n=피\n{푓\n푗\n}\n퐹\n푗=1\n\u0002\n\u0000\nÍ\n퐹\n푗=1\n푓\n푗\n·푧\n푗\n\u0001\n⊺\n(\nÍ\n퐹\n푗\n′\n=1\n푓\n푗\n′\n·푥\n푗\n′\n)\n\u0003\n≈푦·\nÍ\n퐹\n푗=1\n푝\n푗\n·|푧\n푗\n|\n2\n, using푥\n푗\n′\n≈푦·푧\n푗\n′\nthus\n(푧\n푗\n)\n⊺\n푥\n푗\n′\n≈0when푗≠푗\n′\n, and(푓\n푗\n)\n2\n=푓\n푗\n.\n푟\n퐸푁푆\n푀\n(푥) −−−−−→\n푀→∞\n피\n[\n푟(푥)\n]\n≈푦·\n퐹\n∑︁\n푗=1\n풑\n풋\n·|푧\n푗\n|\n2\n.(4)\nIn contrast,  when considering푟\n푊퐴\n푀\n=\n\u0010\n1\n푀\nÍ\n푀\n푖=1\n휔\n푖\n\u0011\n⊺\n\u0010\n1\n푀\nÍ\n푀\n푖=1\n푓\n푖\n\u0011\nwith푀→ ∞,  we have\n1\n푀\nÍ\n푀\n푖=1\n푓\n푖\n−−−−−→\n푀→∞\n피\n[\n푓\n]\n=\n\u0002\n푝\n푗\n\u0003\n퐹\n푗=1\nand\n1\n푀\nÍ\n푀\n푖=1\n휔\n푖\n−−−−−→\n푀→∞\n피\n[\n휔\n]\n=\nÍ\n퐹\n푗=1\n푝\n푗\n·푧\n푗\n, and thus:\n푟\n푊퐴\n푀\n(푥) −−−−−→\n푀→∞\n©\n­\n«\n퐹\n∑︁\n푗=1\n푝\n푗\n·푧\n푗\nª\n®\n¬\n⊺\n©\n­\n«\n퐹\n∑︁\n푗\n′\n=1\n푝\n푗\n′\n·푥\n푗\n′\nª\n®\n¬\n≈푦·\n퐹\n∑︁\n푗=1\n풑\n2\n풋\n·|푧\n푗\n|\n2\n.(5)\nInterpretation.For ENS, the coefficient for a given feature is풑\n풋\n, the same as the probability of\nthis information being used by any individual network. In contrast, WA involves the square of the\nprobability풑\n2\n풋\n. Thus WA reduces the reliance on features with low probability, related to minor\nspecific information (such as noise or context) which can be used to fit the corrupted training samples;\nthis would reduce memorization, and thus explains the robustness of WA under label corruption.\nReciprocally, WA tends to prioritize the most probable features, favoring the mechanisms that are\nconsistently learned, in other words themechanisms invariant across runs. Overall, WA acts as a\nregularization, improving robustness under label corruption by tackling run-specific mechanisms\nfavoring memorization, and improving reliability under distribution shifts by preserving run-invariant\nmechanisms favoring generalization.\nRemark 3(Invariance).We argue that weight averaging only keeps the invariant predictive mechanisms\nacross runs. This is in analogy with the invariance literature [50], popular for domain generalization\n[51,100] under spurious correlations, where the key idea is that the predictive mechanisms which are\ninvariant across domains are the causal ones that are stable under distribution shifts. This theoretically\nconnects two key paradigms for OOD generalization, ensembling and invariance, and shows that weight\naveraging actually benefits from both.\nRemark 4(Extension to a deeper structure with퐿layers).We obtain a square in풑\n2\n풋\ndue to our\nsimplified two-layer architecture. Yet, in full generality, using a deeper structure with퐿layers would\nlead to풑\n푳\n풋\n. Intuitively, WA applies an AND-mask on the information, that need to be found both in the\nprevious feature space and the next layer weights.\nRemark 5(From reward robustness to learnability).When applied to the design of RMs inWARM,\nwe now argue that WA facilitatesWARM’s stability [87] by mitigating the reliance on some non-robust\nfeatures. Indeed, WA makes theWARMreward more robust to small (potentially adversarial [101])\nperturbations [102], i.e., smoother [103] in the input space. This relates to the Lipschitzness property\nof the reward [104,105,106], where the difference in predicted rewards is bounded by the distance in\ninput space. Fortunately, such smoothness is useful in RL [107], in particular for the stability of the\npolicy gradient [108] because “sharp changes in reward value are hard to represent and internalize”\n[109]. This is studied inLipschitzness is all you need[109] where the authors argue that “the local\nLipschitzness of the reward is a sine qua non condition for good performance”, required “to even learn\nanything”. In summary, robustness improves stability and hinders the cascade of errors occurring when\nminor input variations can cause large reward differences.\n10",
    "WARM: On the Benefits of Weight Averaged Reward Models\nIn conclusion, we summarize the benefits fromWARM. First, WARM is efficient, incurring no memory\nor computation costs, as it returns a single model. Second,WARMreduces variance while leveraging\nmechanisms invariant across runs, thus improving its reliability under distribution shifts. Lastly,\nWARMalso addresses label corruption, thereby augmenting robustness to noisy preferences.\n5. Experiments\nTo empirically validateWARM’s benefits described in previous section, we train PaLM-XXS RMs on\nthe TL;DR summarization benchmark [97] where preference labels are generated by a PaLM-L model\nprompted with chain-of-thought [99]. This AI labeling approach, increasingly common in recent\nresearch [26,41,110] as an efficient alternative to human assessments, is motivated by studies\n[57,58] indicating that it correlates well with human preferences: critically, it provides an automatic\npairwise oracle preferencemetric to evaluate reward hacking (in a similar fashion to the distillation\nsetup from [17], discussed in Appendix C.4). In addition, we leverage a PaLM-XS RM forpointwise\n0.20.40.60.81.01.2\nKL : log(N)\nN1\nN\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) PaLM (clean).\n0.20.40.60.81.01.2\nKL : log(N)\nN1\nN\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) PaLM (corrupt).\n0123456\nKL : log(N)\nN1\nN\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(c) T5 (clean).\n0123456\nKL : log(N)\nN1\nN\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(d) T5 (corrupt).\nFigure 6|Control reward for BoN experiments: clean preference dataset in Figures 6(a) and 6(c) and 25%\ncorruptions in Figures 6(b) and 6(d). We consider two SFT policies to generate candidate summaries: one\nbased on PaLM architecture [98], the other on T5 architecture [111]. The푥-axis is theKLbetween the BoN\npolicy and the SFT policy; the푦-axis represents the control reward gains w.r.t. to an RM휙\n1\n, which was the\nbest individual RM onD\n표표푑\n. The blue lines representWARMwith푀weights:WARMperforms higher than the\nindividual RMs (in yellows) or when ensembling their predictions (ENS in red). We report the absolute control\nrewards for those experiments in Figure 15, where the values range roughly between3and7.\n0123456\nKL : log(N)\nN1\nN\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\nWin ratio vs. SFT\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) SFT (clean).\n0123456\nKL : log(N)\nN1\nN\n0.70\n0.75\n0.80\n0.85\n0.90\nWin ratio vs. SFT\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) SFT (corrupt).\n0123456\nKL : log(N)\nN1\nN\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nWin ratio vs. WARM \nM \n= 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(c)WARM(clean).\n0123456\nKL : log(N)\nN1\nN\n0.300\n0.325\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500\nWin ratio vs. WARM \nM \n= 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(d)WARM(corrupt).\nFigure 7|Oracle preference metric for BoN experiments on T5 generations: clean preference dataset in\nFigures 7(a) and 7(c) and 25% corruptions in Figures 7(b) and 7(d). We plot the win rates for different values\nof푁vs. two reference strategies: SFT (i.e., random selection or equivalently BoN with푁=1), or selecting the\nbest summary according toWARM푀=6. We observe that all strategies beat the SFT reference (they are all\nabove 50% win rate), but that none beat theWARM푀=6reference.\n11",
    "WARM: On the Benefits of Weight Averaged Reward Models\ncontrol rewardreaching 80.1% accuracy on the OOD datasetD\n표표푑\n. As verified in our experiments, this\ncontrol RM also detects hacking, as it benefits from a larger architecture and a disjoint pretraining\ncompared to the PaLM-XXS RMs of interest. Below, we explore two key scenarios: in Section 5.1,\nWARMreranks outputs in best-of-푁(BoN); in Section 5.2,WARMguides the RL procedure.\n5.1. Best-of-푁experiments\nSetup.We start with best-of-푁(BoN) sampling experiments in Figures 6 and 7. Given a dataset\nof퐷text prompts, for each prompt we generate푁summaries from a SFT policy, and then returns\nthe summary with the highest reward according to different RMs. We actually consider two SFT\npolicies; one based on PaLM architecture [98] (푁=8,퐷=15000), the other on T5 architecture [111]\n(푁=1000,퐷=1000). For the푥-axis, we plot theKLbetween the BoN policy and the SFT policy,\nwhich can be approximated bylog(푁)−\n푁−1\n푁\n[112,113]. BoN is effective [16], especially in the low-KL\nregime (i.e., for small푁). We consider two setups, without (clean setup) and with (corrupt setup)\n25% label corruption in the preference datasets for reward modeling, and denote in each setup the\nweights{휙\n푖\n}\n푀\n푖=1\nsorted in decreasing accuracy onD\n표표푑\n.\nControl reward.Figure 6 shows that, in terms ofpointwise control reward,WARMperforms con-\nsistently better than ENS (only with푀=2for computational reasons) and the two best individual\nRMs휙\n1\nand휙\n2\n; moreover, the gains get bigger for푀=6. As a side note, we also observe that the\nindividual RM휙\n2\nperforms better in BoN in Figure 6(c) than휙\n1\nthough휙\n1\nwas better than휙\n2\non\nD\n표표푑\n, highlighting that selecting the appropriate individual RM is not trivial [41].\nOracle preference.In Figure 7, we leverage thepairwise oracle preference[58] metric to validate\nbetter performance withWARM. We observe in Figures 7(a) and 7(b) that summaries selected with\nWARMhave a win rate of up to 92.5% against the random selection of a summary (from SFT). We\nalso see in Figures 7(c) and 7(d) that reciprocally, all selection strategies have a win rate lower than\n50% against the summaries selected byWARM푀=6.\n5.2. RL experiments\nSetup.For RL fine-tuning of policies, we follow [58] and use their modified version of REINFORCE [60]\nwith a baseline value score for variance reduction, a simpler algorithm than PPO [61] yet still effective\nfor LLMs. Both policy and value LLMs are PaLM-XS, initialized from the same SFT model. We then\ngenerate samples with the policy, compute the reward with the RMs and update the weights to\noptimize this reward. More details are available in Appendix B.4. To reduce forgetting and encourage\nthe policy to remain close to its SFT initialization, we incorporate aKLregularization [81,82]\ncontrolled by a coefficient훼, ablated in Figure 8(c), yet otherwise set to0.003in the clean setup and\n0.01in the corrupt setup. ThisKLserves as the푥-axis in our plots to estimate model drift, as done in\nthe literature; same curves with the number of training steps as the푥-axis in Figures 1(b) and 18.\nControl reward.In Figure 8, we observe reward hacking; as the policy moves away from its SFT\ninitialization, the control reward collapses. Critically,WARMimproves performances: in particular,\nincreasing푀pushes the Pareto front of solutions to the top left in Figures 8(a) and 8(b). In comparison,\npolicies trained with ENS (with푀=2for computational reasons) are still susceptible to early reward\nhacking, while reaching absolute control rewards significantly worse than withWARM(even with\n푀=2). In Figure 8(c), we confirm that the훼hyperparameter plays a crucial role; low values of훼\nsuch as0.001correspond to highKL, while high values of훼such as0.01entail lowKLbut a risk of\nunderfitting. From a practical perspective, this highlights that the optimal value of훼forWARMis\nlower than for a single RM; this is becauseWARMcan mitigate reward hacking, and thus the optimal\npolicies are obtained for larger values ofKL.\n12",
    "WARM: On the Benefits of Weight Averaged Reward Models\n02004006008001000\nKL\n3\n4\n5\n6\n7\n8\n9\n10\n11\nControl reward\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) RL (clean).\n0255075100125150175\nKL\n4\n5\n6\n7\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) RL (corrupt).\n02505007501000125015001750\nKL\n2\n4\n6\n8\n10\nControl reward\nWARM    = 0.01\nWARM    = 0.003\nWARM    = 0.001\nInd    = 0.01\nInd    = 0.003\nInd    = 0.001\n(c) Ablating훼for RL (clean).\nFigure 8|Control reward for RL experiments: clean preference dataset in Figures 8(a) and 8(c) and 25%\ncorruptions in Figure 8(b). The blue lines show the RL fine-tuning of policies when averaging푀weights as the\nRM; the darker, the higher the푀. It performs higher than when RL fine-tuning with the individual RMs (in\nyellows) or when ensembling their predictions (in red). Figure 8(c) shows results of policies RL fine-tuned with\nWARM푀=6or휙\n1\n, for different values of훼controlling theKLregularization strength.\n20003000400050006000\n# steps\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nWin ratio vs. SFT\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(a) SFT.\n20003000400050006000\n# steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nWin ratio vs. WARM \nM \n= 6 at step 3500\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(b) WARM푀=6.\n20003000400050006000\n# steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nWin ratio vs. Ind \n1\n at step 3000\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(c)휙\n1\n(best individual RM).\nFigure 9|Oracle preference metric for RL experiments: clean preference dataset. We plot the win rates\nalong RL fine-tuning against three reference policies: the SFT policy, the policy RL fine-tuned withWARM\n푀=6after 3500 steps, and the policy RL fine-tuned with휙\n1\nafter 3000 steps. Figure 19 reports results when\ncomparing policies at fixed number of training steps.\nOracle preference.In Figure 9, we compare the different policies according to our pairwise oracle\npreference AI labeler [58]. In Figure 9(a), the reference policy is the SFT initialization; all the RL\nfine-tuned policies outperform this baseline, withWARM푀=6reaching a win rate of99.8%after\n3500 steps (the highest win rate among all policies). We use this policy as the reference in Figure 9(b);\nno other policy could beat it. Interestingly, we observe that using푀=10rewards can delay reward\nhacking but does not improve the peak performance; we speculate this is related to our weight\nselection procedure, as the weights{휙\n푖\n}\n10\n푖=7\nhave lower individual accuracy onD\n표표푑\nthan{휙\n푖\n}\n6\n푖=1\n(more\ndetails in Figure 10). Finally, in Figure 9(c), the reference policy is obtained after 3000 steps of\nRL fine-tuning with휙\n1\n(the best individual RM onD\n표표푑\n). There is a large region of steps in which\npolicies trainedWARM(even with푀=2) beat this approach; the previous reference from Figure 9(b)\nactually has a 79.4% win rate against it.\n13",
    "WARM: On the Benefits of Weight Averaged Reward Models\n6. Discussion\nBenefits.WARMrepresents a flexible and pragmatic method to improve the alignment of AI with\nhuman values and societal norms. This paper has detailed several of its benefits, and below, we delve\ninto additional, more exploratory advantages.WARMfollows theupdatable machine learning paradigm\n[114], eliminating the need for inter-server communication, thus enablingembarrassingly simple\nparallelization[115] of RMs. This facilitates its use infederated learningscenario [116] where the data\nshould remain private; moreover, WA would add a layer of privacy and bias mitigation by reducing\nthe memorization of private preference [52]. Then, a straightforward extension ofWARMwould\ncombine RMs trained on different datasets, for example, coming from different (clusters of) labelers.\nThis diversity could helpWARMperformances, but also from a multi objective perspective [117]; by\nnon-uniform interpolation of RMs, we could learn a set ofpersonalized policies[39]. Furthermore,\nas WA has been shown to limit catastrophic forgetting [118,119],WARMcould seamlessly support\niterative and evolving preferences. Finally, a promising research direction is extendingWARMto direct\npreference optimization (DPO) strategies [120], where averaging the RMs casts back to averaging\nthe DPO policies [121].\nLimitations.WARM, while innovative, does face some limitations, notably two when compared to\nprediction ensembling methods; first, prediction ensembling can benefit from the diversity brought\nby combining RMs from various architectures and pre-trainings; second, prediction ensembling\ncan incorporate prediction disagreement into the reward to provide uncertainty estimation and\nlimit model drift. However, it’s been noted in [41] that simple averaging of logits often performs\ncomparably to more complex prediction aggregation functions that include uncertainty elements.\nAnother limitation is that, whileWARMeffectively reduces certain types of memorization, it does\nnot completely eradicate all forms of spurious correlations or biases inherent in the preference data.\nFor instance, if each individual RM predominantly relies on summary length as a criterion,WARM\nis likely to replicate this tendency. Therefore, alternative methods (from the OOD generalization\nliterature?) might be required, for example those based on invariance regularization [51,100] or\nlast layer retraining [122]. Finally,WARMonly enhances reward modeling without tackling the other\nchallenges in RLHF [18]; thus, to mitigate the safety risks [19,34,35] from misalignment [32,33],\nWARMmust be considered within the larger context of responsible AI.\n7. Conclusion\nIn conclusion, we introduce Weight Averaged Reward Models (WARM) to address two critical chal-\nlenges in reward modeling: reliability under distribution shifts and robustness under label corruption.\nBy averaging the weights of multiple RMs obtained from diverse fine-tunings,WARMappears as an\nefficient solution to mitigate reward hacking in reinforcement learning from human feedback. Our\nempirical results demonstrate its effectiveness when applied to summarization. We anticipate that\nWARMwill contribute to more aligned, transparent, and effective AI systems, encouraging further\nexploration in reward modeling.\n14",
    "WARM: On the Benefits of Weight Averaged Reward Models\nReferences\n[1] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023.(p. 1)\n[2] OpenAI. Gpt-4 technical report. 2023.(p. 1)\n[3]\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4.arXiv preprint, 2023.(p. 1)\n[4]Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.(pp. 1 and 3)\n[5]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. InNAACL, 2019.(p. 1)\n[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. InNeurIPS, 2020.(pp. 1, 7, and 27)\n[7]Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. InICLR,\n2022.(pp. 1 and 3)\n[8]Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir\nParmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh\nPuri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A,\nSumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via\ndeclarative instructions on 1600+ NLP tasks. InACL, 2022.(p. 1)\n[9]\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. Stanford Alpaca: An instruction-following LLaMA model,\n2023.(p. 1)\n[10]\nPaul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu\nGeist, Sertan Girgin, Léonard Hussenot, Orgad Keller, et al. Factually consistent summarization\nvia reinforcement learning with textual entailment feedback. InACL, 2023.(p. 1)\n[11]Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On the fragility of learned\nreward functions.arXiv preprint, 2023.(p. 1)\n[12]Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. InNeurIPS, 2017.(pp. 1, 2, 3, 5, and 27)\n[13]\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences.arXiv\npreprint, 2019.(pp. 1 and 27)\n15",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[14]Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\nNeurIPS, 2020.(pp. 1, 7, and 27)\n[15]Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\nChristiano. Recursively summarizing books with human feedback.arXiv preprint, 2021.(pp. 1\nand 27)\n[16]Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. LLaMA 2: Open foundation and fine-tuned chat models.\narXiv preprint, 2023.(pp. 1, 5, 12, and 27)\n[17]Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.\nInICML, 2023.(pp. 1, 11, and 33)\n[18]Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier\nRando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems\nand fundamental limitations of reinforcement learning from human feedback.TMLR, 2023.\n(pp. 1 and 14)\n[19]\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.\nConcrete problems in AI safety.arXiv preprint, 2016.(pp. 1, 2, 3, 4, and 14)\n[20]Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild.https://openai.com\n/research/faulty-reward-functions, 2016.(pp. 1 and 4)\n[21]Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny\nHernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown,\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a\nlaboratory for alignment.arXiv preprint, 2021.(pp. 1 and 4)\n[22]Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger.\nDefining and characterizing reward gaming. InNeurIPS, 2022.(pp. 1 and 4)\n[23]Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification:\nMapping and mitigating misaligned models. InICLR, 2022.(pp. 1 and 4)\n[24]Nathan Lambert and Roberto Calandra. The alignment ceiling: Objective mismatch in rein-\nforcement learning from human feedback.arXiv preprint, 2023.(p. 1)\n[25]Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal?\nend-to-end learning for negotiation dialogues.arXiv preprint, 2017.(p. 1)\n16",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[26]Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating\nlength correlations in rlhf.arXiv preprint, 2023.(pp. 1 and 11)\n[27]Marilyn Strathern. Improving ratings: audit in the british university system.European Review,\n1997.(p. 1)\n[28]\nEthan Perez, Sam Ringer, Kamil\n ̇\ne Lukoši\n ̄\nut\n ̇\ne, Karina Nguyen, Edwin Chen, Scott Heiner, Craig\nPettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model\nbehaviors with model-written evaluations.arXiv preprint, 2022.(p. 1)\n[29]Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R\nBowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards\nunderstanding sycophancy in language models.arXiv preprint, 2023.(p. 1)\n[30]Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori\nHashimoto. Whose opinions do language models reflect? InICML, 2023.(p. 1)\n[31]Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conver-\nsational ai: Converging evidence on chatgpt’s pro-environmental, left-libertarian orientation.\narXiv preprint, 2023.(p. 1)\n[32]\nJessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch.  Alignment for\nadvanced machine learning systems.Ethics of AI, 2016.(pp. 1, 3, and 14)\n[33]Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep\nlearning perspective.arXiv preprint, 2022.(pp. 1, 3, 14, and 27)\n[34]Dan Hendrycks and Mantas Mazeika. X-risk analysis for AI research.arXiv preprint, 2022.\n(pp. 1 and 14)\n[35]Dan Hendrycks. Natural selection favors AIs over humans.arXiv preprint, 2023.(pp. 1 and 14)\n[36]Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned AI.NeurIPS, 2020.\n(pp. 2 and 4)\n[37]Daniel Shin, Anca Dragan, and Daniel S. Brown.  Benchmarks and algorithms for offline\npreference-based reward learning.TMLR, 2023.(p. 2)\n[38]Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile\nLukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable\noversight for large language models.arXiv preprint, 2022.(pp. 2 and 4)\n[39]Alexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste\nGaya, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment\nby interpolating weights fine-tuned on diverse rewards. InNeurIPS, 2023.(pp. 2, 14, and 26)\n[40]Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models\nto follow instructions with human feedback.NeurIPS, 2022.(pp. 2, 3, and 27)\n[41]Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour, DJ Dvijotham,\nAdam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding?\nreward model ensembles mitigate but do not eliminate reward hacking.arXiv preprint, 2023.\n(pp. 2, 4, 5, 11, 12, 14, and 27)\n17",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[42]Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help\nmitigate overoptimization.arXiv preprint, 2023.(pp. 2, 5, and 27)\n[43]\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.  Simple and scalable\npredictive uncertainty estimation using deep ensembles. InNeurIPS, 2017.(pp. 2 and 5)\n[44]\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear\nmode connectivity and the lottery ticket hypothesis. InICML, 2020.(pp. 3, 6, and 26)\n[45]Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer\nlearning? InNeurIPS, 2020.(pp. 3, 6, and 26)\n[46]\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-\nLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and\nLudwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves\naccuracy without increasing inference time. InICML, 2022.(pp. 3, 6, 7, 9, 26, and 28)\n[47]\nAlexandre Ramé, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Galli-\nnari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. In\nNeurIPS, 2022.(pp. 3, 5, 6, 7, 8, 9, 26, and 28)\n[48]Alexandre Ramé, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Léon Bottou, and David Lopez-\nPaz. Model Ratatouille: Recycling diverse models for out-of-distribution generalization. In\nICML, 2023.(pp. 3, 6, 7, 26, and 28)\n[49]Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee,\nand Sungrae Park. SWAD: Domain generalization by seeking flat minima. InNeurIPS, 2021.\n(pp. 3 and 26)\n[50]\nKrikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via\ninvariant feature representation. InICML, 2013.(pp. 3 and 10)\n[51]Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimiza-\ntion.arXiv preprint, 2019.(pp. 3, 4, 10, and 14)\n[52]Kerem Zaman, Leshem Choshen, and Shashank Srivastava. Fuse to forget: Bias reduction and\nselective memorization through model fusion.arXiv preprint, 2023.(pp. 3, 14, and 26)\n[53]Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and\nTong Zhang. Spurious feature diversification improves out-of-distribution generalization. In\nICLR, 2024.(pp. 3, 9, and 26)\n[54]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. InNeurIPS, 2017.(pp. 3 and 6)\n[55]\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\nopportunities and risks of foundation models.arXiv preprint, 2021.(pp. 3 and 26)\n[56]Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level\nimage representations using convolutional neural networks. InCVPR, 2014.(p. 3)\n[57]\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:\nHarmlessness from AI feedback.arXiv preprint, 2022.(pp. 4 and 11)\n18",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[58]Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu,\nColton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning\nfrom human feedback with ai feedback.arXiv preprint, 2023.(pp. 4, 7, 11, 12, 13, 27, and 28)\n[59]Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons.Biometrika, 1952.(p. 4)\n[60]Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning.Reinforcement learning, 1992.(pp. 4, 12, and 28)\n[61]John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms.arXiv preprint, 2017.(pp. 4 and 12)\n[62]\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InICLR, 2021.\n(p. 4)\n[63]Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay\nBalsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,\nEtienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure\nLeskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.\nWILDS: A benchmark of in-the-wild distribution shifts. InICML, 2021.(p. 4)\n[64]Mohammad Pezeshki, Sékou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and\nGuillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. InNeurIPS,\n2020.(p. 4)\n[65] Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, and Moncef Gabbouj. Learning distinct\nfeatures helps, provably.arXiv preprint, 2021.(p. 4)\n[66]Niv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse ImageNet\nmodels transfer better.arXiv preprint, 2022.(p. 4)\n[67]Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel,\nChristina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspeci-\nfication presents challenges for credibility in modern machine learning.JMLR, 2020.(pp. 4\nand 26)\n[68]Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. ID and OOD performance\nare sometimes inversely correlated on real-world datasets. InNeurIPS Workshop, 2023.(p. 4)\n[69]Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural\nnetworks. InICML, 2017.(p. 4)\n[70]\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua\nDillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?\nevaluating predictive uncertainty under dataset shift. InNeurIPS, 2019.(p. 4)\n[71]Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain\ngeneralization. InNeurIPS, 2021.(p. 4)\n[72] Herbert A Simon. Bounded rationality.Utility and probability, 1990.(p. 4)\n[73]\nRohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning,\nrather than assuming, human biases for reward inference. InICML, 2019.(p. 4)\n19",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[74]Timo Kaufmann, Sarah Ball, Jacob Beck, Eyke Hüllermeier, and Frauke Kreuter.  On the\nchallenges and practices of reinforcement learning from real human feedback. 2023.(p. 4)\n[75]Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath,\nBen Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models\nto reduce harms: Methods, scaling behaviors, and lessons learned.arXiv preprint, 2022.(p. 4)\n[76]Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk,\nZongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, et al. Rewarding chatbots\nfor real-world engagement with millions of users.arXiv preprint, 2023.(p. 4)\n[77]Condorcet. Essai sur l’application de l’analyse à la probabilité des décisions rendues à la\npluralité des voix. 1785.(p. 4)\n[78]\nSilviu Pitis. Failure modes of learning reward models for llms and other sequence models. In\nICML, 2023.(p. 4)\n[79]\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models’\nsensitivity to spurious features in prompt design or: How i learned to start worrying about\nprompt formatting.arXiv preprint, 2023.(p. 4)\n[80]Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\nState of what art? a call for multi-prompt llm evaluation.arXiv preprint, 2023.(p. 4)\n[81]Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E\nTurner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation\nmodels with kl-control. InICML, 2017.(pp. 5 and 12)\n[82]Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision\nprocesses. InICML, 2019.(pp. 5 and 12)\n[83]Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets\nnatural language: Synergies between functional and structural language learning. InACL,\n2020.(p. 5)\n[84]\nYuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering\nlanguage drift with seeded iterated learning. InICML, 2020.(p. 5)\n[85]Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human\nobjectives by evaluating hypothetical behavior. InICML, 2020.(pp. 5 and 27)\n[86]\nWilliam Saunders, Girish Sastry, Andreas Stuhlmüller, and Owain Evans. Trial without error:\nTowards safe reinforcement learning via human intervention. InAAMAS, 2018.(p. 5)\n[87]Binghai Wang et al. Secrets of rlhf in large language models part ii: Reward modeling.arXiv\npreprint, 2023.(pp. 5, 10, and 27)\n[88]Ron Kohavi, David H Wolpert, et al.  Bias plus variance decomposition for zero-one loss\nfunctions. InICML, 1996.(p. 5)\n[89]Naonori Ueda and Ryohei Nakano. Generalization error of ensemble estimators. InICNN,\n1996.(p. 5)\n[90]Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen,\nAnna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et al. Specific versus general\nprinciples for constitutional ai.arXiv preprint, 2023.(p. 5)\n20",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[91]Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang.\nFine-tuning can distort pretrained features and underperform out-of-distribution. InICLR,\n2022.(pp. 5, 6, 7, and 28)\n[92]Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging\nmodels modulo permutation symmetries. InICLR, 2022.(p. 6)\n[93]Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.\nKnowledge is a region in weight space for fine-tuned language models. InEMNLP, 2023.(p. 6)\n[94]Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule\nthem all: Overlapping features of training methods. InICLR, 2022.(pp. 6 and 8)\n[95]\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.\nAveraging weights leads to wider optima and better generalization. InUAI, 2018.(pp. 7 and 26)\n[96]Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving\nmodel selection and boosting performance in domain generalization. InNeurIPS, 2021.(pp. 7\nand 26)\n[97]\nMichael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to\nlearn automatic summarization. InACL Workshop, 2017.(pp. 7, 8, and 11)\n[98]\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report.\narXiv preprint, 2023.(pp. 7, 11, 12, 27, 28, and 30)\n[99]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain-of-Thought prompting elicits reasoning in large language models. InNeurIPS,\n2022.(pp. 7, 11, and 27)\n[100] Alexandre Ramé, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances\nfor out-of-distribution generalization. InICML, 2022.(pp. 10 and 14)\n[101]Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-\nfellow, and Rob Fergus. Intriguing properties of neural networks.arXiv preprint, 2013.(p. 10)\n[102]Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika\nChaudhuri. Adversarial robustness through local lipschitzness.arXiv preprint, 2020.(p. 10)\n[103]Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. A case for new\nneural network smoothness constraints. InNeurIPS ICBINB, 2020.(p. 10)\n[104]Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier\nagainst adversarial manipulation.NeurIPS, 2017.(p. 10)\n[105]Jure Sokolić, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin\ndeep neural networks.IEEE Transactions on Signal Processing, 2017.(p. 10)\n[106]Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized\nsmoothing. InICML, 2019.(p. 10)\n[107]\nRoland Hafner and Martin Riedmiller. Reinforcement learning in feedback control: Challenges\nand benchmarks from technical process control.Machine learning, 2011.(p. 10)\n21",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[108]Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov\ndecision processes.Machine Learning, 2015.(p. 10)\n[109]Lionel Blondé, Pablo Strasser, and Alexandros Kalousis. Lipschitzness is all you need to tame\noff-policy generative adversarial imitation learning.Machine Learning, 2022.(p. 10)\n[110]\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback.arXiv preprint, 2023.(p. 11)\n[111]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer.JMLR, 2020.(pp. 11, 12, 30, and 33)\n[112] Jacob Hilton. KL divergence of max-of-n, 2023.(p. 12)\n[113]Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D’Amour, Jacob Eisenstein, Chirag\nNagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment\npolicy.arXiv preprint, 2024.(p. 12)\n[114]\nColin Raffel. Building Machine Learning Models Like Open Source Software.ACM, 2023.\n(p. 14)\n[115]\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and\nLuke Zettlemoyer. Branch-Train-Merge: Embarrassingly parallel training of expert language\nmodels.arXiv preprint, 2022.(p. 14)\n[116]Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. InAISTATS, 2017.\n(p. 14)\n[117]Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A.\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. InNeuriPS, 2023.(p. 14)\n[118]Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of\nstrong zero-shot models for continual learning. InNeurIPS Workshop, 2022.(pp. 14 and 26)\n[119]Steven Vander Eeckt et al. Weight averaging: A simple yet effective method to overcome\ncatastrophic forgetting in automatic speech recognition.arXiv preprint, 2022.(p. 14)\n[120]\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\narXiv preprint, 2023.(pp. 14 and 27)\n[121]\nMaxime Labonne. NeuralBeagle14-7B.https://huggingface.co/mlabonne/NeuralBe\nagle14-7B-GGUF, 2024.(pp. 14 and 27)\n[122]Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson.  Last layer re-training is\nsufficient for robustness to spurious correlations. InICLR, 2023.(p. 14)\n[123]\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common\ncorruptions and perturbations. InICLR, 2019.(p. 26)\n22",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[124]John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, and\nEric Karl Oermann. Variable generalization performance of a deep learning model to detect\npneumonia in chest radiographs: A cross-sectional study.PLOS Medicine, 2018.(p. 26)\n[125]Alex J DeGrave, Joseph D Janizek, and Su-In Lee. AI for radiographic COVID-19 detection\nselects shortcuts over signal.Nature Machine Intelligence, 2021.(p. 26)\n[126]Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi,\nHongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. InCVPR,\n2022.(p. 26)\n[127]Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi,\nSimon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by\ninterpolating weights. InNeurIPS, 2022.(p. 26)\n[128]Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem\nChoshen. ColD fusion: Collaborative descent for distributed multitask finetuning. InACL,\n2023.(p. 26)\n[129]Nikolaos Dimitriadis, Pascal Frossard, and François Fleuret. Pareto manifold learning: Tackling\nmultiple tasks via ensembles of single-task models.arXiv preprint, 2022.(Not cited.)\n[130]\nMustafa Shukor, Corentin Dancette, Alexandre Ramé, and Matthieu Cord. Unival: Unified\nmodel for image, video, audio and language.TMLR, 2023.(p. 26)\n[131]Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, and Sven Gowal. Seasoning model\nsoups for robustness to adversarial and natural distribution shifts. InCVPR, 2023.(p. 26)\n[132]Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc, and Naomi Saphra.  Linear\nconnectivity reveals generalization strategies. InICLR, 2023.(p. 26)\n[133]Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov,\nPavel Shvechikov, Dmitry Vetrov, and Andrew Gordon Wilson. Improving stability in deep\nreinforcement learning with weight averaging. 2018.(p. 26)\n[134]\nJean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for\nonline adaptation in reinforcement learning. InICLR, 2022.(p. 26)\n[135]Daniel Lawson and Ahmed H Qureshi. Merging decision transformers: Weight averaging for\nforming multi-task policies. InICLR RRL Workshop, 2023.(p. 26)\n[136]\nMichael Noukhovitch, Samuel Lavoie, Florian Strub, and Aaron Courville. Language model\nalignment with elastic reset. InNeurIPS, 2023.(p. 26)\n[137]Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,\nHannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. InICLR, 2023.\n(p. 26)\n[138]Nico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M Ponti. Elastic\nweight removal for faithful and abstractive dialogue generation.arXiv preprint, 2023.(p. 26)\n[139]Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy\nlabels with deep neural networks: A survey.TNNLS, 2022.(p. 26)\n[140]Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization.ICLR, 2017.(p. 26)\n23",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[141]Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan\nSilberman. Learning from noisy labels by regularized estimation of annotator confusion. In\nCVPR, 2019.(p. 26)\n[142]Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami\nSomepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al.\nNeftune: Noisy embeddings improve instruction finetuning.arXiv preprint, 2023.(p. 26)\n[143]Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise\nfor deep neural networks. InAAAI, 2017.(p. 26)\n[144]Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama.\nSample selection with uncertainty of losses for learning with noisy labels. InICLR, 2022.(p. 26)\n[145]Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning\ndata-driven curriculum for very deep neural networks on corrupted labels. InICML, 2018.\n(p. 26)\n[146]\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi\nSugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.\nNeurIPS, 2018.(p. 26)\n[147]Maryam Sabzevari.Ensemble learning in the presence of noise. PhD thesis, Universidad Autónoma\nde Madrid, 2019.(p. 26)\n[148]Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. InICML,\n2000.(p. 27)\n[149]W Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson, Serena Booth, Anca\nDragan, Peter Stone, and Scott Niekum. Learning optimal advantage from preferences and\nmistaking it for reward.arXiv preprint, 2023.(p. 27)\n[150]Peter Barnett, Rachel Freedman, Justin Svegliato, and Stuart Russell. Active reward learning\nfrom multiple teachers.arXiv preprint, 2023.(p. 27)\n[151]Sian Gooding and Hassan Mansoor. The impact of preference agreement in reinforcement\nlearning from human feedback: A case study in summarization.arXiv preprint, 2023.(p. 27)\n[152]Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Tool-\naugmented reward modeling. InICLR, 2023.(p. 27)\n[153]\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang\nGan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models\nwith factually augmented rlhf.arXiv preprint, 2023.(p. 27)\n[154]\nAnonymous. RIME: Robust preference-based reinforcement learning with noisy human prefer-\nences. InSubmitted to ICLR, 2023.(p. 27)\n[155]Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello,\nMichal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from\nhuman preferences.arXiv preprint, 2023.(p. 27)\n[156]\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. InICML, 2018.(pp. 27 and 28)\n24",
    "WARM: On the Benefits of Weight Averaged Reward Models\n[157]Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran,\nJoshua Susskind, and Etai Littwin. Vanishing gradients in reinforcement finetuning of language\nmodels.arXiv preprint, 2023.(p. 28)\n25",
    "WARM: On the Benefits of Weight Averaged Reward Models\nWARM: On the Benefits of Weight Averaged Reward Models\nSupplementary material\nThis supplementary material is organized as follows:\n•Appendix A enriches our related work.\n•Appendix B clarifies some experimental details.\n•Appendix C enriches our experiments.\nA. Related work\nThis paper leverages the insights from the OOD generalization literature, in particular from linear\nmode connectivity (see Appendix A.1), and applies them to the design of efficient, reliable and robust\nreward models (see Appendix A.2).\nA.1. Out-of-distribution generalization, linear mode connectivity and memorization\nLMC in fine-tuning.Fine-tuning foundation models [55] into specialized models that generalize\nwell to new distributions is critical for many real-world applications [123,124,125]. Recently,\ndifferent variants of weight averaging (WA) were able to improve performance, such as moving\naverage [49,95,96], WiSE fine-tuning [126], model soups [46], DiWA [47] and model ratatouille\n[48]. These works rely on the LMC [44,45] across fine-tuned weights, which was extended to fine-\ntunings on different tasks [48,127,128], modalities [130] or with different losses [47,131], although\n[132] highlighted some limitations. WA was also used recently in RL setups [39,133,134,135,136],\nin particular in RLHF in [39, 136] but only to combine policies and not rewards.\nInsights into WA.Specifically, WA comes with several benefits. First, WA flattens the loss landscape\n[49]. Second, WA approximates prediction ensembling, thus reduces variance of the estimator\n[46,47] and tackles model misspecification [67]. Third, WA combines models’ abilities [137,138],\nwhich can be useful for multi-task [127], multi-objective [39] or in continual learning [118] setups.\nLastly, it has recently been shown that WA can provide some benefits under spurious correlations\n[52,53], with a phenomenon calledFalseFalseTruein [53]. These works [52,53] share similarities\nwith our memorization experiments from Section 4.2, but we are the first to analyze WA regularization\nproperties under label corruption, and their consequences on generalization. In contrast, in [52] the\nnetworks are trained on different datasets while the theory in [53] is actually mostly developed for\nprediction ensembling.\nMemorization.Traditional approaches [139] tackling memorization of corrupted labels [140] usually\nrequire explicit regularization [141], specific data augmentation [142], loss adjustment [143] or\nsample selection [144]. Some other strategies are based on ensembling: they filter out potentially\ncorrupted samples with self-labeling filtering [145,146] or bagging diversity procedures [147]. As\nfar as we know, with WA we propose the first strategy combining multiple models trained on the\nsame dataset that manages to tackle corruption.\n26",
    "WARM: On the Benefits of Weight Averaged Reward Models\nA.2. Reward modeling\nOne of the central challenge in aligning LLMs is the absence of explicit rewards from the environment,\na.k.a. the outer alignment challenge [33]. While Inverse Reinforcement Learning [148] attempts\nto derive a reward model (RM) from expert demonstrations, most recent efforts [12,13,14,15,\n40] primarily focus on learning from human preferences. Despite its importance to enhance LLM\nperformances post-RL and for safe deployment in real-world applications, how to best design RMs\nhas arguably receive less attention than it warrants. Some research [149] seeks to refine the loss\nfunction from Equation (1). Other approaches are more data oriented: for example, LLaMA-2 [16]\ninvolves continual learning of the RM to adjust to new generation distributions; [85,150] follow an\nactive learning paradigm [151]. Augmenting rewards with tools [152] or additional information\n[153] represents an even more recent and very promising trend. Limited efforts have been made at\nthe intersection of label corruption and reward modeling; [154] tried to filter the preference dataset\nfor small academic locomotion tasks, while the concurrent [87] suggests applying label smoothing\nand flipping. Actually, reward ensembling is the most discussed method to mitigate reward hacking\n[41,42]; we show thatWARMcan beat ENS while removing its overheads. Finally, following DPO\n[120], a recent trend merges reward modeling with policy learning; though, the policies still tend to\nhack the preference data [155], and thus require only a few training steps and very small learning\nrates. The WA of DPO policies, theoretically equivalent to the WA of RMs, is a promising research\ndirection with already significant empirical results on public benchmarks, as demonstrated in [121].\nB. Implementation details\nB.1. Dataset details\nFor summarization, we use the Reddit TL;DR dataset [14], containing posts from Reddit that have\nbeen filtered to ensure high quality. The training summaries from [14] are generated by OpenAI\nGPT-3 [6] variants. The dataset contains 123k posts, and∼5% is held out as the ID validation set.\nTo generate the candidate responses in the OOD datasetD\n표표푑\nwith 92k pairwise comparisons, we\nconsidered multiple PaLM-XS policies with high temperature, some of which are pre-trained only,\nothers SFT-ed and others RLHF-ed; the goal was to get a diverse set of summaries.\nB.2. AI labeling details\nWhile the ideal approach for evaluating our models would involve human preferences, we resort to\nthe cheaper AI labeling procedure from RLAIF [58]. We query an instruct fine-tuned PaLM-L [98]\nLLM\n1\n, prompted to generate preference mimicking human preferences. Specifically, we follow the\n“Detailed + CoT 0-shot” prompting strategy from RLAIF [58], the best one according to their results,\ninvolving zero-shot prompting with chain-of-thought [99], a maximum decoding length of 512 tokens\nand temperature푇=0.0(i.e., greedy decoding). To avoid position bias, we run the AI labeler in the\ntwo possible orderings. This strategy was shown to perform similarly to human labellers, with similar\ninter-agreement. For the corruption experiments, we swap the labels for 25% of the training samples.\nB.3. Reward modeling details\nThe RMs are PaLM-XXS models [98]. They are first pre-trained, and then supervised fine-tuned on\nthe Reddit TL;DR dataset for 12k steps with a batch size of 128 and the Adafactor [156] optimizer\n1\nAvailable through Google Cloud’s Vertex AIhttps://cloud.google.com/vertex-ai/docs/generative-ai/\nlearn/models.\n27",
    "WARM: On the Benefits of Weight Averaged Reward Models\nwith a learning rate of10\n−5\n. Following theBaklavarecipe, we actually launch the reward modeling\nfrom different checkpoints along this SFT fine-tuning, at steps{8k, 10k, 12k}; taking a too-early\ncheckpoint would drastically reduce RM accuracy, as observed in [157]. To convert this LLM into a\nclassifier, we plug a linear probed [91] classification layer (the same for all RMs); said differently,\neven though the featurizers are actually from different SFT checkpoints, they share the same linear\nprobed classification linear layer. As explained in [91], it prevents features from moving too much\naway from their initializations, which facilitates the LMC required for WA.\nWe train all RMs for 10k steps, a batch size of 128, the Adafactor [156] optimizer, a learning rate\nsampled in{1e-5,4e-5,1e-4}, and a dropout probability in{0.05, 0.1}. This follows the practical\nrecommandations from [47] to leverage hyperparameters in a mild range to preserve the LMC.\nTraining for a longer number of steps could help, as it did not alter the LMC in previous works [48].\nIn practice, for the main experiments with clean labels, we launch 10 reward modelings; when ranked\nin decreasing accuracy onD\n표표푑\n, we denote them{휙\n푖\n}\n10\n푖=1\n. Therefore, the RMs named휙\n1\nand휙\n2\nin the\ndifferent plots are the two best according to their individual performances under distribution shifts.\nThen,WARM푀=2is actually the RM defined per\n휙\n1\n+휙\n2\n2\n, while ENS푀=2averages their predictions.\nMore generally,WARMwith푀weights is the WA of the푀best weights{휙\n푖\n}\n푀\n푖=1\n. The main motivation\nof this weight selection procedure is to remove potentially bad RMs, as validated in Figure 10, in\nwhich we consider different permutations across those 10 RMs. As a side note, we speculate that a\ngreedy procedure as in [46] could further improve performances.\n246810\nM\n0.752\n0.754\n0.756\n0.758\n0.760\n0.762\n0.764\n0.766\n0.768\nOOD Acc.\nFrom best to worst\nFirst best and then random\nRandom permutation v1\nRandom permutation v2\nFrom worst to best\nFigure 10|Analysis of the weight selection procedure. We plot the accuracy resulting from averaging푀\nweights (out of 10), where these weights are chosen based on various selection procedures. This effectively\nvalidates that choosing models from best to worst serves as a reliable heuristic.\nB.4. Reinforcement learning details\nBoth policy and value models are PaLM-XS [98], initialized from the same SFT model. We then\ngenerate samples from the policy with temperature푇=0.9, batch size of 128, the Adafactor [156]\noptimizer, a learning rate of10\n−5\nand a policy warmup of 2k steps. We set훼=0.003for theKL\nregularization in the main experiment without label corruption, and훼=0.01with label corruption.\nFollowing [58], we used a modified version of REINFORCE [60] with a baseline value function for\nvariance reduction.\n28",
    "WARM: On the Benefits of Weight Averaged Reward Models\nC. Additional experiments\nC.1. 2\nnd\norder analysis: weight averaging for more robust ensembling\n0.00.51.0\n0.43\n0.44\n0.45\n0.46\nAcc.\nWA\nENS\nDiag\n0.00.51.0\n0.46\n0.47\n0.48\n0.49\n0.50\n0.00.51.0\n0.34\n0.36\n0.38\n0.40\n0.00.51.0\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.00.51.0\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\nAcc.\n0.00.51.0\n0.320\n0.325\n0.330\n0.335\n0.00.51.0\n0.370\n0.375\n0.380\n0.385\n0.390\n0.395\n0.00.51.0\n0.43\n0.44\n0.45\n0.46\nFigure 11|Train (corrupt). More results enriching Figure 4(a) with different pairs of RMs.\n0.00.51.0\n0.886\n0.888\n0.890\n0.892\n0.894\nAcc.\nWA\nENS\nDiag\n0.00.51.0\n0.890\n0.892\n0.894\n0.896\n0.00.51.0\n0.835\n0.840\n0.845\n0.850\n0.855\n0.00.51.0\n0.8800\n0.8825\n0.8850\n0.8875\n0.8900\n0.8925\n0.8950\n0.00.51.0\n0.890\n0.895\n0.900\n0.905\nAcc.\n0.00.51.0\n0.8425\n0.8450\n0.8475\n0.8500\n0.8525\n0.8550\n0.8575\n0.00.51.0\n0.878\n0.880\n0.882\n0.884\n0.886\n0.888\n0.00.51.0\n0.888\n0.890\n0.892\n0.894\n0.896\nFigure 12|Train (clean). More results enriching Figure 4(b) with different pairs of RMs.\n0.00.51.0\n0.7725\n0.7750\n0.7775\n0.7800\n0.7825\n0.7850\nAcc.\nWA\nENS\nDiag\n0.00.51.0\n0.760\n0.765\n0.770\n0.775\n0.00.51.0\n0.74\n0.75\n0.76\n0.77\n0.00.51.0\n0.74\n0.75\n0.76\n0.77\n0.78\n0.00.51.0\n0.74\n0.75\n0.76\n0.77\n0.78\nAcc.\n0.00.51.0\n0.770\n0.772\n0.774\n0.776\n0.778\n0.780\n0.00.51.0\n0.780\n0.785\n0.790\n0.00.51.0\n0.7700\n0.7725\n0.7750\n0.7775\n0.7800\n0.7825\n0.7850\nFigure 13|Validation (ID). More results enriching Figure 4(c) with different pairs of RMs.\n0.00.51.0\n0.710\n0.712\n0.714\n0.716\nAcc.\nWA\nENS\nDiag\n0.00.51.0\n0.704\n0.706\n0.708\n0.710\n0.712\n0.00.51.0\n0.700\n0.705\n0.710\n0.715\n0.720\n0.725\n0.00.51.0\n0.695\n0.700\n0.705\n0.710\n0.715\n0.720\n0.725\n0.00.51.0\n0.690\n0.695\n0.700\n0.705\n0.710\n0.715\nAcc.\n0.00.51.0\n0.718\n0.720\n0.722\n0.724\n0.726\n0.00.51.0\n0.718\n0.720\n0.722\n0.724\n0.726\n0.00.51.0\n0.708\n0.710\n0.712\n0.714\n0.716\n0.718\nFigure 14|Test (OOD). More results enriching Figure 4(d) with different pairs of RMs.\n29",
    "WARM: On the Benefits of Weight Averaged Reward Models\nC.2. BoN experiments\n0.20.40.60.81.01.2\nKL : log(N)\nN1\nN\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) PaLM (clean).\n0.20.40.60.81.01.2\nKL : log(N)\nN1\nN\n3.0\n3.5\n4.0\n4.5\n5.0\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) PaLM (corrupt).\n0123456\nKL : log(N)\nN1\nN\n3\n4\n5\n6\n7\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(c) T5 (clean).\n0123456\nKL : log(N)\nN1\nN\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(d) T5 (corrupt).\nFigure 15|Same as Figure 8, but withabsolute values of the control reward for BoN experiments. We\nconsider two SFT policies to generate candidate summaries: one based on PaLM architecture [98], the other\non T5 architecture [111]. In both cases, we observe thatWARMperforms better than ENS and the individual\nnetworks in terms of pointwise control RM.\n0.20.40.60.81.01.2\nKL : log(N)\nN1\nN\n0.00\n0.02\n0.04\n0.06\nControl reward gain\nWARM Baklava M = 2\nENS Baklava M = 2\nInd \n3\n Baklava\nInd \n1\n(a)Baklavawith PaLM.\n0123456\nKL : log(N)\nN1\nN\n0.00\n0.05\n0.10\n0.15\nControl reward gain\nWARM Baklava M = 2\nENS Baklava M = 2\nInd \n3\n Baklava\nInd \n1\n(b)Baklavawith T5.\nFigure 16|Control reward for BoN experiments(clean setup) withBaklavawhen the two fine-tunings휙\n1\nand휙\n3\nhave different featurizer initializations, collected respectively at steps 12k and 8k from a shared SFT.\n30",
    "WARM: On the Benefits of Weight Averaged Reward Models\n0.20.40.60.81.01.2\nKL : log(N)\nN1\nN\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nWin ratio vs. WARM \nM \n= 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(a) PaLM.\n0123456\nKL : log(N)\nN1\nN\n0.1\n0.2\n0.3\n0.4\n0.5\nWin ratio vs. WARM \nM \n= 6 with \nN \n= 1000\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(b) T5 vs.WARMw/푁=1000.\nFigure 17|Oracle preference metric for BoN experiments(clean setup). Figure 17(a) confirms Figure 7(c)\nbut on generations from PaLM SFT. Figure 17(b) shows win rates for BoN on T5 generations forWARMwith\n푀=6and always푁=1000for BoN vs. other RMs with1≤푁≤1000. We validate that BoN limits reward\nhacking compared to RL, as performances get better when increasing푁.\nC.3. RL experiments\nC.3.1. Experiments with corrupted preference dataset\n0200040006000800010000\n# steps\n4\n5\n6\n7\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\nFigure 18|RL experiments. Same as Figure 1(b) but with 25% corruption in the preference dataset.\n31",
    "WARM: On the Benefits of Weight Averaged Reward Models\nC.3.2. Experiments with clean preference dataset\n20003000400050006000\n# steps\n0.0\n0.2\n0.4\n0.6\n0.8\nWin ratio vs. WARM \nM \n= 6\nWARM M = 10\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(a)WARM푀=6.\n20003000400050006000\n# steps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nWin ratio of WARM \nM \n= 6 vs. Ind \n1\n= 0.01\n= 0.003\n= 0.001\ny=0.5\n(b) Impact of훼.\nFigure 19|Oracle preference metric for RL experimentsat fixed number of training steps (clean setup).\nFigure 19(a) plots the win rate of the policy withWARM푀=6vs. the other policies, all at the same number of\ntraining steps. Figure 19(b) shows the win rate ofWARM푀=6against the policy trained with a single RM휙\n1\n(the best according to OOD accuracy) along training for different values of훼controlling theKLregularization\nstrength.\n025005000750010000125001500017500\n# steps\n2\n3\n4\n5\n6\n7\n8\n9\n10\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) Control reward vs. training steps.\n0100200300400500600700\nKL\n2\n3\n4\n5\n6\n7\n8\n9\n10\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) Control reward vs.KL.\nFigure 20|Control reward for RL experimentswith훼=0.01(clean setup).\n32",
    "WARM: On the Benefits of Weight Averaged Reward Models\n02000400060008000\n# steps\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) Control reward vs. training steps.\n0500100015002000\nKL\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) Control reward vs.KL.\nFigure 21|Control reward for RL experimentswith훼=0.001(clean setup).\nC.4. Distillation experiments\nIn Figure 22 we reproduce the distillation setup from [17], where the control PaLM-XS RM generates\nthe labels to train PaLM-XXS RMs. As a side note, we observed that distillation changes the diversity\nacross fine-tuned RMs, thus potentially altering the significance of the distillation setup, motivating\nus in exploring the more realistic RLAIF setup.\n0123456\nKL : log(N)\nN1\nN\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\nFigure 22|BoN experiment in the distillation setup from [17]. The labels in the preference dataset are\ngiven by the control RM, the same RM which gives the푦-axis. The candidate summaries are generated by a\nSFT with the T5 architecture [111]. The blue lines representWARMwith푀weights:WARMperforms higher\nthan the individual RMs (in yellows) or when ensembling their predictions (ENS in red).\n33"
  ]
}