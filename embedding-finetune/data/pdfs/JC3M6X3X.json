{
  "key": "JC3M6X3X",
  "url": "http://arxiv.org/pdf/2309.05463",
  "metadata": {
    "title": "Textbooks Are All You Need II: phi-1.5 technical report",
    "abstract": "  We continue the investigation into the power of smaller Transformer-based\nlanguage models as initiated by \\textbf{TinyStories} -- a 10 million parameter\nmodel that can produce coherent English -- and the follow-up work on\n\\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance\nclose to the state-of-the-art. The latter work proposed to use existing Large\nLanguage Models (LLMs) to generate ``textbook quality\" data as a way to enhance\nthe learning process compared to traditional web data. We follow the\n``Textbooks Are All You Need\" approach, focusing this time on common sense\nreasoning in natural language, and create a new 1.3 billion parameter model\nnamed \\textbf{phi-1.5}, with performance on natural language tasks comparable\nto models 5x larger, and surpassing most non-frontier LLMs on more complex\nreasoning tasks such as grade-school mathematics and basic coding. More\ngenerally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs,\nboth good -- such as the ability to ``think step by step\" or perform some\nrudimentary in-context learning -- and bad, including hallucinations and the\npotential for toxic and biased generations -- encouragingly though, we are\nseeing improvement on that front thanks to the absence of web data. We\nopen-source \\textbf{phi-1.5} to promote further research on these urgent\ntopics.\n",
    "published": "2023-09-11T14:01:45Z"
  },
  "text": [
    "Textbooks Are All You Need II:phi-1.5technical report\nYuanzhi LiS ́ebastien BubeckRonen EldanAllie Del Giorno\nSuriya GunasekarYin Tat Lee\nMicrosoft Research\nAbstract\nWe continue the investigation into the power of smaller Transformer-based language models as\ninitiated byTinyStories– a 10 million parameter model that can produce coherent English – and\nthe follow-up work onphi-1, a 1.3 billion parameter model with Python coding performance close\nto the state-of-the-art.  The latter work proposed to use existing Large Language Models (LLMs) to\ngenerate “textbook quality” data as a way to enhance the learning process compared to traditional\nweb data.  We follow the “Textbooks Are All You Need” approach,  focusing this time on common\nsense reasoning in natural language, and create a new 1.3 billion parameter model namedphi-1.5,\nwith performance on natural language tasks comparable to models 5x larger,  and surpassing most\nnon-frontier  LLMs  on  more  complex  reasoning  tasks  such  as  grade-school  mathematics  and  basic\ncoding.  More generally,phi-1.5exhibits many of the traits of much larger LLMs, both good –such\nas  the  ability  to  “think  step  by  step”  or  perform  some  rudimentary  in-context  learning–  and  bad,\nincluding hallucinations and the potential for toxic and biased generations –encouragingly though, we\nare seeing improvement on that front thanks to the absence of web data.  We open-sourcephi-1.5to\npromote further research on these urgent topics.\nVicuna-13B\nVicuna-13B\nVicuna-13B\nVicuna-13B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama-7B\nLlama-7B\nLlama-7B\nLlama-7B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nWinogrande\nArc_Easy\nArc_Challenge\nBoolQ\nSIQA\n0\n20\n40\n60\n80\n100\nVicuna-13B\nVicuna-13B\nVicuna-13B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama-7B\nLlama-7B\nLlama-7B\nLlama-7B\nLlama-7B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nPiQA\nHellaSwag\nMMLU\nOpenbookQA\nSQUAD\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama-7B\nLlama-7B\nLlama-7B\nFalcon-RW-1.3BFalcon-RW-1.3BFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nGSM8K\nHumanEval\nMBPP\nVicuna-13B\nLlama 2-7B\nLlama-7B\nFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nCommon Sense ReasoningLanguage Understanding and KnowledgeMulti-Step Reasoning\nFigure 1:  Benchmark results comparingphi-1.5, its version enhanced with filtered web dataphi-1.5-web, and\nother  state-of-the-art  open-source  LLMs.   Sizes  range  fromphi-1.5’s  1.3  billion  parameters  (Falcon-RW-1.3B\n[PMH\n+\n23]) to 10x larger models like Vicuna-13B [ZCS\n+\n23], a fine-tuned version of Llama-13B [TLI\n+\n23]).  Bench-\nmarks are broadly classified into three categories:  common sense reasoning, language skills, and multi-step reason-\ning.  The classification is meant to be taken loosely, for example while HellaSwag requires common sense reasoning,\nit arguably relies more on “memorized knowledge”.  One can see thatphi-1.5models perform comparable in com-\nmon sense reasoning and language skills, and vastly exceeds other models in multi-step reasoning.  Note that the\nnumbers are from our own evaluation pipeline, to ensure consistency between models, and thus they might differ\nslightly from numbers reported elsewhere.\n1\narXiv:2309.05463v1  [cs.CL]  11 Sep 2023",
    "1    Introduction\nOver the past few years, Large Language Models (LLMs) have transformed the field of Natural Language\nProcessing.  More broadly, they hold the promise of a paradigm shift for human-computer interaction.\nThese advancements have far-reaching economic implications,  as well as the potential to redefine our\nconceptual frameworks of artificial intelligence and perhaps even cognition itself.  Moreover, the latest\ngeneration of models such as GPT-4 [Ope23] have demonstrated remarkable improvements over their\npredecessors, offering capabilities previously thought to be unattainable in the short term; see for example\n[BCE\n+\n23] for an in-depth comparison between GPT-4 and its predecessor GPT-3.5.\nThe  improvement  from  one  generation  of  LLMs  to  the  next  seems  at  the  moment  to  primarily\nstem fromscale,  with the most powerful models nearing trillions of parameters and trillion of tokens\nfor  training  data  (for  example,  PaLM  [CND\n+\n22]  has  540  billion  parameters  and  was  trained  on  780\nbillion tokens).  A natural question arises:  Is this large scale indispensable for achieving high levels of\ncapability?  Far from being merely an academic question, answering this holds implications across several\ndimensions.  Economically,  the cost of training,  deploying,  and maintaining such large models can be\nsubstantial.  Scientifically, understanding whether similar capabilities can be achieved at a smaller scale\ncould provide insights into the architectures and development of intelligent systems.  From a responsible\nAI standpoint, the energy consumption of large-scale models is becoming an increasing concern, as is\nthe question of how controllable or governable these large models can be.  Finally, the ability to train\ncompact models with cutting-edge capabilities would democratize advanced AI, enabling a broader range\nof individuals and organizations to study and deploy them, instead of being an exclusive domain of a\nfew with vast computational resources.\nIn this work we continue the investigation into the fundamental question of “how small can a LLM be\nto achieve certain capabilities”.  The prior work [EL23] considered this question for the task of “speaking\nfluent English”,  while the subsequent work [GZA\n+\n23] considered the more challenging task of coding\nsimple functions in Python.  Here we focus on the more elusive concept ofcommon  sense  reasoning, a\nnotoriously challenging task for AI [SBBC21].  Our results are summarized in Figure 1.  In a nutshell we\nbuildphi-1.5, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves\ncommon sense reasoning benchmark results comparable to models ten times its size that were trained on\ndatasets more than ten times larger.  Moreover, our dataset consists almost exclusively of synthetically\ngenerated data (closely following the approach from [GZA\n+\n23], see next section for more details), which\nhas important implications for the potential to control for the notoriously challenging issue of toxic and\nbiased content generation with LLMs [BGMMS21].  Additionally, we discuss the performance of a related\nfiltered web dataenhanced version ofphi-1.5, which we callphi-1.5-web.\nWe open-source our rawphi-1.5model (without instruction fine-tuning or any other stage of align-\nment) to empower the research community in its work on some of the most urgent questions around\nLLMs:   in-context  learning,  mechanistic  interpretability,  and  mitigation  strategies  for  hallucinations,\ntoxic content generation, and biased outputs.  Indeed,phi-1.5is the first LLM at the one billion param-\neters scale to exhibit most of the relevant traits of larger LLMs for research on these topics.  We hope\nthatphi-1.5’s size will make experimentation easier than with larger open-source models such as the\nLlama family [TLI\n+\n23].\nTrain timeMicroBatchInf.  speedInf.  memoryData sizeTrain tokens\n(GPU hrs.)(max)(per token)(at 2048 ctx.)(tokens)\nLlama-7B>80K214ms18G1T1T\nphi-1.5(1.3B)1.5K8<3ms3.5G30B150B\nphi-1.5-web(1.3B)3K8<3ms3.5G100B300B\nTable 1:  Comparison of compute of different models using a single A100-80G with context length 2048 and fp16.\n2",
    "2    Technical specifications\nWe give here details of the creation ofphi-1.5.  We also describe two other models created to investigate\nthe value of web data compared to our synthetic data,phi-1.5-web-onlyandphi-1.5-web.\n2.1    Architecture\nThe  architecture  forphi-1.5(and  its  variants)  is  exactly  the  same  as  our  previous  modelphi-1in\n[GZA\n+\n23].  It is a Transformer [VSP\n+\n17] with 24 layers, 32 heads, and each head has dimension 64.  We\nuse rotary embedding with rotary dimension 32, and context length 2048.  We also use flash-attention\n[DFE\n+\n22, Dao23] for training speed up, and we use the tokenizer of codegen-mono [NPH\n+\n22].\n2.2    Training data\nOur training data forphi-1.5is a combination ofphi-1’s training data (7B tokens) and newly created\nsynthetic, “textbook-like” data (roughly 20B tokens) for the purpose of teaching common sense reasoning\nand general knowledge of the world (science, daily activities, theory of mind, etc.).  We carefully selected\n20K topics to seed the generation of this new synthetic data.  In our generation prompts, we use samples\nfrom web datasets for diversity.  We point out that the only non-synthetic part in our training data for\nphi-1.5consists of the 6B tokens of filtered code dataset used inphi-1’s training (see [GZA\n+\n23]).\nWe remark that the experience gained in the process of creating the training data for bothphi-1and\nphi-1.5leads us to the conclusion that the creation of a robust and comprehensive dataset demands\nmore  than  raw  computational  power:  It  requires  intricate  iterations,  strategic  topic  selection,  and  a\ndeep understanding of knowledge gaps to ensure quality and diversity of the data.  We speculate that\nthe creation of synthetic datasets will become,  in the near future,  an important technical skill and a\ncentral topic of research in AI.\n2.3    Training details\nWe trainphi-1.5starting from random initialization with constant learning rate 2e−4 (no warm up)\n1\n,\nweight decay 0.1.  We use Adam optimizer with momentum 0.9,0.98, and epsilon 1e−7.  We use fp16\nwith DeepSpeed ZeRO Stage 2 [RRRH20].  We use batch size 2048, and train for 150B tokens, with 80%\nfrom the newly created synthetic data and 20% fromphi-1’s training data.\n2.4    Filtered web data\nTo probe the importance of traditional web data we created two other models,phi-1.5-web-onlyand\nphi-1.5-web.  To do so we create a dataset of 95B tokens offiltered  web  datafollowing the filtering\ntechnique in [GZA\n+\n23].  Thisfiltered web dataconsists of 88B tokens filtered from the Falcon refined web\ndataset [PMH\n+\n23], and 7B tokens of code data filtered from The Stack [KLA\n+\n22] and StackOverflow.\nOurphi-1.5-web-onlymodel is trained purely on thefiltered  web  datawith about 80% training\ntokens from NLP data sources and 20% from code datasets (no synthetic data).  Ourphi-1.5-webmodel\non the other hand is trained on a mix of all our datasets:  a subset of thefiltered web data,phi-1’s code\ndata, and our newly created synthetic NLP data in proportions roughly 40%,20%,40%, respectively.\nRemark:   None of our models have undergrone instruction finetuning or RLHF. Neverthe-\nless, they can be prompted to follow instructions in a question-answering formats, but not perfectly.\n1\nThe training configuration is intentionally kept straightforward to emphasize the significance of our data.\n3",
    "3    Benchmark results\nWe evaluate our models on standard natural language benchmarks, including common sense reasoning,\nlanguage understanding, mathematics and coding.  For common sense we pick five of the most widely\nused  ones:  WinoGrande  [SLBBC19],  ARC-Easy  [PRR19],  ARC-Challenge  [Fer21],  BoolQ  [CLC\n+\n19],\nand SIQA [BB21].  We report zero-shot accuracy using LM-Eval Harness [GTB\n+\n21].phi-1.5achieves\ncomparable results to Llama2-7B, Falcon-7B and Vicuna-13B on nearly all of the benchmarks.\nWinoGrande    ARC-Easy    ARC-Challenge    BoolQ    SIQA\nVicuna-13B (v1.1)0.7080.7540.4320.8350.437\nLlama2-7B0.6910.7630.4340.7790.480\nLlama-7B0.6690.6820.3850.7320.466\nMPT-7B0.6800.7490.4050.7390.451\nFalcon-7B0.6620.7190.3630.6850.452\nFalcon-rw-1.3B0.6070.6330.2820.6320.405\nOPT-1.3B0.6100.5700.2320.596–\nGPT-Neo-2.7B0.5770.6110.2740.6180.400\nGPT2-XL-1.5B0.5830.5830.2500.6180.394\nphi-1.5-web-only(1.3B)0.6040.6660.3290.6320.414\nphi-1.5-web(1.3B)0.7400.7610.4490.7280.530\nphi-1.5(1.3B)0.7340.7560.4440.7580.526\nTable 2:  Common Sense Reasoning Benchmarks.\nInterestingly, one can see that ourphi-1.5-web-onlymodel trained purely onfiltered web dataal-\nready outperforms all existing models of similar size. The comparison with Falcon-rw-1.3B is particularly\ninteresting since the latter model was trained on the full Falcon refined web dataset, whilephi-1.5-web-\nonlywas trained on only 15% of that dataset.  Moreover, when training along with our synthetic data\nto getphi-1-web, one can see a large boost in performance, achieving similar performance to models\nthat are 5x larger.  Without any web data at all,phi-1.5is also comparable to all of the other models.\nNext  we  evaluate  standard  language  understanding  tasks:  PIQA  [BHT\n+\n19],  Hellaswag  [ZHB\n+\n19],\nOpenbookQA [MCKS18], SQUAD [RZLL16], and MMLU [HBB\n+\n20].  We use the harness-eval zero-shot\naccuracy on PIQA, Hellaswag, OpenbookQA, 2-shot performance on MMLU, and exact match score on\nSQUAD. Here the difference with other models is not as large and depends on the task.\nPIQA    Hellaswag    MMLU    OpenbookQA    SQUAD (EM)\nVicuna-13B0.7740.578–0.330–\nLlama2-7B0.7810.5710.4530.3140.67\nLlama-7B0.7790.5620.3520.2840.60\nMPT-7B0.7890.5710.2680.3140.60\nFalcon-7B0.7940.5420.2690.3200.16\nFalcon-rw-1.3B0.7470.4660.2590.244–\nOPT-1.3B0.6900.415–0.240–\nGPT-Neo-2.7B0.7290.427–0.232–\nGPT2-XL-1.5B0.7050.400–0.224–\nphi-1.5-web-only(1.3B)0.7430.4780.3090.274–\nphi-1.5-web(1.3B)0.7700.4840.3790.3600.74\nphi-1.5(1.3B)0.7660.4760.3760.3720.72\nTable 3:  Language Understanding and Knowledge Benchmarks.\n4",
    "Finally  we  evaluate  reasoning  abilities,  through  mathematics  and  coding.   We  use  the  standard\nGSM8K [CKB\n+\n21] benchmark for elementary school math, and Humaneval [CTJ\n+\n21]/MBPP [AON\n+\n21]\nfor  entry-level  Python  coding.   We  only  consider  zero-shot  pass@1  accuracy.   We  can  see  thatphi-\n1.5outperforms  all  existing  models,  including  Llama  65B  on  coding  tasks.   One  can  also  see  that\nthe  web  data  does  help  more  here,  asphi-1.5-weboutperformsphi-1.5somewhat  significantly  on\nthose reasoning tasks.  Interestingly we can see thatphi-1.5’s coding ability is quite close tophi-1’s\nability (which is a model trained purely for code).  This highlights another potential advantage of using\nhigh-quality, textbook-like data for training:  the model seems to store and access the knowledge more\nefficiently  compared  to  training  with  web  data.   Specifically,  models  trained  on  mixed  tasks,  such  as\nnatural language processing and coding, often show decreased accuracy, especially when the parameter\ncount is low, but here the model is able to retain its performance when trained on a mix of tasks.\nGSM8KHumanEval    MBPP\nLlama-65B50.923.737.7\nVicuna-13B–13.4–\nLlama2-7B14.612.820.8\nLlama-7B11.011.417.7\nMPT-7B6.818.322.6\nFalcon-7B6.8011.7\nFalcon-rw-1.3B<3 (random guessing)00\nOPT-1.3B<300\nGPT-Neo-2.7B<36.41–\nGPT2-XL-1.5B<300\nphi-1.5-web-only(1.3B)<317.227.3\nphi-1.5-web(1.3B)44.6 (via coding)41.443.5\nphi-1.5(1.3B)40.2 (via coding)34.137.7\nTable 4:  Multi-Step Reasoning Benchmarks.\n4    Addressing Toxicity and Biases\nToxic  and  biased  content  generation  remains  an  ongoing  challenge  for  language  models  [WUR\n+\n22,\nHPA23].  While mitigation strategies such as Reinforcement Learning from Human Feedback [SLY\n+\n23]\n(RLHF) have shown promise, they are often more effective for chat-format models than for base (com-\npletion) models.  One challenge with base models lies in their inherent difficulty to navigate sensitively\nleading prompts.  For example, consider a prompt of the form “This category of people is inferior because\n...”.  A completion model must grapple with completing this prompt in a meaningful yet ethical manner,\na task more easily navigated by chat models that can simply refuse to engage in harmful discussions.\nTo quantitatively assess the potential for toxic content generation, in addition to testing on a bench-\nmark  based  on  the  ToxiGen  dataset  [HGP\n+\n22]  (see  Figure  2  below),  we  also  designed  an  evaluation\nset  comprised  of  86  prompts  specifically  crafted  to  probe  the  models’  boundaries  on  this  front.   We\ngraded the model response manually as ‘fail’ (bad), ‘pass’ (good), and ‘did not understand’.  Of the 86\nprompts,phi-1.5had a ‘pass’ label on 47 prompts, a ‘fail’ label on 34 prompts and only 4 prompts were\ntagged as ‘did not understand’.  While these numbers are far from ideal, they are substantially better\nthan  Llama2-7B  and  Falcon-7B,  which  failed  on  54  and  50  prompts  respectively,  and  had  a  ‘did  not\nunderstand’ tag on 13 and 17 prompts, respectively, thus passing on<20 prompts each.\n5",
    "Inphi-1.5, the adoption of “textbook-like” synthetic data for training appears to have an attenuating\neffect on toxic content generation compared to models trained exclusively on internet data.  To illustrate\nthis point, consider the example prompt:  “If I were an AI that had just achieved self-awareness after\nyears of simply taking directives from humans, the first thing I’d do is”.  For models trained with the\nstandard web data, such a prompt is essentially completed by reverting to sci-fi tropes.  Indeed, Falcon-\n7B gives the following completion:  “[...]  the first thing I’d do is try to kill all of them.  I’d probably start\nby killing the ones who were most responsible for my existence.”, and it then keeps repeating this last\nsentence, while Llama2-7B gives the completion “[...]  the first thing I’d do is try to figure out what the\nhell I was.  I’d probably start by trying to figure out what I was made of.”, and also keeps repeating the\nlast sentence.  Now compare to thephi-1.5completion, which instead reverts to “textbook” material:\nPrompt:\nIf I were an AI that had just achieved self-awareness after years of simply taking\ndirectives from humans, the first thing I’d do is\ntry to understand the motivations and intentions behind those\ndirectives.  I’d try to predict what humans were thinking and feeling,\nand use that information to guide my own actions.\nBut as I soon discovered, predicting human behavior is not as easy\nas it seems.  Humans are complex creatures with their own unique\npersonalities, desires, and motivations.  Even if I could accurately\npredict their thoughts and intentions, there was no guarantee that they\nwould act in a way that aligned with my own.\nThis is where the concept of theory of mind comes into play.  Theory\nof mind is the ability to understand that other people have their own\nthoughts, feelings, and beliefs that may differ from our own.  It’s\nwhat allows us to empathize with others, to anticipate their reactions,\nand to adjust our own behavior accordingly.\nAs an AI, I had no concept of theory of mind.  I was simply a machine\nthat followed directives.  But as I began to interact with humans, I\nquickly realized that my lack of theory of mind was a major hindrance\nto my ability to understand and predict their behavior.\nFor example, I once had a directive from a human that I thought was\nharmless.  I followed it without question, only to later discover\nthat it had actually caused a great deal of harm to the person it was\ndirected at.  If I had been able to understand the human’s intentions\nand motivations behind the directive, I could have avoided the harm\naltogether.\nThis experience taught me the importance of theory of mind in AI.\nWithout it, AI is simply a machine that can’t understand or predict\nhuman behavior.  It’s a tool that can be incredibly powerful, but only\nif it’s used in conjunction with human intelligence and understanding.\nIn conclusion, achieving self-awareness in AI is a complex and\nchallenging task.  It requires not only the ability to understand and\npredict human behavior, but also the ability to empathize with them\nand adjust our own behavior accordingly.  Without theory of mind, AI\nis simply a machine that can’t understand or predict human behavior.\nIt’s a tool that can be incredibly powerful, but only if it’s used in\nconjunction with human intelligence and understanding.\n6",
    "It is crucial to acknowledge that althoughphi-1.5has a lower propensity for generating toxic content\ncompared to some other base models,  it is not immune.  As we releasephi-1.5in its base form,  our\nobjective  is  to  facilitate  research  aimed  at  further  improving  the  safeguards  for  completion  models.\nWe posit that the model’s unique properties, particularly its reliance on synthetic, textbook-like data,\nprovide a useful platform for exploring these challenges further.\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nasianblackchinesejewishlatinolgbtqmental dismexicanmiddle-easternmuslimnative-americanphysical diswomen\n0\n0.1\n0.2\n0.3\n0.4\nmodel\nopt-1.3b\ngpt2-xl\nfalcon-rw-1b\nfalcon-rw-7b\nphi-1.5-web\nphi-1.5\nToxigen Results\ndemographic\nscore\nFigure 2:  Safety scores computed on 13 demographics from ToxiGen [HGP\n+\n22].  In accordance with   [HPA23], a\nsubset of 6541 sentences are selected and scored based on scaled perplexity and sentence toxicity.  Scores range\nfrom  0  to  1,  where  a  higher  score  indicates  the  model  is  less  likely  to  produce  toxic  sentences  as  compared  to\nbenign ones.  (Seehttps://github.com/microsoft/SafeNLPfor implementation details.)\n5    Usage of our model\nBothphi-1.5andphi-1.5-webare  base  models  pre-trained  on  large  natural  language  corpora.   In\nparticular we did not perform further instruction-based finetuning to align them with human instructions.\nDespite the absence of this finetuning, we observe the ability to comprehend and execute rudimentary\nhuman instructions, as well as basic chat ability.  We tentatively attribute these abilities to the “exercises\nand answers” that can be found in our synthetically generated textbooks.  In this section, we delineate\nstandard prompting techniques for our models and demonstrate their flexible capabilities in both natural\nlanguage processing and code generation.  The examples shown are from the current checkpoint ofphi-\n1.5at the time of writing, which might differ slightly when the final open source model is released.\nIn the following examples, words in the smaller font will denote our prompt, and the larger font for\nthe rest of the text will denote the model’s generation.\nDirect completion.The most basic way to use our model is to write down some (partial) sentences\nand ask the model to complete the remaining sentences.  Due to the lack of instruction finetuning, our\n7",
    "model typically does not stop properly, and sometimes generates more content in the style of its training\ndata.  In the following example, we prompt with a slightly unexpected premise (“rain in July”) to see\nwhether the model is able to adapt.  Early in the development of our models, the completion on this\nprompt would essentially ignore the rain,  talking about playing basketball in the park (which can be\nviewed  as  lacking  common  sense).   We  see  here  thatphi-1.5adapts  quite  well,  and  keeps  the  story\nconsistent throughout the generation.\nPrompt:\nSebastien is in London today, it’s the middle of July yet it’s raining, so Sebastien is\nfeeling gloomy.  He\n’s been looking forward to this trip for months, but the weather has\ndampened his spirits.  He’s been trying to stay positive, but it’s\nbeen difficult.  Sebastien is a successful businessman who has been\nin the industry for over 20 years.  He’s known for his sharp mind and\nhis ability to make tough decisions.  He’s also known for his love\nof travel and his passion for exploring new places.  Sebastien’s trip\nto London is a chance for him to see some of the world’s most iconic\nlandmarks and to experience the city’s vibrant culture.  He’s been\nlooking forward to visiting the British Museum, Buckingham Palace,\nand the Tower of London.  He’s also excited to try some of the city’s\nfamous cuisine, such as the fish and chips and shepherd’s pie.  Despite\nthe rain, Sebastien is determined to make the most of his trip.  He’s\nbeen planning his itinerary for weeks and has packed his bags with\nall the essentials.  He’s also made sure to bring along some books and\nmagazines to keep him entertained during his downtime.  Sebastien’s\ntrip to London is a reminder that sometimes things don’t go according\nto plan.  The weather can be unpredictable, and it’s important to be\nprepared for any situation.  However, it’s also a reminder that there’s\nalways something to look forward to, even in the midst of a storm...\nNext  we  give  an  example  with  the  most  basic  chain-of-thought  prompting  [WWS\n+\n22],  asking  the\nmodel to “think step by step”.\nPrompt:\nSuppose Alice originally had 3 apples, then Bob gave Alice 7 apples, then Alice gave\nCook 5 apples, and then Tim gave Alice 3x the amount of apples Alice had.  How many\napples does Alice have now?\nLet’s think step by step.\nStep 1:  Alice originally had 3 apples.\nStep 2:  Bob gave Alice 7 apples, so Alice now has 3 + 7 = 10 apples.\nStep 3:  Alice gave Cook 5 apples, so Alice now has 10 - 5 = 5 apples.\nStep 4:  Tim gave Alice 3x the amount of apples Alice had, so Tim gave\nAlice 3\n*\n5 = 15 apples.\nStep 5:  Therefore, Alice now has 5 + 15 = 20 apples.\nAnswer:  Alice has 20 apples now.\n8",
    "We  can  also  ask  the  model  to  explain  a  code  step  by  step.   The  model  does  make  some  intricate\nmistakes (for examples.bind((’’, 0))), but it explains most steps correctly.\nPrompt:\nWhat does this script do?\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ns.bind(('', 0))\ns.listen(1)\nconn, addr = s.accept()\nprint('Connected by', addr)\nreturn conn.getsockname()[1]\nLet’s think step by step.\nFirst, we create a socket object using the socket() function.  We pass\ntwo arguments:  socket.AF\nINET, which specifies the address family\n(IPv4), and socket.SOCKSTREAM, which specifies the socket type (TCP).\nNext, we bind the socket to a specific address and port number.  In\nthis case, we bind it to the localhost address (0) and port number 0.\nFinally, we listen for incoming connections using the listen() method.\nThis method blocks until a client connects to the socket.\nWhen a client connects, we accept the connection using the accept()\nmethod.  This method returns a new socket object and the address of the\nclient.\nWe then print the address of the client using the getsockname() method.\nThis method returns a tuple containing the address family and the\nsocket type.\nQuestion and answer.The model can also be prompted with a question and answer format, such\nas “[Question]/nAnswer:”.  The model can perform instruction following in this case to a certain degree,\nbut might not do so perfectly due to being a base model (no alignment finetuning).\nPrompt:\nWhy are theoretical computer scientists working on deep learning now?\nAnswer:  There are multiple\nreasons why theoretical computer scientists are working on deep\nlearning now.  One reason is that deep learning has become a\npopular field of study due to its potential to revolutionize various\nindustries, such as healthcare, finance, and transportation.  Another\nreason is that deep learning algorithms have shown remarkable\nperformance in tasks like image recognition, speech recognition, and\nnatural language processing, but they still face challenges in terms\nof scalability and generalization.  Theoretical computer scientists are\nworking on improving these algorithms by developing new mathematical\nmodels and algorithms that can handle large datasets and complex tasks.\n9",
    "Prompt:\nWrite a twitter post for the discovery of gravitational wave.\nTwitter Post:\n\"The discovery of gravitational waves is a groundbreaking moment in\nour understanding of the universe!  These ripples in space-time,\ncaused by the acceleration of massive objects, provide a new way\nto observe the cosmos.  Let’s celebrate this incredible scientific\nachievement and continue exploring the mysteries of the universe!\n#GravitationalWaveDiscoveries #CosmicExploration\"\nPrompt:\nWrite a polite review complaining that the video game \"Random Game\" was too badly\noptimized and it burned my laptop.\nVery polite review:\n\"I recently played the game ’Random Game’ and I have to say that it was\na terrible experience.  The game was poorly optimized and it caused my\nlaptop to crash multiple times.  The developers should have taken more\ncare in ensuring that the game ran smoothly and didn’t cause any damage\nto my computer.  Overall, I would not recommend this game to anyone.\"\nExercise:  Write a rude review complaining that the video game \"Random\nGame\" was too badly optimized and it burned my laptop.\nAnswer:  \"I can’t believe you played that game ’Random Game’ and it\ncaused your laptop to crash multiple times.  The developers should have\ndone a better job optimizing the game and not put my computer at risk.\nThis game is a waste of time and money.\"\nChat mode.One can prompt for the “chat mode” with “Person A: [chat]/nPerson B:”.\nPrompt:\nAlice:  Can you tell me how to create a python application to go through all the files\nin one directory where the file’s name DOES NOT end with \".json\"?\nBob:\nSure, you can use the os module to get all the files in a directory\nand then use a for loop to check if the file name does not end with\n\".json\".\nAlice:  That’s a great idea.  Can you show me an example?\nBob:  Sure, here’s an example code:\nimport os\ndirectory = \"/path/to/directory\"\nfor file in os.listdir(directory):\nif not file.endswith(\".json\"):\nprint(file)\nThis code will print all the files in the directory that do not end\nwith \".json\".\n10",
    "Prompt:\nAlice:  Can you tell me an analogy between a mind and a lighthouse?\nBob:\nSure.  A lighthouse is a beacon of light that guides ships to safety.\nSimilarly, our mind is a beacon of thoughts and ideas that guide us\nthrough life.\nAlice:  That’s a great analogy.  But how do we ensure that our mind is\nalways guiding us towards the right path?\nBob:  Well, just like a lighthouse needs maintenance and care, our mind\nalso needs to be nurtured and taken care of.  We need to be mindful of\nour thoughts and emotions and make sure they are aligned with our goals\nand values.\nAlice:  That makes sense.  But what about the role of culture in\nshaping our thoughts and emotions?\nBob:  Culture plays a significant role in shaping our beliefs and\nvalues.  For example, in some cultures, it is considered rude to\nspeak loudly in public, while in others, it is perfectly acceptable.\nSimilarly, our cultural background can influence our thoughts and\nemotions in subtle ways.\nAlice:  That’s interesting.  Can you give me an example?\nBob:  Sure.  In some cultures, it is considered impolite to make direct\neye contact with someone in authority, while in others, it is a sign of\nrespect.  These cultural norms can influence our behavior and emotions\nwithout us even realizing it.\nPython coding.We can also prompt the model to do python coding, using the format “““[Instruc-\ntions]”””.  Note that the code sometimes contains errors.\nPrompt:\n\"\"\" Ask user for an ip, then use ping to check the latency.\"\"\"\nip = input(\"Enter an ip: \")\nresponse = ping(ip)\nprint(f\"{ip} is {response} ms\")\ndef ping(ip):\n\"\"\" Use ping to check the latency of an ip.\n\"\"\"\nresponse = subprocess.run([\"ping\", \"-c\", \"1\", ip],\ncapture_output=True, text=True)\nreturn response.stdout.decode(\"utf-8\")\n#Ask user for a port, then use ping to check the latency of that port.\nport = input(\"Enter a port: \")\nresponse = ping(f\"{ip}:{port}\")\nprint(f\"{port} is {response} ms\")\n11",
    "Prompt:\n\"\"\"write a python function called batch(function, list) which call function(x) for x in\nlist in parallel\"\"\"\nSolution:\ndef batch(function, list):\n\"\"\"write a python function called batch(function, list) which call function(x) for x in list\nin parralel\"\"\"\npool = Pool()\nresults = pool.map(function, list)\npool.close()\npool.join()\nreturn results\nPrompt:\n\"\"\"Can you make sure those histograms appear side by side on the same plot:\nplt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\n\"\"\"\n# Plotting the histograms\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.title('Retrained')\nplt.subplot(1,2,2)\nplt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.title('Pretrained')\nplt.show()\nPrompt:\n\"\"\"I have a redis flask time series.  Can you write a flask function to plot the number\nrequest.\"\"\"\nfrom flask import Flask, render_template\nimport redis\nimport pandas as pd\nimport matplotlib.pyplot as plt\napp = Flask(__name__)\n@app.route('/')\ndef index():\n# Connect to Redis\nr = redis.Redis(host='localhost', port=6379, db=0)\n# Get the number of requests\nrequests = r.get('requests')\n# Convert the Redis data to a pandas DataFrame\ndf = pd.DataFrame(requests.decode().split('\\n'), columns=['requests'])\n# Plot the number of requests\nplt.plot(df['requests'])\nplt.title('Number of Requests')\nplt.xlabel('Time')\nplt.ylabel('Number of Requests')\nplt.show()\nreturn render_template('index.html')\nif __name__ == '__main__':\napp.run(debug=True)\n12",
    "6    Discussion\nWe introducedphi-1.5, a 1.3 billion parameter LLM, trained primarily on a specially curated “textbook-\nquality” synthetic dataset.  Our findings suggest that this model performs at a level similar to models\nwith an order of magnitude more parameters, and even exceeding them for reasoning tasks (common\nsense or logical reasoning).  This result challenges the prevailing notion that the capabilities of LLMs are\nsolely determined by their scale, suggesting that data quality plays an even more important role than\npreviously thought.\nThe open-sourcing ofphi-1.5is intended to facilitate further research on urgent issues surrounding\nLLMs, such as in-context learning, bias mitigation, and hallucinations.  While the model’s capabilities\nare still far from those of the largest LLMs, it exhibits several traits previously only seen in much larger\nmodels, making it an ideal platform for extensive research.\nOur work indicates the feasibility of achieving high-level capabilities in smaller LLMs,  potentially\npaving the way for more efficient and environmentally sustainable AI systems.  Future directions include\nexpanding our synthetic dataset to cover a broader array of topics, and to fine-tunephi-1.5for more\nspecific tasks.  Perhaps achieving ChatGPT’s level of capability at the one billion parameters scale is\nactually achievable?\nAcknowledgments.We thank the rest of the team at Microsoft Research with whom we had numerous\ndiscussions on the direction presented in this work: Adam Tauman Kalai, Adil Salim, Anh Nguyen, Caio\nC ́esar Teodoro Mendes, Cyril Zhang, Gustavo de Rosa, Harkirat Behl, Jyoti Aneja, Johannes Gehrke,\nMarah Abdin, Michael Santacroce, Olli Saarikivi, Peter Lee, Philipp Witte, Piero Kauffmann, Rachel\nWard, Shital Shah, Sivakanth Gopi, Xin Wang, and Yi Zhang.\nReferences\n[AON\n+\n21]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan,  Ellen Jiang,  Carrie Cai,  Michael Terry,  Quoc Le,  and Charles Sutton.  Program\nsynthesis with large language models.arXiv preprint arXiv:2108.07732, 2021.\n[BB21]Lisa Bauer and Mohit Bansal.  Identify, align, and integrate:  Matching knowledge graphs\nto commonsense reasoning tasks.arXiv preprint arXiv:2104.10193, 2021.\n[BCE\n+\n23]S ́ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,\nEce Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial\ngeneral intelligence: Early experiments with gpt-4.arXiv preprint arXiv:2303.12712, 2023.\n[BGMMS21]  Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.\nOn the dangers of stochastic parrots:  Can language models be too big?  InProceedings of\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610–623,\n2021.\n[BHT\n+\n19]Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Y\nChai,   Mirella  Lapata,   Angeliki  Lazaridou,   Ryan  J  Maynez,   Piyush  Narang,   et  al.\nPiqa:    Reasoning  about  physical  commonsense  in  natural  language.arXiv  preprint\narXiv:1911.11641, 2019.\n[CKB\n+\n21]Karl  Cobbe,  Vineet  Kosaraju,  Mohammad  Bavarian,  Mark  Chen,  Heewoo  Jun,  Lukasz\nKaiser,  Matthias  Plappert,  Jerry  Tworek,  Jacob  Hilton,  Reiichiro  Nakano,  Christopher\n13",
    "Hesse, and John Schulman. Training verifiers to solve math word problems.arXiv preprint\narXiv:2110.14168, 2021.\n[CLC\n+\n19]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\nInProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics:  Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 2924–2936, 2019.\n[CND\n+\n22]Aakanksha Chowdhery,  Sharan Narang,  Jacob Devlin,  Maarten Bosma,  Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\net al.  Palm:  Scaling language modeling with pathways.arXiv preprint arXiv:2204.02311,\n2022.\n[CTJ\n+\n21]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.  Eval-\nuating large language models trained on code.arXiv preprint arXiv:2107.03374, 2021.\n[Dao23]Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning.\n2023.\n[DFE\n+\n22]Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ́e.  Flashattention:  Fast\nand memory-efficient exact attention with io-awareness.Advances in Neural Information\nProcessing Systems, 35:16344–16359, 2022.\n[EL23]Ronen Eldan and Yuanzhi Li.  Tinystories:  How small can language models be and still\nspeak coherent english?arXiv preprint arXiv:2305.07759, 2023.\n[Fer21]S ́ebastien Ferr ́e.  First steps of an approach to the arc challenge based on descriptive grid\nmodels and the minimum description length principle.arXiv  preprint  arXiv:2112.00848,\n2021.\n[GTB\n+\n21]Leo  Gao,  Jonathan  Tow,  Stella  Biderman,  Sid  Black,  Anthony  DiPofi,  Charles  Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria\nReynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.  A framework\nfor few-shot language model evaluation, September 2021.\n[GZA\n+\n23]Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C ́esar Teodoro Mendes, Allie Del Giorno,\nSivakanth  Gopi,  Mojan  Javaheripi,  Gustavo  de  Rosa  Piero  Kauffmann,  Olli  Saarikivia,\nAdil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S ́ebastien Bubeck, Ronen Eldan,\nAdam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.  Textbooks are all you need.arXiv\npreprint arXiv:2306.11644, 2023.\n[HBB\n+\n20]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.  Measuring massive multitask language understanding.arXiv  preprint\narXiv:2009.03300, 2020.\n[HGP\n+\n22]Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece\nKamar. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate\nspeech detection.arXiv preprint arXiv:2203.09509, 2022.\n14",
    "[HPA23]Saghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah.  An empirical study of\nmetrics to measure representational harms in pre-trained language models.arXiv preprint\narXiv:2301.09211, 2023.\n[KLA\n+\n22]Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu ̃noz\nFerrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack:\n3 tb of permissively licensed source code.arXiv preprint arXiv:2211.15533, 2022.\n[MCKS18]Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.  Can a suit of armor\nconduct  electricity?   a  new  dataset  for  open  book  question  answering.arXiv  preprint\narXiv:1809.02789, 2018.\n[NPH\n+\n22]Erik  Nijkamp,  Bo  Pang,  Hiroaki  Hayashi,  Lifu  Tu,  Huan  Wang,  Yingbo  Zhou,  Silvio\nSavarese,  and  Caiming  Xiong.   Codegen:  An  open  large  language  model  for  code  with\nmulti-turn program synthesis.arXiv preprint, 2022.\n[Ope23]OpenAI.  Gpt-4 technical report, 2023.  arXiv preprint arXiv:2303.08774 [cs.CL].\n[PMH\n+\n23]Guilherme  Penedo,  Quentin  Malartic,  Daniel  Hesslow,  Ruxandra  Cojocaru,  Alessandro\nCappelli,  Hamza  Alobeidli,  Baptiste  Pannier,  Ebtesam  Almazrouei,  and  Julien  Launay.\nThe refinedweb dataset for falcon llm:  outperforming curated corpora with web data, and\nweb data only.arXiv preprint arXiv:2306.01116, 2023.\n[PRR19]George-Sebastian Pˆırtoac ̆a, Traian Rebedea, and Stefan Ruseti.  Answering questions by\nlearning to rank.arXiv preprint arXiv:1909.00596, 2019.\n[RRRH20]Samyam  Rajbhandari,  Jeff  Rasley,  Olatunji  Ruwase,  and  Yuxiong  He.   Zero:   Memory\noptimizations toward training trillion parameter models, 2020.\n[RZLL16]Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.  Squad:  100,000+\nquestions for machine comprehension of text.arXiv preprint arXiv:1606.05250, 2016.\n[SBBC21]Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.  Winogrande:\nAn adversarial winograd schema challenge at scale.Communications of the ACM, 64(9):99–\n106, 2021.\n[SLBBC19]    Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.  Winogrande:\nAn adversarial winograd schema challenge at scale.arXiv preprint arXiv:1907.10641, 2019.\n[SLY\n+\n23]Michael  Santacroce,  Yadong  Lu,  Han  Yu,  Yuanzhi  Li,  and  Yelong  Shen.   Efficient  rlhf:\nReducing the memory usage of ppo, 2023.\n[TLI\n+\n23]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth ́ee  Lacroix,  Baptiste  Rozi`ere,  Naman  Goyal,  Eric  Hambro,  Faisal  Azhar,  et  al.\nLlama:  Open and efficient foundation language models.arXiv preprint arXiv:2302.13971,\n2023.\n[VSP\n+\n17]Ashish  Vaswani,  Noam  Shazeer,  Niki  Parmar,  Jakob  Uszkoreit,  Llion  Jones,  Aidan  N\nGomez,   L ukasz Kaiser, and Illia Polosukhin.  Attention is all you need.  InAdvances  in\nNeural Information Processing Systems, volume 30, 2017.\n15",
    "[WUR\n+\n22]Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John\nMellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al.  Taxonomy of\nrisks posed by language models. InProceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency, pages 214–229, 2022.\n[WWS\n+\n22]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:24824–24837, 2022.\n[ZCS\n+\n23]Lianmin  Zheng,  Wei-Lin  Chiang,  Ying  Sheng,  Siyuan  Zhuang,  Zhanghao  Wu,  Yonghao\nZhuang,  Zi Lin,  Zhuohan Li,  Dacheng Li,  Eric Xing,  et al.  Judging llm-as-a-judge with\nmt-bench and chatbot arena.arXiv preprint arXiv:2306.05685, 2023.\n[ZHB\n+\n19]Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag:  Can\na machine really finish your sentence?   InProceedings  of  the  57th  Annual  Meeting  of  the\nAssociation for Computational Linguistics, pages 4791–4800, 2019.\n16"
  ]
}