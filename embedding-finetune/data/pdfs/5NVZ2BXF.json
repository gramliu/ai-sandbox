{
  "key": "5NVZ2BXF",
  "url": "http://arxiv.org/pdf/2303.16203",
  "metadata": {
    "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
    "abstract": "  The recent wave of large-scale text-to-image diffusion models has\ndramatically increased our text-based image generation abilities. These models\ncan generate realistic images for a staggering variety of prompts and exhibit\nimpressive compositional generalization abilities. Almost all use cases thus\nfar have solely focused on sampling; however, diffusion models can also provide\nconditional density estimates, which are useful for tasks beyond image\ngeneration. In this paper, we show that the density estimates from large-scale\ntext-to-image diffusion models like Stable Diffusion can be leveraged to\nperform zero-shot classification without any additional training. Our\ngenerative approach to classification, which we call Diffusion Classifier,\nattains strong results on a variety of benchmarks and outperforms alternative\nmethods of extracting knowledge from diffusion models. Although a gap remains\nbetween generative and discriminative approaches on zero-shot recognition\ntasks, our diffusion-based approach has significantly stronger multimodal\ncompositional reasoning ability than competing discriminative approaches.\nFinally, we use Diffusion Classifier to extract standard classifiers from\nclass-conditional diffusion models trained on ImageNet. Our models achieve\nstrong classification performance using only weak augmentations and exhibit\nqualitatively better \"effective robustness\" to distribution shift. Overall, our\nresults are a step toward using generative over discriminative models for\ndownstream tasks. Results and visualizations at\nhttps://diffusion-classifier.github.io/\n",
    "published": "2023-03-28T17:59:56Z"
  },
  "text": [
    "Your Diffusion Model is Secretly a Zero-Shot Classifier\nAlexander C. LiMihir PrabhudesaiShivam DuggalEllis BrownDeepak Pathak\nCarnegie Mellon University\nAbstract\nThe  recent  wave  of  large-scale  text-to-image  diffusion\nmodels  has  dramatically  increased  our  text-based  image\ngeneration  abilities.   These  models  can  generate  realistic\nimages for a staggering variety of prompts and exhibit im-\npressive compositional generalization abilities.  Almost all\nuse cases thus far have solely focused on sampling;  how-\never, diffusion models can also provide conditional density\nestimates, which are useful for tasks beyond image genera-\ntion. In this paper, we show that the density estimates from\nlarge-scale text-to-image diffusion models like Stable Dif-\nfusion can be leveraged to perform zero-shot classification\nwithout any additional training.  Our generative approach\nto classification, which we callDiffusion Classifier, attains\nstrong results on a variety of benchmarks and outperforms\nalternative methods of extracting knowledge from diffusion\nmodels.   Although a gap remains between generative and\ndiscriminative  approaches  on  zero-shot  recognition  tasks,\nour  diffusion-based  approach  has  significantly  stronger\nmultimodal compositional reasoning ability than competing\ndiscriminative approaches.  Finally, we use Diffusion Clas-\nsifier to extract standard classifiers from class-conditional\ndiffusion models trained on ImageNet. Our models achieve\nstrong classification performance using only weak augmen-\ntations  and  exhibit  qualitatively  better  “effective  robust-\nness” to distribution shift. Overall, our results are a step to-\nward using generative over discriminative models for down-\nstream  tasks.   Results  and  visualizations  on  our  website:\ndiffusion-classifier.github.io/\n1. Introduction\nTo  Recognize  Shapes,   First  Learn  to  Generate  Im-\nages[31]—in this seminal paper, Geoffrey Hinton empha-\nsizes generative modeling as a crucial strategy for training\nartificial neural networks for discriminative tasks like image\nrecognition.   Although generative models tackle the more\nchallenging task of accurately modeling the underlying data\ndistribution, they can create a more complete representation\nof  the  world  that  can  be  utilized  for  various  downstream\nCorrespondence to: Alexander Li<alexanderli@cmu.edu>\ntasks.  As a result, a plethora of implicit and explicit gen-\nerative  modeling  approaches  [26,  42,  46,  21,  77,  70,  79]\nhave  been  proposed  over  the  last  decade.   However,  the\nprimary  focus  of  these  works  has  been  content  creation\n[18, 8, 39, 40, 76, 34] rather than their ability to perform dis-\ncriminative tasks.  In this paper, we revisit this classic gen-\nerative vs. discriminative debate in the context of diffusion\nmodels, the current state-of-the-art generative model fam-\nily.  In particular, we examinehow diffusion models com-\npare against the state-of-the-art discriminative models on\nthe task of image classification.\nDiffusion models are a recent class of likelihood-based\ngenerative  models  that  model  the  data  distribution  via  an\niterative  noising  and  denoising  procedure  [70,  35].   They\nhave  recently  achieved  state-of-the-art  performance  [20]\non  several  text-based  content  creation  and  editing  tasks\n[24,  67,  34,  66,  59].    Diffusion  models  operate  by  per-\nforming two iterative processes—the fixedforward process,\nwhich  destroys  structure  in  the  data  by  iteratively  adding\nnoise, and the learnedbackward process, which attempts to\nrecover the structure in the noised data.  These models are\ntrained via a variational objective, which maximizes an evi-\ndence lower bound (ELBO) [5] of the log-likelihood.  For\nmost  diffusion  models,  computing  the  ELBO  consists  of\nadding noiseεto a sample, using the neural network to pre-\ndict the added noise, and measuring the prediction error.\nConditional generative models like diffusion models can\nbe easily converted into classifiers [54].  Given an inputx\nand a finite set of classescthat we want to choose from,\nwe can use the model to compute class-conditional likeli-\nhoodsp\nθ\n(x|c).  Then, by selecting an appropriate prior\ndistributionp(c)and applying Bayes’ theorem, we can get\npredicted class probabilitiesp(c|x). For conditional diffu-\nsion models that use an auxiliary input, like a class index for\nclass-conditioned models or prompt for text-to-image mod-\nels, we can do this by leveraging the ELBO as an approxi-\nmate class-conditional log-likelihoodlogp(x|c). In prac-\ntice, obtaining a diffusion model classifier through Bayes’\ntheorem consists of repeatedly adding noise and computing\na Monte Carlo estimate of the expected noise reconstruction\nlosses (also calledε-prediction loss) for every class. We call\nthis approachDiffusion Classifier. Diffusion Classifier can\n1\narXiv:2303.16203v3  [cs.LG]  13 Sep 2023",
    "––\nKV \nQ\n2\n<latexit sha1_base64=\"1wWc0GGRaknl4pv1RRZvH9lz9zY=\">AAAB83icbVDLSsNAFL2pr1pfVZdugkVwVZIi6rLoxmUF+4AmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATX6DjfVmltfWNzq7xd2dnd2z+oHh51dJwqyto0FrHqBUQzwSVrI0fBeoliJAoE6waT29zvPjKleSwfcJowPyIjyUNOCRrJ8yKC4yDMnmYDHFRrTt2Zw14lbkFqUKA1qH55w5imEZNIBdG67zoJ+hlRyKlgs4qXapYQOiEj1jdUkohpP5tnntlnRhnaYazMk2jP1d8bGYm0nkaBmcwz6mUvF//z+imG137GZZIik3RxKEyFjbGdF2APuWIUxdQQQhU3WW06JopQNDVVTAnu8pdXSadRdy/rjfuLWvOmqKMMJ3AK5+DCFTThDlrQBgoJPMMrvFmp9WK9Wx+L0ZJV7BzDH1ifP5I2kgo=</latexit>\nx\nt\nKV \nQ\n<latexit sha1_base64=\"56hsVbtdGBpzTtu/VRwSPNgalD4=\">AAAB+HicbZDLSsNAFIYnXmu9NOrSzWARXJWkiLosunFZwV6gCWUyPWmHTi7MnAg19EncuFDErY/izrdx2mahrT8MfPznHM6ZP0il0Og439ba+sbm1nZpp7y7t39QsQ+P2jrJFIcWT2SiugHTIEUMLRQooZsqYFEgoROMb2f1ziMoLZL4AScp+BEbxiIUnKGx+nbFg1QLadDDESDr21Wn5sxFV8EtoEoKNfv2lzdIeBZBjFwyrXuuk6KfM4WCS5iWvUxDyviYDaFnMGYRaD+fHz6lZ8YZ0DBR5sVI5+7viZxFWk+iwHRGDEd6uTYz/6v1Mgyv/VzEaYYQ88WiMJMUEzpLgQ6EAo5yYoBxJcytlI+YYhxNVmUTgrv85VVo12vuZa1+f1Ft3BRxlMgJOSXnxCVXpEHuSJO0CCcZeSav5M16sl6sd+tj0bpmFTPH5I+szx8uXJNv</latexit>\n✏\n✓\n<latexit sha1_base64=\"56hsVbtdGBpzTtu/VRwSPNgalD4=\">AAAB+HicbZDLSsNAFIYnXmu9NOrSzWARXJWkiLosunFZwV6gCWUyPWmHTi7MnAg19EncuFDErY/izrdx2mahrT8MfPznHM6ZP0il0Og439ba+sbm1nZpp7y7t39QsQ+P2jrJFIcWT2SiugHTIEUMLRQooZsqYFEgoROMb2f1ziMoLZL4AScp+BEbxiIUnKGx+nbFg1QLadDDESDr21Wn5sxFV8EtoEoKNfv2lzdIeBZBjFwyrXuuk6KfM4WCS5iWvUxDyviYDaFnMGYRaD+fHz6lZ8YZ0DBR5sVI5+7viZxFWk+iwHRGDEd6uTYz/6v1Mgyv/VzEaYYQ88WiMJMUEzpLgQ6EAo5yYoBxJcytlI+YYhxNVmUTgrv85VVo12vuZa1+f1Ft3BRxlMgJOSXnxCVXpEHuSJO0CCcZeSav5M16sl6sd+tj0bpmFTPH5I+szx8uXJNv</latexit>\n✏\n✓\n<latexit sha1_base64=\"3TI8V8cX099fkVQmqy6nYpDaQUA=\">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKewGUY9BLx4jmIckS5idzCZD5rHMzAphyVd48aCIVz/Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWyc99mhiGFey1C9X/Ko/B1olQU4qkKPRL3/1BoqkgkpLODamG/iJDTOsLSOcTku91NAEkzEe0q6jEgtqwmx+8BSdOWWAYqVdSYvm6u+JDAtjJiJynQLbkVn2ZuJ/Xje18XWYMZmklkqyWBSnHFmFZt+jAdOUWD5xBBPN3K2IjLDGxLqMZiEEyy+vklatGlxWa/cXlfpNHkcRTuAUziGAK6jDHTSgCQQEPMMrvHnae/HevY9Fa8HLZ47hD7zPH4aLkDs=</latexit>\n✏\n<latexit sha1_base64=\"DyoYokN8kOj0I2NP2cbbPxB/37w=\">AAACaHicbVDLjtMwFHXCq3R4hJcQYmNNhdRKUCVdDCwrEBLLQaIzI9Uhctyb1hrHjuwbNCUT8Qls+QI+iB0fwIavwGkHBDNcydLxOefq3nvySkmHcfw9CC9dvnL1Wu96f+fGzVu3ozt3D5yprYCZMMrYo5w7UFLDDCUqOKos8DJXcJgfv+r0ww9gnTT6Ha4rSEu+1LKQgqOnsugTq/XC64CNaBtmKrAcjdW8hIbbZSl121KmoMAhZSXHVZ43r9uswaeUQeWkMrqlc3b6+5MxXAHy4dZbNCdt5q1iRJ/98bPT95OUMiuXKxxl0SAex5uiF0FyBgbT0eePVe/rl/0s+sYWRtQlaBSKOzdP4gpTvytKoaDts9pBxcUxX8Lcw+4QlzaboFr6xDMLWhjrn0a6Yf/uaHjp3LrMvbPb353XOvJ/2rzG4kXaSF3VCFpsBxW1omholzpdSAsC1doDLqz0u1Kx4pYL9Nn3fQjJ+ZMvgoPJONkbT94mg+lLsq0eeUx2yZAk5DmZkjdkn8yIID+CneB+8CD4GUbhw/DR1hoGZz33yD8V7v4Cq5G+hQ==</latexit>\nargmin\nc\n\u0000\nE\nt,✏\n[k✏\n✓\n(x\nt\n,c)\u0000✏k\n2\n]\n\u0000\n<latexit sha1_base64=\"wXxrnsnojZTp0+85EvDANFahYFI=\">AAACCnicbVC7TsNAEDyHVwgvAyXNQYJEFdkpgDKChnRBIg8pjqLz5UxOOT90t0aJLNc0/AoNBQjR8gV0/A3nxAUkjLTSaGZXuztuJLgCy/o2Ciura+sbxc3S1vbO7p65f9BWYSwpa9FQhLLrEsUED1gLOAjWjSQjvitYxx1fZ37ngUnFw+AOphHr++Q+4B6nBLQ0MI8dYBNwvaQRRDHghrZZiiuOT2Ck1UlaGZhlq2rNgJeJnZMyytEcmF/OMKSxzwKggijVs60I+gmRwKlgacmJFYsIHetNPU0D4jPVT2avpPhUK0PshVJXAHim/p5IiK/U1Hd1Z3aiWvQy8T+vF4N32U949iUL6HyRFwsMIc5ywUMuGQUx1YRQyfWtmI6IJBR0eiUdgr348jJp16r2ebV2WyvXr/I4iugInaAzZKMLVEc3qIlaiKJH9Ixe0ZvxZLwY78bHvLVg5DOH6A+Mzx80u5qU</latexit>\nInput Image x\n<latexit sha1_base64=\"gcHqBrzeOG8wGMskCfpomwH7USY=\">AAACEHicbVC7SgNBFJ2Nrxhfq5Y2g4kYQcJuCrUM2mgjEc0DsiHMTm6SITOzy8ysEEI+wcZfsbFQxNbSzr9x8ig0euDC4Zx7ufeeMOZMG8/7clILi0vLK+nVzNr6xuaWu71T1VGiKFRoxCNVD4kGziRUDDMc6rECIkIOtbB/MfZr96A0i+SdGcTQFKQrWYdRYqzUcg9viYg54FwAsWY8kjjQTOBAENOjhOPrvHeMr45yLTfrFbwJ8F/iz0gWzVBuuZ9BO6KJAGkoJ1o3fC82zSFRhlEOo0yQaIgJ7ZMuNCyVRIBuDicPjfCBVdq4Eylb0uCJ+nNiSITWAxHazvGhet4bi/95jcR0zppDJuPEgKTTRZ2EYxPhcTq4zRRQwweWEKqYvRXTHlGEGpthxobgz7/8l1SLBf+kULwpZkvnszjSaA/tozzy0SkqoUtURhVE0QN6Qi/o1Xl0np03533amnJmM7voF5yPb4d8mvU=</latexit>\nSample✏⇠N(0,I)\n<latexit sha1_base64=\"7jO/wKq456c9VQx8ZwdaPRzsRss=\">AAAB/3icbVA9SwNBEJ2LXzF+nQo2NouJYCHhLoVaBm0sI+YLLkfY2+wlS3bvjt09IcQU/hUbC0Vs/Rt2/hs3yRWa+GDg8d4MM/OChDOlHefbyq2srq1v5DcLW9s7u3v2/kFTxakktEFiHst2gBXlLKINzTSn7URSLAJOW8HwZuq3HqhULI7qepRQX+B+xEJGsDZS1z66xyLhFJU06igmkOeeo7pf6tpFp+zMgJaJm5EiZKh17a9OLyapoJEmHCvluU6i/TGWmhFOJ4VOqmiCyRD3qWdohAVV/nh2/wSdGqWHwliaijSaqb8nxlgoNRKB6RRYD9SiNxX/87xUh1f+mEVJqmlE5ovClCMdo2kYqMckJZqPDMFEMnMrIgMsMdEmsoIJwV18eZk0K2X3oly5qxSr11kceTiGEzgDFy6hCrdQgwYQeIRneIU368l6sd6tj3lrzspmDuEPrM8fHRSUOA==</latexit>\nSamplet⇠[1,T]\n<latexit sha1_base64=\"tA6EB5xvVjriuxAt3mNfE12GWbg=\">AAACAXicbVC7SgNBFJ2NrxhfURvBZjAIVmE3hVoGtbARIpgHJEuYndxNhsw+mLkrhiU2/oqNhSK2/oWdf+Mk2UITDwwczjmXO/d4sRQabfvbyi0tr6yu5dcLG5tb2zvF3b2GjhLFoc4jGamWxzRIEUIdBUpoxQpY4EloesPLid+8B6VFFN7hKAY3YP1Q+IIzNFK3eNBBeEDPT6+E7yeTGL2JeiDH3WLJLttT0EXiZKREMtS6xa9OL+JJACFyybRuO3aMbsoUCi5hXOgkGmLGh6wPbUNDFoB20+kFY3pslB71I2VeiHSq/p5IWaD1KPBMMmA40PPeRPzPayfon7upCOMEIeSzRX4iKUZ0UgftCQUc5cgQxpUwf6V8wBTjaEormBKc+ZMXSaNSdk7LldtKqXqR1ZEnh+SInBCHnJEquSY1UiecPJJn8krerCfrxXq3PmbRnJXN7JM/sD5/APnRlzk=</latexit>\nDi↵usion Model\n<latexit sha1_base64=\"41WincqQVVpTHZI60nJQrVZEynw=\">AAACLnicbVDLSgMxFM3UVx1fVZdugkVwVWa6UHFVFMFlhb6gM5RMJtOGZpIhyYhlmC9y46/oQlARt36GmbYLbb0Qcjjn3NzcEySMKu04b1ZpZXVtfaO8aW9t7+zuVfYPOkqkEpM2FkzIXoAUYZSTtqaakV4iCYoDRrrB+LrQu/dEKip4S08S4sdoyGlEMdKGGlRuPC4oDwnXtqfJgw6irGUuiAUPaWGhfJhDfOl5NoLJSGgBRQQR9DLMkFKQo5h4+aBSdWrOtOAycOegCubVHFRevFDgNDZzp+/0XSfRfoakppiR3PZSRRKEx2hI+gYWU5SfTdfN4YlhQhgJaQ4vvmrY3x0ZipWaxIFxxkiP1KJWkP9p/VRHF35GeZJqwvFsUJQyaJYusoMhlQRrNjEAYWniwRCPkERYm4RtE4K7uPIy6NRr7lmtflevNq7mcZTBETgGp8AF56ABbkETtAEGj+AZvIMP68l6tT6tr5m1ZM17DsGfsr5/AAcaqRk=</latexit>\nText conditioningc:\naphotoofa{classname}\n<latexit sha1_base64=\"y0jZ1XY01OvGgZow+oPnNb8N1Cc=\">AAACCnicbVC7TgJBFJ31ifhCLW1GiYkV2aVQSyKNnZjIIwFCZoe7MDL7yMxdItlQ2/grNhYaY+sX2Pk3zgIxCp5kkpNz7p2Zc9xICo22/WUtLa+srq1nNrKbW9s7u7m9/ZoOY8WhykMZqobLNEgRQBUFSmhECpjvSqi7g3Lq14egtAiDWxxF0PZZLxCe4AyN1MkdtRDu0fWSsmRa/xj02r0DjmII404ubxfsCegicWYkT2aodHKfrW7IYx8C5OmlTceOsJ0whYJLGGdbsYaI8QHrQdPQgPmg28kkypieGKVLvVCZEyCdqL83EuZrPfJdM+kz7Ot5LxX/85oxehftRARRjBDw6UNeLCmGNO2FdoUygeXIEMaVMH+lvM8U42jay5oSnPnIi6RWLDhnheJNMV+6nNWRIYfkmJwSh5yTErkiFVIlnDyQJ/JCXq1H69l6s96no0vWbOeA/IH18Q1EGJtC</latexit>\nClassification Objective\nFigure 1.Overview of our Diffusion Classifier approach:Given an input imagexand a set of possible conditioning inputs (e.g., text for\nStable Diffusion or class index for DiT, an ImageNet class-conditional model), we use a diffusion model to choose the one that best fits this\nimage. Diffusion Classifier is theoretically motivated through the variational view of diffusion models and uses the ELBO to approximate\nlogp\nθ\n(x|c).  Diffusion Classifier chooses the conditioningcthat best predicts the noise added to the input image.Diffusion Classifier\ncan be used to extract a zero-shot classifier from Stable Diffusion and a standard classifier from DiT without any additional training.\nextract  zero-shot  classifiers  from  text-to-image  diffusion\nmodels and standard classifiers from class-conditional dif-\nfusion models,without any additional training. We develop\ntechniques for appropriately choosing diffusion timesteps to\ncompute errors at, reducing variance in the estimated prob-\nabilities, and speeding up classification inference.\nWe  highlight  the  surprising  effectiveness  of  our  pro-\nposed Diffusion Classifier on zero-shot classification, com-\npositional reasoning, and supervised classification tasks by\ncomparing  against  multiple  baselines  on  eleven  different\nbenchmarks.  By utilizing Stable Diffusion [65], Diffusion\nClassifier  achieves  strong  zero-shot  accuracy  and  outper-\nforms alternative approaches for extracting knowledge from\nthe pretrained diffusion model.  Our approach alsooutper-\nforms  the  strongest  contrastive  methods  on  the  challeng-\ning Winoground compositional reasoning benchmark[75].\nFinally,  we  use  our  approach  to  perform  standard  classi-\nfication  with  Diffusion  Transformer  (DiT),  an  ImageNet-\ntrained  class-conditional  diffusion  model.Our  genera-\ntive approachachieves 79.1% accuracy on ImageNet using\nonly weak augmentations and exhibits better robustness to\ndistribution shiftthan competing discriminative classifiers\ntrained on the same dataset. Our results suggest that it may\nbe time to revisit generative approaches to classification.\n2. Related Work\nGenerative Models for Discriminative Tasks:Machine\nlearning  algorithms  designed  to  solve  common  classifi-\ncation  or  regression  tasks  generally  operate  under  two\nparadigms:discriminativeapproaches  directly  learn  to\nmodel the decision boundary of the underlying task, while\ngenerative approacheslearn to model the distribution of the\ndata  and  then  address  the  underlying  task  as  a  maximum\nlikelihood estimation problem. Algorithms like naive Bayes\n[54],  VAEs  [42],  GANs  [26],  EBMs  [23,  46],  and  diffu-\nsion models [70, 35] fall under the category of generative\nmodels.  The idea of modeling the data distribution to bet-\nter learn the discriminative feature has been highlighted by\nseveral seminal works [31, 54, 63]. These works train deep\nbelief networks [32] to model the underlying image data as\nlatents,  which  are  later  used  for  image  recognition  tasks.\nRecent works on generative modeling have also learned ef-\nficient representations for both global and dense prediction\ntasks like classification [28, 33, 13, 8, 19] and segmenta-\ntion [47, 83, 10, 3, 9].  Moreover, such models [27, 51, 37]\nhave been shown to be more adversarially robust and bet-\nter calibrated. However, most of the aforementioned works\neither train jointly for discriminative and generative model-\ning or fine-tune generative representations for downstream\ntasks.  Directly utilizing generative models for discrimina-\ntive  tasks  is  a  relatively  less-studied  problem,  and  in  this\nwork, we particularly highlight theefficacy of directly using\nrecent diffusion models as image classifiers.\nDiffusion  Models:Diffusion  models  [35,  70]  have  re-\ncently gained significant attention from the research com-\nmunity due to their ability to generate high-fidelity and di-\nverse content like images [67, 55, 24], videos [69, 34, 78],\n3D [59, 50], and audio [43, 52] from various input modal-\nities  like  text.   Diffusion  models  are  also  closely  tied  to\nEBMs  [46,  23],  denoising  score  matching  [72,  80],  and\nstochastic differential equations [73, 84].  In this work, we\ninvestigate to what extent the impressive high-fidelity gen-\nerative abilities of these diffusion models can be utilized for\ndiscriminative  tasks  (namely  classification).   We  take  ad-\nvantage of the variational view of diffusion models for ef-\nficient and parallelizable density estimates. The prior work\nof Dhariwal & Nichol [20] proposed using a classifier net-\nwork to modify the output of an unconditional generative\nmodel to obtain class-conditional samples.  Our goal is the\nreverse: using diffusion models as classifiers.\n2",
    "Zero-Shot   Image   Classification:Classifiers   thus   far\nhave  usually  been  trained  in  a  supervised  setting  where\nthe  train  and  test  sets  are  fixed  and  limited.   CLIP  [61]\nshowed that exploiting large-scale image-text data can re-\nsult in zero-shot generalization to various new tasks.  Since\nthen, there has been a surge toward building a new category\nof classifiers, known as zero-shot or open-vocabulary clas-\nsifiers, that are capable of detecting a wide range of class\ncategories [25, 48, 49, 1]. These methods have been shown\nto learn robust representations that generalize to various dis-\ntribution shifts [38, 16, 74]. Note that in spite of them being\ncalled “zero-shot,” it is still unclear whether evaluation sam-\nples lie in their training data distribution.  In contrast to the\ndiscriminative approaches above,  we propose extracting a\nzero-shot classifier from a large-scalegenerativemodel.\n3. Method: Classification via Diffusion Models\nWe  describe  our  approach  for  calculating  class  condi-\ntional density estimates in a practical and efficient manner\nusing diffusion models. We first provide an overview of dif-\nfusion models (Sec. 3.1), discuss the motivation and deriva-\ntion of our Diffusion Classifier method (Sec. 3.2),  and fi-\nnally propose techniques to improve its accuracy (Sec. 3.3).\n3.1. Diffusion Model Preliminaries\nDiffusion  probabilistic  models  (“diffusion  models”  for\nshort) [70, 35] are generative models with a specific Markov\nchain structure. Starting at a clean samplex\n0\n, the fixed for-\nward processq(x\nt\n|x\nt−1\n)adds Gaussian noise,  whereas\nthe learned reverse processp\nθ\n(x\nt−1\n|x\nt\n,c)tries to denoise\nits input, optionally conditioning on a variablec. In our set-\nting,xis an image andcrepresents a low-dimensional text\nembedding (for text-to-image synthesis) or class index (for\nclass-conditional generation).  Diffusion models define the\nconditional probability ofx\n0\nas:\np\nθ\n(x\n0\n|c) =\nZ\nx\n1:T\np(x\nT\n)\nT\nY\nt=1\np\nθ\n(x\nt−1\n|x\nt\n,c) dx\n1:T\n(1)\nwherep(x\nT\n)is typically fixed toN(0,I).  Directly maxi-\nmizingp\nθ\n(x\n0\n)is intractable due to the integral, so diffusion\nmodels are instead trained to minimize the variational lower\nbound (ELBO) of the log-likelihood:\nlogp\nθ\n(x\n0\n|c)≥E\nq\n\u0014\nlog\np\nθ\n(x\n0:T\n,c)\nq(x\n1:T\n|x\n0\n)\n\u0015\n(2)\nDiffusion models parameterizep\nθ\n(x\nt−1\n|x\nt\n,c)as a Gaus-\nsian and train a neural network to map a noisy inputx\nt\nto a\nvalue used to compute the mean ofp\nθ\n(x\nt−1\n|x\nt\n,c). Using\nthe fact that each noised samplex\nt\n=\n√\n ̄α\nt\nx+\n√\n1− ̄α\nt\nε\ncan be written as a weighted combination of a clean inputx\nand Gaussian noiseε∼N(0,I), diffusion models typically\nAlgorithm 1Diffusion Classifier\n1:Input:test imagex, conditioning inputs{c\ni\n}\nn\ni=1\n(e.g.,\ntext embeddings), # of trialsTper input\n2:InitializeErrors[c\ni\n] =list()for eachc\ni\n3:fortrialj= 1,...,Tdo\n4:Samplet∼[1,1000];ε∼N(0,I)\n5:x\nt\n=\n√\n ̄α\nt\nx+\n√\n1− ̄α\nt\nε\n6:forconditioningc\nk\n∈{c\ni\n}\nn\ni=1\ndo\n7:Errors[c\nk\n].append(∥ε−ε\nθ\n(x\nt\n,c\nk\n)∥\n2\n)\n8:end for\n9:end for\n10:returnarg min\nc\ni\n∈C\nmean(Errors[c\ni\n])\nlearn a networkε\nθ\n(x\nt\n,c)that estimates the added noise. Us-\ning this parameterization, the ELBO can be written as:\n−E\nε\n\"\nT\nX\nt=2\nw\nt\n∥ε−ε\nθ\n(x\nt\n,c)∥\n2\n−logp\nθ\n(x\n0\n|x\n1\n,c)\n#\n+C\n(3)\nwhereCis a constant term that does not depend onc. Since\nT= 1000is large andlogp\nθ\n(x\n0\n|x\n1\n,c)is typically small,\nwe choose to drop this term. Finally, [35] find that removing\nw\nt\nimproves sample quality metrics,  and many follow-up\nworks also choose to do so.  We found that deviating from\nthe uniform weighting used at training time hurts accuracy,\nso we setw\nt\n= 1.  Thus, this gives us our final approxima-\ntion that we treat as the ELBO:\n−E\nt,ε\n\u0002\n∥ε−ε\nθ\n(x\nt\n,c)∥\n2\n\u0003\n+C(4)\n3.2. Classification with diffusion models\nIn general, classification using a conditional generative\nmodel can be done by using Bayes’ theorem on the model\npredictionsp\nθ\n(x|c\ni\n)and the priorp(c)over labels{c\ni\n}:\np\nθ\n(c\ni\n|x) =\np(c\ni\n)p\nθ\n(x|c\ni\n)\nP\nj\np(c\nj\n)p\nθ\n(x|c\nj\n)\n(5)\nA uniform prior over{c\ni\n}(i.e.,p(c\ni\n) =\n1\nN\n) is natural and\nleads to all of thep(c)terms cancelling. For diffusion mod-\nels, computingp\nθ\n(x|c)is intractable, so we use the ELBO\nin place oflogp\nθ\n(x|c)and use Eq. 4 and Eq. 5 to obtain a\nposterior distribution over{c\ni\n}\nN\ni=1\n:\np\nθ\n(c\ni\n|x) =\nexp{−E\nt,ε\n[∥ε−ε\nθ\n(x\nt\n,c\ni\n)∥\n2\n]}\nP\nj\nexp{−E\nt,ε\n[∥ε−ε\nθ\n(x\nt\n,c\nj\n)∥\n2\n]}\n(6)\nWe compute an unbiased Monte Carlo estimate of each ex-\npectation by samplingN(t\ni\n,ε\ni\n)pairs, witht\ni\n∼[1,1000]\n3",
    "05001000\n−150\n−100\n−50\n0\n50\nError:\n‖\n\u000f\n−\n\u000f\nθ\n(\nx\nt\n,c\n)\n‖\n2\n05001000\n−100\n0\n100\nSamoyed\nGreat Pyrenees\n05001000\nTimestept\n−50\n−25\n0\n25\n50\nError:\n‖\n\u000f\n−\n\u000f\nθ\n(\nx\nt\n,c\n)\n‖\n2\n05001000\nTimestept\n0\n25\n50\n75\nFigure 2. We show theε-prediction error for an image of a Great\nPyrenees  dog  and  two  prompts  (“Samoyed”  and  “Great  Pyre-\nnees”).   Each  subplot  corresponds  to  a  singleε\ni\n,  with  the  error\nevaluated at everyt∈ {1,2,...,1000}.  Errors are normalized to\nbe zero-mean at each timestep across the 4 plots, and lower is bet-\nter. Variance inε-prediction error is high across differentε, but the\nvariance in the error difference between prompts is much smaller.\nandε∼N(0,I), and computing:\n1\nN\nN\nX\ni=1\n\r\n\r\n\r\nε\ni\n−ε\nθ\n(\np\n ̄α\nt\ni\nx+\np\n1− ̄α\nt\ni\nε\ni\n,c\nj\n)\n\r\n\r\n\r\n2\n(7)\nBy  plugging  Eq.  7  into  Eq.  6,  we  can  extract  a  classifier\nfrom any conditional diffusion model.  We call this method\nDiffusion  Classifier.Diffusion  Classifier  is  a  powerful,\nhyperparameter-free approach to extracting classifiers from\npretrained  diffusion  models  without  any  additional  train-\ning.Diffusion Classifier can be used to extract a zero-shot\nclassifier from a text-to-image model like Stable Diffusion\n[65], to extract a standard classifier from a class-conditional\ndiffusion model like DiT [58], and so on.  We outline our\nmethod in Algorithm 1 and show an overview in Figure 1.\n3.3. Variance Reduction via Difference Testing\nAt  first  glance,   it  seems  that  accurately  estimating\nE\nt,ε\n\u0002\n∥ε−ε\nθ\n(x\nt\n,c)∥\n2\n\u0003\nfor   each   classcrequires   pro-\nhibitively many samples.  Indeed, a Monte Carlo estimate\neven using thousands of samples is not precise enough to\ndistinguish classes reliably.  However, a key observation is\nthat classification only requires therelativedifferences be-\ntween the prediction errors, not theirabsolutemagnitudes.\nWe can rewrite the approximatep\nθ\n(c\ni\n|x)from Eq. 6 as:\n1\nP\nj\nexp{E\nt,ε\n[∥ε−ε\nθ\n(x\nt\n,c\ni\n)∥\n2\n−∥ε−ε\nθ\n(x\nt\n,c\nj\n)∥\n2\n]}\n(8)\n02004006008001000\nTimestep used (1 trial)\n20\n40\nAccuracy\nFigure 3.Pets accuracy, evaluating only a single timestep per\nclass.  Smalltcorresponds to less noise added, and largetcorre-\nsponds to significant noise. Accuracy is highest when an interme-\ndiate amount of noise is added (t= 500).\nEq. 8 makes apparent that we only need to estimate thedif-\nferencein prediction errors across each conditioning value.\nPractically,  instead  of  using  different  random  samples  of\n(t\ni\n,ε\ni\n)to  estimate  the  ELBO  for  each  conditioning  input\nc, we simply sample a fixed setS={(t\ni\n,ε\ni\n)}\nN\ni=1\nand use\nthe same samples to estimate theε-prediction error for every\nc. This is reminiscent of paired difference tests in statistics,\nwhich  increase  their  statistical  power  by  matching  condi-\ntions across groups and computing differences.\nIn  Figure  2,  we  use  4  fixedε\ni\n’s  and  evaluate∥ε\ni\n−\nε\nθ\n(\n√\n ̄α\nt\nx+\n√\n1− ̄α\nt\nε\ni\n,c)∥\n2\nfor everyt∈1,...,1000, two\nprompts (“Samoyed dog” and “Great Pyrenees dog”), and\na fixed input image of a Great Pyrenees.  Even for a fixed\nprompt, theε-prediction error varies wildly across the spe-\ncificε\ni\nused.  However, the error difference between each\nprompt is much more consistent for eachε\ni\n.Thus, by using\nthe same(t\ni\n,ε\ni\n)for each conditioning input, our estimate\nofp\nθ\n(c\ni\n|x)is much more accurate.\n4. Practical Considerations\nOur Diffusion Classifier method requires repeated error\nprediction  evaluations  for  every  class  in  order  to  classify\nan input image.   These evaluations naively require signif-\nicant inference time, even with the technique presented in\nSection 3.3. In this section, we present further insights and\noptimizations that reduce our method’s runtime.\n4.1. Effect of timestep\nDiffusion Classifier, which is a theoretically principled\nmethod for estimatingp\nθ\n(c\ni\n|x), uses a uniform distribu-\ntion over the timesteptfor estimating theε-prediction er-\nror.   Here,  we check if alternate distributions overtyield\nmore  accurate  results.   Figure  3  shows  the  Pets  accuracy\nwhen using only a single timestep evaluation per class. Per-\nhaps intuitively, accuracy is highest when using intermedi-\nate timesteps (t≈500).  This begs the question:  can we\nimprove accuracy by oversampling intermediate timesteps\nand undersampling low or high timesteps?\n4",
    "We  try  a  variety  of  timestep  sampling  strategies,  in-\ncluding  repeatedly  tryingt=  500with  many  randomε,\ntryingNevenly  spaced  timesteps,  and  trying  the  middle\nt−N/2,...,t+N/2timesteps. The tradeoff between dif-\nferent strategies is whether to try a fewt\ni\nrepeatedly with\nmanyεor to try manyt\ni\nonce. Figure 4 shows that all strate-\ngies improve when taking using average error of more sam-\nples, but simply using evenly spaced timesteps is best.  We\nhypothesize that repeatedly trying a small set oft\ni\nscales\npoorly since this biases the ELBO estimate.\n4.2. Efficient Classification\nA naive implementation of our method requiresC×N\ntrials to classify a given image, whereCis the number of\nclasses andNis the number of(t,ε)samples to evaluate\nto  estimate  each  conditional  ELBO.  However,  we  can  do\nbetter. Since we only care aboutarg max\nc\np(c|x), we can\nstop computing the ELBO for classes we can confidently\nreject.  Thus, one option to classify an image is to use an\nupper confidence bound algorithm [2] to allocate most of\nthe compute to the top candidates.  However, this requires\nassuming  that  the  distribution  of∥ε−ε\nθ\n(x\nt\n,c\nj\n)∥\n2\nis  the\nsame across timestepst, which does not hold.\nWe found that a simpler method works just as well.  We\nsplit our evaluation into a series of stages,  where in each\nstage we try each remainingc\ni\nsome number of times and\nthen remove the ones that have the highest average error.\nThis allows us to efficiently eliminate classes that are almost\ncertainly not the final output and allocate more compute to\nreasonable classes.   For example,  on the Pets dataset,  we\nhaveN\nstages\n= 2. We try each class 25 times in the first stage,\nthen prune to the 5 classes with the smallest average error.\nFinally,  in the second stage we try each of the 5 remain-\ning classes 225 additional times.  In Algorithm 2, we write\nthis asKeepList= (5,1)andTrialList= (25,250).\nWith this evaluation strategy, classifying one Pets image re-\nquires 18 seconds on a RTX 3090 GPU. As our work fo-\ncuses on understanding diffusion model capabilities and not\non developing a fully practical inference algorithm, we do\nnot significantly tune the evaluation strategies.  Further de-\ntails on adaptive evaluation are in Appendix A.\nFurther reducing inference time could be a valuable av-\nenue  for  future  work.   Inference  is  still  impractical  when\nthere are many classes.  Classifying a single ImageNet im-\nage, with 1000 classes, takes about 1000 seconds with Sta-\nble Diffusion at512×512resolution, even with our adaptive\nstrategy. Table 7 shows inference times for each dataset, and\nwe discuss promising approaches for speedups in Section 7.\n5. Experimental Details\nWe  provide  setup  details,  baselines,  and  datasets  for\nzero-shot and supervised classification.\n10\n0\n10\n1\n10\n2\nNumber of trials per class\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy\nUniform\n0, 500, 1000\n0\n500\n1000\n475, 500, 525\nEven 5\nEven 10\nFigure 4.Zero-shot scaling curves for different timestep sam-\npling strategies.  We evaluate a variety of strategies for choosing\nthe timesteps at which we evaluate theε-prediction error.   Each\nstrategy name indicates which timesteps it uses— e.g., “0” only\nuses the first timestep, “0,500,1000” uses only the first, middle\nand last, “Even 10” uses 10 evenly spaced timesteps. We allocate\nmoreεevaluations at the chosen timesteps as the number of trials\nincreases.  Strategies that repeatedly sample from a restricted set\nof timesteps, like “475, 500, 525”, scale poorly with trials. Using\ntimesteps uniformly from the full range [1, 1000] scales best.\n5.1. Zero-shot Classification\nDiffusion  Classifier  Setup:Zero-shot  Diffusion  Classi-\nfier  utilizes  Stable  Diffusion  2.0  [65],  a  text-to-image  la-\ntent diffusion model trained on a filtered subset of LAION-\n5B [68]. Additionally, instead of using the squaredℓ\n2\nnorm\nto compute theε-prediction error, we leave the choice be-\ntweenℓ\n1\nandℓ\n2\nas a per-dataset inference hyperparameter.\nSee Appendix C for more discussion. We also use the adap-\ntive Diffusion Classifier from Algorithm 2.\nBaselines:We provide results using two strong discrimi-\nnative zero-shot models:  (a) CLIP ResNet-50 [60] and (b)\nOpenCLIP ViT-H/14 [11].  We provide these for reference\nonly, as these models are trained on different datasets with\nvery  different  architectures  from  ours  and  thus  cannot  be\ncompared  apples-to-apples.   We  further  compare  our  ap-\nproach against two alternative ways to extract class labels\nfrom diffusion models: (c)Synthetic SD Data: We train a\nResNet-50 classifier on synthetic data generated using Sta-\nble Diffusion (with class names as prompts), (d)SD Fea-\ntures:  This baseline is not a zero-shot classifier,  as it re-\nquires  alabeled  datasetof  real-world  images  and  class-\nnames.   Inspired  by  Label-DDPM  [3],  we  extract  Stable\nDiffusion features (mid-layer U-Net features at a resolution\n[8×8×1024] at timestept= 100), and then fit a ResNet-\n50  classifier  on  the  extracted  features  and  corresponding\nground-truth labels. Details are in Appendix F.4.\nDatasets:We evaluate the zero-shot classification perfor-\nmance across eight datasets: Food-101 [6], CIFAR-10 [45],\n5",
    "Zero-shot?FoodCIFAR10AircraftPetsFlowersSTL10ImageNetObjectNet\nSynthetic SD Data✓12.635.39.431.322.138.018.95.2\nSD Features✗73.084.035.275.970.087.256.610.2\nDiffusion Classifier (ours)✓77.788.526.487.366.395.461.443.4\nCLIP ResNet-50✓81.175.619.385.465.994.358.240.0\nOpenCLIP ViT-H/14✓92.797.342.394.679.998.376.869.2\nTable 1.Zero-shot classification performance. Our zero-shot Diffusion Classifier method (which utilizes Stable Diffusion) significantly\noutperforms the zero-shot diffusion model baseline that trains a classifier on synthetic SD data.  Diffusion Classifier also generally out-\nperforms the baseline trained on Stable Diffusion features, despite “SD Features” using the entire training set to train a classifier. Finally,\nalthough making a fair comparison is difficult due to different training datasets, our generative approach surprisingly outperforms CLIP\nResNet-50 and is competitive with OpenCLIP ViT-H. We report average accuracy or mean-per-class accuracy in accordance with [44].\nFGVC-Aircraft  [53],  Oxford-IIIT  Pets  [57],  Flowers102\n[56], STL-10 [12], ImageNet [17], and ObjectNet [4]. Due\nto computational constraints, we evaluate on 2000 test im-\nages  for  ImageNet.   We  also  evaluate  zero-shot  composi-\ntional reasoning ability on the Winoground benchmark [75].\n5.2. Supervised Classification\nDiffusion  Classifier  Setup:We  build  Diffusion  Clas-\nsifier   on   top   of   Diffusion   Transformer   (DiT)   [58],   a\nclass-conditional  latent  diffusion  model  trained  only  on\nImageNet-1k [17]. We use DiT-XL/2 at resolution256\n2\nand\n512\n2\nand evaluate each class 250 times per image.\nBaselines:We compare against the following discrimina-\ntive models trained with cross-entropy loss on ImageNet-\n1k:ResNet-18,   ResNet-34,   ResNet-50,   and   ResNet-\n101 [29], as well as ViT-L/32, ViT-L/16, and ViT-B/16 [22].\nDatasets:We evaluate models on their in-distribution ac-\ncuracy on ImageNet [17] and out-of-distribution generaliza-\ntion to ImageNetV2 [64], ImageNet-A [30], and ObjectNet\n[4].   ObjectNet  accuracy  is  computed  on  the  113  classes\nshared with ImageNet.  Due to computational constraints,\nwe evaluate Diffusion Classifier accuracy on 10,000 vali-\ndation  images  for  ImageNet.   We  compute  the  baselines’\nImageNet accuracies on the same 10,000 image subset.\n6. Experimental Results\nIn this section, we conduct detailed experiments aimed\nat addressing the following questions:\n1.  How does Diffusion Classifier compare against zero-\nshot state-of-the-art classifiers such as CLIP?\n2.  How does our method compare against alternative ap-\nproaches for classification with diffusion models?\n3.  How well does our method do on compositional rea-\nsoning tasks?\n4.  How well does our method compare to discriminative\nmodels trained on the same dataset?\n5.  How robust is our model compared to discriminative\nclassifiers over various distribution shifts?\n6.1. Zero-shot Classification Results\nTable  1  shows  that  Diffusion  Classifier  significantly\noutperforms  the  Synthetic  SD  Data  baseline,  an  alternate\nzero-shot  approach  of  extracting  information  from  diffu-\nsion models.   This is likely because the model trained on\nsynthetically generated data learns to rely on features that\ndo not transfer to real data.  Surprisingly, our method also\ngenerally outperforms the SD Features baseline, which is a\nclassifier trained in asupervisedmanner using the entirela-\nbeled training setfor each dataset.  In contrast, our method\nis zero-shot and requires no additional training or labels. Fi-\nnally, while it is difficult to make a fair comparison due to\ntraining dataset differences, our method outperforms CLIP\nResNet-50 and is competitive with OpenCLIP ViT-H.\nThis is a major advancement in the performance of gen-\nerative approaches, and there are clear avenues for improve-\nment.   First,  we performed no manual prompt tuning and\nsimply used the prompts used by the CLIP authors.  Tun-\ning  the  prompts  to  the  Stable  Diffusion  training  distribu-\ntion should improve its recognition abilities.   Second,  we\nsuspect that Stable Diffusion classification accuracy could\nimprove  with  a  wider  training  distribution.   Stable  Diffu-\nsion  was  trained  on  a  subset  of  LAION-5B  [68]  filtered\naggressively to remove low-resolution, potentially NSFW,\nor unaesthetic images.   This decreases the likelihood that\nit  has  seen  relevant  data  for  many  of  our  datasets.   The\nrightmost column in Table 2 shows that only 0-3% of the\ntest images in CIFAR10, Pets, Flowers, STL10, ImageNet,\nand  ObjectNet  would  remain  after  applying  all  three  fil-\nters.Thus, many of these zero-shot test sets are completely\nout-of-distribution for Stable Diffusion.Diffusion Classi-\nfier performance would likely improve significantly if Sta-\nble Diffusion were trained on a less curated training set.\n6.2. Improved Compositional Reasoning Abilities\nLarge text-to-image diffusion models are capable of gen-\nerating samples with impressive compositional generaliza-\ntion.  In this section, we test whether this generative ability\ntranslates to improved compositionalreasoning.\n6",
    "DatasetResolutionAestheticSFWA + SR + A + S\nFood61.590.599.990.556.3\nCIFAR100.03.490.33.20.0\nAircraft98.695.7100.095.694.4\nPets1.189.1100.089.10.9\nFlowers0.082.4100.082.40.0\nSTL100.031.693.130.60.0\nImageNet4.584.198.082.53.4\nObjectNet98.820.598.820.320.2\nTable 2.How in-distribution is each test set for Stable Diffu-\nsion?We  show  the  percentage  of  each  test  set  that  would  re-\nmain after the Stable Diffusion 2.0 data filtering process. The first\nthree columns show the percentage of images that pass resolution\n(≥512\n2\n), aesthetic (≥4.5), and safe-for-work (≤0.1) thresholds,\nrespectively. The last two columns show the proportion of images\nthat pass multiple filters, and the last column (R + A + S) corre-\nsponds to the actual filtering criteria used to train SD 2.0.\nWinogroundBenchmark:WecompareDiffusion\nClassifier   to   contrastive   models   like   CLIP   [60]   on\nWinoground  [75],   a  popular  benchmark  for  evaluating\nthe  visio-linguistic  compositional  reasoning  abilities  of\nvision-language  models.Each  example  in  Winoground\nconsists of 2 (image, caption) pairs. Notably, both captions\nwithin an example contain the exact same set of words, just\nin  a  different  order.   Vision-language  multimodal  models\nare scored on Winoground by their ability to match captions\nC\ni\nto their corresponding imagesI\ni\n.   Given a model that\ncomputes a score for each possible pairscore(C\ni\n,I\nj\n), the\ntext scoreof a particular example((C\n0\n,I\n0\n),(C\n1\n,I\n1\n))is 1 if\nand only if it independently prefers captionC\n0\nover caption\nC\n1\nfor imageI\n0\nand vice-versa for imageI\n1\n. Precisely, the\nmodel’s text score on an example is:\nI[score(C\n0\n,I\n0\n)> score(C\n1\n,I\n0\n)AND\nscore(C\n1\n,I\n1\n)> score(C\n0\n,I\n1\n)]\n(9)\nAchieving a high text score is extremely challenging.  Hu-\nmans  (via  Mechanical  Turk)  achieve  89.5%  accuracy  on\nthis benchmark, but even the best models do barely above\nchance.  Models can only do well if they understand com-\npositional structure within each modality.  CLIP has been\nfound to do poorly on this benchmark since its embeddings\ntend to be more like a “bag of concepts” that fail to bind\nsubjects to attributes or verbs [81].\nEach example is tagged by the type of linguistic swap\n(object, relation and both) between the two captions:\n1.  Object:  reorder elements like noun phrases that typi-\ncally refer to real-world objects/subjects.\n2.  Relation:reorder  elements  like  verbs,   adjectives,\nprepositions, and/or adverbs that modify objects.\n3.  Both: a combination of the previous two types.\nWe show examples of each swap type in Figure 5.\nFigure 5.Example visualizations of Winoground swap types.\nEach category corresponds to a different type of linguistic swap\nin the caption.  Object swaps noun phrases, Relation swaps verbs,\nadjectives, or adverbs, and Both can swap entities of both kinds.\nModelObjectRelationBothAverage\nRandom Chance25.025.025.025.0\nCLIP ViT-L/1427.025.857.728.2\nOpenCLIP ViT-H/1439.026.657.733.0\nDiffusion Classifier (ours)46.129.280.838.5\nTable 3.Compositional reasoning results on Winoground. Dif-\nfusion Classifier obtains signficantly better text score (Eq. 9) than\nthe contrastive baselines for all three swap categories.\nResultsTable  3  compares  Diffusion  Classifier  to  two\nstrong contrastive baselines:  OpenCLIP ViT-H/14 (whose\ntext embeddings Stable Diffusion conditions on) and CLIP\nViT-L/14.Diffusion  Classifier  significantly  outperforms\nboth  discriminative  approaches  on  Winoground.Our\nmethod is stronger on all three swap types, even the chal-\nlenging  “Relation”  swaps  where  the  contrastive  baselines\ndo no better than random guessing. This indicates that Dif-\nfusion Classifier’s generative approach exhibits better com-\npositional reasoning abilities.  Since Stable Diffusion uses\nthe  same  text  encoder  as  OpenCLIP  ViT-H/14,  this  im-\nprovement comes from better cross-modal binding of con-\ncepts  to  images.   Overall,  we  find  it  surprising  that  Sta-\nble Diffusion, trained with only sample generation in mind,\ncan be repurposed into such a strong classifier and reasoner\nwithout any additional training.\n6.3. Supervised Classification Results\nWe    compare    Diffusion    Classifier,leveraging    the\nImageNet-trained DiT-XL/2 model [58], to ViTs [22] and\nResNets [29] trained on ImageNet.  This setting is partic-\nularly interesting because it enables a fair comparison be-\ntween models trained on the same dataset.  Table 4 shows\nthat Diffusion Classifier outperforms ResNet-101 and ViT-\nL/32. Diffusion Classifier achieves ImageNet accuracies of\n7",
    "Method\nIDOOD\nININ-V2IN-AObjectNet\nResNet-1870.357.31.127.2\nResNet-3473.861.01.931.6\nResNet-5076.763.20.036.4\nResNet-101\n77.765.54.739.1\nViT-L/3277.964.411.932.1\nViT-L/1680.467.516.736.8\nViT-B/1681.269.6\n20.839.9\nDiffusion Classifier256\n2\n77.564.620.032.1\nDiffusion Classifier512\n2\n79.166.730.233.9\nTable 4.Standard classification on ImageNet.We compare Dif-\nfusion Classifier (using DiT-XL/2 at256\n2\nand512\n2\nresolutions)\nto discriminative models trained on ImageNet.  We highlight cells\nwhere Diffusion Classifier does better. All models (generative and\ndiscriminative) have only been trained on ImageNet.\n77.5% and 79.1% at resolutions256\n2\nand512\n2\nrespectively.\nTo the best of our knowledge, we are the first to show that a\ngenerative model trained to learnp\nθ\n(x|c)can achieve Im-\nageNet classification accuracy comparable to highly com-\npetitive discriminative methods.\n6.3.1    Better Out-of-distribution Generalization\nWe find that Diffusion Classifier surprisingly has stronger\nout-of-distribution  (OOD)  performance  on  ImageNet-A\nthan all of the baselines.  In fact, our method shows qual-\nitatively different and better OOD generalization behavior\nthan discriminative approaches.  Previous work [74] evalu-\nated hundreds of discriminative models and found a tight\nlinear  relationship  between  their  in-distribution  (ID)  and\nOOD accuracy — for a given ID accuracy,  no models do\nbetter OOD than predicted by the linear relationship.  For\nmodels trained on only ImageNet-1k (no extra data), none\nof a wide variety of approaches, from adversarial training\nto targeted augmentations to different architectures, achieve\nbetter  OOD  accuracy  than  predicted.   We  show  the  rela-\ntionship between ID ImageNet accuracy (subsampled to the\nclasses that overlap with ImageNet-A) and OOD accuracy\non ImageNet-A for these discriminative models as the blue\npoints (“standard training”) in Figure 6. The OOD accuracy\nis described well by a piecewise linear fit, with a kink at the\nImageNet accuracy of the ResNet-50 model used to identify\nthe hard images that comprise ImageNet-A. No discrimina-\ntive models show meaningful “effective robustness,” which\nis the gap between the actual OOD accuracy of a model and\nthe OOD accuracy predicted by the linear fit [74].\nHowever, in contrast to these hundreds of discriminative\nmodels, Diffusion Classifier achieves much higher OOD ac-\ncuracy on ImageNet-A than predicted.  Figure 6 shows that\nDiffusion Classifier lies far above the linear fit and achieves\nan effective robustness of 15-25%.To the best of our knowl-\n8090959697\nImageNet (class-subsampled) (top-1, %)\n5\n10\n20\n30\n40\nImageNet-A (top-1, %)\nDistribution Shift to Imagenet-A\nLinear fit (piecewise)\nStandard training\nDiffusion Classifier\nFigure 6.Diffusion Classifier exhibits effective robustness with-\nout using extra labeled data.  Compared to discriminative mod-\nels trained on the same amount of labeled data (“standard train-\ning”), Diffusion Classifier achieves much higher ImageNet-A ac-\ncuracy than predicted by its ImageNet accuracy.  Diffusion Clas-\nsifier points correspond to DiT-XL/2 at resolution256\n2\nand512\n2\n.\nPoints are shown with 99.5% Clopper-Pearson confidence inter-\nvals.  The red lines show the linear relationship between ID and\nOOD accuracy for discriminative models,  with a “break” at the\naccuracy of the model used to create ImageNet-A. The axes were\nadjusted using logit scaling, since accuracies fall within[0,100].\nedge, this is the first approach to achieve significant effec-\ntive robustness without using any extra data during training.\nThere are a few caveats to our finding. Diffusion Classi-\nfier does not show improved effective robustness on the Im-\nageNetV2 or ObjectNet distribution shifts, though perhaps\nthe nature of those shifts is different from that of ImageNet-\nA. Diffusion Classifier may do better on ImageNet-A since\nits predictions could be less correlated with the (discrimina-\ntive) ResNet-50 used to find hard examples for ImageNet-A.\nNevertheless, the dramatic improvement in effective robust-\nness on ImageNet-A is exciting and suggests that genera-\ntive classifiers are promising approaches to achieve better\nrobustness to distribution shift.\n6.3.2    Stable Training and No Overfitting\nDiffusion Classifier’s ImageNet accuracy is especially im-\npressive since DiT was trained withonly random horizon-\ntal flips, unlike typical classifiers that use RandomResized-\nCrop, Mixup [82], RandAugment [14], and other tricks to\navoid overfitting.   Training DiT with more advanced aug-\nmentations  should  further  improve  its  accuracy.   Further-\nmore, DiT training is stable with fixed learning rate and no\nregularization  other  than  weight  decay  [58].   This  stands\nin stark contrast with ViT training,  which is unstable and\nfrequently  suffers  from  NaNs,  especially  for  large  mod-\nels [28]. These results indicate that the generative objective\nlogp\nθ\n(x|c)could be a promising way to scale up training\nto even larger models without overfitting or instability.\n8",
    "ResolutionObjectiveININ-v2IN-AObjectNet\nℓ\n2\n77.564.620.033.9\n256\n2\nVLB71.657.717.924.7\nℓ\n2\n+ VLB77.564.620.033.8\nℓ\n2\n79.166.730.233.9\n512\n2\nVLB74.059.124.924.7\nℓ\n2\n+ VLB79.066.630.233.8\nTable  5.Effect  of  classification  objective.DiT  trainsε\nθ\nwith  the  uniformly  weightedℓ\n2\nloss  to  evaluate\nP\nt\nw\nt\n∥ε−\nε\nθ\n(x\nt\n,c)∥\n2\nfrom Eq. 3.  DiT also trains the learned varianceΣ\nθ\nofp\nθ\n(x\nt−1\n|x\nt\n)with  the  exact  variational  lower  bound,  which\nweights timesteps unevenly.   Since both of these weightings are\ninvolved in DiT training,  we try each objective,  as well as their\nsum,  to see which one achieves the best accuracy.  We find that\nuniformly weightingℓ\n2\nerrors across timesteps performs best.\n6.3.3    Choice of classification objective\nWhile Stable Diffusion parameterizesp\nθ\n(x\nt−1\n|x\nt\n,c)as\na  Gaussian  with  fixed  variance,  DiT  learns  the  variance\nΣ\nθ\n(x\nt\n,c). A single network outputsε\nθ\nandΣ\nθ\n, but they are\ntrained via two separate losses.ε\nθ\nis trained via a uniform\nweighting ofℓ\n2\nerrorsE\nε,t\n[∥ε−ε\nθ\n(x\nt\n,c)∥\n2\n], as this is found\nto improve sample quality.  In contrast,Σ\nθ\nis trained with\nthe exact variational lower bound. This keeps the timestep-\ndependent weighting termw\nt\nin Eq. 3 and weights theε-\nprediction  errors  by  the  inverse  of  the  variancesΣ\nθ\n(see\n[58] for more details). Since both losses are used at training\ntime,  we run an experiment to see which objective yields\nthe best accuracy as an inference-time objective. Instead of\nchoosing the class with the lowest error based on uniform\nℓ\n2\nweighting,  as  is  done  in  Algorithm  1,  we  additionally\ntry using the variational bound or the sum of the uniform\nweighting  and  the  variational  bound.   Table  5  shows  that\nthe uniformℓ\n2\nweighting does best across all datasets. This\njustifies the approximation we made to the ELBO in Eq. 4.\nThe sum of the uniformℓ\n2\nand the variational bound does\nalmost  as  well,  likely  because  the  magnitude  of  the  vari-\national  bound  is  much  smaller  than  that  of  the  uniformly\nweightedℓ\n2\n, so their sum is dominated by theℓ\n2\nterm.\n7. Conclusion and Discussion\nWe investigated the zero-shot and standard classification\nabilities of diffusion models by leveraging them as condi-\ntional  density  estimators.   By  performing  a  simple,  unbi-\nased Monte Carlo estimate of the learned conditional ELBO\nfor each class, we extractDiffusion Classifier—apower-\nful approach to turn any conditional diffusion model into\na classifier without any additional training.We find that\nthis classifier narrows the gap with state-of-the-art discrim-\ninative approaches on zero-shot and standard classification\nand significantly outperforms them on multimodal compo-\nsitional reasoning. Diffusion Classifier also exhibits far bet-\nter “effective robustness” to distribution shift.\nAccelerating InferenceWhile inference time is currently\na practical bottleneck, there are several clear ways to accel-\nerate Diffusion Classifier.  Decreasing resolution from the\ndefault512×512(for SD) would yield a dramatic speedup.\nInference at256×256is at least4×faster, and inference\nat128×128would  be  over16×faster.   Another  option\nis to use a weak discriminative model to quickly eliminate\nclasses that are clearly incorrect.  Appendix B shows that\nthis would simultaneously improve accuracy and reduce in-\nference time.  Gradient-based search could backpropagate\nthrough the diffusion model to solvearg max\nc\nlogp(x|c),\nwhich could eliminate the runtime dependency on the num-\nber of classes. New architectures could be designed to only\nuse the class conditioningctoward the end of the network,\nenabling  reuse  of  intermediate  activations  across  classes.\nFinally, note that the error prediction process is easily par-\nallelizable.   With  sufficient  scaling  or  better  GPUs  in  the\nfuture, all Diffusion Classifier steps can be done in parallel\nwith thelatency of a single forward pass.\nRole  of  Diffusion  Model  Design  DecisionsSince  we\ndon’t change the base diffusion model of Diffusion Clas-\nsifier, the choices made during diffusion training affect the\nclassifier.   For  instance,  Stable  Diffusion  [65]  conditions\nthe image generation on the text embeddings from Open-\nCLIP [38].  However, the language model in OpenCLIP is\nmuch weaker than open-ended large-language models like\nT5-XXL [62] because it is only trained on text data avail-\nable from image-caption pairs, a minuscule subset of total\ntext data on the Internet.  Hence, we believe that diffusion\nmodels trained on top of T5-XXL embeddings, such as Im-\nagen [67], should display better zero-shot classification re-\nsults, but these are not open-source to empirically validate.\nOther design choices, such as whether to perform diffusion\nin latent space (e.g. Stable Diffusion) or in pixel space (e.g.\nDALLE 2), can also affect the adversarial robustness of the\nclassifier and present interesting avenues for future work.\nIn conclusion, while generative models have previously\nfallen short of discriminative ones for classification, today’s\npace of advances in generative modeling means that they are\nrapidly catching up.  Our strong classification, multimodal\ncompositional  reasoning,  and  robustness  results  represent\nan encouraging step in this direction.\nAcknowledgementsWe  thank  Patrick  Chao  for  helpful\ndiscussions and Christina Baek and Rishi Veerapaneni for\npaper  feedback.   Stability.AI  contributed  compute  to  run\nsome  experiments.    AL  is  supported  by  the  NSF  GRFP\nDGE1745016 and DGE2140739.   This work is supported\nby NSF IIS-2024594 and ONR MURI N00014-22-1-2773.\n9",
    "References\n[1]  Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al.  Flamingo:  a\nvisual language model for few-shot learning.Advances in\nNeural  Information  Processing  Systems,  35:23716–23736,\n2022. 3\n[2]  Peter  Auer.Using  confidence  bounds  for  exploitation-\nexploration  trade-offs.Journal  of  Machine  Learning  Re-\nsearch, 3(Nov):397–422, 2002. 5\n[3]  Dmitry   Baranchuk,    Andrey   Voynov,    Ivan   Rubachev,\nValentin Khrulkov, and Artem Babenko.  Label-efficient se-\nmantic segmentation with diffusion models. InInternational\nConference on Learning Representations, 2022. 2, 5, 17\n[4]  Andrei  Barbu,  David  Mayo,  Julian  Alverio,  William  Luo,\nChristopher Wang,  Dan Gutfreund,  Joshua B. Tenenbaum,\nand  Boris  Katz.    Objectnet:   A  large-scale  bias-controlled\ndataset for pushing the limits of object recognition models.\nInNeural Information Processing Systems, 2019. 6\n[5]  David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Vari-\national inference:  A review for statisticians.Journal of the\nAmerican Statistical Association, 2017. 1\n[6]  Lukas  Bossard,  Matthieu  Guillaumin,  and  Luc  Van  Gool.\nFood-101 – mining discriminative components with random\nforests.  InEuropean Conference on Computer Vision, 2014.\n5\n[7]  Andrew Brock, Jeff Donahue, and Karen Simonyan.  Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 18\n[8]  Tom  Brown,  Benjamin  Mann,  Nick  Ryder,  Melanie  Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners.Advances in neural in-\nformation processing systems, 33:1877–1901, 2020. 1, 2\n[9]  Ryan   Burgert,   Kanchana   Ranasinghe,   Xiang   Li,   and\nMichael S Ryoo.  Peekaboo: Text to image diffusion models\nare zero-shot segmentors.arXiv preprint arXiv:2211.13224,\n2022. 2\n[10]  Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya\nSutskever,  and  Pieter  Abbeel.   Infogan:  Interpretable  rep-\nresentation learning by information maximizing generative\nadversarial nets.Advances in neural information processing\nsystems, 29, 2016. 2\n[11]  Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning  laws  for  contrastive  language-image  learning.arXiv\npreprint arXiv:2212.07143, 2022. 5\n[12]  Adam Coates, Andrew Ng, and Honglak Lee.  An analysis\nof single-layer networks in unsupervised feature learning. In\nGeoffrey Gordon, David Dunson, and Miroslav Dud\n ́\nık, ed-\nitors,Proceedings  of  the  Fourteenth  International  Confer-\nence on Artificial Intelligence and Statistics,  volume 15 of\nProceedings of Machine Learning Research, pages 215–223,\nFort Lauderdale, FL, USA, 11–13 Apr 2011. PMLR. 6\n[13]  Danilo  Croce,  Giuseppe  Castellucci,  and  Roberto  Basili.\nGAN-BERT: Generative adversarial learning for robust text\nclassification with a bunch of labeled examples. InProceed-\nings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics, 2020. 2\n[14]  Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.Randaugment:    Practical  automated  data  augmen-\ntation  with  a  reduced  search  space.InProceedings  of\nthe  IEEE/CVF  conference  on  computer  vision  and  pattern\nrecognition workshops, pages 702–703, 2020. 8\n[15]  Tri  Dao,  Daniel  Y.  Fu,  Stefano  Ermon,  Atri  Rudra,  and\nChristopher R\n ́\ne.  FlashAttention:  Fast and memory-efficient\nexact attention with IO-awareness.   InAdvances in Neural\nInformation Processing Systems, 2022. 16\n[16]  Mostafa  Dehghani,  Josip  Djolonga,  Basil  Mustafa,  Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner,\nMathilde  Caron,  Robert  Geirhos,  Ibrahim  Alabdulmohsin,\net al.  Scaling vision transformers to 22 billion parameters.\narXiv preprint arXiv:2302.05442, 2023. 3\n[17]  Jia  Deng,  Wei  Dong,  Richard  Socher,  Li-Jia  Li,  Kai  Li,\nand Li Fei-Fei.  Imagenet:  A large-scale hierarchical image\ndatabase.  In2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 6\n[18]  Jacob    Devlin,Ming-Wei    Chang,Kenton    Lee,and\nKristina  Toutanova.Bert:   Pre-training  of  deep  bidirec-\ntional  transformers  for  language  understanding.preprint\narXiv:1810.04805, 2018. 1\n[19]  Jacob Devlin,  Ming-Wei Chang,  Kenton Lee,  and Kristina\nToutanova.   Bert:  Pre-training  of  deep  bidirectional  trans-\nformers for language understanding.ArXiv, abs/1810.04805,\n2019. 2\n[20]  Prafulla Dhariwal and Alexander Nichol.  Diffusion models\nbeat gans on image synthesis.NeurIPS, 2021. 1, 2\n[21]  Laurent  Dinh,  Jascha  Sohl-Dickstein,  and  Samy  Bengio.\nDensity estimation using real nvp.CoRR, abs/1605.08803,\n2016. 1\n[22]  Alexey  Dosovitskiy,  Lucas  Beyer,  Alexander  Kolesnikov,\nDirk   Weissenborn,   Xiaohua   Zhai,   Thomas   Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain  Gelly,   et  al.An  image  is  worth  16x16  words:\nTransformers  for  image  recognition  at  scale.preprint\narXiv:2010.11929, 2020. 6, 7\n[23]  Yilun Du and Igor Mordatch.  Implicit generation and gen-\neralization in energy-based models.ArXiv, abs/1903.08689,\n2019. 2\n[24]  Aditya  Ramesh  et  al.   Hierarchical  text-conditional  image\ngeneration with clip latents, 2022. 1, 2\n[25]  Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco,\nLudwig Schmidt, and Shuran Song.  Clip on wheels:  Zero-\nshot object navigation as object localization and exploration.\narXiv preprint arXiv:2203.10421, 2022. 3\n[26]  Ian  Goodfellow,  Jean  Pouget-Abadie,  Mehdi  Mirza,  Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio.   Generative adversarial nets.   InAdvances\nin neural information processing systems, pages 2672–2680,\n2014. 1, 2\n[27]  Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen,\nDavid  Duvenaud,  Mohammad  Norouzi,  and  Kevin  Swer-\nsky.   Your classifier is secretly an energy based model and\n10",
    "you should treat it like one.  InInternational Conference on\nLearning Representations, 2020. 2\n[28]  Kaiming He,  Xinlei Chen,  Saining Xie,  Yanghao Li,  Piotr\nDoll\n ́\nar, and Ross Girshick. Masked autoencoders are scalable\nvision learners.arXiv:2111.06377, 2021. 2, 8\n[29]  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep  residual  learning  for  image  recognition.    InCVPR,\n2016. 6, 7\n[30]  Dan  Hendrycks,  Kevin  Zhao,  Steven  Basart,  Jacob  Stein-\nhardt, and Dawn Song. Natural adversarial examples.CVPR,\n2021. 6\n[31]  Geoffrey E. Hinton.  To recognize shapes, first learn to gen-\nerate images.Progress in brain research, 2007. 1, 2\n[32]  Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A\nfast learning algorithm for deep belief nets.Neural Comput.,\n2006. 2\n[33]  R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\nKaran Grewal, Phil Bachman, Adam Trischler, and Yoshua\nBengio.  Learning deep representations by mutual informa-\ntion estimation and maximization.  InInternational Confer-\nence on Learning Representations, 2019. 2\n[34]  Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi  Gao,  Alexey  Gritsenko,  Diederik  P.  Kingma,  Ben\nPoole,  Mohammad Norouzi,  David J. Fleet,  and Tim Sali-\nmans.  Imagen video: High definition video generation with\ndiffusion models, 2022. 1, 2\n[35]  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models.Advances in Neural Information\nProcessing Systems, 33:6840–6851, 2020. 1, 2, 3\n[36]  Jonathan  Ho  and  Tim  Salimans.    Classifier-free  diffusion\nguidance.arXiv preprint arXiv:2207.12598, 2022. 18\n[37]  Yujia  Huang,  James  Gornet,  Sihui  Dai,  Zhiding  Yu,  Tan\nNguyen, Doris Tsao, and Anima Anandkumar.  Neural net-\nworks with recurrent generative feedback.  In H. Larochelle,\nM. Ranzato,  R. Hadsell,  M.F. Balcan,  and H. Lin,  editors,\nAdvances in Neural Information Processing Systems. Curran\nAssociates, Inc., 2020. 2\n[38]  Gabriel   Ilharco,   Mitchell   Wortsman,   Nicholas   Carlini,\nRohan  Taori,   Achal  Dave,   Vaishaal  Shankar,   Hongseok\nNamkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi,\net al. Openclip.Zenodo, 4:5, 2021. 3, 9\n[39]  Phillip  Isola,  Jun-Yan  Zhu,  Tinghui  Zhou,  and  Alexei  A\nEfros.   Image-to-image  translation  with  conditional  adver-\nsarial networks.CVPR, 2017. 1\n[40]  Tero Karras,  Samuli Laine,  and Timo Aila.   A style-based\ngenerator  architecture  for  generative  adversarial  networks.\nIEEE Trans. Pattern Anal. Mach. Intell., 43(12):4217–4228,\n2021. 1\n[41]  Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye.  Dif-\nfusionclip:  Text-guided  diffusion  models  for  robust  image\nmanipulation.  InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n2426–2435, June 2022. 15\n[42]  Diederik Kingma and Max Welling.   Auto-encoding varia-\ntional bayes.ICLR, 12 2013. 1, 2\n[43]  Zhifeng  Kong,  Wei  Ping,  Jiaji  Huang,  Kexin  Zhao,  and\nBryan Catanzaro.  Diffwave: A versatile diffusion model for\naudio  synthesis.   InInternational  Conference  on  Learning\nRepresentations, 2021. 2\n[44]  Simon  Kornblith,  Jonathon  Shlens,  and  Quoc  V  Le.    Do\nbetter imagenet models transfer better?    InProceedings of\nthe  IEEE/CVF  conference  on  computer  vision  and  pattern\nrecognition, pages 2661–2671, 2019. 6\n[45]  Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10\n(canadian institute for advanced research), 2010. 5\n[46]  Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and\nFujie Huang. A tutorial on energy-based learning.Predicting\nstructured data, 1(0), 2006. 1, 2\n[47]  Daiqing Li,  Junlin Yang,  Karsten Kreis,  Antonio Torralba,\nand  Sanja  Fidler.    Semantic  segmentation  with  generative\nmodels: Semi-supervised learning and strong out-of-domain\ngeneralization.  InConference on Computer Vision and Pat-\ntern Recognition (CVPR), 2021. 2\n[48]  Junnan  Li,  Dongxu  Li,  Caiming  Xiong,  and  Steven  Hoi.\nBlip:   Bootstrapping  language-image  pre-training  for  uni-\nfied  vision-language  understanding  and  generation.   InIn-\nternational Conference on Machine Learning, pages 12888–\n12900. PMLR, 2022. 3, 15\n[49]  Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei  Yang,  Chunyuan  Li,  Yiwu  Zhong,  Lijuan  Wang,  Lu\nYuan,  Lei  Zhang,  Jenq-Neng  Hwang,  et  al.Grounded\nlanguage-image   pre-training.InProceedings   of   the\nIEEE/CVF  Conference  on  Computer  Vision  and  Pattern\nRecognition, pages 10965–10975, 2022. 3\n[50]  Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui   Zeng,   Xun   Huang,   Karsten   Kreis,   Sanja   Fi-\ndler,  Ming-Yu  Liu,  and  Tsung-Yi  Lin.Magic3d:   High-\nresolution   text-to-3d   content   creation.arXiv   preprint\narXiv:2211.10440, 2022. 2\n[51]  Hao  Liu  and  P.  Abbeel.   Hybrid  discriminative-generative\ntraining  via  contrastive  learning.ArXiv,  abs/2007.09070,\n2020. 2\n[52]  Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley. Audi-\noldm: Text-to-audio generation with latent diffusion models.\narXiv preprint arXiv:2301.12503, 2023. 2\n[53]  Subhransu   Maji,   Esa   Rahtu,   Juho   Kannala,   Matthew\nBlaschko, and Andrea Vedaldi.   Fine-grained visual classi-\nfication of aircraft.arXiv preprint arXiv:1306.5151, 2013.\n6\n[54]  Andrew Ng and Michael Jordan.  On discriminative vs. gen-\nerative classifiers:  A comparison of logistic regression and\nnaive bayes.Advances in neural information processing sys-\ntems, 14, 2001. 1, 2\n[55]  Alex  Nichol,  Prafulla  Dhariwal,  Aditya  Ramesh,  Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen.   GLIDE: towards photorealistic image genera-\ntion and editing with text-guided diffusion models.CoRR,\nabs/2112.10741, 2021. 2\n[56]  Maria-Elena Nilsback and Andrew Zisserman.   Automated\nflower classification over a large number of classes.   InIn-\ndian Conference on Computer Vision, Graphics and Image\nProcessing, Dec 2008. 6\n11",
    "[57]  Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nC. V. Jawahar. Cats and dogs. InIEEE Conference on Com-\nputer Vision and Pattern Recognition, 2012. 6\n[58]  William Peebles and Saining Xie. Scalable diffusion models\nwith transformers.arXiv preprint arXiv:2212.09748, 2022.\n4, 6, 7, 8, 9, 17\n[59]  Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.   Dreamfusion:  Text-to-3d  using  2d  diffusion.arXiv\npreprint arXiv:2209.14988, 2022. 1, 2\n[60]  Alec  Radford,   Jong  Wook  Kim,   Chris  Hallacy,   Aditya\nRamesh,  Gabriel  Goh,  Sandhini  Agarwal,  Girish  Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al.  Learn-\ning transferable visual models from natural language super-\nvision.   InInternational Conference on Machine Learning,\npages 8748–8763. PMLR, 2021. 5, 7, 17\n[61]  Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al.   Language models are unsu-\npervised multitask learners.OpenAI blog, 1(8):9, 2019. 3\n[62]  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu.   Exploring the limits of transfer learning with\na unified text-to-text transformer.The Journal of Machine\nLearning Research, 21(1):5485–5551, 2020. 9\n[63]  Marc’Aurelio Ranzato, Joshua Susskind, Volodymyr Mnih,\nand Geoffrey Hinton. On deep generative models with appli-\ncations to recognition. InCVPR 2011, 2011. 2\n[64]  Benjamin  Recht,  Rebecca  Roelofs,  Ludwig  Schmidt,  and\nVaishaal Shankar.  Do imagenet classifiers generalize to im-\nagenet?  InInternational Conference on Machine Learning,\n2019. 6\n[65]  Robin  Rombach,   Andreas  Blattmann,   Dominik  Lorenz,\nPatrick  Esser,  and  Bj\n ̈\norn  Ommer.    High-resolution  image\nsynthesis  with  latent  diffusion  models.   InProceedings  of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684–10695, 2022. 2, 4, 5, 9, 16\n[66]  Nataniel  Ruiz,  Yuanzhen  Li,  Varun  Jampani,  Yael  Pritch,\nMichael Rubinstein, and Kfir Aberman.  Dreambooth:  Fine\ntuning  text-to-image  diffusion  models  for  subject-driven\ngeneration.arXiv preprint arXiv:2208.12242, 2022. 1\n[67]  Chitwan  Saharia,   William  Chan,   Saurabh  Saxena,   Lala\nLi,   Jay   Whang,   Emily   Denton,   Seyed   Kamyar   Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi.  Photorealistic text-to-image diffusion models with\ndeep language understanding.  InAdvances in Neural Infor-\nmation Processing Systems, 2022. 1, 2, 9\n[68]  Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade   Gordon,    Ross   Wightman,    Mehdi   Cherti,    Theo\nCoombes,  Aarush  Katta,  Clayton  Mullis,  Mitchell  Worts-\nman,  et  al.Laion-5b:   An  open  large-scale  dataset  for\ntraining next generation image-text models.arXiv preprint\narXiv:2210.08402, 2022. 5, 6\n[69]  Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang  Zhang,  Qiyuan  Hu,  Harry  Yang,  Oron  Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video:  Text-to-video generation without text-video\ndata. InThe Eleventh International Conference on Learning\nRepresentations, 2023. 2\n[70]  Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand  Surya  Ganguli.Deep  unsupervised  learning  using\nnonequilibrium thermodynamics.   InInternational Confer-\nence on Machine Learning, pages 2256–2265. PMLR, 2015.\n1, 2, 3\n[71]  Jiaming Song, Chenlin Meng, and Stefano Ermon.  Denois-\ning diffusion implicit models.   InInternational Conference\non Learning Representations, 2021. 15\n[72]  Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution.Advances in neural\ninformation processing systems, 32, 2019. 2\n[73]  Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole.  Score-based\ngenerative  modeling  through  stochastic  differential  equa-\ntions.arXiv preprint arXiv:2011.13456, 2020. 2\n[74]  Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Car-\nlini, Benjamin Recht, and Ludwig Schmidt.  Measuring ro-\nbustness  to  natural  distribution  shifts  in  image  classifica-\ntion.Advances in Neural Information Processing Systems,\n33:18583–18599, 2020. 3, 8\n[75]  Tristan   Thrush,   Ryan   Jiang,   Max   Bartolo,   Amanpreet\nSingh,  Adina  Williams,  Douwe  Kiela,  and  Candace  Ross.\nWinoground: Probing vision and language models for visio-\nlinguistic compositionality. InProceedings of the IEEE/CVF\nConference  on  Computer  Vision  and  Pattern  Recognition,\npages 5238–5248, 2022. 2, 6, 7, 16\n[76]  A\n ̈\naron van den Oord,  Sander Dieleman,  Heiga Zen,  Karen\nSimonyan, Oriol Vinyals, Alexander Graves, Nal Kalchbren-\nner, Andrew Senior, and Koray Kavukcuoglu.  Wavenet:  A\ngenerative model for raw audio. InArxiv, 2016. 1\n[77]  A\n ̈\naron   Van   Den   Oord,   Nal   Kalchbrenner,   and   Koray\nKavukcuoglu.  Pixel recurrent neural networks.  InInterna-\ntional  conference  on  machine  learning,  pages  1747–1756.\nPMLR, 2016. 1\n[78]  Ruben  Villegas,  Mohammad  Babaeizadeh,  Pieter-Jan  Kin-\ndermans,  Hernan Moraldo,  Han Zhang,  Mohammad Taghi\nSaffar,   Santiago   Castro,   Julius   Kunze,   and   D.   Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description.ArXiv, abs/2210.02399, 2022. 2\n[79]  Pascal Vincent.  A connection between score matching and\ndenoising autoencoders.Neural Computation, 23(7):1661–\n1674, 2011. 1\n[80]  P. Vincent,  H. Larochelle,  Y. Bengio,  and P.-A. Manzagol.\nExtracting and composing robust features with denoising au-\ntoencoders. InICML, 2008. 2\n[81]  Yutaro Yamada, Yingtian Tang, and Ilker Yildirim. When are\nlemons purple?  the concept association bias of clip.arXiv\npreprint arXiv:2212.12043, 2022. 7\n[82]  Hongyi  Zhang,  Moustapha  Cisse,  Yann  N  Dauphin,  and\nDavid Lopez-Paz.  mixup: Beyond empirical risk minimiza-\ntion.arXiv preprint arXiv:1710.09412, 2017. 8\n[83]  Yuxuan  Zhang,  Huan  Ling,  Jun  Gao,  Kangxue  Yin,  Jean-\nFrancois  Lafleche,  Adela  Barriuso,  Antonio  Torralba,  and\nSanja Fidler.  Datasetgan: Efficient labeled data factory with\nminimal human effort. InCVPR, 2021. 2\n[84]  Roland  S  Zimmermann,  Lukas  Schott,  Yang  Song,  Ben-\njamin A Dunn, and David A Klindt.  Score-based generative\nclassifiers.arXiv preprint arXiv:2110.00473, 2021. 2\n12",
    "Appendix\nA. Efficient Diffusion Classifier Algorithm\nThough  Diffusion  Classifier  works  straightforwardly  with  the  procedure  described  in  Algorithm  1,  we  are  interested\nin speeding up inference as described in Section 4.2.  Algorithm 2 shows the efficient Diffusion Classifier procedure that\nadaptively  chooses  which  classes  to  continue  evaluating.   Table  6  shows  the  evaluation  strategy  used  for  each  zero-shot\ndataset.  We hand-picked the strategies based on the number of classes in each dataset.  Further gains in accuracy may be\npossible with more evaluations.\nAlgorithm 2Diffusion Classifier (Adaptive)\n1:Input:test imagex, conditioning inputsC={c\ni\n}\nn\ni=1\n(e.g., text embeddings or class indices), number of stagesN\nstages\n,\nlistKeepListof number ofc\ni\nto keep after each stage, listTrialListof number of trials done by each stage\n2:InitializeErrors[c\ni\n] =list()for eachc\ni\n3:InitializePrevTrials= 0// How many times we’ve tried each remaining element ofCso far\n4:forstage i= 1,...,N\nstages\ndo\n5:fortrialj= 1,...,TrialList[i]−PrevTrialsdo\n6:Samplet∼[1,1000]\n7:Sampleε∼N(0,I)\n8:x\nt\n=\n√\n ̄α\nt\nx+\n√\n1− ̄α\nt\nε\n9:forconditioningc\nk\n∈Cdo\n10:Errors[c\nk\n].append(∥ε−ε\nθ\n(x\nt\n,c\nk\n)∥\n2\n)\n11:end for\n12:end for\n13:C ←arg min\nS⊂C;\n|S|=KeepList[i]\nX\nc\nk\n∈S\nmean(Errors[c\nk\n])// Keep topKeepList[i]conditioningsc\nk\nwith the lowest errors\n14:PrevTrials=TrialList[i]\n15:end for\n16:returnarg min\nc\ni\n∈C\nmean(Errors[c\ni\n])\nDatasetPrompts kept per stageEvaluations per stageAvg. evaluations per classTotal evaluations\nFood10120 10 5 120 50 100 50050.75120\nCIFAR105 150 5002752750\nAircraft20 10 5 120 50 100 500515100\nPets5 125 250511890\nFlowers10220 10 5 120 50 100 50050.45140\nSTL105 1100 5003003000\nImageNet500 50 10 150 100 500 1000100100000\nObjectNet25 10 5 150 100 500 1000118.613400\nTable 6. Adaptive evaluation strategy for each zero-shot dataset.\nB. Inference Costs and Hybrid Classification Approach\nTable 7 shows the inference time of Diffusion Classifier when using the efficient Diffusion Classifier algorithm (Algo-\nrithm 2).  Classifying a single image takes anywhere between 18 seconds (Pets) to 1000 seconds (ImageNet).  The issue\nwith ImageNet is that Diffusion Classifier inference time still approximately scales linearly with the number of classes, even\nwhen using the adaptive strategy. One way to address this problem is to use a weak discriminative model to quickly “prune”\naway classes that are almost certainly incorrect.  Table 7 shows that using Diffusion Classifier to choose among the top 20\nclass predictions made by CLIP ResNet-50 for an image greatly reduces inference time, while even improving performance.\nThis pruning procedure only requires the top-20 accuracy of the fast discriminative model to be high (close to 100%), so\n13",
    "Food101CIFAR10AircraftOxford PetsFlowers102STL10ImageNet\nDiffusion Classifier77.788.526.487.366.395.461.4\nTime/img (s)5130511851301000\nDiffusion Classifier w/ discriminative pruning78.788.526.886.467.095.462.6\nTime/img (s)353035183530150\nEst. Time/img (s) at128\n2\nres2221229\nCLIP ResNet-5081.175.619.385.465.994.358.2\nTable  7.Zero-shot  accuracy  and  inference  time  with  Stable  Diffusion512×512.“Pruning”  away  unlikely  classes  with  a  weak\ndiscriminative classifier (e.g., CLIP ResNet-50) increases accuracy and reduces inference time. Additionally, reducing resolution to128×\n128would reduce inference time by roughly16×.  However, its impact on accuracy is difficult to estimate without retraining the Stable\nDiffusion model to expect lower resolutions. All times are estimated using a RTX 3090 GPU.\nit works even when the top-1 accuracy of the ResNet-50 is low, like on Aircraft.  We chose top-20 intuitively, without any\nhyperparameter search, and tuning thekfor top-kpruning will trade off between inference time and accuracy. Note that no\nother results in this paper use the discriminative pruning procedure, to avoid conflating the capabilities of Diffusion Classifier\nwith those of the weak discriminative model used to prune.\nC. Inference Objective Function\nFood101CIFAR10AircraftOxford PetsFlowers102STL10ImageNetObjectNet\nSquaredℓ\n2\n77.784.426.486.362.295.461.443.4\nℓ\n1\n73.888.422.187.366.395.459.636.8\nHuber77.784.626.786.662.695.460.943.5\nTable 8.Diffusion Classifier zero-shot performance with different loss functionsL(ε−ε\nθ\n(x\nt\n,c)).\nResolutionObjectiveImageNetImageNetV2ImageNet-AObjectNet\n256\n2\nSquaredℓ\n2\n77.564.620.032.1\n256\n2\nℓ\n1\n74.960.59.724.7\n512\n2\nSquaredℓ\n2\n79.166.730.233.9\n512\n2\nℓ\n1\n75.662.113.226.2\nTable 9.Diffusion Classifier supervised performance with different loss functionsL(ε−ε\nθ\n(x\nt\n,c)).\nWhile  the  theory  in  Section  3  justifies  using∥ε−ε\nθ\n(x\nt\n,c)∥\n2\n2\nwithin  the  Diffusion  Classifier  inference  objective,  we\nsurprisingly find that other loss functions can work better in some cases.  Table 8 shows that∥ε−ε\nθ\n(x\nt\n,c)∥\n1\n(theℓ\n1\nloss)\ninstead of the squaredℓ\n2\nloss does better on roughly half of the datasets that we use to evaluate the Stable Diffusion-based\nzero-shot classifier.  This is puzzling, since theℓ\n1\nloss is neither theoretically justified nor appears in the Stable Diffusion\ntraining objective.  We hope followup work can explain the empirical success of theℓ\n1\nloss.  Combining these two losses\ndoes not get the “best of both worlds.”  The Huber loss, which is the squaredℓ\n2\nloss for values less than 1 and is theℓ\n1\nloss for values greater than 1,  roughly achieves the same performance as the theoretically-justified squaredℓ\n2\nloss.   We\nchoose between squaredℓ\n2\nandℓ\n1\nas a hyperparameter for Section 6.1. Table 9 shows thatℓ\n1\ndoes not help with supervised\nclassification (Section 6.3) using DiT-XL/2.\nD. Interpretability via Image Generation\nIn contrast to discriminative classifiers, where it is difficult to understand what features the model has learned or why a\nmodel has made a certain decision, generative classifiers are easier to visualize.  In this section, we examine how samples\nfrom the generative model can help us understand class-dependent features that the model has learned as well as failures in\nthe model’s understanding.\n14",
    "Input ImageDDIM Inversion\nw/ BLIP caption \nDDIM Inversion\nw/ human-modified \nBLIP caption \nDDIM Inversion\nw/ correct class name\nas prompt\nDDIM Inversion\nw/ incorrect class\nname as prompt\nDDIM Inversion\nw/ incorrect class\nname as prompt\nFigure 7.Analyzing Diffusion Classifier for Zero-Shot Classification:We analyze the role of different text/captions (BLIP, Human-\nmodified BLIP, correct class-name,  incorrect class-name) for zero-shot classification using text-based diffusion models.   To do so,  we\ninvert the input image using the corresponding caption and then reconstruct it using deterministic DDIM sampling.  The image inverted\nand reconstructed using a human-modified BLIP caption aligns the most with the input image since this caption is the most descriptive.\nThe images reconstructed usingcorrect class names as prompts (column 4)   align much better with the input image in terms of class-\ndescriptive features of the underlying object than the images reconstructed usingincorrect class names as prompts (columns 5 and 6) .\nRow 3 (columns 4 and 5) demonstrates an example where the base Stable Diffusion does not understand the difference between the two\ncat breeds, Birman and Ragdoll, and hence cannot invert/sample them differently. As a result, our classifier also fails.\nExperiment SetupGiven an input image,  we first perform DDIM inversion [71, 41] (with 50 timesteps) using Stable\nDiffusion 2.0 and different captions as prompts:  BLIP [48] generated caption, human-refined BLIP generated caption, “a\nphoto of{correct-class-name}, a type of pet” and “a photo of{incorrect-class-name}, a type of pet.”.  Next, we leverage\nthe inverted DDIM latent and the corresponding prompt to attempt to reconstruct the original image (using a deterministic\ndiffusion scheduler [71]). The underlying intuition behind this experiment is that the inverted image should look more similar\nto the original image when a correct and appropriate/descriptive prompt is used for DDIM inversion and sampling.\nExperimental  EvaluationFigure  7  shows  the  results  of  this  experiment  for  the  Oxford-IIIT  Pets  dataset.   The  image\ninverted using a human-modified BLIP caption (column 3) is the most similar to the original image (column 1). This aligns\nwith our intuition as this caption is most descriptive of the input image.  The human-modified caption only adds the correct\nclass name (Bengal Cat, American Bull Dog, Birman Cat) ahead of the BLIP predicted “cat or dog” token for the foreground\nobject and slightly enhances the description for the background.  Comparing the BLIP-caption results (column 2) with the\nhuman-modified  BLIP-caption  results  (column  3),  we  can  see  that  by  just  using  the  class-name  as  the  extra  token,  the\ndiffusion model can inherit class-descriptive features.  The Bengal cat has stripes, the American Bulldog has a wider chin,\nand the Birman cat has a black patch on its face in the reconstructed image.\nCompared to the images generated using the human-generated caption as a prompt, the images reconstructed using only\nclass names as prompts (columns 4,5,6) align less with the input image (column 1).  This is expected, as class names by\nthemselves  are  not  dense  descriptions  of  the  input  images.   Comparing  the  results  of  column  4  (correct  class  names  as\nprompt)  with  those  of  column  5,6  (incorrect  class  names  as  prompt),  we  can  see  that  the  foreground  object  has  similar\nclass-descriptive features (brown and black stripes in row 1 and black face patches in row 3) to the input image for the\ncorrect-prompt reconstructions.  This highlights the fact that although using class names as approximate prompts will not\nlead to perfect denoising (Eq. 7),for the global prediction task of classification,  the correct class names should provide\nenough descriptive features for denoising, relative to the incorrect class names.\nRow 3 of Figure 7 further highlights an example of a failure mode where Stable Diffusion generates very similar inverted\nimages for correct Birman and incorrect Ragdoll text prompts.  As a result, our model also incorrectly classifies the Birman\ncat as a Ragdoll.  To fix this failure mode, we tried finetuning the Stable Diffusion model on a dataset of Ragdoll/Birman\n15",
    "cats (175 images in total).  Using this finetuned model, Diffusion Classifier accuracy on these two classes increases to 85%,\nfrom an initial zero-shot accuracy of 45%.  In addition to minimizing the standardε-prediction error∥ε−ε\nθ\n(x\nt\n,c\ni\n)∥\n2\n, we\nfound that adding a loss term toincreasethe error∥ε−ε\nθ\n(x\nt\n,c\nj\n)∥\n2\nfor the wrong classc\nj\nhelped the model distinguish these\ncommonly-confused classes.\nE. How Does Stable Diffusion Version Affect Zero-Shot Accuracy?\nSD VersionFood101CIFAR10AircraftOxford PetsFlowers102STL10ImageNetObjectNet\n1.160.383.420.178.843.192.651.738.1\n1.275.785.926.385.454.494.457.339.4\n1.377.587.527.887.254.594.959.740.9\n1.477.886.028.687.454.294.859.241.2\n1.578.485.529.187.555.094.559.641.6\n2.077.788.526.487.366.395.461.443.4\n2.177.987.124.386.259.495.358.438.3\nTable 10.Effect of Stable Diffusion version on Diffusion Classifier zero-shot accuracy.  We bold the best version within SD 1.x and\n2.x. For SD 1, accuracy tends to increase with more training. The main exception is on low-resolution datasets like CIFAR10 and STL10.\nSD 2 performance consistently decreases from SD 2.0 to SD 2.1 on almost every dataset.\nWe investigate how much the Stable Diffusion checkpoint version affects Diffusion Classifier’s zero-shot classification\naccuracy.  Table 10 shows zero-shot accuracy for each Stable Diffusion release version so far.  We use the same adaptive\nevaluation strategy (Algorithm 2) for each version.  Accuracy improves with each new release for SD 1.x, as more training\nlikely reduces underfitting on the training data.  However, accuracy actually decreases when going from SD 2.0 to 2.1.  The\ncause  of  this  is  not  clear,  especially  without  access  to  intermediate  training  checkpoints.   One  hypothesis  is  that  further\ntraining on512\n2\nresolution images causes the model to forget knowledge from its initial256\n2\nresolution training set, which\nis closer to the distribution of these zero-shot benchmarks.  SD 2.1 was finetuned using a more permissive NSFW threshold\n(≥0.98instead of≥0.1), so another hypothesis is that this introduced a lot of human images that hurt performance on our\nobject-centric benchmarks.\nF. Additional Implementation Details\nF.1. Zero-shot classification using Diffusion Classifier\nTraining DataFor our zero-shot Diffusion Classifier, we utilize Stable Diffusion 2.0 [65].  This model was trained on a\nsubset of the LAION-5B dataset, filtered so that the training data is aesthetic and appropriately safe-for-work. LAION clas-\nsifiers were used to remove samples that are too small (less than512×512), potentially not-safe-for-work (punsafe≥0.1),\nor unaesthetic (aesthetic score≤4.5). These thresholds are conservative, since false negatives (NSFW or undesirable images\nleft in the training set) are worse than removing extra images from a large starting dataset. As discussed in Section 6.1, these\nfiltering criteria bias the distribution of Stable Diffusion training data and likely negatively affect Diffusion Classifier’s per-\nformance on datasets whose images do not satisfy these criteria. SD 2.0 was trained for 550k steps at resolution256×256on\nthis subset, followed by an additional 850k steps at resolution512×512on images that are at least that large. This checkpoint\ncan be downloaded online through the diffusers repository atstabilityai/stable-diffusion-2-0-base.\nInference DetailsWe use FP16 and Flash Attention [15] to improve inference speed. This enables efficient inference with\na batch size of 32, which works across a variety of GPUs, from RTX 2080Ti to A6000. We found that adding these two tricks\ndid not affect test accuracy compared to using FP32 without Flash Attention. Given a test image, we resize the shortest edge\nto 512 pixels using bicubic interpolation, take a512×512center crop, and normalize the pixel values to[−1,1].  We then\nuse the Stable Diffusion autoencoder to encode the512×512×3RGB image into a64×64×4latent. We finally classify\nthe test image by applying the method described in Sections 3 and 4 to estimateε-prediction error in this latent space.\nF.2. Compositional reasoning using Diffusion Classifier\nFor our experiments on the Winoground benchmark [75], most details are the same as the zero-shot details described in\nAppendix F.1. We use Stable Diffusion 2.0, and we evaluate each image-caption pair with 1000 evenly spaced timesteps. We\n16",
    "ArchConv1Conv2Conv3 x2Conv4 x2Conv5 x2\nResNet-187x7x643x3 max-pool3x3x1283x3x2563x3x512\nResNet-18 (SD Features)3x3x1280-3x3x12803x3x25603x3x2560\nTable 11. Comparison of SD Features’ ResNet-18 classifier architecture with the original ResNet-18\nomit the adaptive inference strategy since there are only 4 image-caption pairs to evaluate for each Winoground example.\nF.3. ImageNet classification using Diffusion Classifier\nFor this task, we use the recently proposed Diffusion Transformer (DiT) [58] as the backbone of our Diffusion Classifier.\nDiT was trained on ImageNet-1k, which contains about 1.28 million images from 1,000 classes.  While it was originally\ntrained to produce high-quality samples with strong FID scores, we repurpose the model and compare it against discriminative\nmodels with the same ImageNet-1k training data.  We use the DiT-XL/2 model size at resolution256\n2\nand512\n2\n.  Notably,\nDiT achieves strong performance while using much weaker data augmentations than what discriminative models are usually\ntrained with. During training time for the256\n2\ncheckpoint, the smaller edge of the input image is resized to 256 pixels. Then,\na256×256center crop is taken, followed by a random horizontal flip, followed by embedding with the Stable Diffusion\nautoencoder. A similar process is done for the512\n2\nmodel. At test time, we follow the same preprocessing pipeline, but omit\nthe random horizontal flip.  Classification performance could improve if stronger augmentations, like RandomResizedCrop\nor color jitter, are used during the diffusion model training process.\nF.4. Baselines for Zero-Shot Classification\nSynthetic SD Data:We provide the implementation details of the “Synthetic SD Data” baseline (row 1 of Table 1) for\nthe task of zero-shot image classification.  Our Diffusion Classifier approach builds on the intuition that a model capable of\ngenerating examples of desired classes should be able to directly discriminate between them. In contrast, this baseline takes\nthe simple approach of using our generative model, Stable Diffusion, as intended to generatesynthetic training datafor a\ndiscriminative model.  For a given dataset, we use pre-trained Stable Diffusion 2.0 with default settings to generate10,000\nsynthetic512×512pixel images per class as follows: we use the English class name and randomly sample a template from\nthose provided by the CLIP [60] authors to form the prompt for each generation.  We then train a supervised ResNet-50\nclassifier using the synthetic data and the labels corresponding to the class name that was used to generate each image.  We\nuse batch size= 256, weight decay= 1e−4, learning rate= 0.1with a cosine schedule, the AdamW optimizer, and use\nrandom resized crop & horizontal flip transforms.  We create a validation set using the synthetic data by randomly selecting\n10% of the images for each class; we use this for early stopping to prevent over-fitting. Finally, we report the accuracy on the\ntarget dataset’s proper test set.\nSD Features:We provide the implementation details of the “SD Features” baseline (row 2 of Table 1) for the task of\nimage classification. This baseline is inspired by Label-DDPM [3], a recent work on diffusion-based semantic segmentation.\nUnlike Label-DDPM, which leverages a category-specific diffusion model,  we directly build on top of the open-sourced\nStable Diffusion model (trained on the LAION dataset). We then approach the task of classification as follows: given the pre-\ntrained Stable Diffusion model, we extract the intermediate U-Net features corresponding to the input image. These features\nare then passed through a ResNet-based classifier to predict logits for the potential classes. To extract the intermediate U-Net\nfeatures,  we add a noise equivalent to the100thtimestep noise to the input image and evaluate the corresponding noisy\nlatent using the forward diffusion process.  We then pass the noisy latent through the U-Net model, conditioned on timestep\nt= 100and text conditioning (c) as an empty string, and extract the features from the mid-layer of the U-Net at a resolution\nof [8 × 8 × 1024].  Next, we train a supervised classifier on top of these features.Thus, this baseline is not zero-shot.The\narchitecture of our classifier is similar to ResNet-18, with small modifications to make it compatible with an input size of\n[8×8×1024].  Table 11 defines these modifications.  We set batch size= 16, learning rate= 1e−4, and use the AdamW\noptimizer. During training, we apply image augmentations typically used by discriminative classifiers (RandomResizedCrop\nand horizontal flip). We do early stopping using the validation set to prevent overfitting.\nG. Techniques that did not help\nDiffusion Classifier requires many samples to accurately estimate the ELBO. In addition to using the techniques in Sec-\ntion 3 and 4, we tried several other options for variance reduction. None of the following methods worked, however. We list\nnegative results here for completeness, so others do not have to retry them.\n17",
    "Classifier-free guidanceClassifier-free guidance [36] is a technique that improves the match between a prompt and gener-\nated image, at the cost of mode coverage. This is done by training a conditionalε\nθ\n(x\nt\n,c)and unconditionalε\nθ\n(x\nt\n)denoising\nnetwork and combining their predictions at sampling time:\n ̃ε(x\nt\n,c) = (1 +w)ε\nθ\n(x\nt\n,c)−wε\nθ\n(x\nt\n)(10)\nwherewis a guidance weight that is typically in the range[0,10].  Most diffusion models are trained to enable this trick by\noccasionally replacing the conditioningcwith an empty token. Intuitively, classifier-free guidance “sharpens”logp\nθ\n(x|c)\nby encouraging the model to move away from regions that unconditionally have high probability. We test Diffusion Classifier\nto see if using the ̃εfrom classifier-free guidance can improve confidence and classification accuracy.  Our newε-prediction\nmetric is now∥ε−(1 +w)ε\nθ\n(x\nt\n,c)−wε\nθ\n(x\nt\n)∥\n2\n.  However, Figure 8 shows thatw= 0(i.e., no classifier-free guidance)\nperforms best.  We hypothesize that this is because Diffusion Classifier fails on uncertain examples, which classifier-free\nguidance affects unpredictably.\nError map croppingThe ELBOE\nt,ε\n[∥ε−ε\nθ\n(x\nt\n,c)∥\n2\n]depends on accurately estimating the added noise at every location\nof the64×64×4image latent. We try to reduce the impact of edge pixels (which are less likely to contain the subject) by\ncomputingx\nt\nas normal, but only measuring the ELBO on a center crop ofεandε\nθ\n(x\nt\n,c). We compute:\n∥ε\n[i:−i,i:−i]\n−ε\nθ\n(x\nt\n,c)\n[i:−i,i:−i]\n∥\n2\n(11)\nwhereiis the number of latent “pixels” to remove from each edge.  However, Figure 9 shows that any amount of cropping\nreduces accuracy.\n20246810\nGuidance Scale\n0\n25\n50\n75\nAccuracy\nFigure 8. Accuracy plot of classifier-free guidance on Pets.\n051015202530\nAmount cropped off each side\n20\n40\n60\n80\nAccuracy\nFigure 9. Croppingεandε\nθ\n(x\nt\n,c)reduces accuracy on Pets.\nImportance samplingImportance sampling is a common method for reducing the variance of a Monte Carlo estimate.\nInstead of samplingε∼N(0,I), we sampleεfrom a narrower distribution. We first tried fixingε= 0, which is the mode of\nN(0,I), and only varying the timestept.  We also tried the truncation trick [7] which samplesε∼ N(0,I)but continually\nresamples elements that fall outside the interval[a,b].  Finally, we tried samplingε∼ N(0,I)and rescaling them to the\nexpected norm (ε→\nε\n∥ε∥\n2\nE\nε\n′\n[∥ε\n′\n∥\n2\n])) so that there are no outliers.  Table 12 shows that none of these importance sampling\nstrategies improve accuracy. This is likely because the noiseεsampled with these strategies are completely out-of-distribution\nfor the noise prediction model. For computational reasons, we performed this experiment on a 10% subset of Pets.\nSampling distribution forεPets accuracy\nε= 041.3\nTruncatedNormal,[−1,1]49.9\nTruncatedNormal,[−2.5,2.5]81.5\nExpected norm86.9\nε∼N(0,I)87.5\nTable 12. Every importance sampling strategy underperforms sampling the noiseεfrom a standard normal distribution.\n18"
  ]
}