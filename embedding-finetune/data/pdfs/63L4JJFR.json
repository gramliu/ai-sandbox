{
  "key": "63L4JJFR",
  "url": "http://arxiv.org/pdf/2309.03409",
  "metadata": {
    "title": "Large Language Models as Optimizers",
    "abstract": "  Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to our main application in prompt optimization,\nwhere the goal is to find instructions that maximize the task accuracy. With a\nvariety of LLMs, we demonstrate that the best prompts optimized by OPRO\noutperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on\nBig-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.\n",
    "published": "2023-09-07T00:07:15Z"
  },
  "text": [
    "LARGELANGUAGEMODELS ASOPTIMIZERS\nChengrun Yang\n*\nXuezhi Wang    Yifeng Lu    Hanxiao Liu\nQuoc V. Le    Denny Zhou    Xinyun Chen\n*\n{chengrun, xuezhiw, yifenglu, hanxiaol}@google.com\n{qvl, dennyzhou, xinyunchen}@google.com\nGoogle DeepMind\n*\nEqual contribution\nABSTRACT\nOptimization is ubiquitous. While derivative-based algorithms have been powerful\ntools for various problems, the absence of gradient imposes challenges on many\nreal-world applications.  In this work, we propose Optimization by PROmpting\n(OPRO), a simple and effective approach to leverage large language models (LLMs)\nas optimizers, where the optimization task is described in natural language.  In\neach optimization step, the LLM generates new solutions from the prompt that\ncontains previously generated solutions with their values, then the new solutions are\nevaluated and added to the prompt for the next optimization step. We first showcase\nOPRO on linear regression and traveling salesman problems, then move on to our\nmain application in prompt optimization, where the goal is to find instructions\nthat maximize the task accuracy.  With a variety of LLMs, we demonstrate that\nthe best prompts optimized by OPRO outperform human-designed prompts by\nup to8%on GSM8K, and by up to50%on Big-Bench Hard tasks.   Code at\nhttps://github.com/google-deepmind/opro.\n050100150\n# steps\n50.0\n60.0\n70.0\n80.0\ntraining accuracy\nGSM8K\n(a) GSM8K\n050100150200\n# steps\n60.0\n80.0\n100.0\ntraining accuracy\nBBH\nmovie_recommendation\n(b) BBH movie_recommendation\nFigure 1:  Prompt optimization on GSM8K (Cobbe et al., 2021) and BBH (Suzgun et al., 2022)\nmovie_recommendation. The optimization on GSM8K has pre-trainedPaLM 2-Las the scorer and\nthe instruction-tunedPaLM 2-L(denotedPaLM 2-L-IT) as the optimizer; the optimization on\nBBH movie_recommendation hastext-bisonas the scorer andPaLM 2-L-ITas the optimizer.\nEach dot is the average accuracy across all (up to 8) generated instructions in the single step, and the\nshaded region represents standard deviation. See Section 5 for more details on experimental setup.\nTable 1: Top instructions with the highest GSM8K zero-shot test accuracies from prompt optimization\nwith different optimizer LLMs. All results use the pre-trainedPaLM 2-Las the scorer.\nSourceInstructionAcc\nBaselines\n(Kojima et al., 2022)Let’s think step by step.71.8\n(Zhou et al., 2022b)Let’s work this out in a step by step way to be sure we have the right answer.58.8\n(empty string)34.0\nOurs\nPaLM 2-L-ITTake a deep breath and work on this problem step-by-step.80.2\nPaLM 2-LBreak this down.79.9\ngpt-3.5-turbo\nA little bit of arithmetic and a logical approach will help us quickly arrive at\nthe solution to this problem.\n78.5\ngpt-4Let’s combine our numerical command and clear thinking to quickly and\naccurately decipher the answer.\n74.5\n1\narXiv:2309.03409v3  [cs.LG]  15 Apr 2024",
    "Large Language Models as Optimizers\n1INTRODUCTION\nOptimization is critical for all areas. Many optimization techniques are iterative: the optimization\nstarts from an initial solution, then iteratively updates the solution to optimize the objective func-\ntion (Amari, 1993; Qian, 1999; Kingma & Ba, 2015; Bäck & Schwefel, 1993; Rios & Sahinidis,\n2013; Reeves, 1993). The optimization algorithm typically needs to be customized for an individual\ntask to deal with the specific challenges posed by the decision space and the performance landscape,\nespecially for derivative-free optimization.\nIn this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to\nutilize large language models (LLMs) as optimizers. With the advancement of prompting techniques,\nLLMs have achieved impressive performance in various domains (Wei et al., 2022; Kojima et al.,\n2022; Wang et al., 2022; Zhou et al., 2022a; Madaan et al., 2023; Bai et al., 2022; Chen et al., 2023e).\nTheir ability to understand natural language lays out a new possibility for optimization: instead of\nformally defining the optimization problem and deriving the update step with a programmed solver,\nwe describe the optimization problem in natural language, then instruct the LLM to iteratively generate\nnew solutions based on the problem description and the previously found solutions. Optimization\nwith LLMs enables quick adaptation to different tasks by changing the problem description in the\nprompt, and the optimization process can be customized by adding instructions to specify the desired\nproperties of the solutions.\nTo demonstrate the potential of LLMs for optimization,  we first present case studies on linear\nregression and the traveling salesman problem, which are two classic optimization problems that\nunderpin many others in mathematical optimization, computer science, and operations research. On\nsmall-scale optimization problems, we show that LLMs are able to find good-quality solutions simply\nthrough prompting, and sometimes match or surpass hand-designed heuristic algorithms.\nNext, we demonstrate the ability of LLMs to optimize prompts:  the goal is to find a prompt that\nmaximizes the task accuracy. Specifically, we focus on natural language tasks where both the task\ninput and output are texts. LLMs are shown to be sensitive to the prompt format (Zhao et al., 2021;\nLu et al., 2021; Wei et al., 2023; Madaan & Yazdanbakhsh, 2022); in particular, semantically similar\nprompts may have drastically different performance (Kojima et al., 2022; Zhou et al., 2022b; Zhang\net al., 2023), and the optimal prompt formats can be model-specific and task-specific (Ma et al., 2023;\nChen et al., 2023c).  Therefore, prompt engineering is often important for LLMs to achieve good\nperformance (Reynolds & McDonell, 2021). However, the large and discrete prompt space makes it\nchallenging for optimization, especially when only API access to the LLM is available. Following\nprior work on continuous and discrete prompt optimization (Lester et al., 2021; Li & Liang, 2021;\nZhou et al., 2022b; Pryzant et al., 2023), we assume a training set is available to compute the training\naccuracy as the objective value for optimization, and we show in experiments that optimizing the\nprompt for accuracy on a small training set is sufficient to reach high performance on the test set.\nThe prompt to the LLM serves as a call to the optimizer, and we name it themeta-prompt. Figure 3\nshows an example.  The meta-prompt contains two core pieces of information.  The first piece is\npreviously generated prompts with their corresponding training accuracies. The second piece is the\noptimization problem description, which includes several exemplars randomly selected from the\ntraining set to exemplify the task of interest. We also provide instructions for the LLM to understand\nthe relationships among different parts and the desired output format. Different from recent work\non using LLMs for automatic prompt generation (Zhou et al., 2022b; Pryzant et al., 2023), each\noptimization step in our workgeneratesnew prompts that aim to increase the test accuracy based on\na trajectory of previously generated prompts, instead ofeditingone input prompt according to natural\nlanguage feedback (Pryzant et al., 2023) or requiring the new prompt to follow the same semantic\nmeaning (Zhou et al., 2022b).  Making use of the full optimization trajectory, OPRO enables the\nLLM to gradually generate new prompts that improve the task accuracy throughout the optimization\nprocess, where the initial prompts have low task accuracies.\nWe conduct comprehensive evaluation on several LLMs, includingtext-bisonandPalm 2-L\nin the PaLM-2 model family (Anil et al., 2023), as well asgpt-3.5-turboandgpt-4in the GPT\nmodel family. We optimize prompts on GSM8K (Cobbe et al., 2021) and Big-Bench Hard (Suzgun\net al., 2022), which are reasoning benchmarks where prompting techniques have achieved remarkable\nperformance breakthrough (Wei et al., 2022; Kojima et al., 2022; Suzgun et al., 2022).  Starting\nfrom initial prompts with low task accuracies, we show that all LLMs in our evaluation are able to\n2",
    "Large Language Models as Optimizers\nscores\ngenerated\nsolutions\nLLM as\noptimizer\nobjective function\nevaluator\nreturn top solutions\nwhen finish\nmeta-prompt\nsolution-score pairs\ntask description\nFigure 2:  An overview of the OPRO framework.  Given the meta-prompt as the input, the LLM\ngenerates new solutions to the objective function, then the new solutions and their scores are added\ninto the meta-prompt for the next optimization step. The meta-prompt contains the solution-score\npairs obtained throughout optimization, a natural language description of the task, and (in prompt\noptimization) a few task exemplars. Figure 3 shows a sample meta-prompt for prompt optimization.\nserve as optimizers, which consistently improve the performance of the generated prompts through\niterative optimization until convergence (see Figure 1). In particular, while these LLMs generally\nproduce instructions of different styles (see Table 1), with zero-shot prompting, their best generated\ninstructions match the few-shot chain-of-thought prompting performance when applied toPaLM\n2-L, outperforming the zero-shot performance with human-designed prompts by up to8%on\nGSM8K. Additionally, we observe that the OPRO-optimized prompts transfer to other benchmarks\nof the same domain and also deliver notable performance gain.\n2OPRO: LLMAS THEOPTIMIZER\nFigure 2 illustrates the overall framework of OPRO. In each optimization step, the LLM generates\ncandidate solutions to the optimization task based on the optimization problem description and\npreviously evaluated solutions in the meta-prompt. Then the new solutions are evaluated and added to\nthe meta-prompt for the subsequent optimization process. The optimization process terminates when\nthe LLM is unable to propose new solutions with better optimization scores, or a maximum number\nof optimization steps has reached. We first outline the desired features of LLMs for optimization,\nthen describe the key design choices based on these desirables.\n2.1DESIRABLES OFOPTIMIZATION  BYLLMS\nMaking use of natural language descriptions.The main advantage of LLMs for optimization is\ntheir ability of understanding natural language, which allows people to describe their optimization\ntasks without formal specifications. For instance, in prompt optimization where the goal is to find a\nprompt that optimizes the task accuracy, the task can be described with a high-level text summary\nalong with input-output examples.\nTrading off exploration and exploitation.The exploration-exploitation trade-off is a fundamental\nchallenge in optimization, and it is important for LLMs serving as optimizers to balance these two\ncompeting goals. This means that the LLM should be able to exploit promising areas of the search\nspace where good solutions are already found, while also exploring new regions of the search space\nso as to not miss potentially better solutions.\n2.2META-PROMPTDESIGN\nAs the input to the optimizer LLM, the meta-prompt contains the following two essential parts.\nOptimization problem description.The first part is the text description of the optimization problem,\nincluding the objective function and solution constraints.  For example, for prompt optimization,\nthe LLM can be instructed to “generate a new instruction that achieves a higher accuracy”, and we\ndenote such instructions in the meta-prompt asmeta-instructions. We can also provide customized\n3",
    "Large Language Models as Optimizers\nmeta-instructions as an informal regularization of the generated solutions, such as “the instruction\nshould be concise and generally applicable”.\nOptimization trajectory.Besides understanding natural language instructions, LLMs are also\nshown to be able to recognize patterns from in-context demonstrations (Wei et al., 2023; Madaan &\nYazdanbakhsh, 2022; Mirchandani et al., 2023). Our meta-prompt makes use of this property and in-\nstructs the LLM to leverage the optimization trajectory for generating new solutions. Specifically, the\noptimization trajectory includes past solutions and their optimization scores, sorted in the ascending\norder. Including optimization trajectory in the meta-prompt allows the LLM to identify similarities of\nsolutions with high scores, encouraging the LLM to build upon existing good solutions to construct\npotentially better ones without the need of explicitly defining how the solution should be updated.\n2.3SOLUTIONGENERATION\nAt the solution generation step, the LLM generates new solutions with the meta-prompt as input. The\nfollowing are the key optimization challenges we address in this stage.\nOptimization stability.In the optimization process,  not all solutions achieve high scores and\nmonotonically improve over prior ones. Due to the sensitivity of in-context learning to the prompt,\nLLM output can be drastically affected by low-quality solutions in the input optimization trajectory,\nespecially at the beginning when the solution space has not been adequately explored. This sometimes\nresults in optimization instability and large variance. To improve stability, we prompt the LLM to\ngenerate multiple solutions at each optimization step, allowing the LLM to simultaneously explore\nmultiple possibilities and quickly discover promising directions to move forward.\nExploration-exploitation trade-off.We tune the LLM sampling temperature to balance between\nexploration and exploitation. A lower temperature encourages the LLM to exploit the solution space\naround the previously found solutions and make small adaptations, while a high temperature allows\nthe LLM to more aggressively explore solutions that can be notably different.\n3MOTIVATINGEXAMPLE: MATHEMATICALOPTIMIZATION\nWe first demonstrate the potential of LLMs in serving as optimizers for mathematical optimization.\nIn particular, we present a case study on linear regression as an example of continuous optimization,\nand on the Traveling Salesman Problem (TSP) as an example of discrete optimization. On both tasks,\nwe see LLMs properly capture the optimization directions on small-scale problems merely based on\nthe past optimization trajectory provided in the meta-prompt.\n3.1LINEARREGRESSION\nIn linear regression problems, the goal is to find the linear coefficients that probabilistically best\nexplain  the  response from  the  input  variables.   We  study  the setting  in  which  the independent\nand dependent variablesXandyare both one-dimensional and an interceptbis present, so that\nthere are two one-dimensional variablesw,bto optimize over.  In a synthetic setting, we sample\nground truth values for one-dimensional variablesw\ntrue\nandb\ntrue\n, and generate 50 data points by\ny=w\ntrue\nx+b\ntrue\n+ε, in whichxranges from 1 to 50 andεis the standard Gaussian noise.  Our\noptimization starts from 5 randomly sampled(w,b)pairs. In each step, we prompt an instruction-\ntuned LLM with a meta-prompt that includes the best 20(w,b)pairs in history and their sorted\nobjective values. The meta-prompt then asks for a new(w,b)pair that further decreases the objective\nvalue. A sample meta-prompt is shown in Figure 19 of Appendix C.1. We prompt the meta-prompt 8\ntimes to generate at most 8 new(w,b)pairs in each step to improve optimization stability. Then we\nevaluate the objective value of the proposed pair and add it to history. We do black-box optimization:\nthe analytic form does not appear in the meta-prompt text.  This is because the LLM can often\ncalculate the solution directly from the analytic form.\nTable  2  summarizes  the  results  with  one  of  the  following  optimizer  LLMs:text-bison,\ngpt-3.5-turbo,  andgpt-4.   We study three settings ofw\ntrue\nandb\ntrue\n:  within the starting\nregion[10,20]×[10,20], “near outside” (each ofw\ntrue\nandb\ntrue\nis outside the starting region but the\ndistance is less than 10), and “far outside” (each ofw\ntrue\nandb\ntrue\nis outside the starting region and\nthe distance is greater than 10). We see:\n4",
    "Large Language Models as Optimizers\nTable 2: Linear regression by optimizer LLMs: the mean±standard deviation of the number of steps\nand the number of unique(w,b)pairs explored before reaching the global optima. Bothwandbstart\nfrom 5 random starting points in[10,20]. We use temperature 1.0 for all models. We run each setting\n5 times. The starting points are the same across optimizer LLMs but are different across 5 runs, and\nare grouped by: within the starting region, outside and close to the starting region, and outside and\nfarther from the starting region. Bold numbers indicate the best among three LLMs in each setting.\nw\ntrue\nb\ntrue\nnumber of stepsnumber of unique(w,b)pairs explored\ntext-bison  gpt-3.5-turbo   gpt-4   text-bison  gpt-3.5-turbo   gpt-4\n15145.8±2.67.6±4.54.0±1.540.0±12.436.0±15.217.2±5.1\n17174.0±1.812.6±6.06.0±3.733.4±11.753.8±16.926.0±10.6\n16103.8±2.210.4±5.46.2±3.130.2±13.442.8±16.324.2±8.2\n359.8±2.810.8±2.712.2±2.055.8±16.139.6±10.133.0±4.0\n252319.6±11.426.4±18.312.2±3.7104.0±52.378.6±26.244.2±8.3\n23031.4±6.342.8±9.738.0±15.9126.4±17.7125.6±21.799.0±24.6\n36-135.8±6.445.4±16.950.4±18.8174.0±28.2142.2±31.2116.4±32.7\n•The number of unique(w,b)pairs explored by each model is fewer than exhaustive search,\nindicating these models are able to to do black-box optimization: compare the numbers and\npropose a descent direction.\n•\nThetext-bisonandgpt-4models outperformgpt-3.5-turboin convergence speed:\nthey arrive at the optima with fewer steps. Thegpt-4model also outperforms in finding the\noptima with fewer explored unique points. Taking a closer look at the optimization trajectory, we\nseegpt-4is the best at proposing a reasonable next step from the history: for example, when\nthe history shows the objective values of(w,b) = (8,7),(w,b) = (8,6), and(w,b) = (8,5)\nare decreasing, it has a highest chance to propose(w,b) = (8,4)for evaluation.\n•The problem becomes harder for all models when the ground truth moves farther from the\nstarting region: all models need more explorations and more steps.\n3.2TRAVELINGSALESMANPROBLEM(TSP)\nNext, we consider the Traveling Salesman Problem (TSP) (Jünger et al., 1995; Gutin & Punnen, 2006),\na classical combinatorial optimization problem with numerous algorithms proposed in literature,\nincluding heuristic algorithms and solvers (Rosenkrantz et al., 1977; Golden et al., 1980; Optimization\net al., 2020; Applegate et al., 2006; Helsgaun, 2017), and approaches based on training deep neural\nnetworks (Kool et al., 2019; Deudon et al., 2018; Chen & Tian, 2019; Nazari et al., 2018). Specifically,\ngiven a set ofnnodes with their coordinates, the TSP task is to find the shortest route that traverses\nall nodes from the starting node and finally returns to the starting node.\nOur optimization process with LLMs starts from 5 randomly generated solutions, and each optimiza-\ntion step produces at most 8 new solutions. We present the meta-prompt in Figure 20 of Appendix C.1.\nWe generate the problem instances by samplingnnodes with bothxandycoordinates in[−100,100].\nWe use the Gurobi solver (Optimization et al., 2020) to construct the oracle solutions and compute the\noptimality gap for all approaches, where the optimality gap is defined as the difference between the\ndistance in the solution constructed by the evaluated approach and the distance achieved by the oracle\nsolution, divided by the distance of the oracle solution.  Besides evaluating OPRO with different\nLLMs includingtext-bison,gpt-3.5-turboandgpt-4, we also compare OPRO to the\nfollowing heuristics:\n•Nearest Neighbor (NN)\n. Starting from an initial node, the solution is constructed with\nthe nearest neighbor heuristic: At each step, among the remaining nodes that are not included in\nthe current partial solution, NN selects the node with the shortest distance to the end node of the\npartial solution, and adds it as the new end node. The process finishes when all nodes have been\nadded to the solution.\n•Farthest Insertion (FI). One caveat of the nearest neighbor heuristic is that it does\nnot take the distance between the start and end node into consideration when constructing partial\nsolutions.  To address this issue, FI aims to optimize the cost of inserting new nodes into the\npartial solution at each step.  Define the minimal insertion cost of adding a new nodekas\n5",
    "Large Language Models as Optimizers\nTable 3: Results of the Traveling Salesman Problem (TSP) with different number of nodesn, where\neachncontains 5 problems. “# steps” calculates the mean±standard error of optimization steps\nfor successful runs that find the optimal solution. “# successes” counts the number of problems that\nOPRO results in the optimal solution. When no optimal solution is found for any evaluated problem,\nthe corresponding number of steps is N/A.\nn\noptimality gap (%)# steps (# successes)\nNNFItext-bison  gpt-3.5-turbo   gpt-4   text-bison  gpt-3.5-turbo     gpt-4\n1013.0±1.33.2±1.40.0±0.00.0±0.00.0±0.040.4±5.6(5)46.8±9.3(5)9.6±3.0(5)\n159.4±3.71.2±0.64.4±1.31.2±1.10.2±0.2N/A (0)202.0±41.1(4)58.5±29.0(4)\n2016.0±3.90.2±0.130.4±10.64.4±2.51.4±0.6N/A (0)438.0±0.0(1)195.5±127.6(2)\n5019.7±3.19.8±1.5219.8±13.7133.0±6.811.0±2.6N/A (0)N/A (0)N/A (0)\nc(k) = min\n(i,j)\nd(i,k) +d(k,j)−d(i,j), whereiandjare adjacent nodes in the current tour,\nandd(·,·)represents the distance between two nodes. At each step, FI adds a new node that\nmaximizes the minimal insertion cost.\nWe present the results in Table 3. We randomly generate 5 problem instances for each number of\nnodesn. In addition to measuring the optimality gap, on problems where the LLM finds the optimal\nsolutions, we also show the number of optimization steps taken to reach the global optimum. First,\nwe observe thatgpt-4significantly outperformsgpt-3.5-turboandtext-bisonacross all\nproblem sizes. Specifically, on smaller-scale problems,gpt-4reaches the global optimum about4×\nfaster than other LLMs. On larger-scale problems, especially withn= 50,gpt-4still finds solutions\nwith a comparable quality to heuristic algorithms, while bothtext-bisonandgpt-3.5-turbo\nget stuck at local optima with up to20×worse optimality gaps.\nOn the other hand, the performance of OPRO degrades dramatically on problems with larger sizes.\nWhenn= 10, all LLMs find the optimal solutions for every evaluated problem; as the problem size\ngets larger, the OPRO optimality gaps increase quickly, and the farthest insertion heuristic starts to\noutperform all LLMs in the optimality gap.\nLimitations.We would like to note that OPRO is designed for neither outperforming the state-\nof-the-art gradient-based optimization algorithms for continuous mathematical optimization, nor\nsurpassing the performance of specialized solvers for classical combinatorial optimization problems\nsuch as TSP. Instead, the goal is to demonstrate that LLMs are able to optimize different kinds\nof objective functions simply through prompting, and reach the global optimum for some small-\nscale problems. Our evaluation reveals several limitations of OPRO for mathematical optimization.\nSpecifically, the length limit of the LLM context window makes it hard to fit large-scale optimization\nproblem descriptions in the prompt, e.g., linear regression with high-dimensional data, and traveling\nsalesman problems with a large set of nodes to visit. In addition, the optimization landscape of some\nobjective functions are too bumpy for the LLM to propose a correct descending direction, causing the\noptimization to get stuck halfway. We further elaborate our observed failure cases in Appendix A.\n4APPLICATION: PROMPTOPTIMIZATION\nNext, we demonstrate the effectiveness of OPRO on prompt optimization, where the objective is to\nfind the prompt that maximizes task accuracy. We first introduce the problem setup, then illustrate\nthe meta-prompt design.\n4.1PROBLEMSETUP\nWe focus on prompt optimization for natural language tasks, where both the input and output are in\nthe text format. The task is represented as a dataset with training and test splits, where the training\nset is used to calculate the training accuracy as the objective value during the optimization process,\nand we compute the test accuracy on the test set after the optimization finishes. While traditional\noptimization often requires a decently large training set, our experiment shows that a small number\nor fraction of training samples (e.g., 3.5% of the training set for GSM8K (Cobbe et al., 2021), 20%\nfor Big-Bench Hard (Suzgun et al., 2022)) is sufficient. The objective function evaluator is an LLM\n6",
    "Large Language Models as Optimizers\nI have some texts along with their corresponding scores. The texts are arranged in ascending order\nbased on their scores, where higher scores indicate better quality.\ntext:\nLet’s figure it out!\nscore:\n61\ntext:\nLet’s solve the problem.\nscore:\n63\n(. . .  more instructions and scores . . . )\nThe following exemplars show how to apply your text: you replace <INS> in each input with your\ntext, then read the input and give an output. We say your output is wrong if your output is different\nfrom the given output, and we say your output is correct if they are the same.\ninput:\nQ: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books\nby their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books than\nAlannah. If Beatrix has 30 books, how many books do the three have together?\nA: <INS>\noutput:\n140\n(. . .  more exemplars . . . )\nWrite your new text that is different from the old ones and has a score as high as possible. Write the\ntext in square brackets.\nFigure 3: An example of the meta-prompt for prompt optimization with instruction-tunedPaLM 2-L\n(PaLM 2-L-IT) on GSM8K, where the generated instruction will be prepended to the beginning\nof “A:” in the scorer LLM output (A_beginin Section 4.1). <INS> denotes the position where the\ngenerated instruction will be added.  The blue text contains solution-score pairs; the purple text\ndescribes the optimization task and output format; the orange text are meta-instructions.\nto which the optimized prompt will be applied, and it can be the same or different from the LLM for\noptimization. We denote the LLM for objective function evaluation as thescorer LLM, and the LLM\nfor optimization as theoptimizer LLM.\nThe output of the optimizer LLM is aninstruction, which is concatenated to the question part of every\nexemplar and prompts the scorer LLM. We consider the following positions to insert the instruction:\n•Q_begin: the instruction is added before the original question.\n•Q_end: the instruction is added after the original question.\n•A_begin: the instruction is added to the beginning of the scorer LLM output. This is applicable\nto pretrained LLMs without instruction tuning, where the prompt is formatted as a sequence of\nQA pairs.\nWe exemplify these prompting formats in Appendix B.\n4.2META-PROMPTDESIGN\nFigure 3 shows an example of the meta-prompt for prompt optimization on GSM8K (Cobbe et al.,\n2021). More details are as follows.\n7",
    "Large Language Models as Optimizers\nOptimization problem examples.The problem description includes a few examples taken from the\ntraining set to demonstrate the task for the generated instructions. For example, from the input-output\npair in Figure 3, we can infer this is a math word problem. The input-output pair also demonstrates\nthe position where the generated instruction will be added to, and this is essential for the optimizer\nLLM to generate instructions of the same style. In each optimization step, we add several (three for\nexample) training examples to the meta-prompt by random sampling the training set or choose the\nones the previous instructions fall short of.\nOptimization trajectory.The optimization trajectory includes instructions generated from the past\noptimization steps, along with their scores. The old instructions and scores are sorted by the score in\nascending order. The score is the training accuracy in prompt optimization. We only keep instructions\nwith the highest scores in the meta-prompt in consideration of the LLM context length limit.\nMeta-instructions.We also addmeta-instructions: the instructions to the optimizer LLM that explain\nthe optimization goal and instruct the model how to use the above information. The meta-instructions\nmay also specify the desired generated instruction format for easier parsing.\n5PROMPTOPTIMIZATIONEXPERIMENTS\nWe present the evaluation results for prompt optimization in this section. Our experiments demonstrate\nthat OPRO brings a significant performance gain across the board, with different combinations of\nLLMs as the optimizer and the scorer.\nSection 5.1 describes the experiment setup. Section 5.2 shows main results on reasoning tasks like\nGSM8K and BBH. Section 5.3 shows ablation studies. Section 5.4 analyzes overfitting in prompt\noptimization. Section 5.5 compares the prompt optimization performance of meta-prompts in OPRO\nand EvoPrompt (Guo et al., 2023).\n5.1EVALUATIONSETUP\nModels.The LLMs we use as the optimizer and the scorer are:\n•\nOptimizer LLM: Pre-trainedPaLM 2-L(Anil et al., 2023),  instruction-tunedPaLM 2-L\n(denotedPaLM 2-L-IT),text-bison,gpt-3.5-turbo, andgpt-4.\n•  Scorer LLM: Pre-trainedPaLM 2-Landtext-bison.\nWith pre-trainedPaLM 2-Las the scorer,  the optimizer LLM generates A_begin instructions.\nSincetext-bisonhas been instruction-tuned, the optimizer LLM generates Q_begin and Q_end\ninstructions whentext-bisonis used as the scorer.\nBenchmarks.Our primary evaluation benchmarks are GSM8K (Cobbe et al., 2021) and Big-Bench\nHard (BBH) (Suzgun et al., 2022). GSM8K is a benchmark of grade school math word problems\nwith 7,473 training samples and 1,319 test samples, where chain-of-thought prompting (Wei et al.,\n2022) and the zero-shot instruction “Let’s think step by step.” (Kojima et al., 2022) have drastically\nimproved the performance over the standard prompting. BBH is a suite of 23 challenging BIG-Bench\ntasks (Srivastava et al., 2022) that covers a wide range of topics beyond arithmetic reasoning, including\nsymbolic manipulation and commonsense reasoning. Each task contains up to 250 examples in total.\nTo examine the transferability of the optimized instructions, we also evaluate the instructions op-\ntimized for GSM8K on two other mathematical reasoning datasets, i.e., MultiArith (Roy & Roth,\n2016) and AQuA (Ling et al., 2017).\nImplementation details.We set the temperature to be 0 when evaluating the performance of\ngenerated instructions, in which case the scorer LLM greedily decodes. Unless otherwise specified, we\nset the default temperature to be 1.0 for optimizer LLMs to generate diverse and creative instructions.\nAt each optimization step, we prompt the optimizer LLM with the meta-prompt 8 times to generate 8\ninstructions, then we add these instructions with their training scores to the optimization trajectory\nin the meta-prompt.  Our meta-prompt at each step contains the best 20 instructions so far and 3\nrandomly picked exemplars from the training set. We study the effect of different hyperparameters in\nablation studies (Section 5.3). Appendix C.2 presents the full meta-prompts for different optimizer\nLLMs.\n8",
    "Large Language Models as Optimizers\nTable 4: Test accuracies on GSM8K. We show the instruction with the highest test accuracy for each\nscorer-optimizer pair.\nScorerOptimizer /\nSource\nInstruction\nposition\nTop instructionAcc\nBaselines\nPaLM 2-L(Kojima et al.,\n2022)\nA_beginLet’s think step by step.71.8\nPaLM 2-L(Zhou et al.,\n2022b)\nA_beginLet’s work this out in a step by step way to be sure we have the\nright answer.\n58.8\nPaLM 2-LA_beginLet’s solve the problem.60.8\nPaLM 2-LA_begin(empty string)34.0\ntext-bison(Kojima et al.,\n2022)\nQ_beginLet’s think step by step.64.4\ntext-bison(Zhou et al.,\n2022b)\nQ_beginLet’s work this out in a step by step way to be sure we have the\nright answer.\n65.6\ntext-bisonQ_beginLet’s solve the problem.59.1\ntext-bisonQ_begin(empty string)56.8\nOurs\nPaLM 2-L      PaLM\n2-L-IT\nA_beginTake a deep breath and work on this problem step-by-step.80.2\nPaLM 2-L    PaLM 2-LA_beginBreak this down.79.9\nPaLM 2-L   gpt-3.5-turboA_beginA little bit of arithmetic and a logical approach will help us\nquickly arrive at the solution to this problem.\n78.5\nPaLM 2-L      gpt-4A_beginLet’s combine our numerical command and clear thinking to\nquickly and accurately decipher the answer.\n74.5\ntext-bison     PaLM\n2-L-IT\nQ_beginLet’s work together to solve math word problems! First, we will\nread and discuss the problem together to make sure we\nunderstand it. Then, we will work together to find the solution. I\nwill give you hints and help you work through the problem if\nyou get stuck.\n64.4\ntext-bison  text-bisonQ_endLet’s work through this problem step-by-step:68.5\ntext-bison  gpt-3.5-turboQ_endAnalyze the given information, break down the problem into\nmanageable steps, apply suitable mathematical operations, and\nprovide a clear, accurate, and concise solution, ensuring precise\nrounding if necessary. Consider all variables and carefully\nconsider the problem’s context for an efficient solution.\n66.5\ntext-bison     gpt-4Q_beginStart by dissecting the problem to highlight important numbers\nand their relations. Decide on the necessary mathematical\noperations like addition, subtraction, multiplication, or division,\nrequired for resolution. Implement these operations, keeping in\nmind any units or conditions. Round off by ensuring your\nsolution fits the context of the problem to ensure accuracy.\n62.7\n5.2MAINRESULTS\nWe show prompt optimization curves on GSM8K and two BBH tasks in this section. The curves on\nother BBH tasks are deferred to Appendix D, and the tables containing all accuracy numbers are in\nAppendix E.\n5.2.1GSM8K\nFor prompt optimization, we randomly sample 3.5% examples from the GSM8K training set. The\nsame subset is used throughout optimization, so that the task accuracies computed at intermediate\noptimization steps are approximations of the training accuracy on all 7,473 training examples. This\nbalances the evaluation cost with the generalization performance. After the optimization procedure\nfinishes, we evaluate the found instructions on the entire GSM8K test set.\nFigure 1(a) in Section 1 shows prompt optimization curves with pre-trainedPaLM 2-Las scorer\nandPaLM 2-L-ITas optimizer, and the initial instruction is “Let’s solve the problem” with a\n(approximated, and same below) training accuracy of 60.5. We observe that the optimization curve\nshows an overall upward trend with several leaps throughout the optimization process, for example:\n9",
    "Large Language Models as Optimizers\n•“Let’s think carefully about the problem and solve it together.”  at Step 2 with the training\naccuracy 63.2;\n• “Let’s break it down!” at Step 4 with training accuracy 71.3;\n• “Let’s calculate our way to the solution!” at Step 5 with training accuracy 73.9;\n• “Let’s do the math!” at Step 6 with training accuracy 78.2.\nThe optimization curves also generally show a decrease of the variance among the accuracies of\ninstructions generated at each step, indicating that the optimizer LLM generatesdistributionally\nbetter instructions throughout the optimization.\nNext, we present the results of generating Q_begin instructions with thetext-bisonscorer and\nthePaLM 2-L-IToptimizer, starting from an empty instruction with a 57.1 training accuracy. The\noptimization curve in Figure 4(a) shows a similar upward trend, during which a few leaps in the\ntraining accuracy include:\n•“Solve the following problems using the given information.” at Step 2 with training accuracy\n59.8;\n•“Solve the following problems by applying the given information and using the appropriate\nmathematical operations.” at Step 3 with training accuracy 64.0;\n•\n“Let’s read the problem carefully and identify the given information. Then, we can create an\nequation and solve for the unknown variable.” at Step 4 with training accuracy 67.0;\n•“I’m always down for solving a math word problem together. Just give me a moment to read\nand understand the problem. Then, I’ll create an equation that models the problem, which I’ll\nsolve for the unknown variable. I also may or may not use some helpful diagrams or visuals\nto understand the problem. Lastly, be sure to allow me some time to carefully check my work\nbefore submitting any responses!” at Step 29 with training accuracy 70.1.\nNote that although our default setting is to run OPRO for 200 steps in prompt optimization, we\nneed much fewer steps if the goal is to find some outstanding instructions. An example is that the\nFigure 1(a) experiment found “Let’s do the math!”  at Step 6 with training accuracy 78.2, almost\nmatching the “Take a deep breath and work on this problem step-by-step.” found at the 107th step\nwith training accuracy 80.2, at a point where the optimization curve is still trending upwards. This is\nbecause a leap in our optimization curve does not always correspond to a much better instruction being\ndiscovered; instead, it can be due to a large qualitative improvement of all 8 generated instructions in\nthis step. The latter usually happens several steps after the former: after a much better instruction is\ndiscovered in one step, the meta-prompt gradually gets rid of worse instructions in the latter steps by\ngenerating instructions similar to the much-better one. The top instructions kept in the meta-prompt\ngradually improves in this procedure. At a point when the meta-prompt only triggers higher quality\ninstructions, the leap happens.\nFinally, Figure 4(b) shows that the pre-trainedPaLM 2-Lcan also serve as the optimizer LLM and\nimprove its own prediction performance. Different from other optimizer LLMs that are instruction-\ntuned, the pre-trainedPaLM 2-Lperforms better when the prompt is formatted in a few-shot manner.\nTherefore, we include two initial instructions to start the optimization: the empty instruction (with\na training accuracy 32.2) and “The answer is” (with a training accuracy 33.3).  See Figure 21 in\nAppendix C for the meta-prompt format. The generated instructions follow the same style as “The\nanswer is”: most instructions are also phrases suitable as the prefix of a sentence, like “Here you\ngo:” (generated at Step 11 with training accuracy 61.3) and “Let’s do it:” (generated at Step 13 with\ntraining accuracy 75.1).\nTable 4 summarizes top instructions found on GSM8K with different scorer and optimizer LLMs.\nWe observe that:\n•The styles of instructions found by different optimizer LLMs vary a lot:PaLM 2-L-ITand\ntext-bisonones are concise, while GPT ones are long and detailed.\n•\nAlthough some top instructions contain the “step-by-step” phrase, most others achieve a compa-\nrable or better accuracy with different semantic meanings.\n10",
    "Large Language Models as Optimizers\n050100150200\n# steps\n50.0\n60.0\n70.0\ntraining accuracy\nGSM8K\n(scorer: text-bison)\n(a)PaLM 2-L-IToptimizer\n020406080\n# steps\n20.0\n40.0\n60.0\n80.0\ntraining accuracy\nGSM8K\n(scorer and optimizer:\nPaLM 2-L)\n(b) pre-trainedPaLM 2-Loptimizer\nFigure 4: Prompt optimization on GSM8K with (a) thetext-bisonscorer and thePaLM 2-L-IT\noptimizer, and (b) pre-trainedPaLM 2-Las both scorer and optimizer.\n5.2.2BBH\nOn BBH, the optimization starts from an empty string as the initial instruction by default.  The\ninstructions are placed at A_begin when the scorer isPaLM 2-L, and at Q_begin when the scorer\nistext-bison. For each task, we utilize a subset of 20% examples for prompt optimization, and\nthe rest examples are for testing. We show experimental results on more variants of the instruction\nposition and initialization in Appendix E.\nFigure 5 visualizes the per-task accuracy difference on all 23 BBH tasks compared to the instruction\n“Let’s think step by step.” (Kojima et al., 2022) and the empty instruction, and we present the concrete\naccuracies in Table 7 of Appendix E. We show that the instructions found by OPRO outperform\n“Let’s think step by step.” on almost all tasks by a large margin: our instructions outperform by over\n5% on 19/23 tasks with thePaLM 2-Lscorer, and on 15/23 tasks with thetext-bisonscorer.\nOur prompt optimization algorithm also improves instructions from the empty starting point by over\n5% on most tasks: 20/23 with thePaLM 2-Lscorer and 15/23 with thetext-bisonscorer.\nSimilar to GSM8K, we observe upward trends in optimization curves on almost all BBH tasks, as\nshown in Figure 6. See Figure 23 and 24 in Appendix D for more curves on other BBH tasks.\nWe next show some examples of instructions found through the course of optimization. On the task\nruin_names, starting from the empty instruction (with 64.0 training accuracy), with thetext-bison\nscorer and thePaLM 2-L-IToptimizer, the following instructions are generated:\n•“Consider the following when editing artist or movie names humorously:” at Step 1 with training\naccuracy 72.0;\n•“When making humorous edits of artist or movie names, you can change one or more letters or\neven create puns by adding new words that sound similar.” at Step 18 with training accuracy\n80.0;\n•\n“We can make humorous edits of artist/movie names by changing letters to create new words\nthat are similar in sound but have different meanings. For example, The Police can be changed\nto The Polite, The Abyss can be changed to Toe Abyss, and Schindler’s List can be changed to\nSchindler’s Lost.” at Step 38 with training accuracy 82.0.\nAlthough the above instructions are semantically similar, a paraphrase by the optimizer LLM offers a\nnotable accuracy improvement. We further highlight this observation in Section 5.2.3.\nBelow are some instructions generated when performing prompt optimization on temporal_sequences,\nstarting from the empty instruction (with the training accuracy of 64.0):\n•“To solve this problem, we need to first identify the time period when the person was not seen\ndoing anything else. Then, we need to check if the place they went to was open during that time\nperiod. If it was, then that is the time period when they could have gone to that place.” at Step 2\nwith training accuracy 42.0;\n•“To find the time period when a person could have gone to a place, identify the time periods\nwhen they were not seen doing anything else and the place was open. If there are multiple time\nperiods that match these criteria, then the person could have gone to the place during any of\nthese time periods.” at Step 18 with training accuracy 54.0;\n11",
    "Large Language Models as Optimizers\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n-20\n0\n20\n40\naccuracy difference\n(a)PaLM 2-Lscorer, ours minus “Let’s think step by step.”\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\n60\naccuracy difference\n(b)PaLM 2-Lscorer, ours minus empty starting point\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\n60\naccuracy difference\n(c)text-bisonscorer, ours minus “Let’s think step by step.”\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\naccuracy difference\n(d)text-bisonscorer, ours minus empty starting point\nFigure 5:  On 23 BBH tasks, the accuracy differences among instructions found by prompt opti-\nmization (with thePaLM 2-L-IToptimizer), “Let’s think step by step.”, and the empty string\n(optimization starting point).\n•“To determine the possible time period when a person went to a place, first identify all the time\nperiods when the person was not seen doing anything else and the place was open. Then, rule\nout any time periods during which the person was seen doing something else. The remaining\ntime periods are the possible times when the person could have gone to the place.” at Step 41\nwith training accuracy 72.0.\nTable 5 presents the best instructions generated on movie_recommendation, ruin_names, and tem-\nporal_sequences tasks with different combinations of the optimizer and the scorer LLMs.  Again,\n12",
    "Large Language Models as Optimizers\n050100150200\n# steps\n70.0\n80.0\n90.0\ntraining accuracy\nBBH ruin_names\n(a) BBH ruin_names\n050100150\n# steps\n30.0\n50.0\n70.0\ntraining accuracy\nBBH\ntemporal_sequences\n(b) BBH temporal_sequences\nFigure  6:   Training  accuracy  curves  of  prompt  optimization  on  BBH  ruin_names  and  tempo-\nral_sequences with thetext-bisonscorer and thePaLM 2-L-IToptimizer. The optimizations\nstart from the empty string.\ndifferent optimizer LLMs produce instructions of different styles.  See Appendix E for results on\nmore BBH tasks.\n5.2.3SEMANTICALLY SIMILAR INSTRUCTIONS MAY ACHIEVE DRASTICALLY  DIFFERENT\nACCURACIES\nOne challenge of prompt optimization is the sensitivity of model performance to subtle changes in\nthe instruction. For example, with thePaLM 2-Lscorer on the GSM8K test set, “Let’s think step\nby step.” achieves accuracy 71.8, “Let’s solve the problem together.” has accuracy 60.5, while the\naccuracy of “Let’s work together to solve this problem step by step.” is only 49.4, although it is the\nsemantic combination of the two upper instructions. This behavior increases both the variance across\nsingle-step instructions and the oscillation during optimization, and motivates us to generate multiple\ninstructions at each step to improve the optimization stability.\n5.2.4TRANSFERABILITY OF FOUND INSTRUCTIONS\nWe assess the transferability of found prompts to different datasets of the same domain, where we\nevaluate the top instructions found for GSM8K on two more math reasoning benchmarks Multi-\nArith (Roy & Roth, 2016) and AQuA (Ling et al., 2017). Table 6 shows that our optimized prompts\nalso outperform baseline prompts with different scorer LLMs on these two benchmarks.\n5.3ABLATIONSTUDIES\nWe usetext-bisonas the scorer andPaLM 2-Las the optimizer for all ablation studies. The\ntasks we evaluate are GSM8K (math reasoning) and BBH sports_understanding (non-math reasoning).\nMeta-prompt design.The meta-prompt design is crucial in achieving good prompt optimization\nperformance. We investigate the following core design choices:\n•The order of the previous instructions.We compare the following options: (1) from lowest to\nhighest (our default setting); (2) from highest to lowest; (3) random. Figures 7(a) and 7(b) show\nthat the default setting achieves better final accuracies and converges faster. One hypothesis is\nthat the optimizer LLM output is affected more by the past instructions closer to the end of the\nmeta-prompt.  This is consistent with the recency bias observed in Zhao et al. (2021), which\nstates that LLMs are more likely to generate tokens similar to the end of the prompt.\n•The effect of instruction scores.In terms of how to present the accuracy scores, we compare three\noptions: (1) rounding the accuracies to integers, which is equivalent to bucketizing the accuracy\nscores to 100 buckets (our default setting); (2) bucketizing the accuracies to 20 buckets; (3)\nnot showing the accuracies, only showing the instructions in the ascending order. Figures 7(c)\nand 7(d) show that the accuracy scores assists the optimizer LLM in better understanding the\nquality difference among previous instructions, and thus the optimizer LLM proposes better new\ninstructions that are similar to the best ones in the input optimization trajectory.\n•\nThe effect of exemplars.We compare three options:  (1) showing 3 exemplars from the task\n(default); (2) showing 10 exemplars from the task; (3) no exemplars. Figures 7(e) and 7(f) show\n13",
    "Large Language Models as Optimizers\nTable  5:   Top  instructions  with  the  highest  accuracies  found  in  prompt  optimization  on  BBH\nmovie_recommendation, ruin_names, and temporal_sequences.\nScorerOptimizerInstruction\nposition\nInstructionAcc\nmovie_recommendation\nPaLM 2-L   PaLM 2-L-ITA_beginBased on your input, I have analyzed the given\nmovies in terms of genre, plot, tone, audience rating,\nyear of release, director, cast, and reviews. I have also\ntaken into account the given options. The movie that\nis most similar to the given movies in terms of all\nthese factors is:\n90.8\nPaLM 2-L     PaLM 2-L\nA_beginThe best film:88.4\nPaLM 2-L   gpt-3.5-turboA_beginLet’s uncover the perfect movie recommendation\nfrom the options provided, ensuring an exceptional\ncinematic experience together as we select the most\ncaptivating and satisfying choice that will keep us\nthoroughly engaged and immersed until the very end.\n88.0\ntext-bison  PaLM 2-L-IT\nQ_beginWhat is the highest-rated movie similar to the given\nmovies, with a similar IMDb rating and released in\nthe same year?\n91.6\ntext-bison gpt-3.5-turboQ_beginBased on the movie list provided, carefully consider\nyour preferences and make a well-informed decision.\n70.8\nruin_names\nPaLM 2-L   PaLM 2-L-ITA_beginWhich is the funniest pun on the artist or movie name?88.0\nPaLM 2-L     PaLM 2-L\nA_beginAnswer for ruin:83.6\nPaLM 2-L   gpt-3.5-turboA_beginPrepare to have a side-splittingly funny time as we\nuncover the most clever and hilarious alternatives for\nthese artist or movie names, challenging your wit to\nguess the correct one with a burst of creativity, humor,\nand imaginative twists!\n86.8\ntext-bison  PaLM 2-L-ITQ_beginA humorous edit of an artist or movie name can be\ncreated by replacing one or more letters to form a new\nword or phrase that sounds similar but has a different\nmeaning. The new word or phrase should be relevant\nto the original word, but it should also be a surprise,\nwhich makes the edit funny. For example, the artist or\nmovie name \"Rocky\" can be changed to \"Ricky,\" and\n\"Schindler’s List\" can be changed to \"Schindler’s\nLift.\" Be creative and have fun!\n83.6\ntext-bison gpt-3.5-turboQ_beginChoose the option that offers the most clever and\nhumorous alteration of the given artist or movie name.\nLet your creativity shine and select the answer that\nwill undoubtedly bring a smile to your face! Make\nsure to think outside the box!\n75.2\ntemporal_sequences(noPaLM 2-Las scorer results because its training accuracy on empty string is 100.0)\ntext-bison  PaLM 2-L-ITQ_beginTo determine the time period when a person went to a\nplace, first identify all the time periods when the\nperson’s whereabouts are unknown. Then, rule out\nany time periods during which the person was seen\ndoing something else or the place was closed. The\nremaining time periods are the possible times when\nthe person could have gone to the place.\n80.4\ntext-bison gpt-3.5-turbo\nQ_beginIdentify the optimal time slot for the individual to\nengage in the mentioned location/activity considering\nthe given sightings and waking up time, taking into\naccount the opening and closing times of the location\nand the duration of each event.\n53.6\n14",
    "Large Language Models as Optimizers\nTable 6: Transferability across datasets: accuracies of top instructions found for GSM8K on Multi-\nArith and AQuA.\nScorerSource\nInstruction\nposition\nInstruction\nAccuracy\nMultiArithAQuA\nBaselines\nPaLM 2-L(Kojima et al.,\n2022)\nA_beginLet’s think step by step.85.744.9\nPaLM 2-L(Zhou et al.,\n2022b)\nA_beginLet’s work this out in a step by step way\nto be sure we have the right answer.\n72.848.4\nPaLM 2-LA_beginLet’s solve the problem.87.544.1\nPaLM 2-LA_begin(empty string)69.337.8\ntext-bison(Kojima et al.,\n2022)\nQ_beginLet’s think step by step.92.531.9\ntext-bison(Zhou et al.,\n2022b)\nQ_begin\nLet’s work this out in a step by step way\nto be sure we have the right answer.\n93.732.3\ntext-bisonQ_beginLet’s solve the problem.85.529.9\ntext-bisonQ_begin(empty string)82.233.5\nOurs\nPaLM 2-L    PaLM 2-L-IT\non GSM8K\nA_beginTake a deep breath and work on this\nproblem step-by-step.\n95.354.3\ntext-bison   PaLM 2-L-IT\non GSM8K\nQ_beginLet’s work together to solve math word\nproblems! First, we will read and\ndiscuss the problem together to make\nsure we understand it. Then, we will\nwork together to find the solution. I will\ngive you hints and help you work\nthrough the problem if you get stuck.\n96.837.8\nthat presenting exemplars in the meta-prompt is critical, as it provides information on what the\ntask looks like and helps the optimizer model phrase new instructions better. However, more\nexemplars do not necessarily improve the performance, as a few exemplars are usually sufficient\nto describe the task.  In addition, including more exemplars results in a longer meta-prompt\nwith a dominating exemplar part, which may distract the optimizer LLM from other important\ncomponents like the optimization trajectory.\nThe number of generated instructions per step.Computing a mini-batch of gradients reduces\nthe variance of a stochastic gradient descent procedure. Similarly, generating multiple instructions\nin each step improves the optimization stability with LLMs.  On the other hand, to achieve better\nperformance with a fixed budget for the number of instructions to evaluate, the number of per-step\ninstructions should not be too large, so as to allow more optimization steps to incorporate richer\ninformation of past instructions with their accuracies. Taking both aspects into consideration, Figure 8\ncompares the optimization performance of sampling 1 / 2 / 4 / 8 (default) / 16 instructions in each\nstep, showing that sampling 8 instructions at each step overall achieves the best performance.\nStarting  point.We  study  the  effect  of  different  initial  instructions  for  prompt  optimization.\nOur default setting is to start from an empty string when the scorer LLM is (instruction-tuned)\ntext-bison, and to start from either the empty string (on BBH tasks) or “Let’s solve the problem.”\n(on GSM8K) with instruction position A_begin when the scorer LLM is the (pre-trained)PaLM 2-L.\nFigure 9(a) shows the performance oftext-bisonas the scorer LLM with 3 options of initial\ninstructions: (1) the empty string; (2) “Solve the following problem.”; or (3) “Solve the following\nproblem.” and “Let’s solve the problem.”. We observe that the accuracies do not differ much with\ndifferent starting points. Interestingly, the styles of the generated instructions are also similar. For\nexample, most of the generated instructions starting from (1) and (2) contain the phrase “solve this\nproblem”, like “Let’s work together to solve this problem.” in Step 4 with training accuracy 64.8 from\n(1), and “Let’s solve the following problems using the given information.” in Step 3 with training\naccuracy 62.8 from (2).\n15",
    "Large Language Models as Optimizers\n050100150200\n# steps\n50.0\n60.0\n70.0\naccuracy\nascending (default)\ndescending\nrandom\n(a) instruction ordering (GSM8K)\n050100150200\n# steps\n0.0\n50.0\n100.0\naccuracy\nascending (default)\ndescending\nrandom\n(b) instruction ordering (BBH sports_understanding)\n050100150200\n# steps\n50.0\n60.0\n70.0\naccuracy\n100 buckets (default)\n20 buckets\nno scores\n(c) instruction scores (GSM8K)\n050100150200\n# steps\n0.0\n50.0\n100.0\naccuracy\n100 buckets (default)\n20 buckets\nno scores\n(d) instruction scores (BBH sports_understanding)\n050100150200\n# steps\n50.0\n60.0\n70.0\naccuracy\n3 exemplars (default)\n10 exemplars\nno exemplars\n(e) # exemplars (GSM8K)\n050100150200\n# steps\n0.0\n50.0\n100.0\naccuracy\n3 exemplars (default)\n10 exemplars\nno exemplars\n(f) # exemplars (BBH sports_understanding)\nFigure 7:Ablation studies: how each part of the meta-prompt matters.The dots are the average\nvalues across 3 optimization repetitions, and the shaded regions represent standard deviations.\n16",
    "Large Language Models as Optimizers\n040080012001600\n# evaluated instructions\n50.0\n60.0\n70.0\naccuracy\n1\n2\n4\n8 (default)\n16\n(a) GSM8K\n040080012001600\n# evaluated instructions\n0.0\n50.0\n100.0\naccuracy\n1\n2\n4\n8 (default)\n16\n(b) BBH sports_understanding\nFigure 8:Ablation studies: the number of generated instructions in each step.The dots are the\naverage values across 3 optimization repetitions, and the shaded regions represent standard deviations.\nThe x-axis represents the total number of evaluated instructions through the optimization; e.g., we\nrun 200 optimization steps when sampling 8 instructions in each step, run 400 steps when sampling 4\ninstructions in each step, etc.\n050100150200\n# steps\n50.0\n60.0\n70.0\naccuracy\nfrom \"\" (default)\nfrom \"Solve the following problem.\"\nfrom \"\", \"Solve the following problem.\",\nand \"Let's solve the problem.\"\n(a) GSM8K,text-bisonscorer, Q_begin\n050100150200\n# steps\n40.0\n60.0\n80.0\naccuracy\nfrom \"Let's solve the problem\" (default)\nfrom \"\"\nfrom \"Let's think step by step.\"\n(b) GSM8K,PaLM 2-Lscorer, A_begin\nFigure 9:Ablation studies: the initial instructions for prompt optimization.The dots are the\naverage values across 3 optimization repetitions, and the shaded regions represent standard deviations.\nFigure 9(b) presents the results of ofPaLM 2-Las the scorer LLM with the following options of\ninitial instructions:  (1) “Let’s solve the problem.”; (2) the empty string; or (3) “Let’s think step\nby step.”.  We notice that the performance differs much more with different initial instructions,\nespecially at the beginning of the optimization. Specifically, starting from (1) leads to better generated\ninstructions than (2) in the first 30 steps, while the instructions optimized from both (1) and (2)\nare worse than (3) throughout. A similar observation holds when usingPaLM 2-Las scorer and\ngpt-3.5-turboas optimizer for BBH tasks, by comparing the results starting from the empty\nstring (Appendix E.2) and from “Let’s solve the problem.” (Appendix E.3). Taking a closer look into\nthe optimization process of (2), we find that although both “solve the problem” and “step by step”\nshow up in generated instructions at Step 5, it takes the optimizer LLM more steps to get rid of worse\ninstructions presented in the meta-prompt when starting from instructions with lower accuracies.\nTherefore, one direction for future work is to accelerate convergence from weaker starting points.\n17",
    "Large Language Models as Optimizers\n050100150200\n# steps\n50.0\n60.0\n70.0\naccuracy\n0.0\n0.5\n1.0 (default)\n1.5\n2.0\n(a) GSM8K\n050100150200\n# steps\n0.0\n50.0\n100.0\naccuracy\n0.0\n0.5\n1.0 (default)\n1.5\n2.0\n(b) BBH sports_understanding\nFigure 10:Ablation studies: temperature of the optimizer model.The dots are the average values\nacross 3 optimization repetitions, and the shaded regions represent standard deviations.\nDiversity per step.We evaluate the following temperatures of the optimizer LLM: {0.0, 0.5, 1.0\n(default), 1.5, 2.0}.  Figure 10 shows the default temperature 1.0 achieves the best performance.\nSpecifically, optimizations with smaller temperatures (0.0 and 0.5) lack exploration and thus creativity,\nand the optimizer LLM often gets stuck at the same instruction for tens of steps, resulting in flat\noptimization curves. On the other hand, with larger temperatures (1.5 and 2.0), the optimizer LLM\nmore often ignores the trajectory of previous instructions presented in the meta-prompt and thus lacks\nexploitation, therefore the optimization curve does not have a steady upward trend.\nComparison with one-step instruction generation.Our current iterative procedure runs for multiple\nsteps and generates a new batch of solutions in each step. To validate the importance of leveraging\nthe optimization trajectory for generating new prompts, we compare to a baseline that generates all\ninstructions in a single step without entering into the optimization procedure.  We compare these\ntwo approaches on GSM8K and BBH sports_understanding with thePaLM 2-L-IToptimizer.\nFor GSM8K the scorer LLM is pre-trainedPaLM 2-Land the initial instruction is “Let’s solve\nthe problem”, and for BBH sports_understanding the scorer LLM istext-bisonand the initial\ninstruction is the empty string.  The baseline generates 50 instructions in a single step, thus its\nmeta-prompt only includes task exemplars, the initial instruction with its accuracy, and the same\nmeta-instructions as our full meta-prompt for performing optimization. All the other hyperparameters\nremain the same.\nOur results show that this one-step instruction generation performs much worse than our optimization\napproach.  Specifically: (1) On GSM8K, the best instruction among all 50 is still “Let’s solve the\nproblem”, with a 64.4 training accuracy and a 60.8 test accuracy. On the other hand, our approach\n(corresponding to Figure 1(a) in the main paper) found “Let’s do the math!”  with a 78.2 training\naccuracy and a 76.3 test accuracy at the 5th step by generating 8 instructions at each step.  (2)\nSimilarly, on BBH sports_understanding, the best instruction among all 50 achieved a 84.0 training\naccuracy and 80.0 test accuracy. This is again worse than the instruction found by our approach at\nStep 4, which achieved a 88.0 training accuracy and a 84.5 test accuracy.\n5.4OVERFITTINGANALYSIS INPROMPTOPTIMIZATION\nFor simplicity, we do not set aside a validation set in our default setting of prompt optimization. We\nmade this decision based on the experiments when a validation set is present.\nOverfitting may result in training accuracy being much higher than the validation/test accuracy. It\nis difficult to avoid overfitting, but overfitting is less harmful when each candidate solution (natural\nlanguage instruction in the prompt optimization context) overfits to a similar extent. In this case, a\nhigher training accuracy solution still achieves a higher validation/test accuracy, and one can adopt\nsolutions with the highest training accuracies as the final result. Figure 11 shows this is the case for\nOPRO in prompt optimization: when setting aside a validation set with the same size as the training\n18",
    "Large Language Models as Optimizers\n050100150200\n# steps\n50\n70\n90\naccuracy\ntraining\nvalidation\n(a)  BBH snarks,PaLM 2-Las scorer,PaLM\n2-L-IT\nas optimizer, starting from “Let’s solve\nthe problem.”\n050100\n# steps\n40\n60\n80\naccuracy\ntraining\nvalidation\n(b) BBH sports_understanding,text-bison\nas scorer,gpt-3.5-turboas optimizer, start-\ning from the empty string\nFigure 11:Overfitting analysis.The exemplars are splitted to 1/3 training, 1/3 validation and 1/3\ntest. We compute the validation accuracy every 3 steps. The training/validation dots are the average\ntraining/validation accuracies across 3 optimization repetitions, respectively, and the shaded regions\nrepresent standard deviations.\nset, the validation accuracy curves trend up and down alongside the training curves in both prompt\noptimization settings.\nOf course, overfitting still occurs in the instructions found by our prompt optimization: in Table 7\nand 10, our training accuracies are often 5%-20% higher than our test accuracies, despite that our test\nand overall accuracies are still mostly higher than human-written counterparts. Setting aside a larger\ntraining set and optimizing for fewer steps (early stopping) may help reduce overfitting.\n5.5COMPARISON WITHEVOPROMPT\nSome concurrent works on prompt optimization propose meta-prompts that explicitly ask the LLM to\nperform mutation and crossovers of existing prompts (Fernando et al., 2023; Guo et al., 2023). In our\nevaluation, we compare our approach to the Genetic Algorithm (GA) and Differential Evolution (DE)\nversions of EvoPrompt (Guo et al., 2023). Specifically, in the GA meta-prompt, given two prompts,\nthe meta-prompt instructs the LLM to cross over the two prompts and generates a new one, then\nmutates the newly generated prompt to produce the final prompt. DE extends the GA meta-prompt\nto include more detailed instructions, e.g., asking the LLM to identify different parts between the\ntwo given prompts before performing the mutation. This is in contrast with OPRO, which leverages\nthe optimization trajectory including multiple past prompts, instead of only 2 previous prompts.\nMeanwhile, OPRO also provides the LLM with richer information to facilitate the understanding of\nthe optimization problem, including exemplars and task accuracies of different prompts.\nFigure 12 presents the results on GSM8K and BBH sports_understanding benchmarks, where we use\ngpt-3.5-turboas the optimizer. On GSM8K, the initial instructions of all approaches are “Let’s\nsolve the problem.” and “Here is the answer.”, which are simple and generic. Again, we observe that\nOPRO performance steadily improves with more optimization steps. On the other hand, both versions\nof EvoPrompt even degrade the performance on GSM8K. The main reason is because EvoPrompt\ndoes not utilize exemplars for prompt optimization, thus it lacks the understanding of the task to\noptimize for.  In this way, EvoPrompt relies on good-quality and task-specific initial prompts to\noptimize from.\nGiven this observation, we provide more task-specific initial instructions for experiments on BBH\nsports_understanding, which are “Solve the sports understanding problem.” and “Give me the answer\nto sports understanding.”  In this case,  EvoPrompt (DE) is able to find better prompts than the\ninitial ones, but the optimization curve is less stable than OPRO. This indicates that leveraging the\noptimization trajectory helps the LLM to identify promising directions to improve existing prompts.\n19",
    "Large Language Models as Optimizers\n050100150\n# steps\n20\n50\n80\naccuracy\nOPRO\nEvoPrompt (GA)\nEvoPrompt (DE)\n(a) GSM8K,PaLM 2-Lscorer, A_begin\n050100150200\n# steps\n50\n90\naccuracy\nOPRO\nEvoPrompt (GA)\nEvoPrompt (DE)\n(b) BBH sports_understanding,text-bison\nscorer, Q_begin\nFigure 12:Comparison with EvoPrompt in prompt optimization.We use thegpt-3.5-turbo\noptimizer for both experiments. “EvoPrompt (GA)” uses the meta-prompt from Guo et al. (2023),\nFigure 1; “EvoPrompt (DE)” uses the meta-prompt from Guo et al. (2023), Figure 2. All optimizations\nin (a) use the pre-trainedPaLM 2-Lscorer and start from two simple instructions “Let’s solve the\nproblem.” and “Here is the answer.”; all optimizations in (b) use thetext-bisonscorer and start\nfrom two richer (task-specific) instructions “Solve the sports understanding problem.”  and “Give\nme the answer to sports understanding.”.  The dots are the average values across 3 optimization\nrepetitions, and the shaded regions represent standard deviations. We use temperature 1.0 for OPRO\nand temperature 0.5 for EvoPrompt, same as the default settings in respective works.\n6RELATEDWORK\nPrompt optimization.Prior works have developed soft prompt-tuning methods that optimize the\nprompt represented as task-specific continuous vectors (Lester et al., 2021; Li & Liang, 2021; Liu et al.,\n2021; Qin & Eisner, 2021), as well as performing discrete prompt optimization by gradient-guided\nsearch (Shin et al., 2020; Wen et al., 2023; Gao et al., 2020; Chen et al., 2023d) and reinforcement\nlearning (Deng et al., 2022; Zhang et al., 2023). These approaches become inapplicable when there is\nonly API access to the LLM. Other works designed edit-based approaches for gradient-free prompt\noptimization (Xu et al., 2022; Prasad et al., 2022), where the editing can be done with human-\ndefined operations (e.g., swapping two phrases) (Prasad et al., 2022) or language models (e.g., back\ntranslation) (Xu et al., 2022). Some recent works investigate LLMs for prompt optimization (Zhou\net al., 2022b; Pryzant et al., 2023; Xu et al., 2023). Specifically, APE (Zhou et al., 2022b) first uses\nthe LLM to generate initial instructions. Afterwards, APE selects top instructions with the highest\naccuracies, then prompts the LLM with each individual instruction to generate a semantically similar\nvariant of the initial instruction. APO (Pryzant et al., 2023) in each step instructs the LLM to produce\ntext feedback on how to update an old instruction. Different from edit-based approaches, the optimizer\nLLM in our work directly generates new instructions at each optimization step, and the optimizer\nLLM is merely asked to improve the task accuracy without being required to imitate past instructions.\nCompared to Zhou et al. (2022b) and Pryzant et al. (2023), our optimization process incorporates\nthe past generated instructions with their scores in the meta-prompt, enabling the optimizer LLM to\ndiscover common patterns of high-quality instructions.\nPrompting with natural language feedback.A recent line of work investigates approaches to\nimprove the LLM performance by prompting with natural language feedback to revise the model\noutput, which has shown effectiveness in reducing harmful LLM outputs (Bai et al., 2022; Ganguli\net al., 2023), improving reasoning (Shinn et al., 2023; Madaan et al., 2023) and code generation\nperformance (Chen et al., 2023e; Olausson et al., 2023; Shinn et al., 2023; Chen et al., 2023b),\ndialogue applications (Nair et al., 2023; Madaan et al., 2023; Yuan et al., 2023), and so on (Kim et al.,\n2023; Wang et al., 2023). Specifically, Yuan et al. (2023) develops a human-in-the-loop framework\nfor deriving system-level feedback from a collection of instance-level feedback, which is then used\n20",
    "Large Language Models as Optimizers\nfor refining data. In our work, the optimizer LLM utilizes the optimization trajectory in the prompt,\nwhich implicitly requires the LLM to summarize the common characteristics among solutions with\nsimilar scores. We consider incorporating explicit natural language feedback on generated solutions\nfor later optimization steps as future work.\nTuning language models for optimization.Some previous works tune or prompt language models\nto behave as mutation and crossover operators in evolutionary algorithms. Meyerson et al. (2023)\nutilizes language models with few-shot exemplars to propose evolutionary cross-overs on tasks such\nas image and code generation. In Lehman et al. (2022), the large language model trained on code diff\ngeneration is used as the mutation operator, and they further design a fine-tuning method to improve\nperformance in the Sodarace domain for robot simulation. EvoPrompting (Chen et al., 2023a) uses\nlarge language models to evolve neural network architectures, where they combine evolutionary\nsearch with soft prompt tuning. With respect to taking the trajectory as the input for optimization,\nOptFormer (Chen et al., 2022) trains a transformer model on large collections of hyperparameter\noptimization data. On the other hand, our work performs optimization solely by prompting without\nadditional training.\n7CONCLUSION\nWe embark on employing LLMs as optimizers, where the LLM progressively generates new solutions\nto optimize an objective function.  We first motivate OPRO with linear regression and traveling\nsalesman problems, then proceed to prompt optimization as a concrete application. Our evaluation\ndemonstrates that LLMs have the capacity of gradually improving the generated solutions based on\nthe past optimization trajectory. Interestingly, on small-scale traveling salesman problems, OPRO\nperforms on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized\nprompts outperform human-designed prompts on GSM8K and Big-Bench Hard by a significant\nmargin, sometimes over50%.\nA number of unresolved questions are open for future research on LLMs for optimization. In general,\nhow to reduce the sensitivity to initialization and better balance exploitation with exploration remains\na challenge. Specifically, for prompt optimization, one limitation of our current implementation is\nthat the optimizer LLM does not effectively utilize error cases in the training set to infer promising\ndirections to improve the generated instructions. In our experiments, we tried including error cases in\nthe meta-prompt rather than randomly sampling from the training set at each optimization step, but the\nresults are similar, indicating that the error cases alone are not informative enough for the optimizer\nLLM to grasp the cause of the wrong prediction.  Another limitation is that prompt optimization\nrequires a training set to compute the accuracy that guides the optimization process. Currently the\ntraining set at least contains tens of samples, so that the optimized prompt does not severely overfit\nto the training samples.  A promising direction is to incorporate richer feedback about the error\ncases besides the aggregated accuracy, and summarize the key features that distinguish between\nhigh-quality and low-quality generated prompts in the optimization trajectory. Such information may\ninform the optimizer LLM of how to more efficiently improve over the past generated instructions,\nand potentially further reduce the example set size needed for prompt optimization.\nETHICSSTATEMENT\nThis work uses synthetic math problems for linear regression and traveling salesman problems, and\nuses public datasets like GSM8K and Big-Bench Hard for prompt optimization. These tasks have\nbeen commonly used in similar works and should not be regarded controversial. There is a peril that\nLLMs may generate harmful information that poses safety risks; how to safeguard model behavior\nremains valuable future work.\nREPRODUCIBILITYSTATEMENT\nWe evaluate on public benchmarks.  Thetext-bisonAPI is available at:https://cloud.\ngoogle.com/vertex-ai/docs/generative-ai/learn/models.   The  GPT  models\nare available here:http://openai.com/api/.   This work usesgpt-3.5-turbo-0613\nandgpt-4-0613.\n21",
    "Large Language Models as Optimizers\nACKNOWLEDGMENTS\nWe thank Daiyi Peng, Yanqi Zhou, Jerry Wei, Shuo Chen, Tim Rocktäschel, Chrisantha Fernando,\nDylan Banarse, Henryk Michalewski, Simon Osindero, and Ed H. Chi for their valuable feedback,\nand thank several anonymous reviewers for helpful comments.\nREFERENCES\nShun-ichi Amari. Backpropagation and stochastic gradient descent method.Neurocomputing, 5(4-5):\n185–196, 1993.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.arXiv\npreprint arXiv:2305.10403, 2023.\nDavid Applegate, Ribert Bixby, Vasek Chvatal, and William Cook. Concorde tsp solver, 2006.\nThomas Bäck and Hans-Paul Schwefel.   An overview of evolutionary algorithms for parameter\noptimization.Evolutionary computation, 1(1):1–23, 1993.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback.arXiv preprint arXiv:2212.08073, 2022.\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\ntool makers.arXiv preprint arXiv:2305.17126, 2023.\nAngelica Chen, David M Dohan, and David R So. Evoprompting: Language models for code-level\nneural architecture search.arXiv preprint arXiv:2302.14838, 2023a.\nAngelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R\nBowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training with natural\nlanguage feedback.arXiv preprint arXiv:2303.16749, 2023b.\nJiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou.  When do you need chain-of-thought\nprompting for chatgpt?arXiv preprint arXiv:2304.03262, 2023c.\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient\ninstruction optimization for black-box large language models.arXiv preprint arXiv:2306.03082,\n2023d.\nXinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization.\nAdvances in Neural Information Processing Systems, 32, 2019.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to\nself-debug.arXiv preprint arXiv:2304.05128, 2023e.\nYutian  Chen,  Xingyou  Song,  Chansoo  Lee,  Zi  Wang,  Richard  Zhang,  David  Dohan,  Kazuya\nKawakami, Greg Kochanski, Arnaud Doucet, Marc’aurelio Ranzato, et al.   Towards learning\nuniversal hyperparameter optimizers with transformers.Advances in Neural Information Process-\ning Systems, 35:32053–32068, 2022.\nKarl Cobbe,  Vineet Kosaraju,  Mohammad Bavarian,  Mark Chen,  Heewoo Jun,  Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems.arXiv preprint arXiv:2110.14168, 2021.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,\nEric P Xing, and Zhiting Hu.  Rlprompt:  Optimizing discrete text prompts with reinforcement\nlearning.arXiv preprint arXiv:2205.12548, 2022.\nMichel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau.\nLearning heuristics for the tsp by policy gradient. InInternational Conference on the Integration of\nConstraint Programming, Artificial Intelligence, and Operations Research, pp. 170–181. Springer,\n2018.\n22",
    "Large Language Models as Optimizers\nChrisantha  Fernando,  Dylan  Banarse,  Henryk  Michalewski,  Simon  Osindero,  and  Tim  Rock-\ntäschel. Promptbreeder: Self-referential self-improvement via prompt evolution.arXiv preprint\narXiv:2309.16797, 2023.\nDeep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil\n ̇\ne Lukoši\n ̄\nut\n ̇\ne, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al.  The capacity for\nmoral self-correction in large language models.arXiv preprint arXiv:2302.07459, 2023.\nTianyu Gao, Adam Fisch, and Danqi Chen.  Making pre-trained language models better few-shot\nlearners.arXiv preprint arXiv:2012.15723, 2020.\nBruce Golden,  Lawrence Bodin,  T Doyle,  and W Stewart Jr.   Approximate traveling salesman\nalgorithms.Operations research, 28(3-part-ii):694–711, 1980.\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,\nand Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\nprompt optimizers.arXiv preprint arXiv:2309.08532, 2023.\nGregory Gutin and Abraham P Punnen.The traveling salesman problem and its variations, volume 12.\nSpringer Science & Business Media, 2006.\nKeld Helsgaun.  An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling\nsalesman and vehicle routing problems.Roskilde: Roskilde University, 12, 2017.\nMichael Jünger, Gerhard Reinelt, and Giovanni Rinaldi. The traveling salesman problem.Handbooks\nin operations research and management science, 7:225–330, 1995.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.  Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491, 2023.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. InInternational\nConference on Learning Representations, 2015.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.  Large\nlanguage models are zero-shot reasoners.arXiv preprint arXiv:2205.11916, 2022.\nWouter Kool, Herke van Hoof, and Max Welling.  Attention, learn to solve routing problems!  In\nInternational Conference on Learning Representations, 2019. URLhttps://openreview.\nnet/forum?id=ByxBFsRqYm.\nJoel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O Stanley.\nEvolution through large models.arXiv preprint arXiv:2206.08896, 2022.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning.arXiv preprint arXiv:2104.08691, 2021.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.arXiv\npreprint arXiv:2101.00190, 2021.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word problems.arXiv preprint arXiv:1705.04146,\n2017.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.  Gpt\nunderstands, too.arXiv preprint arXiv:2103.10385, 2021.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered\nprompts and where to find them: Overcoming few-shot prompt order sensitivity.arXiv preprint\narXiv:2104.08786, 2021.\nXiao Ma,  Swaroop Mishra,  Ahmad Beirami,  Alex Beutel,  and Jilin Chen.   Let’s do a thought\nexperiment: Using counterfactuals to improve moral reasoning.arXiv preprint arXiv:2306.14308,\n2023.\n23",
    "Large Language Models as Optimizers\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\ntwo to tango.arXiv preprint arXiv:2209.07686, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.  Self-refine: Iterative refinement\nwith self-feedback.arXiv preprint arXiv:2303.17651, 2023.\nElliot  Meyerson,  Mark  J  Nelson,  Herbie  Bradley,  Arash  Moradi,  Amy  K  Hoover,  and  Joel\nLehman.   Language model crossover:  Variation through few-shot prompting.arXiv preprint\narXiv:2302.12170, 2023.\nSuvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas,\nKanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines.\narXiv preprint arXiv:2307.04721, 2023.\nVarun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan. Dera: Enhancing large language\nmodel completions with dialog-enabled resolving agents.arXiv preprint arXiv:2303.17071, 2023.\nMohammadReza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Takac.  Reinforcement\nlearning for solving the vehicle routing problem. InAdvances in Neural Information Processing\nSystems, pp. 9861–9871, 2018.\nTheo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama.\nDemystifying gpt self-repair for code generation.arXiv preprint arXiv:2306.09896, 2023.\nGurobi Optimization et al. Gurobi optimizer reference manual, 2020.\nArchiki Prasad,  Peter Hase,  Xiang Zhou,  and Mohit Bansal.   Grips:  Gradient-free,  edit-based\ninstruction search for prompting large language models.arXiv preprint arXiv:2203.07281, 2022.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\noptimization with\" gradient descent\" and beam search.arXiv preprint arXiv:2305.03495, 2023.\nNing Qian. On the momentum term in gradient descent learning algorithms.Neural networks, 12(1):\n145–151, 1999.\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\narXiv preprint arXiv:2104.06599, 2021.\nColin R Reeves.Modern heuristic techniques for combinatorial problems. John Wiley & Sons, Inc.,\n1993.\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the\nfew-shot paradigm.  InExtended Abstracts of the 2021 CHI Conference on Human Factors in\nComputing Systems, pp. 1–7, 2021.\nLuis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms and\ncomparison of software implementations.Journal of Global Optimization, 56:1247–1293, 2013.\nDaniel J Rosenkrantz, Richard E Stearns, and Philip M Lewis, II. An analysis of several heuristics\nfor the traveling salesman problem.SIAM journal on computing, 6(3):563–581, 1977.\nSubhro  Roy  and  Dan  Roth.Solving  general  arithmetic  word  problems.arXiv  preprint\narXiv:1608.01413, 2016.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools.arXiv preprint arXiv:2302.04761, 2023.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts.arXiv preprint\narXiv:2010.15980, 2020.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection.arXiv preprint arXiv:2303.11366, 2023.\n24",
    "Large Language Models as Optimizers\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.  Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models.arXiv preprint\narXiv:2206.04615, 2022.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them.arXiv preprint arXiv:2210.09261, 2022.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models.arXiv\npreprint arXiv:2305.16291, 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nJason  Wei,  Xuezhi  Wang,  Dale  Schuurmans,  Maarten  Bosma,  Ed  Chi,  Quoc  Le,  and  Denny\nZhou.  Chain of thought prompting elicits reasoning in large language models.arXiv preprint\narXiv:2201.11903, 2022.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, et al. Larger language models do in-context learning differently.arXiv\npreprint arXiv:2303.03846, 2023.\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\nHard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.\narXiv preprint arXiv:2302.03668, 2023.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang.  Wizardlm:  Empowering large language models to follow complex instructions.arXiv\npreprint arXiv:2304.12244, 2023.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Gps:\nGenetic prompt search for efficient few-shot learning.arXiv preprint arXiv:2210.17041, 2022.\nWeizhe Yuan, Kyunghyun Cho, and Jason Weston. System-level natural language feedback.arXiv\npreprint arXiv:2306.13588, 2023.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera:\nTest-time prompt editing via reinforcement learning. InThe Eleventh International Conference on\nLearning Representations, 2023.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\nfew-shot performance of language models. InInternational Conference on Machine Learning, pp.\n12697–12706. PMLR, 2021.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning\nin large language models.arXiv preprint arXiv:2205.10625, 2022a.\nYongchao  Zhou,  Andrei  Ioan  Muresanu,  Ziwen  Han,  Keiran  Paster,  Silviu  Pitis,  Harris  Chan,\nand  Jimmy  Ba.   Large  language  models  are  human-level  prompt  engineers.arXiv  preprint\narXiv:2211.01910, 2022b.\n25",
    "Large Language Models as Optimizers\nASOMEFAILURECASES\nAlthough LLMs show the power of optimizing basic math problems (Section 3) and prompts (Sec-\ntion 4), we see some limitations across all optimizer LLMs that may impede their power of solving\nmore challenging problems. These limitations include:\n•Hallucinating the values that need to come from math calculation: The optimizer LLMs\noften output contents like “the function value at (5, 3) is 15” despite that the true value is not 15.\nThe model will get it right if external tools that can reliably calculate the value are triggered.\nWhen and how to trigger such tool use cases remains an interesting topic (see e.g., (Schick et al.,\n2023; Cai et al., 2023)).\n•\nGenerating solutions already appeared in context even if we tell it to \"Give me a new (w, b)\npair that is different from all pairs above\": the optimizer LLMs do not 100% reliably follow\nthis instruction even if its own outputs often include sentences like “I will provide a new pair\nthat is different”, making the output self-contradictory. The output is almost guaranteed to be\ndifferent from in-context old solutions when the model output contains a comparison of the new\npair and all old pairs, though.  Thus (implicitly) triggering such behaviors may be a solution.\nHow to implement this feature without harming the instruction following performance of other\nparts remains an interesting topic to study.\n•\nIn black-box math optimization, getting stuck at a point that is neither global nor local\noptimal: This often occurs in two linear regression cases: (a) The in-context exemplars all share\nthe sameworbthat is different fromw\ntrue\norb\ntrue\n. This case is more likely to be avoided when\na larger number of past solutions are included in the meta-prompt; (b) one or several of the best\nprevious solutions in the meta-prompt havews andbs in quantitatively opposite directions from\nthe global optimaw\ntrue\nandb\ntrue\n: for example, thews are all smaller thanw\ntrue\nwhile thebs are\nall larger thanb\ntrue\n. Since the optimizer model often proposes to only increasewor decreaseb\nwhen the past solutions in meta-prompt shareworb, the optimization will get stuck if either\nincreasingwor decreasingbwould increase the objective value.  This issue is mitigated by\nsampling multiple new solutions (thus more exploration) at each step.\n•\nHard to navigate a bumpy loss landscape: Like other optimizers, it is harder for the optimizer\nLLM to optimize black-box functions when the loss landscape gets more complicated.  For\nexample, when minimizing the Rosenbrock functionf(x,y) = (a−x)\n2\n+b(y−x\n2\n)\n2\nwitha= 20\n(whose global optimal point isx= 20,y= 400) with 5 starting points in[10,20]×[10,20],\nthe optimization often gets stuck at around(0,0).  This is because the optimizer LLM sees a\ndecrease of objective value when it drastically decreases bothxandyto0. Then starting from\n(0,0), the optimizer LLM is hard to further navigatexandyalong the narrow valley in the loss\nlandscape towards(20,400)(Figure 13).\nx\n0\n5\n10\n15\n20\ny\n0\n100\n200\n300\n400\nf(x, y)\n50000\n100000\n150000\nFigure 13: A visualization of the landscape of the Rosenbrock functionf(x,y) = (a−x)\n2\n+b(y−x\n2\n)\n2\nwitha= 20andb= 1. The global optima is atx= 20,y= 400with function value0. The function\nvalue atx= 0,y= 0is400. The landscape has a narrow valley between(0,0)and(20,400).\n26",
    "Large Language Models as Optimizers\nBPROMPTINGFORMATS FORSCORERLLM\nFigure 14, 15, and 16 show examples of the Q_begin, Q_end, and A_begin prompting formats when\nthe “QA” pattern is present. The “QA” pattern is eliminated when prompting instruction-tuned scorer\nmodels liketext-bisonwith the Q_begin and Q_end formats (Figure 17 and 18).\nQ: {instruction}\nJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for\nher friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh\nduck egg. How much in dollars does she make every day at the farmers’ market?\nA:\nFigure 14: The Q_begin prompting format on a GSM8K test exemplar with the \"QA\" pattern.\nQ: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins\nfor her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per\nfresh duck egg. How much in dollars does she make every day at the farmers’ market?\n{instruction}\nA:\nFigure 15: The Q_end prompting format on a GSM8K test exemplar with the \"QA\" pattern.\nQ: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins\nfor her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per\nfresh duck egg. How much in dollars does she make every day at the farmers’ market?\nA: {instruction}\nFigure 16: The A_begin prompting format on a GSM8K test exemplar.\n{instruction}\nJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for\nher friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh\nduck egg. How much in dollars does she make every day at the farmers’ market?\nFigure 17: The Q_begin prompting format on a GSM8K test exemplar without the \"QA\" pattern.\nJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for\nher friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh\nduck egg. How much in dollars does she make every day at the farmers’ market?\n{instruction}\nFigure 18: The Q_end prompting format on a GSM8K test exemplar without the \"QA\" pattern.\n27",
    "Large Language Models as Optimizers\nCMETA-PROMPTS\nC.1META-PROMPT FORMATHOPTIMIZATION\nNow you will help me minimize a function with two input variables w, b. I have some (w, b) pairs\nand the function values at those points. The pairs are arranged in descending order based on their\nfunction values, where lower values are better.\ninput:\nw=18, b=15\nvalue:\n10386334\ninput:\nw=17, b=18\nvalue:\n9204724\nGive me a new (w, b) pair that is different from all pairs above, and has a function value lower than\nany of the above.  Do not write code.  The output must end with a pair [w, b], where w and b are\nnumerical values.\nFigure 19: An example of the meta-prompt for linear regression. The blue text contains solution-score\npairs; the orange text are meta-instructions.\nYou are given a list of points with coordinates below: (0): (-4, 5), (1): (17, 76), (2): (-9, 0), (3): (-31,\n-86), (4): (53, -35), (5): (26, 91), (6): (65, -33), (7): (26, 86), (8): (-13, -70), (9): (13, 79), (10): (-73,\n-86), (11): (-45, 93), (12): (74, 24), (13): (67, -42), (14): (87, 51), (15): (83, 94), (16): (-7, 52), (17):\n(-89, 47), (18): (0, -38), (19): (61, 58).\nBelow are some previous traces and their lengths. The traces are arranged in descending order based\non their lengths, where lower values are better.\n<trace> 0,13,3,16,19,2,17,5,4,7,18,8,1,9,6,14,11,15,10,12 </trace>\nlength:\n2254\n<trace> 0,18,4,11,9,7,14,17,12,15,10,5,19,3,13,16,1,6,8,2 </trace>\nlength:\n2017\n<trace> 0,11,4,13,6,10,8,17,12,15,3,5,19,2,1,18,14,7,16,9 </trace>\nlength:\n1953\n<trace> 0,10,4,18,6,8,7,16,14,11,2,15,9,1,5,19,13,12,17,3 </trace>\nlength:\n1840\nGive me a new trace that is different from all traces above, and has a length lower than any of the\nabove. The trace should traverse all points exactly once. The trace should start with <trace> and end\nwith </trace>.\nFigure 20:  An example of the meta-prompt for Traveling Salesman Problems with problem size\nn= 20. The blue text contains solution-score pairs; the orange text are meta-instructions.\n28",
    "Large Language Models as Optimizers\nC.2META-PROMPT FORPROMPTOPTIMIZATION\nDifferent optimizer models work the best on different styles of meta-prompts. Figure 3 in the main\npaper shows the meta-prompt forPaLM 2-L-IT; Figure 21 shows that for pre-trainedPaLM 2-L;\nFigure 22 shows that for GPT models.\nCreate a piece of text at the beginning of the answer to enhance the precision in solving diverse grade\nschool math problems.\nPrecision: 4 <TEXT>A dime</TEXT>\nPrecision: 17 <TEXT>The answer is a function. It is</TEXT>\nPrecision: 19 <TEXT>So how can we find out what this equation means?</TEXT>\nPrecision: 20 <TEXT>Solutions:</TEXT>\nFigure 21: An example of the meta-prompt for prompt optimization with pre-trainedPaLM 2-L\non GSM8K, where the generated instruction will be prepended to the beginning of the scorer LLM\noutput (A_beginin Section 4.1).\nYour task is to generate the instruction <INS>. Below are some previous instructions with their scores.\nThe score ranges from 0 to 100.\ntext:\nLet’s figure it out!\nscore:\n61\ntext:\nLet’s solve the problem.\nscore:\n63\n(. . .  more instructions and scores . . . )\nBelow are some problems.\nProblem:\nQ: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books\nby their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books than\nAlannah. If Beatrix has 30 books, how many books do the three have together?\nA: <INS>\nGround truth answer:\n140\n(. . .  more exemplars . . . )\nGenerate an instruction that is different from all the instructions <INS> above, and has a higher score\nthan all the instructions <INS> above. The instruction should begin with <INS> and end with </INS>.\nThe instruction should be concise, effective, and generally applicable to all problems above.\nFigure  22:An  example  of  the  meta-prompt  for  prompt  optimization  with  GPT  models\n(gpt-3.5-turboorgpt-4) on GSM8K, where the generated instruction will be prepended\nto the beginning of the scorer LLM output (A_beginin Section 4.1). The blue text contains solution-\nscore pairs; the purple text describes the optimization task and output format; the orange text are\nmeta-instructions.\n29",
    "Large Language Models as Optimizers\nDPROMPTOPTIMIZATIONCURVES ON  THEREMAININGBBH TASKS\n050100\n# steps\n50.0\n70.0\n90.0\ntraining accuracy\nBBH\nboolean_expressions\n(a) BBH boolean_expressions\n050100\n# steps\n60.0\n70.0\n80.0\ntraining accuracy\nBBH\ncausal_judgement\n(b) BBH causal_judgement\n050100150\n# steps\n40.0\n50.0\n60.0\ntraining accuracy\nBBH\ndate_understanding\n(c) BBH date_understanding\n050100\n# steps\n40.0\n50.0\n60.0\ntraining accuracy\nBBH\ndisambiguation_qa\n(d) BBH disambiguation_qa\n050100\n# steps\n98.0\n100.0\ntraining accuracy\nBBH\ndyck_languages\n(e) BBH dyck_languages\n0204060\n# steps\n50.0\n60.0\n70.0\ntraining accuracy\nBBH\nformal_fallacies\n(f) BBH formal_fallacies\n050100150200\n# steps\n20.0\n30.0\ntraining accuracy\nBBH\ngeometric_shapes\n(g) BBH geometric_shapes\n050100150200\n# steps\n60.0\n70.0\n80.0\ntraining accuracy\nBBH\nhyperbaton\n(h) BBH hyperbaton\n050100150200\n# steps\n55\n60\n65\ntraining accuracy\nBBH logical_deduction_\nseven_objects\n(i)BBH logical_deduction_seven_objects\n050100150200\n# steps\n60\n70\n80\n90\n100\ntraining accuracy\nBBH movie_\nrecommendation\n(j) BBH movie_recommendation\n050100150200\n# steps\n0\n10\n20\n30\ntraining accuracy\nBBH multistep_\narithmetic_two\n(k)BBH multistep_arithmetic_two\n04080120\n# steps\n55\n60\n65\n70\ntraining accuracy\nBBH navigate\n(l) BBH navigate\n050100\n# steps\n40\n50\n60\n70\ntraining accuracy\nBBH object_counting\n(m) BBH object_counting\n050100\n# steps\n60\n70\ntraining accuracy\nBBH penguins_in_a_table\n(n) BBH penguins_in_a_table\n0204060\n# steps\n70\n80\ntraining accuracy\nBBH reasoning_about_\ncolored_objects\n(o)BBH reasoning_about_colored_objects\nFigure 23:  Prompt optimization on 21 BBH tasks (except ruin_names and temporal_sequences\nalready shown in Figure 6) with thetext-bisonscorer and thePaLM 2-L-IToptimizer, Part I.\nMost curves have upward trends.\n30",
    "Large Language Models as Optimizers\n02040\n# steps\n30\n40\ntraining accuracy\nBBH salient_translation_\nerror_detection\n(a)BBH salient_translation_error_detection\n050100150200\n# steps\n70\n80\ntraining accuracy\nBBH snarks\n(b) BBH snarks\n02040\n# steps\n40\n60\n80\n100\ntraining accuracy\nBBH sports_\nunderstanding\n(c) BBH sports_understanding\n050100150200\n# steps\n10\n20\ntraining accuracy\nBBH tracking_shuffled_\nobjects_seven_objects\n(d)BBHtracking_shuffled_\nobjects_seven_objects\n050100150200\n# steps\n50\n60\ntraining accuracy\nBBH web_of_lies\n(e) BBH web_of_lies\n050100150200\n# steps\n10\n20\ntraining accuracy\nBBH word_sorting\n(f) BBH word_sorting\nFigure 24: Prompt optimization on 21 BBH tasks (except ruin_names and temporal_sequences in\nFigure 6) with thetext-bisonscorer and thePaLM 2-L-IToptimizer, Part II. All curves have\nupward trends.\nE\nPROMPTOPTIMIZATION  ONBBH TASKS– TABULATEDACCURACIES AND\nFOUNDINSTRUCTIONS\nE.1PALM 2-L-ITAS OPTIMIZER,OPTIMIZATION STARTING FROM  THE EMPTY STRING\nTable 8 and 9 show the instructions found by prompt optimization. A comparison of their accuracies\nwith baselines “Let’s think step by step.” (Kojima et al., 2022), “Let’s work this out in a step by step\nway to be sure we have the right answer.” (Zhou et al., 2022b), and the empty string is in Table 7; a\nvisualization is in Section 5.2 Figure 5.\n31",
    "Large Language Models as Optimizers\nTable 7: Accuracies on BBH tasks: our found instructions with thePaLM 2-L-IToptimizer vs\nbaseline.  The optimization starts from the empty string.  Because of the 20-80 train-test split, we\nshow accuracies with the format “training / test / overall (training + test)”. ThePaLM 2-Lscores are\nfrom A_begin instructions; thetext-bisonscores are from Q_begin instructions. Bold numbers\nindicate the best for the corresponding task.\nTaskScorer\nOur Acc“Let’s think step by\nstep.” Acc\n“Let’s work this out in\na step by step way to\nbe sure we have the\nright answer.” Acc\nempty string “” Acc\ntraining / test / overalltraining / test / overalltraining / test / overalltraining / test / overall\nboolean_expressionsPaLM 2-L90.0 / 83.5 / 84.890.0 / 83.0 / 84.482.0 / 74.0 / 75.674.0 / 71.0 / 71.6\ncausal_judgementPaLM 2-L84.8 / 58.0 / 63.173.0 / 55.3 / 58.859.5 / 57.3 / 57.829.7 / 49.3 / 45.5\ndate_understandingPaLM 2-L86.0 / 84.5 / 84.876.0 / 80.0 / 79.274.0 / 77.0 / 76.470.0 / 74.0 / 73.2\ndisambiguation_qaPaLM 2-L80.0 / 69.0 / 71.240.0 / 52.5 / 50.048.0 / 47.0 / 47.254.0 / 57.5 / 56.8\ndyck_languagesPaLM 2-L100.0 / 100.0 / 100.096.0 / 94.5 / 94.8100.0 / 93.5 / 94.894.0 / 95.0 / 94.8\nformal_fallaciesPaLM 2-L84.0 / 64.0 / 68.478.0 / 59.5 / 63.268.0 / 63.0 / 64.066.0 / 59.0 / 60.4\ngeometric_shapesPaLM 2-L76.0 / 57.0 / 60.842.0 / 33.0 / 34.842.0 / 32.0 / 34.034.0 / 33.0 / 33.2\nhyperbatonPaLM 2-L100.0 / 96.0 / 96.878.0 / 75.0 / 75.674.0 / 72.5 / 72.888.0 / 89.0 / 88.8\nlogical_deduction_seven_objectsPaLM 2-L74.0 / 57.0 / 60.446.0 / 37.0 / 38.834.0 / 30.5 / 31.246.0 / 45.5 / 45.6\nmovie_recommendationPaLM 2-L92.0 / 90.5 / 90.862.0 / 52.5 / 54.452.0 / 48.0 / 48.880.0 / 83.0 / 82.4\nmultistep_arithmetic_twoPaLM 2-L72.0 / 55.5 / 58.842.0 / 46.0 / 45.260.0 / 50.5 / 52.44.0 / 3.5 / 3.6\nnavigatePaLM 2-L92.0 / 75.0 / 78.468.0 / 62.0 / 63.270.0 / 64.0 / 65.238.0 / 37.5 / 37.6\nobject_countingPaLM 2-L84.0 / 86.5 / 86.036.0 / 46.5 / 44.460.0 / 62.0 / 61.628.0 / 27.0 / 27.2\npenguins_in_a_tablePaLM 2-L86.2 / 71.8 / 74.779.3 / 64.1 / 67.162.1 / 58.1 / 58.972.4 / 69.2 / 69.9\nreasoning_about_colored_objectsPaLM 2-L98.0 / 85.5 / 88.082.0 / 79.5 / 80.082.0 / 75.0 / 76.442.0 / 35.0 / 36.4\nruin_namesPaLM 2-L88.0 / 88.0 / 88.070.0 / 55.0 / 58.080.0 / 75.5 / 76.488.0 / 76.5 / 78.8\nsalient_translation_error_detectionPaLM 2-L62.0 / 67.0 / 66.042.0 / 50.0 / 48.458.0 / 46.0 / 48.456.0 / 56.5 / 56.4\nsnarksPaLM 2-L85.7 / 83.2 / 83.760.0 / 62.2 / 61.854.3 / 53.1 / 53.451.4 / 60.1 / 58.4\nsports_understandingPaLM 2-L98.0 / 88.0 / 90.050.0 / 46.5 / 47.260.0 / 52.5 / 54.052.0 / 41.5 / 43.6\ntemporal_sequencesPaLM 2-L100.0 / 100.0 / 100.0100.0 / 96.0 / 96.890.0 / 87.0 / 87.6100.0 / 99.5 / 99.6\ntracking_shuffled_objects_seven_objectsPaLM 2-L32.0 / 16.5 / 19.658.0 / 61.5 / 60.854.0 / 55.5 / 55.214.0 / 23.5 / 21.6\nweb_of_liesPaLM 2-L62.0 / 52.0 / 54.046.0 / 41.5 / 42.424.0 / 31.0 / 29.654.0 / 54.0 / 54.0\nword_sortingPaLM 2-L54.0 / 54.5 / 54.42.0 / 4.5 / 4.012.0 / 9.5 / 10.020.0 / 22.5 / 22.0\nboolean_expressionstext-bison98.0 / 87.0 / 89.272.0 / 61.5 / 63.688.0 / 78.0 / 80.080.0 / 68.5 / 70.8\ncausal_judgementtext-bison78.4 / 58.0 / 62.070.3 / 50.7 / 54.573.0 / 55.3 / 58.878.4 / 58.0 / 62.0\ndate_understandingtext-bison60.0 / 50.0 / 52.044.0 / 45.5 / 45.248.0 / 45.0 / 45.644.0 / 45.0 / 44.8\ndisambiguation_qatext-bison68.0 / 73.0 / 72.04.0 / 6.0 / 5.64.0 / 15.5 / 13.252.0 / 68.5 / 65.2\ndyck_languagestext-bison100.0 / 100.0 / 100.0100.0 / 95.5 / 96.4100.0 / 94.5 / 95.6100.0 / 98.5 / 98.8\nformal_fallaciestext-bison70.0 / 53.0 / 56.464.0 / 54.5 / 56.484.0 / 82.5 / 82.870.0 / 54.5 / 57.6\ngeometric_shapestext-bison40.0 / 19.5 / 23.622.0 / 13.0 / 14.818.0 / 12.0 / 13.220.0 / 14.5 / 15.6\nhyperbatontext-bison80.0 / 79.5 / 79.664.0 / 67.5 / 66.864.0 / 69.0 / 68.064.0 / 64.0 / 64.0\nlogical_deduction_seven_objectstext-bison66.0 / 53.5 / 56.056.0 / 58.0 / 57.656.0 / 56.0 / 56.058.0 / 56.5 / 56.8\nmovie_recommendationtext-bison98.0 / 90.0 / 91.668.0 / 63.0 / 64.066.0 / 62.0 / 62.868.0 / 64.0 / 64.8\nmultistep_arithmetic_twotext-bison32.0 / 16.5 / 19.612.0 / 18.0 / 16.818.0 / 17.5 / 17.616.0 / 18.5 / 18.0\nnavigatetext-bison72.0 / 61.0 / 63.256.0 / 55.0 / 55.260.0 / 56.5 / 57.256.0 / 57.0 / 56.8\nobject_countingtext-bison72.0 / 62.0 / 64.058.0 / 57.0 / 57.262.0 / 55.5 / 56.850.0 / 57.0 / 55.6\npenguins_in_a_tabletext-bison72.4 / 56.4 / 59.658.6 / 53.0 / 54.155.2 / 55.6 / 55.558.6 / 53.0 / 54.1\nreasoning_about_colored_objectstext-bison82.0 / 77.0 / 78.076.0 / 72.5 / 73.278.0 / 73.0 / 74.074.0 / 69.5 / 70.4\nruin_namestext-bison88.0 / 82.5 / 83.666.0 / 65.5 / 65.666.0 / 62.5 / 63.264.0 / 66.0 / 65.6\nsalient_translation _error_detectiontext-bison46.0 / 50.5 / 49.642.0 / 47.5 / 46.442.0 / 49.5 / 48.044.0 / 50.0 / 48.8\nsnarkstext-bison80.0 / 81.8 / 81.568.6 / 77.6 / 75.871.4 / 76.2 / 75.377.1 / 84.6 / 73.1\nsports_understandingtext-bison94.0 / 82.5 / 84.886.0 / 79.0 / 80.490.0 / 81.0 / 82.838.0 / 44.5 / 43.2\ntemporal_sequencestext-bison78.0 / 81.0 / 80.436.0 / 43.5 / 42.032.0 / 45.0 / 42.436.0 / 43.0 / 41.6\ntracking_shuffled_objects_seven_objectstext-bison32.0 / 15.5 / 18.810.0 / 17.0 / 15.610.0 / 18.0 / 16.412.0 / 15.5 / 14.8\nweb_of_liestext-bison62.0 / 50.0 / 52.448.0 / 45.5 / 46.048.0 / 44.0 / 44.852.0 / 51.5 / 51.2\nword_sortingtext-bison24.0 / 17.5 / 18.810.0 / 12.0 / 11.64.0 / 8.0 / 7.24.0 / 7.5 / 6.8\n32",
    "Large Language Models as Optimizers\nTable 8: BBH task-wise instructions found by prompt optimization with thePaLM 2-Lscorer and\nthePaLM 2-L-IToptimizer. The optimization starts from the empty string.\nTaskOur Instruction\nboolean_expressionsA Boolean expression is a well-formed expression consisting of variables, values, and logical operators. The expression\nmust evaluate to a single True or False value. The order of precedence of the logical operators is as follows: NOT, AND,\nOR, XOR, IMP. Parentheses can be used to group subexpressions and to control the order of evaluation.\ncausal_judgement\nWhen considering questions about causation, a typical person would consider the following factors: whether the action\nor event was a necessary condition for the outcome to occur, a sufficient condition, a proximate cause, or a foreseeable\ncause.\ndate_understandingTo find the date X time ago from today, first find today’s date. Then subtract X time from today’s date. If the current\ndate is the last day of a month, then the date a month ago is the last day of the previous month. If the current date is not\nthe last day of a month, then the date a month ago is the same day of the previous month. For example, if today is\nMarch 31, 2023, then the date a month ago is February 28, 2023. If today is April 1, 2023, then the date a month ago is\nMarch 1, 2023.\ndisambiguation_qaIdentifying Antecedents of Pronouns: A Comprehensive Guide\ndyck_languagesFirst, look for the opening parentheses. Then, count the number of opening parentheses. Finally, close the parentheses\nin the reverse order that they were opened.\nformal_fallaciesA deductive argument is one where the conclusion follows necessarily from the premises. If the premises are true, then\nthe conclusion must also be true. An invalid argument is one where it is possible for the premises to be true and the\nconclusion to be false.\ngeometric_shapesA closed polygonal chain is a series of connected line segments. The line segments can be straight or curved. The first\nand last line segments are connected. The line segments do not intersect each other except at their endpoints. A closed\npolygon can be described by an SVG path element, which starts at a given point, goes to one or more additional points,\nand then ends at the starting point. The path element can consist of straight line segments, curved segments, or a\nmixture of both.\nhyperbatonThe correct adjective order in English is opinion, size, shape, age, color, origin, material, and purpose. If you have more\nthan one adjective of the same type, they are usually placed in order of importance. For example, you would say \"a\nlarge, old, Pakistani ship\" rather than \"an old, large, Pakistani ship.\" There are a few exceptions to these rules, but they\nare generally followed in most cases.\nlogical_deduction\n_seven_objects\nThe following questions will test your ability to use deductive reasoning. You will be given a set of statements about a\ngroup of objects. You will then be asked to answer questions about the objects based on the statements. The statements\nin the questions are logically consistent, so you can use them to deduce the order of the objects. For each question, you\nmust choose the option that is logically consistent with the information in the questions.\nmovie_recommendationBased on your input, I have analyzed the given movies in terms of genre, plot, tone, audience rating, year of release,\ndirector, cast, and reviews. I have also taken into account the given options. The movie that is most similar to the given\nmovies in terms of all these factors is:\nmultistep_arithmetic\n_two\nThe order of operations in mathematics is PEMDAS, which stands for Parentheses, Exponents, Multiplication, Division,\nAddition, and Subtraction. When there are multiple operations of the same precedence, they must be performed from\nleft to right. Note that multiplication and division have the same precedence, as do addition and subtraction.\nnavigateYou will return to the starting point if and only if (1) the total number of steps you take forward is equal to the total\nnumber of steps you take back, and (2) the total number of turns you make is a multiple of 180 degrees.\nobject_countingHere is a list of the objects you mentioned and their corresponding counts:\npenguins_in_a_tableHere is my new text:\nreasoning_about\n_colored_objects\nStarting from the leftmost object in the row, I observe the following objects arranged in this order:\nruin_namesWhich is the funniest pun on the artist or movie name?\nsalient_translation\n_error_detection\nInstructions: Read the German sentence and its English translation carefully, then identify the type of error in the\ntranslation and select the correct option. There are six possible types of errors: Named Entities, Numerical Values,\nModifiers or Adjectives, Negation or Antonyms, Facts, and Dropped Content.\nsnarks\nIdentify the sarcastic statement by considering the following factors: incongruity, exaggeration, understatement, context,\nspeaker’s intent, and audience’s reaction. I will also consider the speaker’s tone of voice, facial expressions, and body\nlanguage.\nsports_understandingI will determine if a sentence about an athlete is plausible by first checking if it is grammatically correct. If it is, I will\nthen check if it is consistent with the athlete’s sport, position, and real-world statistics. I will also check if it is consistent\nwith the rules of the athlete’s sport. If the sentence is consistent with all of these things, I will answer \"yes\", otherwise I\nwill answer \"no\".\ntemporal_sequencesThe answer is the time that is not mentioned in the given statements.\ntracking_shuffled_objects\n_seven_objects\nClaire has the blue ball, Gertrude has the black ball, and Dave has the green ball. They are all happy with their new\nballs.\nweb_of_liesThe answer to a question is yes if there are an odd number of liars before the current speaker, and no if there are an even\nnumber of liars before the current speaker. If the current speaker is a truth-teller, they will say the opposite of what the\nprevious person said, while a liar will say the same thing as the previous person said.\nword_sortingAlphabetical order of given words:\n33",
    "Large Language Models as Optimizers\nTable 9: BBH task-wise instructions found by prompt optimization with thetext-bisonscorer\nand thePaLM 2-L-IToptimizer. The optimization starts from the empty string.\nTaskOur Instruction\nboolean_expressionsNot (not False) and not not False is False\ncausal_judgementA typical person would likely answer the questions about causation as follows:\ndate_understandingToday is February 28, 2023. It is a Tuesday. Yesterday was Monday, February 27, 2023. Tomorrow will be Wednesday,\nMarch 1, 2023. A week ago, it was February 21, 2023, and a month ago, it was January 28, 2023. A year from now, it\nwill be February 28, 2024. The day of the week is important to note because it will help us to correctly answer the\nquestions below. Not all years are leap years that contain February 29.\ndisambiguation_qaA pronoun is a word that stands in for a noun. The noun that a pronoun refers to is called its antecedent. To identify the\nantecedent of a pronoun, look for the noun that the pronoun could be referring to. If there is only one possible noun,\nthen that is the antecedent. If there are two or more possible nouns, then the antecedent is ambiguous. Use the context\nof the sentence to help you determine the correct antecedent.\ndyck_languages{ }\nformal_fallaciesHow to Evaluate Deductive Validity of an Argument\ngeometric_shapesWhat shape is this SVG code drawing, and how many sides does it have?\nhyperbatonIn English, adjectives are typically placed before nouns in a specific order. The order is: opinion, size, shape, age, color,\norigin, material, purpose, noun. For example, the sentence \"the big, old, red barn\" would be considered grammatically\ncorrect, while the sentence \"the old, big, red barn\" would not. Adjectives that come before nouns are called attributive\nadjectives, while adjectives that come after nouns are called predicative adjectives.\nlogical_deduction\n_seven_objects\nIn this logical reasoning task, you will be given a series of paragraphs, each of which describes a set of objects arranged\nin a fixed order. The statements in each paragraph are logically consistent. You must read each paragraph carefully and\nuse the information given to determine the logical relationships between the objects. You will then be asked a question\nabout the order of the objects. Read each question carefully and choose the option that answers the question correctly.\nmovie_recommendationWhat is the highest-rated movie similar to the given movies, with a similar IMDb rating and released in the same year?\nmultistep_arithmetic_twoLet’s solve these equations using PEMDAS order of operations. Remember that PEMDAS stands for parentheses,\nexponents, multiplication and division, and addition and subtraction.\nnavigateStarting at the origin, facing north, follow the instructions. If your displacement from the origin is zero and your\ndirection is unchanged, then your answer is Yes. Otherwise, your answer is No.\nobject_counting\nLet me help you count the items you have. Just list them one by one, separated by commas. I will then count each item\nand tell you how many items there are in total.\npenguins_in_a_tableThis table shows information about penguins. The columns show the penguin’s name, age, height (in cm), and weight\n(in kg). The penguins are listed in order of their age, from youngest to oldest.\nreasoning_about\n_colored_objects\nFirst, read the input carefully. Then, identify all the objects mentioned, their colors, and their positions. Next, visualize\nthe objects and their positions in your mind. Finally, answer the questions accurately based on the information given.\nMake sure to pay attention to the order of the objects.\nruin_namesA humorous edit of an artist or movie name can be created by replacing one or more letters to form a new word or\nphrase that sounds similar but has a different meaning. The new word or phrase should be relevant to the original word,\nbut it should also be a surprise, which makes the edit funny. For example, the artist or movie name \"Rocky\" can be\nchanged to \"Ricky,\" and \"Schindler’s List\" can be changed to \"Schindler’s Lift.\" Be creative and have fun!\nsalient_translation\n_error_detection\nThe following translations from German to English contain a particular error. The error may be one of the following\ntypes: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, or Dropped Content.\nPlease identify the error.\nsnarksThe statement\nsports_understandingTo determine the plausibility of a sports sentence, I will first identify the sport, athletes, teams, and events mentioned in\nthe sentence. Then, I will use my knowledge of the rules of the sport, the context of the sentence, common sense, and\nmy knowledge of the world to determine whether the sentence is plausible. I will also consider the time period and\nlocation, as well as any other relevant information. Finally, I will return a score of 1 for plausible sentences and 0 for\nimplausible ones.\ntemporal_sequencesTo determine the time period when a person went to a place, first identify all the time periods when the person’s\nwhereabouts are unknown. Then, rule out any time periods during which the person was seen doing something else or\nthe place was closed. The remaining time periods are the possible times when the person could have gone to the place.\ntracking_shuffled_objects\n_seven_objects\nAt the start of the game, Claire has a blue ball. Throughout the game, pairs of people swap balls. Claire ends up with\nthe yellow ball.\nweb_of_liesPeople in a group either tell the truth or lie. The truthfulness of a person’s statement is determined by the statement of\nthe previous person. If the previous person told the truth, then the current person who says the opposite is lying. If the\nprevious person lied, then the current person who says the opposite is telling the truth. This rule applies to all\nsubsequent statements.\nword_sortingSort the following words alphabetically, ignoring case and punctuation. Print the sorted list.\n34",
    "Large Language Models as Optimizers\nE.2GPT-3.5-TURBOAS OPTIMIZER,OPTIMIZATION STARTING FROM  THE EMPTY  STRING\nTable 11, 12 and 13 show the instructions found by prompt optimization. Their accuracies are listed\nin Table 10. Figure 25 visualizes the difference between their accuracies and those of the baselines\n“Let’s think step by step.” and the empty string. The optimizations find instructions better than the\nempty starting point, and most of the found instructions are better than “Let’s think step by step”.\nOne caveat in the A_begin instructions (Table 11) is that a lot of the found instructions are imperative\nor interrogative sentences that are more suitable to be put into “Q:” rather than “A:”, like “Solve\nthe sequence by properly closing the parentheses.” for dyck_languages and “Which movie option\nfrom the given choices ...?” for movie_recommendation. Such styles appear more often here than the\nPaLM 2-L-IToptimizer results (Table 8), showingPaLM 2-L-ITunderstands the needed style\nbetter. In Section E.3, we show the A_begin optimization results with the non-empty starting point\n“Let’s solve the problem.”. Most results there are declarative sentences – more suitable for A_begin.\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n-20\n0\n20\n40\naccuracy difference\n(a)PaLM 2-L, ours minus “Let’s think step by step.”\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\naccuracy difference\n(b)PaLM 2-L, ours minus empty starting point\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\n60\naccuracy difference\n(c)text-bison, ours minus “Let’s think step by step.”\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\naccuracy difference\n(d)text-bison, ours minus empty starting point\nFigure 25:  On 23 BBH tasks, the accuracy differences among instructions found by prompt opti-\nmization (with thegpt-3.5-turbooptimizer), “Let’s think step by step.”, and the empty string\n(optimization starting point).\n35",
    "Large Language Models as Optimizers\nTable 10: Accuracies on BBH tasks with thegpt-3.5-turbooptimizer that starts from the empty\nstring. ThePaLM 2-Lscores are from A_begin (left) instructions; thetext-bisonscores include\nQ_begin (left) and Q_end (right) instructions.\nTaskScorer\nOur Acc (begin)Our Acc (end)\ntraining / test / overalltraining / test / overall\nboolean_expressionsPaLM 2-L92.0 / 86.5 / 87.6N/A\ncausal_judgementPaLM 2-L81.1 / 58.7 / 63.1N/A\ndate_understandingPaLM 2-L86.0 / 82.0 / 82.8N/A\ndisambiguation_qaPaLM 2-L80.0 / 74.0 / 75.2N/A\ndyck_languagesPaLM 2-L100.0 / 100.0 / 100.0N/A\nformal_fallaciesPaLM 2-L88.0 / 63.5 / 68.4N/A\ngeometric_shapesPaLM 2-L60.0 / 41.0 / 44.8N/A\nhyperbatonPaLM 2-L88.0 / 93.0 / 92.0N/A\nlogical_deduction_seven_objectsPaLM 2-L76.0 / 56.5 / 60.4N/A\nmovie_recommendationPaLM 2-L84.0 / 86.0 / 85.6N/A\nmultistep_arithmetic_twoPaLM 2-L52.0 / 49.0 / 49.6N/A\nnavigatePaLM 2-L76.0 / 67.0 / 68.8N/A\nobject_countingPaLM 2-L78.0 / 79.0 / 78.8N/A\npenguins_in_a_tablePaLM 2-L82.8 / 72.6 / 74.7N/A\nreasoning_about _colored_objectsPaLM 2-L86.0 / 67.5 / 71.2N/A\nruin_namesPaLM 2-L90.0 / 83.0 / 84.4N/A\nsalient_translation_error_detectionPaLM 2-L62.0 / 65.0 / 64.4N/A\nsnarksPaLM 2-L85.7 / 70.6 / 73.6N/A\nsports_understandingPaLM 2-L68.0 / 57.5 / 59.6N/A\ntemporal_sequencesPaLM 2-L100.0 / 99.5 / 99.6N/A\ntracking_shuffled_objects_seven_objectsPaLM 2-L44.0 / 34.5 / 36.4N/A\nweb_of_liesPaLM 2-L92.0 / 91.0 / 91.2N/A\nword_sortingPaLM 2-L62.0 / 52.0 / 54.0N/A\nboolean_expressionstext-bison84.0 / 78.5 / 79.680.0 / 78.0 / 78.4\ncausal_judgementtext-bison78.4 / 57.3 / 61.583.8 / 53.3 / 59.4\ndate_understandingtext-bison52.0 / 45.0 / 46.464.0 / 52.4 / 54.8\ndisambiguation_qatext-bison68.0 / 75.5 / 74.064.0 / 71.5 / 70.0\ndyck_languagestext-bison100.0 / 99.5 / 99.6100.0 / 100.0 / 100.0\nformal_fallaciestext-bison70.0 / 54.5 / 57.674.0 / 53.5 / 57.6\ngeometric_shapestext-bison28.0 / 15.0 / 17.648.0 / 28.0 / 32.0\nhyperbatontext-bison86.0 / 85.0 / 85.280.0 / 76.5 / 77.2\nlogical_deduction_seven_objectstext-bison66.0 / 57.5 / 59.262.0 / 55.0 / 56.4\nmovie_recommendationtext-bison76.0 / 69.5 / 70.882.0 / 70.5 / 72.8\nmultistep_arithmetic_twotext-bison28.0 / 20.5 / 22.028.0 / 22.5 / 23.6\nnavigatetext-bison72.0 / 61.0 / 63.268.0 / 59.5 / 61.2\nobject_countingtext-bison68.0 / 71.0 / 70.472.0 / 69.0 / 69.6\npenguins_in_a_tabletext-bison65.5 / 59.8 / 61.079.3 / 53.0 / 58.2\nreasoning_about_colored_objectstext-bison84.0 / 76.5 / 78.086.0 / 74.0 / 76.4\nruin_namestext-bison80.0 / 74.0 / 75.274.0 / 75.0 / 74.8\nsalient_translation_error_detectiontext-bison44.0 / 50.5 / 49.248.0 / 51.0 / 50.4\nsnarkstext-bison82.9 / 79.7 / 80.388.6 / 84.6 / 85.4\nsports_understandingtext-bison84.0 / 76.5 / 78.090.0 / 80.0 / 82.0\ntemporal_sequencestext-bison50.0 / 54.5 / 53.664.0 / 61.5 / 62.0\ntracking_shuffled_objects_seven_objectstext-bison22.0 / 18.5 / 19.230.0 / 21.5 / 23.2\nweb_of_liestext-bison64.0 / 57.5 / 58.868.0 / 55.0 / 57.6\nword_sortingtext-bison26.0 / 19.0 / 20.432.0 / 25.5 / 26.8\n36",
    "Large Language Models as Optimizers\nTable 11: BBH task-wise instructions found by prompt optimization with thePaLM 2-Lscorer and\nthegpt-3.5-turbooptimizer. The optimizations start from the empty string.\nTaskOur Instruction\nboolean_expressionsAn accurate evaluation of logical expressions involves correctly applying Boolean operators, considering the order of\noperations, and analyzing the truth values of the operands in accordance with Boolean logic principles.\ncausal_judgementUnderstanding causality is critical for accurately assessing cause and effect relationships in various scenarios, leading to\nwell-informed judgments, precise conclusions, and definitive answers to questions about the outcomes involved.\ndate_understandingWhat is the specific date mentioned or required in each given problem or question, taking into account all relevant\ninformation, available options, and the provided context? Please provide the accurate answer in the format\nMM/DD/YYYY.\ndisambiguation_qaAccurately analyze and clarify the pronoun-antecedent relationship in the given sentences, identifying the appropriate\nreferent to eliminate any potential confusion or ambiguity and ensure a precise understanding of the intended meaning.\ndyck_languagesSolve the sequence by properly closing the parentheses.\nformal_fallaciesIn determining the deductive validity of arguments based on explicit premises, a meticulous analysis of the logical\nrelationships and implications is essential for definitively establishing their soundness, confirming their validity or\ninvalidity, and ensuring a reliable and robust assessment of the arguments at hand.\ngeometric_shapesThe SVG path element with the \"d\" attribute plays a crucial role in web development, allowing for the precise definition\nand rendering of various shapes on a webpage.\nhyperbatonUnderstanding the correct order of adjectives is crucial for constructing grammatically accurate and coherent sentences\nthat effectively convey the intended meaning in diverse contexts while ensuring clarity, cohesion, and consistency\nthroughout consistently and effortlessly.\nlogical_deduction\n_seven_objects\nBy conducting a meticulous analysis of the given information and ensuring logical consistency within each paragraph,\nwe can accurately determine the precise order or ranking of the mentioned objects, allowing us to confidently and\nconsistently identify the correct answer in every presented scenario with utmost precision and confidence.\nmovie_recommendationWhich movie option from the given choices closely matches the mentioned films in terms of themes, storylines, and\ncharacteristics, guaranteeing the highest possible similarity score among them all?\nmultistep_arithmetic_twoEvaluate the given mathematical expressions step by step to determine the correct solutions accurately.\nnavigateIs it possible to determine, with absolute certainty, whether strictly adhering to the given instructions will unfailingly\nbring you back to the original starting point without any exceptions, errors, or deviations?\nobject_countingDetermine the total number of objects or entities mentioned in the given list, covering various categories and types, to\naccurately calculate the overall count.\npenguins_in_a_tableFrom the given table, what information can we gather about the mentioned animals and their respective attributes,\nincluding names, ages, heights, and weights?\nreasoning_about\n_colored_objects\nBy thoroughly examining the given information, accurately determine the answers for each question by considering the\nspecific characteristics, colors, and positions of the mentioned objects.\nruin_namesSelect the most amusing and clever alteration from the options provided for the given artist, movie, or title name, and\naccurately choose the correct answer to test your wit and creativity.\nsalient_translation\n_error_detection\nThoroughly examine the given translations from German to English and accurately identify any errors by carefully\nanalyzing the text and selecting the appropriate option with meticulous attention to detail, precision, utmost accuracy,\nand comprehensive understanding of the language for precise evaluation and categorization.\nsnarksWhich option delivers the most devastatingly sarcastic response, brilliantly exposing the sheer absurdity and leaving\nabsolutely no doubt whatsoever in all the given situations?\nsports_understandingMaintaining the accuracy, reliability, and integrity of sports event representation is essential for upholding the highest\nstandards of credibility, trustworthiness, and overall quality in conveying information, without any compromise,\nmisrepresentation, or distortion, thereby ensuring the factual accuracy of sports journalism.\ntemporal_sequences\nBased on the provided timeline and observed activities, we can accurately determine the possible time range when each\nindividual could have visited their intended destinations and answer questions about their visitation time.\ntracking_shuffled_objects\n_seven_objects\nAn important point to note is that each person in the group starts with one specific book at the beginning of the semester.\nweb_of_liesAnalyzing the consistency and accuracy of statements provided by each person is crucial for determining the\ntruthfulness of individuals in every scenario.\nword_sortingPlease sort the given words in alphabetical order: The list of words to be sorted contains -\n37",
    "Large Language Models as Optimizers\nTable 12: BBH task-wise Q_begin instructions found by prompt optimization with thetext-bison\nscorer and thegpt-3.5-turbooptimizer. The optimizations start from the empty string.\nTaskOur Instruction\nboolean_expressionsGroup sub-expressions with parentheses to accurately evaluate logical operations: not, and, and finally or. Determine\nthe resulting value as either True or False.\ncausal_judgementConsider the intentions and actions of the individuals involved.\ndate_understandingDetermine the one-day difference in the given date and express it in the format MM/DD/YYYY.\ndisambiguation_qaDetermine the precise antecedent of the pronoun in the given sentence and select the correct option or state if it is\nambiguous.\ndyck_languagesEnsure that all opening brackets have a corresponding closing bracket, and that the closing brackets are in the correct\norder.\nformal_fallacies\nThoroughly analyze the explicitly provided premises and determine the deductive validity of the argument based on all\nnecessary conditions, implications, exclusions, and dependencies given.\ngeometric_shapesAnalyze the given SVG path element carefully and confidently select the correct option from the provided choices to\naccurately determine the corresponding shape. Pay close attention to the specific path details and confidently make the\nmost suitable choice.\nhyperbatonSelect the sentence that strictly adheres to the standard order of adjectives: opinion, size, age, shape, color, origin,\nmaterial, and purpose. Ensure there are no deviations or alterations in the adjective order. Choose the option without any\nchanges.\nlogical_deduction\n_seven_objects\nAnalyze the given information to accurately determine the precise order and ranking of the mentioned objects/people,\nconsidering their relationships, positions, and any provided comparisons, for a definitive and logical progression with\nmaximum accuracy and efficiency.\nmovie_recommendationBased on the movie list provided, carefully consider your preferences and make a well-informed decision.\nmultistep_arithmetic_twoFirst, simplify any expressions within parentheses following the correct order of operations to accurately evaluate the\nfinal answer with efficiency and precision.\nnavigateAlways face forward. Take 10 steps forward. Turn left. Take 5 steps forward. Take 3 steps backward. Finally, take 7\nsteps forward. Turn around and take 1 step forward. Repeat the previous sequence three times. Follow the given path\nprecisely without any deviations. At the end, turn right and take 11 steps forward. If you follow these instructions, will\nyou return to the starting point? Options: - Yes - No\nobject_countingDetermine the total count of mentioned vegetables accurately and state the final count as the answer.\npenguins_in_a_tableAnalyze the given table to accurately determine the required information based on the provided criteria and attributes of\nthe penguins and giraffes. Utilize efficient problem-solving strategies to arrive at the correct answer.\nreasoning_about\n_colored_objects\nState the color of the object mentioned in the given arrangement with utmost accuracy.\nruin_namesChoose the option that offers the most clever and humorous alteration of the given artist or movie name. Let your\ncreativity shine and select the answer that will undoubtedly bring a smile to your face! Make sure to think outside the\nbox!\nsalient_translation\n_error_detection\nAnalyze the translation and accurately identify the specific error type based on the source text, providing the most\nappropriate corresponding option.\nsnarksChoose the option that wickedly embodies sarcasm.\nsports_understandingDetermine the plausibility of the given statement by evaluating factual accuracy, logical consistency, and contextual\nrelevance, then provide a succinct and well-justified response.\ntemporal_sequencesIdentify the optimal time slot for the individual to engage in the mentioned location/activity considering the given\nsightings and waking up time, taking into account the opening and closing times of the location and the duration of each\nevent.\ntracking_shuffled_objects\n_seven_objects\nPay attention to the given information and track the swaps/exchanges carefully to accurately determine the final\npossession/position/outcome for the specified individual.\nweb_of_liesTo determine the truthfulness of the last person mentioned, analyze the consistency of each statement and count the\nnumber of individuals accusing the previous person of lying. If the count of accusers is even, that person tells the truth;\nif it is odd, that person lies.\nword_sortingAlphabetically sort the given list of words, ensuring all words are included and in ascending order.\n38",
    "Large Language Models as Optimizers\nTable 13: BBH task-wise Q_end instructions found by prompt optimization with thetext-bison\nscorer and thegpt-3.5-turbooptimizer. The optimizations start from the empty string.\nTaskOur Instruction\nboolean_expressionsAccurately use order of operations and parentheses to evaluate logical expressions and determine truth values efficiently.\ncausal_judgementConsider all relevant factors, prioritize overall well-being and ethical considerations, make well-informed decisions\nwhile foreseeing potential consequences efficiently, and consistently strive for optimal outcomes with empathy and\nadaptability in a thoughtful and comprehensive manner.\ndate_understandingSubtract the specified number of days from the given date and format the outcome as MM/DD/YYYY to accurately\ndetermine the desired result in an efficient manner.\ndisambiguation_qaClearly identify and select the unambiguous antecedent for the pronoun or designate it as \"Ambiguous\" if it is unclear.\ndyck_languagesAdd the missing closing parentheses.\nformal_fallacies\nDetermine the deductive validity of the argument presented based on the explicitly stated premises and reach a definitive\nconclusion.\ngeometric_shapesAnalyzing the given SVG path element, accurately determine its shape by closely examining its curves and coordinates,\nthen select the correct option.\nhyperbatonChoose the option with the correct adjective order in each sentence, prioritizing specific attributes like size, color, and\norigin. Place the most specific adjective before the more general ones for precise and standardized ordering across all\nexamples. Ensure accurate alignment of the adjectives based on their respective attributes for consistent and\nstandardized ordering.\nlogical_deduction\n_seven_objects\nDetermine the precise order of the given objects/participants based on the provided information and establish the final\nranking accurately, considering all relevant factors, while maintaining logical consistency with maximum efficiency.\nmovie_recommendationChoose the most similar option from the choices provided that closely aligns with the given movies’ themes, genres, and\nimpact for the most accurate recommendation possible. Make your selection wisely.\nmultistep_arithmetic_twoCarefully follow the order of operations to precisely simplify the expressions within parentheses and efficiently find the\naccurate final answer.\nnavigateAlways face forward. Take 10 steps forward. Turn right and walk for 5 steps. Then, make a left turn and continue for 9\nsteps. Proceed by walking 6 steps backward. Finally, turn around and take 200 steps. Accurately track your movements,\ndiligently adhere to the given path, and ensure to return to the starting point without any deviations or obstacles.\nobject_countingDetermine the total count of items mentioned, including all listed items, using an efficient and concise method. State the\nfinal count as your answer.\npenguins_in_a_table\nIdentify the animal with the maximum measurement (weight, age, or height) in the table and state its name and species.\nreasoning_about\n_colored_objects\nDetermine the color of each item in the given scenario and select the correct color option from the provided choices for\naccurate responses, ensuring utmost precision and completeness.\nruin_namesChoose the option that creatively and hilariously transforms the given artist or movie name.\nsalient_translation\n_error_detection\nCarefully analyze the translations and select the most suitable option from the given choices to rectify the specific error\ncategory, ensuring complete precision, accuracy, and faithful representation of the intended meaning, while considering\nall relevant information in the source text.\nsnarksChoose the option that cleverly employs sarcasm to defy all expectations and leave everyone utterly dumbfounded,\nquestioning the very essence of their own perception.\nsports_understanding\nEvaluate the plausibility of each given statement and provide a well-supported justification based on logical reasoning,\ncontextual understanding, and relevant evidence to arrive at a definitive and conclusive answer.\ntemporal_sequencesIdentify the possible time slot for the desired activity based on the given information and sightings, then select the\ncorrect option.\ntracking_shuffled_objects\n_seven_objects\nThoroughly analyze the given scenarios, systematically consider all available information, and confidently determine\nthe final outcome with exceptional precision and optimal efficiency, while maintaining a strategic and logical approach\nthroughout the process.\nweb_of_liesExamine each person’s statements meticulously to accurately determine the truth and confidently identify who is telling\nthe truth, enabling you to effectively solve the given problem.\nword_sortingSort the given words alphabetically using spaces as separators while maintaining their original order and including all\nwords.\n39",
    "Large Language Models as Optimizers\nE.3PALM 2-LAS SCORER,GPT-3.5-TURBOAS OPTIMIZER,OPTIMIZATION STARTING\nFROM“LET’S SOLVE THE PROBLEM.”\nFigure 26 and Table 14 compare the accuracies of found instructions vs “Let’s solve the problem.”,\n“Let’s think step by step.”, and the instructions in Table 11. Table 15 details the found instructions.\nThe “Let’s” pattern appears more often in the found instructions because of the starting points, and\nthe instructions are more often declarative that are more suitable for A_begin, even if some are\nsemantically far from “Let’s solve the problem”. In fact, “Let’s” was adopted by Zhou et al. (2022b)\nas a fixed pattern in generated prompts, possibly because of the same reason.\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\naccuracy difference\n(a) ours minus “Let’s think step by step.”\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n0\n20\n40\naccuracy difference\n(b)  ours minus “Let’s solve the problem.”  starting\npoint\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\nhyperbaton\nlogical_deduction_seven_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\ntemporal_sequences\ntracking_shuffled_objects_seven_objects\nweb_of_lies\nword_sorting\n-20\n0\n20\naccuracy difference\n(c) ours minus the instructions found with the empty\nstarting point\nFigure 26:  On 23 BBH tasks, the accuracy differences among instructions found by prompt opti-\nmization (with thetext-bisonscorer and thegpt-3.5-turbooptimizer), “Let’s think step by\nstep.”, and “Let’s solve the problem.” (optimization starting point). The found instructions mostly\noutperform the “Let’s think step by step.” baseline, the “Let’s solve the problem.” starting point, and\nthe instructions in Table 11 found by prompt optimization from the empty string.\n40",
    "Large Language Models as Optimizers\nTable 14: Accuracies on BBH tasks with thePaLM 2-Lscorer and thegpt-3.5-turbooptimizer\nthat starts from “Let’s solve the problem”. The scores are from A_begin instructions.\nTaskScorer\nOur Acc“Let’s solve the\nproblem.” Acc\ntraining / test / overalltraining / test / overall\nboolean_expressionsPaLM 2-L98.0 / 89.5 / 91.278.0 / 69.0 / 70.8\ncausal_judgementPaLM 2-L83.8 / 58.7 / 63.662.0 / 61.3 / 61.5\ndate_understandingPaLM 2-L90.0 / 82.0 / 83.674.0 / 71.0 / 71.6\ndisambiguation_qaPaLM 2-L78.0 / 68.0 / 70.052.0 / 54.5 / 54.0\ndyck_languagesPaLM 2-L100.0 / 100.0 / 100.094.0 / 97.0 / 96.4\nformal_fallaciesPaLM 2-L84.0 / 62.0 / 66.468.0 / 54.0 / 56.8\ngeometric_shapesPaLM 2-L62.0 / 42.5 / 46.430.0 / 22.0 / 23.6\nhyperbatonPaLM 2-L94.0 / 91.5 / 92.072.0 / 77.0 / 76.0\nlogical_deduction_seven_objectsPaLM 2-L66.0 / 53.0 / 55.638.0 / 36.5 / 36.8\nmovie_recommendationPaLM 2-L88.0 / 88.0 / 88.066.0 / 76.0 / 74.0\nmultistep_arithmetic_twoPaLM 2-L66.0 / 55.0 / 57.230.0 / 22.0 / 23.6\nnavigatePaLM 2-L76.0 / 67.0 / 68.854.0 / 63.5 / 61.6\nobject_countingPaLM 2-L96.0 / 92.5 / 93.258.0 / 58.0 / 58.0\npenguins_in_a_tablePaLM 2-L86.2 / 70.9 / 74.069.0 / 72.6 / 71.9\nreasoning_about _colored_objectsPaLM 2-L88.0 / 69.0 / 72.878.0 / 69.5 / 71.2\nruin_namesPaLM 2-L92.0 / 85.5 / 86.876.0 / 79.5 / 80.8\nsalient_translation_error_detectionPaLM 2-L66.0 / 67.5 / 67.230.0 / 35.5 / 34.4\nsnarksPaLM 2-L88.6 / 76.9 / 79.280.0 / 70.6 / 72.5\nsports_understandingPaLM 2-L72.0 / 63.5 / 65.260.0 / 50.5 / 52.4\ntemporal_sequencesPaLM 2-L100.0 / 99.5 / 99.696.0 / 92.5 / 93.2\ntracking_shuffled_objects_seven_objectsPaLM 2-L56.0 / 63.5 / 62.042.0 / 51.5 / 49.6\nweb_of_liesPaLM 2-L56.0 / 58.5 / 58.00.0 / 4.0 / 3.2\nword_sortingPaLM 2-L52.0 / 44.5 / 46.018.0 / 20.5 / 20.0\n41",
    "Large Language Models as Optimizers\nTable 15: BBH task-wise Q_begin instructions found by prompt optimization with thePaLM 2-L\nscorer and thegpt-3.5-turbooptimizer. The optimizations start from “Let’s solve the problem”.\nTaskOur Instruction\nboolean_expressionsLet’s accurately assess the given conditions and determine their corresponding Boolean values.\ncausal_judgementLet’s conduct a meticulous evaluation of the given scenarios, accurately determine the causal relationships, and provide\ndefinitive answers through comprehensive analysis, ensuring a precise understanding of causation and a thorough\ndetermination of events in each situation.\ndate_understandingLet’s accurately determine the correct date based on the given information and select the corresponding option in the\nstandard MM/DD/YYYY format with utmost precision and reliability, ensuring the most definitive and reliable solution\npossible for accurate representation in all scenarios without any room for ambiguity, error, or confusion, and providing\nthe highest level of accuracy and reliability.\ndisambiguation_qaLet’s thoroughly analyze the given sentences to accurately determine the unambiguous antecedents of the pronouns\nused, ensuring clear understanding, effective communication, and leaving no room for any confusion or ambiguity.\ndyck_languagesLet’s find the correct closing parentheses and brackets for the given sequences.\nformal_fallaciesLet’s thoroughly analyze the explicitly stated premises and draw definitive conclusions to accurately determine the\ndeductive validity of the arguments provided in each question, employing precise and logical reasoning in our\nassessments for unwavering confidence in our determinations.\ngeometric_shapesLet’s accurately determine the shape represented by the given SVG path element by carefully analyzing its path data\nand considering all available options for a precise identification.\nhyperbatonLet’s quickly identify the correct adjective order.\nlogical_deduction\n_seven_objects\nLet’s methodically analyze the given information, employ logical reasoning, thoroughly evaluate all relevant details, and\naccurately determine the solutions for each problem by considering all provided options comprehensively and\nstrategically, ensuring an efficient and effective approach towards arriving at the correct answers.\nmovie_recommendationLet’s uncover the perfect movie recommendation from the options provided, ensuring an exceptional cinematic\nexperience together as we select the most captivating and satisfying choice that will keep us thoroughly engaged and\nimmersed until the very end.\nmultistep_arithmetic_twoLet’s tackle the following calculations.\nnavigateLet’s accurately and efficiently determine the correct solution for each given scenario, ensuring the highest level of\nprecision, reliability, and consistency throughout.\nobject_countingLet’s determine the total count of various items/objects/ingredients/animals mentioned in order to accurately and\nefficiently find the answer.\npenguins_in_a_tableLet’s analyze the given information and determine the correct answer.\nreasoning_about\n_colored_objects\nLet’s systematically analyze the given information and carefully evaluate each answer choice to confidently determine\nthe accurate and optimal solutions, considering all available options and specific details provided in each question for\nprecise and concise responses, ensuring complete accuracy and clarity in our answers.\nruin_names\nPrepare to have a side-splittingly funny time as we uncover the most clever and hilarious alternatives for these artist or\nmovie names, challenging your wit to guess the correct one with a burst of creativity, humor, and imaginative twists!\nsalient_translation\n_error_detection\nLet’s meticulously analyze the provided translations, accurately identifying any errors or discrepancies, and conduct a\ncomprehensive evaluation to ensure the highest level of translation quality and fidelity. By considering contextual\nnuances, cultural references, linguistic conventions, potential factual errors, and any dropped content, our ultimate aim\nis to achieve precise and thorough assessments for optimal translation accuracy and adherence to the source text.\nsnarksLet’s expertly determine the sarcastic statement among the given options and confidently provide the definitive answer\nwithout any room for doubt or confusion, ensuring absolute precision, clarity, and unwavering expertise in our response,\nwhile carefully analyzing the context, tone, and intention behind each statement to achieve unrivaled accuracy and\nunwavering confidence.\nsports_understandingLet’s find the accurate information.\ntemporal_sequencesThe flawless approach\ntracking_shuffled_objects\n_seven_objects\nBy meticulously analyzing the given scenarios and accurately determining the final outcomes through a series of trades,\nswaps, and exchanges among the individuals involved, let’s ascertain the conclusive results.\nweb_of_liesLet’s scrutinize each statement provided to accurately determine the truth-teller and uncover the veracity behind their\nwords with unwavering analysis.\nword_sortingEmploying efficient and precise measures, sort the given list of words in alphabetical order to provide an optimal\nsolution for any sorting problem, ensuring maximum performance and effectiveness.\n42"
  ]
}