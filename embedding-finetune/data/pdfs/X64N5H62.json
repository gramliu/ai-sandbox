{
  "key": "X64N5H62",
  "url": "http://arxiv.org/pdf/2305.18290",
  "metadata": {
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model",
    "abstract": "  While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.\n",
    "published": "2023-05-29T17:57:46Z"
  },
  "text": [
    "Direct Preference Optimization:\nYour Language Model is Secretly a Reward Model\nRafael Rafailov\n∗†\nArchit Sharma\n∗†\nEric Mitchell\n∗†\nStefano Ermon\n†‡\nChristopher D. Manning\n†\nChelsea Finn\n†\n†\nStanford University\n‡\nCZ Biohub\n{rafailov,architsh,eric.mitchell}@cs.stanford.edu\nAbstract\nWhile large-scale unsupervised language models (LMs) learn broad world knowl-\nedge and some reasoning skills,  achieving precise control of their behavior is\ndifficult due to the completely unsupervised nature of their training.  Existing\nmethods for gaining such steerability collect human labels of the relative quality of\nmodel generations and fine-tune the unsupervised LM to align with these prefer-\nences, often with reinforcement learning from human feedback (RLHF). However,\nRLHF is a complex and often unstable procedure, first fitting a reward model that\nreflects the human preferences, and then fine-tuning the large unsupervised LM\nusing reinforcement learning to maximize this estimated reward without drifting\ntoo far from the original model. In this paper we introduce a new parameterization\nof the reward model in RLHF that enables extraction of the corresponding optimal\npolicy in closed form, allowing us to solve the standard RLHF problem with only a\nsimple classification loss. The resulting algorithm, which we callDirect Prefer-\nence Optimization(DPO), is stable, performant, and computationally lightweight,\neliminating the need for  sampling from the LM during fine-tuning or performing\nsignificant hyperparameter tuning. Our experiments show that DPO can fine-tune\nLMs to align with human preferences as well as or better than existing methods.\nNotably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sen-\ntiment of generations, and matches or improves response quality in summarization\nand single-turn dialogue while being substantially simpler to implement and train.\n1    Introduction\nLarge unsupervised language models (LMs) trained on very large datasets acquire surprising capabili-\nties [11,7,40,8]. However, these models are trained on data generated by humans with a wide variety\nof goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for\nexample, while we may want our AI coding assistant tounderstandcommon programming mistakes\nin order to correct them, nevertheless, when generating code, we would like to bias our model toward\nthe (potentially rare) high-quality coding ability present in its training data. Similarly, we might want\nour language model to beawareof a common misconception believed by 50% of people, but we\ncertainly do not want the model to claim this misconception to be true in 50% of queries about it!\nIn other words, selecting the model’sdesired responses and behaviorfrom its very wideknowledge\nand abilitiesis crucial to building AI systems that are safe, performant, and controllable [26]. While\nexisting methods typically steer LMs to match human preferences using reinforcement learning (RL),\n∗\nEqual contribution; more junior authors listed earlier.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18290v2  [cs.LG]  13 Dec 2023",
    "Figure 1:DPO optimizes for human preferences while avoiding reinforcement learning.Existing methods\nfor fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and\nhuman preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.\nIn contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification\nobjective, fitting animplicitreward model whose corresponding optimal policy can be extracted in closed form.\nwe will show that the RL-based objective used by existing methods can be optimized exactly with a\nsimple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\nAt a high level, existing methods instill the desired behaviors into a language model using curated\nsets of human preferences representing the types of behaviors that humans find safe and helpful. This\npreference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\na large text dataset. While the most straightforward approach to preference learning is supervised\nfine-tuning on human demonstrations of high quality responses, the most successful class of methods\nis reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [12,2]). RLHF methods fit\na reward model to a dataset of human preferences and then use RL to optimize a language model\npolicy to produce responses assigned high reward without drifting excessively far from the original\nmodel. While RLHF produces models with impressive conversational and coding abilities, the RLHF\npipeline is considerably more complex than supervised learning, involving training multiple LMs and\nsampling from the LM policy in the loop of training, incurring significant computational costs.\nIn this paper, we show how to directly optimize a language model to adhere to human preferences,\nwithout explicit reward modeling or reinforcement learning. We proposeDirect Preference Optimiza-\ntion (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms\n(reward maximization with a KL-divergence constraint) but is simple to implement and straight-\nforward to train.  Intuitively, the DPO update increases the relative log probability of preferred to\ndispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents\nthe model degeneration that we find occurs with a naive probability ratio objective. Like existing\nalgorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [5]) that\nmeasures how well a given reward function aligns with empirical preference data. However, while\nexisting methods use the preference model to define a preference loss to train a reward model and\nthen train a policy that optimizes the learned reward model, DPO uses a change of variables to define\nthe preference loss as a function of the policy directly. Given a dataset of human preferences over\nmodel responses, DPO can therefore optimize a policy using a simple binary cross entropy objective,\nproducing the optimal policy to an implicit reward function fit to the preference data.\nOur main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for\ntraining language models from preferences. Our experiments show that DPO is at least as effective\nas existing methods, including PPO-based RLHF, for learning from preferences in tasks such as\nsentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\n2    Related Work\nSelf-supervised language models of increasing scale learn to complete some tasks zero-shot [31] or\nwith few-shot prompts [6,25,11]. However, their performance on downstream tasks and alignment\nwith user intent can be significantly improved by fine-tuning on datasets of instructions and human-\nwritten completions [23,36,13,39]. This ‘instruction-tuning’ procedure enables LLMs to generalize\nto instructions outside of the instruction-tuning set and generally increase their usability [13]. Despite\nthe success of instruction tuning,relativehuman judgments of response quality are often easier to\ncollect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of\nhuman preferences, improving proficiency in translation [18], summarization [38,49], story-telling\n[49], and instruction-following [26,32].  These methods first optimize a neural network reward\nfunction for compatibility with the dataset of preferences under a preference model such as the\n2",
    "Bradley-Terry model [5],  then fine-tune a language model to maximize the given reward using\nreinforcement learning algorithms, commonly REINFORCE [45], proximal policy optimization\n(PPO; [37]), or variants [32]. A closely-related line of work leverages LLMs fine-tuned for instruction\nfollowing with human feedback to generate additional synthetic preference data for targeted attributes\nsuch as safety or harmlessness [2], using only weak supervision from humans in the form of a\ntext rubric for the LLM’s annotations.  These methods represent a convergence of two bodies of\nwork:  one body of work on training language models with reinforcement learning for a variety\nof objectives [33,27,46] and another body of work on general methods for learning from human\npreferences [12,19].  Despite the appeal of using relative human preferences, fine-tuning large\nlanguage models with reinforcement learning remains a major practical challenge; this work provides\na theoretically-justified approach to optimizing relative preferences without RL.\nOutside of the context of language, learning policies from preferences has been studied in both bandit\nand reinforcement learning settings, and several approaches have been proposed. Contextual bandit\nlearning using preferences or rankings of actions, rather than rewards, is known as a contextual\ndueling bandit (CDB; [48,14]). In the absence of absolute rewards, theoretical analysis of CDBs\nsubstitutes the notion of an optimal policy with avon Neumann winner, a policy whose expected win\nrate againstanyother policy is at least 50% [14]. However, in the CDB setting, preference labels\nare given online, while in learning from human preferences, we typically learn from a fixed batch of\noffline preference-annotated action pairs [47]. Similarly,preference-based RL(PbRL) learns from\nbinary preferences generated by anunknown‘scoring’ function rather than rewards [9, 35]. Various\nalgorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally\ninvolve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently\noptimizing it [16,9,12,34,19].  We instead present a single stage policy learning approach that\ndirectly optimizes a policy to satisfy preferences.\n3    Preliminaries\nWe review the RLHF pipeline in Ziegler et al. (and later [38,1,26]). It usually includes three phases:\n1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\nSFT: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality\ndata for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a modelπ\nSFT\n.\nReward Modelling Phase:  In the second phase the SFT model is prompted with promptsxto\nproduce pairs of answers(y\n1\n,y\n2\n)∼π\nSFT\n(y|x).  These are then presented to human labelers\nwho express preferences for one answer, denoted asy\nw\n≻y\nl\n|xwherey\nw\nandy\nl\ndenotes the\npreferred and dispreferred completion amongst(y\n1\n,y\n2\n)respectively. The preferences are assumed\nto be generated by some latent reward modelr\n∗\n(y,x), which we do not have access to. There are a\nnumber of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular\nchoice (although more general Plackett-Luce ranking models [30,21] are also compatible with the\nframework if we have access to several ranked answers). The BT model stipulates that the human\npreference distributionp\n∗\ncan be written as:\np\n∗\n(y\n1\n≻y\n2\n|x) =\nexp (r\n∗\n(x,y\n1\n))\nexp (r\n∗\n(x,y\n1\n)) + exp (r\n∗\n(x,y\n2\n))\n.(1)\nAssuming access to a static dataset of comparisonsD=\n\b\nx\n(i)\n,y\n(i)\nw\n,y\n(i)\nl\n\t\nN\ni=1\nsampled fromp\n∗\n, we\ncan parametrize a reward modelr\nφ\n(x,y)and estimate the parameters via maximum likelihood.\nFraming the problem as a binary classification we have the negative log-likelihood loss:\nL\nR\n(r\nφ\n,D) =−E\n(x,y\nw\n,y\nl\n)∼D\n\u0002\nlogσ(r\nφ\n(x,y\nw\n)−r\nφ\n(x,y\nl\n))\n\u0003\n(2)\nwhereσis the logistic function. In the context of LMs, the networkr\nφ\n(x,y)is often initialized from\nthe SFT modelπ\nSFT\n(y|x)with the addition of a linear layer on top of the final transformer layer\nthat produces a single scalar prediction for the reward value [49]. To ensure a reward function with\nlower variance, prior works normalize the rewards, such thatE\nx,y∼D\n[r\nφ\n(x,y)] = 0for allx.\nRL Fine-Tuning Phase:  During the RL phase,  we use the learned reward function to provide\nfeedback to the language model. In particular, we formulate the following optimization problem\nmax\nπ\nθ\nE\nx∼D,y∼π\nθ\n(y|x)\n\u0002\nr\nφ\n(x,y)\n\u0003\n−βD\nKL\n\u0002\nπ\nθ\n(y|x)||π\nref\n(y|x)\n\u0003\n(3)\n3",
    "whereβis a parameter controlling the deviation from the base reference policyπ\nref\n, namely the ini-\ntial SFT modelπ\nSFT\n.  In practice, the language model policyπ\nθ\nis also initialized toπ\nSFT\n.  The\nadded  constraint  is  important,  as  it  prevents  the  model  from  deviating  too  far  from  the  distri-\nbution on which the reward model is accurate,  as well as maintaining the generation diversity\nand preventing mode-collapse to single high-reward answers.  Due to the discrete nature of lan-\nguage  generation,  this  objective  is  not  differentiable  and  is  typically  optimized  with  reinforce-\nment learning.  The standard approach [49,38,1,26] has been to construct the reward function\nr(x,y) =r\nφ\n(x,y)−β(logπ\nθ\n(y|x)−logπ\nref\n(y|x)), and maximize using PPO [37].\n4    Direct Preference Optimization\nMotivated by the challenges of applying reinforcement learning algorithms on large-scale problems\nsuch as fine-tuning language models, our goal is to derive a simple approach for policy optimization\nusing preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it\nvia RL, our approach leverages a particular choice of reward model parameterization that enables\nextraction of its optimal policy in closed form, without an RL training loop.  As we will describe\nnext in detail, our key insight is to leverage an analytical mapping from reward functions to optimal\npolicies, which enables us to transform a loss function over reward functions into a loss function\nover policies. This change-of-variables approach avoids fitting an explicit, standalone reward model,\nwhile still optimizing under existing models of human preferences, such as the Bradley-Terry model.\nIn essence, the policy network represents both the language model and the (implicit) reward.\nDeriving the DPO objective.We start with the same RL objective as prior work, Eq. 3, under a\ngeneral reward functionr. Following prior work [29,28,17,15], it is straightforward to show that\nthe optimal solution to the KL-constrained reward maximization objective in Eq. 3 takes the form:\nπ\nr\n(y|x) =\n1\nZ(x)\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\n,(4)\nwhereZ(x) =\nP\ny\nπ\nref\n(y|x) exp\n\u0010\n1\nβ\nr(x,y)\n\u0011\nis the partition function.  See Appendix A.1 for a\ncomplete derivation. Even if we use the MLE estimater\nφ\nof the ground-truth reward functionr\n∗\n, it is\nstill expensive to estimate the partition functionZ(x)[17,15], which makes this representation hard\nto utilize in practice. However, we can rearrange Eq. 4 to express the reward function in terms of its\ncorresponding optimal policyπ\nr\n, the reference policyπ\nref\n, and the unknown partition functionZ(·).\nSpecifically, we first take the logarithm of both sides of Eq. 4 and then with some algebra we obtain:\nr(x,y) =βlog\nπ\nr\n(y|x)\nπ\nref\n(y|x)\n+βlogZ(x).(5)\nWe can apply this reparameterization to the ground-truth rewardr\n∗\nand corresponding optimal model\nπ\n∗\n. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two\ncompletions, i.e.,p\n∗\n(y\n1\n≻y\n2\n|x) =σ(r\n∗\n(x,y\n1\n)−r\n∗\n(x,y\n2\n)). Substituting the reparameterization\nin Eq. 5 forr\n∗\n(x,y)\ninto the preference model Eq. 1, the partition function cancels, and we can\nexpress the human preference probability in terms of only the optimal policyπ\n∗\nand reference policy\nπ\nref\n. Thus, the optimal RLHF policyπ\n∗\nunder the Bradley-Terry model satisfies the preference model:\np\n∗\n(y\n1\n≻y\n2\n|x) =\n1\n1 + exp\n\u0010\nβlog\nπ\n∗\n(y\n2\n|x)\nπ\nref\n(y\n2\n|x)\n−βlog\nπ\n∗\n(y\n1\n|x)\nπ\nref\n(y\n1\n|x)\n\u0011\n(6)\nThe derivation is in Appendix A.2.  While Eq. 6 uses the Bradley-Terry model, we can similarly\nderive expressions under the more general Plackett-Luce models [30, 21], shown in Appendix A.3.\nNow that we have the probability of human preference data in terms of the optimal policy rather than\nthe reward model, we can formulate a maximum likelihood objective for a parametrized policyπ\nθ\n.\nAnalogous to the reward modeling approach (i.e. Eq. 2), our policy objective becomes:\nL\nDPO\n(π\nθ\n;π\nref\n) =−E\n(x,y\nw\n,y\nl\n)∼D\n\u0014\nlogσ\n\u0012\nβlog\nπ\nθ\n(y\nw\n|x)\nπ\nref\n(y\nw\n|x)\n−βlog\nπ\nθ\n(y\nl\n|x)\nπ\nref\n(y\nl\n|x)\n\u0013\u0015\n.(7)\nThis way, we fit an implicit reward using an alternative parameterization, whose optimal policy is\nsimplyπ\nθ\n.  Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry\n4",
    "model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the\npreference data distribution [4].  In Section 5, we further discuss theoretical properties of DPO in\nrelation to other works.\nWhat does the DPO update do?For a mechanistic understanding of DPO, it is useful to analyze the\ngradient of the loss functionL\nDPO\n. The gradient with respect to the parametersθcan be written as:\n∇\nθ\nL\nDPO\n(π\nθ\n;π\nref\n) =\n−βE\n(x,y\nw\n,y\nl\n)∼D\n\u0014\nσ(ˆr\nθ\n(x,y\nl\n)−ˆr\nθ\n(x,y\nw\n))\n|\n{z}\nhigher weight when reward estimate is wrong\n\u0014\n∇\nθ\nlogπ(y\nw\n|x)\n|{z}\nincrease likelihood ofy\nw\n− ∇\nθ\nlogπ(y\nl\n|x)\n|{z}\ndecrease likelihood ofy\nl\n\u0015\u0015\n,\nwhereˆr\nθ\n(x,y) =βlog\nπ\nθ\n(y|x)\nπ\nref\n(y|x)\nis the reward implicitly defined by the language modelπ\nθ\nand refer-\nence modelπ\nref\n(more in Section 5). Intuitively, the gradient of the loss functionL\nDPO\nincreases the\nlikelihood of the preferred completionsy\nw\nand decreases the likelihood of dispreferred completions\ny\nl\n. Importantly, the examples are weighed by how much higher the implicit reward modelˆr\nθ\nrates\nthe dispreferred completions, scaled byβ, i.e, how incorrectly the implicit reward model orders\nthe completions, accounting for the strength of the KL constraint.  Our experiments suggest the\nimportance of this weighting, as a naïve version of this method without the weighting coefficient can\ncause the language model to degenerate (Appendix Table 3).\nDPO outline.The general DPO pipeline is as follows: 1) Sample completionsy\n1\n,y\n2\n∼π\nref\n(·|x)\nfor every promptx, label with human preferences to construct the offline dataset of preferences\nD={x\n(i)\n,y\n(i)\nw\n,y\nl\n)\n(i)\n}\nN\ni=1\nand 2) optimize the language modelπ\nθ\nto minimizeL\nDPO\nfor the given\nπ\nref\nandDand desiredβ. In practice, one would like to reuse preference datasets publicly available,\nrather than generating samples and gathering human preferences.  Since the preference datasets\nare sampled usingπ\nSFT\n, we initializeπ\nref\n=π\nSFT\nwhenever available.  However, whenπ\nSFT\nis\nnot available, we initializeπ\nref\nby maximizing likelihood of preferred completions(x,y\nw\n), that\nis,π\nref\n= arg max\nπ\nE\nx,y\nw\n∼D\n[logπ(y\nw\n|x)].  This procedure helps mitigate the distribution shift\nbetween the true reference distribution which is unavailable, andπ\nref\nused by DPO. Further details\nrelated to the implementation and hyperparameters can be found in Appendix B.\n5    Theoretical Analysis of DPO\nIn this section, we give further interpretation of the DPO method, provide theoretical backing, and\nrelate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO [37]).\n5.1    Your Language Model Is Secretly a Reward Model\nDPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using\na single maximum likelihood objective.  Note the optimization objective Eq.  5 is equivalent to a\nBradley-Terry model with a reward parameterizationr\n∗\n(x,y) =βlog\nπ\n∗\nθ\n(y|x)\nπ\nref\n(y|x)\nand we optimize our\nparametric modelπ\nθ\n, equivalently to the reward model optimization in Eq. 2 under the change of\nvariables. In this section we will build the theory behind this reparameterization, show that it does\nnot constrain the class of learned reward models, and allows for the exact recovery of the optimal\npolicy. We begin with by defining an equivalence relation between reward functions.\nDefinition   1.We   say   that   two   reward   functionsr(x,y)andr\n′\n(x,y)are   equivalent   iff\nr(x,y)−r\n′\n(x,y) =f(x)for some functionf.\nIt is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions\ninto classes. We can state the following two lemmas:\nLemma 1.Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two\nreward functions from the same class induce the same preference distribution.\nLemma 2.Two reward functions from the same equivalence class induce the same optimal policy\nunder the constrained RL problem.\nThe proofs are straightforward and we defer them to Appendix A.5. The first lemma is a well-known\nunder-specification issue with the Plackett-Luce family of models [30]. Due to this under-specification,\n5",
    "we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE\nestimates from Eq.  2 [4].  The second lemma states that all reward functions from the same class\nyield the same optimal policy, hence for our final objective, we are only interested in recovering an\narbitrary reward function from the optimal class. We prove the following Theorem in Appendix A.6:\nTheorem  1.Under  mild  assumptions,  all  reward  classes  consistent  with  the  Plackett-Luce\n(and  Bradley-Terry  in  particular)  models  can  be  represented  with  the  reparameterization\nr(x,y) =βlog\nπ(y|x)\nπ\nref\n(y|x)\nfor some modelπ(y|x)and a given reference modelπ\nref\n(y|x).\nProof Sketch.\nConsider any reward functionr(x,y), which induces a corresponding optimal model\nπ\nr\n(y|x), specified by Eq. 4. We will show that a reward function from the equivalence class ofr\ncan be represented using the reparameterization given above. We define the projectionfas\nf(r;π\nref\n,β)(x,y) =r(x,y)−βlog\nX\ny\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\n(8)\nThe operatorfsimply normalizes the reward function with the logarithm of the partition function\nofπ\nr\n. Since the added normalization term is only a function of the prefixx,f(r;π\nref\n,β)(x,y)is a\nreward function in the equivalence class ofr(x,y). Finally, replacingrwith the RHS of Eq. 5 (which\nholds for any reward function), we havef(r;π\nref\n,β)(x,y) =βlog\nπ\nr\n(y|x)\nπ\nref\n(y|x)\n. That is, the projection\nfproduces a member of the equivalence class ofrwith the desired form, and we do not lose any\ngenerality in our reward model from the proposed reparameterization.\nWe can alternatively view Theorem 1 as specifying exactly which reward function within each\nequivalence class the DPO reparameterization selects, that is, the reward function satisfying:\nX\ny\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\n|\n{z}\n=π(y|x), using Thm. 1 reparam.\n= 1,(9)\ni.e.,π(y|x)is a valid distribution (probabilities are positive and sum to 1).  However, following\nEq. 4, we can see that Eq. 9 is the partition function of the optimal policy induced by the reward\nfunctionr(x,y). The key insight of the DPO algorithm is that we can impose certain constraints on\nthe under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models,\nsuch that we preserve the class of representable reward models, but explicitly make the optimal policy\nin Eq. 4 analytically tractable for all promptsx.\n5.2    Instability of Actor-Critic Algorithms\nWe can also use our framework to diagnose instabilities with standard actor-critic algorithms used\nfor the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step\noutlined in Section 3. We can draw connections to the control as inference framework [20] for the\nconstrained RL problem outlined in 3. We assume a parameterized modelπ\nθ\n(y|x)and minimize\nD\nKL\n[π\nθ\n(y|x)||π\n∗\n(y|x)]whereπ\n∗\nis the optimal policy from Eq. 7 induced by the reward function\nr\nφ\n(y,x). With some algebra this leads to the optimization objective:\nmax\nπ\nθ\nE\nπ\nθ\n(y|x)\n\u0014\nr\nφ\n(x,y)−βlog\nX\ny\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr\nφ\n(x,y)\n\u0013\n|{z}\nf(r\nφ\n,π\nref\n,β)\n−βlog\nπ\nθ\n(y|x)\nπ\nref\n(y|x)\n|{z}\nKL\n\u0015\n(10)\nThis is the same objective optimized in prior works [49,38,1,26] using the DPO-equivalent reward\nfor the reward class ofr\nφ\n. In this setting, we can interpret the normalization term inf(r\nφ\n,π\nref\n,β)\nas the soft value function of the reference policyπ\nref\n.  While this term does not affect the optimal\nsolution, without it, the policy gradient of the objective could have high variance, making learning\nunstable. We can accommodate for the normalization term using a learned value function, but that\ncan also be difficult to optimize. Alternatively, prior works have normalized rewards using a human\ncompletion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In\ncontrast the DPO reparameterization yields a reward function that does not require any baselines.\n6",
    "0.02.55.07.510.012.515.017.520.0\nKL(\nref\n)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nReward\nIMDb Sentiment Generation\nDPO (Ours)\nUnlikelihood\nPPO (Our impl.)\nPPO-GT (Our impl.)\nPPO-GT (TRL)\nPreferred-FT\n0.000.250.500.751.00\nSampling temperature\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nWin rate\nTL;DR Summarization Win Rate vs Reference\nDPO\nPPO\nPreferred-FT\nSFT\nGPT-J\nBest of 128\nFigure 2:Left.The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected\nreward for all KL values, demonstrating the quality of the optimization.Right.TL;DR summarization win\nrates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO’s best-case performance on\nsummarization, while being more robust to changes in the sampling temperature.\n6    Experiments\nIn this section, we empirically evaluate DPO’s ability to train policies directly from preferences. First,\nin a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing\nreward and minimizing KL-divergence with the reference policy, compared to common preference\nlearning algorithms such as PPO? Next, we evaluate DPO’s performance on larger models and more\ndifficult RLHF tasks, including summarization and dialogue.  We find that with almost no tuning\nof hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with\nPPO as well as returning the best ofNsampled trajectories under a learned reward function. Before\npresenting these results, we describe the experimental set-up; additional details are in Appendix C.\nTasks.Our experiments explore three different open-ended text generation tasks. For all experiments,\nalgorithms learn a policy from a dataset of preferencesD=\n\b\nx\n(i)\n,y\n(i)\nw\n,y\n(i)\nl\n\t\nN\ni=1\n.  Incontrolled\nsentiment generation,xis a prefix of a movie review from the IMDb dataset [22], and the policy\nmust  generateywith  positive  sentiment.   In  order  to  perform  a  controlled  evaluation,  for  this\nexperiment wegeneratepreference pairs over generations using a pre-trained sentiment classifier,\nwherep(positive|x,y\nw\n)> p(positive|x,y\nl\n). For SFT, we fine-tune GPT-2-large until convergence\non reviews from the train split of the IMDB dataset (further details in App C.1). Insummarization,\nxis a forum post from Reddit; the policy must generate a summaryyof the main points in the\npost. Following prior work, we use the Reddit TL;DR summarization dataset [41] along with human\npreferences gathered by Stiennon et al.. We use an SFT model fine-tuned on human-written forum\npost summaries\n2\nwith the TRLX [42] framework for RLHF. The human preference dataset was\ngathered by Stiennon et al. on samples from a different, but similarly-trained, SFT model. Finally, in\nsingle-turn dialogue,xis a human query, which may be anything from a question about astrophysics\nto a request for relationship advice. A policy must produce an engaging and helpful responseyto\na user’s query; we use the Anthropic Helpful and Harmless dialogue dataset [1], containing 170k\ndialogues between a human and an automated assistant. Each transcript ends with a pair of responses\ngenerated by a large (although unknown) language model along with a preference label denoting\nthe human-preferred response. In this setting, no pre-trained SFT model is available; we therefore\nfine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\nEvaluation.Our experiments use two different approaches to evaluation.  In order to analyze the\neffectiveness of each algorithm in optimizing the constrained reward maximization objective, in the\ncontrolled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward\nand KL-divergence from the reference policy; this frontier is computable because we have acccess to\nthe ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth\nreward function is not known; therefore, we evaluate algorithms with theirwin rateagainst a baseline\npolicy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness\nin the summarization and single-turn dialogue settings, respectively.  For summarization, we use\nreference summaries in the test set as the baseline; for dialogue, we use the preferred response in the\n2\nhttps://huggingface.co/CarperAI/openai_summarize_tldr_sft\n7",
    "0.250.500.751.00\nSampling temperature\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWin rate\nAnthropic-HH Dialogue Win Rate vs Chosen\nDPO\nBest of 128\nPreferred-FT\nPythia-2.8B\n030060090012001500180021002400270030003300\nFine-tuning step\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nWin rate\nDialogue Win Rate Evolution\nDPO (temp = 1.0)DPO (temp = 0.7)\nFigure 3:Left.Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method\nthat improves over chosen summaries in the Anthropic-HH test set.Right.Win rates for different sampling\ntemperatures over the course of training. DPO’s improvement over the dataset labels is fairly stable over the\ncourse of training for different sampling temperatures.\ntest dataset as the baseline. While existing studies suggest LMs can be better automated evaluators\nthan existing metrics [10], we conduct a human study to justify our usage of GPT-4 for evaluation\nin Sec. 6.4. We find GPT-4 judgments correlate strongly with humans, with human agreement with\nGPT-4 typically similar or higher than inter-human annotator agreement.\nMethods.In addition to DPO, we evaluate several existing approaches to training language models\nto adhere to human preferences. Most simply, we explore zero-shot prompting withGPT-J[43] in\nthe summarization task and 2-shot prompting withPythia-2.8B[3] in the dialogue task. In addition,\nwe evaluate theSFTmodel as well asPreferred-FT, which is a model fine-tuned with supervised\nlearning on the chosen completiony\nw\nfrom either the SFT model (in controlled sentiment and\nsummarization) or a generic LM (in single-turn dialogue).  Another pseudo-supervised method is\nUnlikelihood[44], which simply optimizes the policy to maximize the probability assigned toy\nw\nand\nminimizethe probability assigned toy\nl\n; we use an optional coefficientα∈[0,1]on the ‘unlikelihood’\nterm.  We also considerPPO[37] using a reward function learned from the preference data and\nPPO-GT, which is an oracle that learns from the ground truth reward function available in the\ncontrolled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT,\none of-the-shelf version [42] as well as a modified version that normalizes rewards and further tunes\nhyperparameters to improve performance (we also use these modifications when running ‘normal’\nPPO with learned rewards). Finally, we consider theBest ofNbaseline, samplingNresponses from\nthe SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to\na reward function learned from the preference dataset. This high-performing method decouples the\nquality of the reward model from the PPO optimization, but is computationally impractical even for\nmoderateNas it requires samplingNcompletions for every query at test time.\n6.1    How well can DPO optimize the RLHF objective?\nThe KL-constrained reward maximization objective used in typical RLHF algorithms balances\nexploitation of reward while restricting the policy from deviating far from the reference policy.\nTherefore, when comparing algorithms, we must take into account both reward achieved as well as the\nKL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable.\nFigure 2 shows the reward-KL frontier for various algorithms in the sentiment setting. We execute\nmultiple training runs for each algorithm, using a different hyperparameter for policy conservativeness\nin each run (target KL∈ {3,6,9,12}for PPO,β∈ {0.05,0.1,1,5},α∈ {0.05,0.1,0.5,1}for\nunlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total.  After each\n100 training steps until convergence, we evaluate each policy on a set of test prompts, computing\nthe average reward under the true reward function as well as the average sequence-level KL\n3\nwith\nthe reference policyKL(π||π\nref\n).  We find that DPO produces by far the most efficient frontier,\nachieving the highest reward while still achieving low KL. This result is particularly notable for\nmultiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient;\n3\nThat is, the sum of the per-timestep KL-divergences.\n8",
    "DPO’s reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO,\neven when PPO can access ground truth rewards(PPO-GT).\n6.2    Can DPO scale to real preference datasets?\nNext, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For\nsummarization, automatic evaluation metrics such as ROUGE can be poorly correlated with human\npreferences [38], and prior work has found that fine-tuning LMs using PPO on human preferences\nto provide more effective summaries. We evaluate different methods by sampling completions on\nthe test split of TL;DR summarization dataset, and computing the average win rate against reference\ncompletions in the test set. The completions for all methods are sampled at temperatures varying\nfrom 0.0 to 1.0, and the win rates are shown in Figure 2 (right).  DPO, PPO and Preferred-FT all\nfine-tune the same GPT-J SFT model\n4\n. We find that DPO has a win rate of approximately 61% at a\ntemperature of 0.0, exceeding the performance of PPO at  57% at its optimal sampling temperature\nof 0.0.  DPO also achieves a higher maximum win rate compared to the best ofNbaseline.  We\nnote that we did not meaningfully tune DPO’sβhyperparameter, so these results may underestimate\nDPO’s potential. Moreover, we find DPO to be much more robust to the sampling temperature than\nPPO, the performance of which can degrade to that of the base GPT-J model at high temperatures.\nPreferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO\nhead-to-head in human evaluations in Section 6.4, where DPO samples at temperature 0.25 were\npreferred 58% times over PPO samples at temperature 0.\nOn single-turn dialogue, we evaluate the different methods on the subset of the test split of the\nAnthropic HH dataset [1] with one step of human-assistant interaction. GPT-4 evaluations use the\npreferred completions on the test as the reference to compute the win rate for different methods. As\nthere is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT\nto train a reference model on the chosen completions such that completions are within distribution\nof the model, and then train using DPO. We also compare against the best of 128 Preferred-FT\ncompletions (we found the Best ofNbaseline plateaus at 128 completions for this task; see Appendix\nFigure 4) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as\nwell or better for the best-performing temperatures for each method.  We also evaluate an RLHF\nmodel trained with PPO on the Anthropic HH dataset\n5\nfrom a well-known source\n6\n, but are unable\nto find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B\nmodel. Based on our results from TL;DR and the fact that both methods optimize the same reward\nfunction, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the\nonly computationally efficient method that improves over the preferred completions in the Anthropic\nHH dataset, and provides similar or better performance to the computationally demanding Best of\n128 baseline. Finally, Figure 3 shows that DPO converges to its best performance relatively quickly.\n6.3    Generalization to a new input distribution\nWin rate vs. ground truth\nAlg.Temp0Temp0.25\nDPO0.360.31\nPPO0.260.23\nTable  1:   GPT-4  win  rates  vs.    ground\ntruth  summaries  for  out-of-distribution\nCNN/DailyMail input articles.\nTo further compare the performance of PPO and DPO un-\nder distribution shifts, we evaluate the PPO and DPO poli-\ncies from our Reddit TL;DR summarization experiment on\na different distribution, news articles in the test split of the\nCNN/DailyMail dataset [24], using the best sampling temper-\natures from TL;DR (0 and 0.25). The results are presented\nin Table 1.  We computed the GPT-4 win rate against the\nground-truth summaries in the datasets, using the same GPT-\n4 (C) prompt we used for Reddit TL;DR, but replacing the words “forum post” with “news article”.\nFor this new distribution, DPO continues to outperform the PPO policy by a significant margin. This\nexperiment provides initial evidence that DPO policies can generalize similarly well to PPO policies,\neven though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n4\nhttps://huggingface.co/CarperAI/openai_summarize_tldr_sft\n5\nhttps://huggingface.co/reciprocate/ppo_hh_pythia-6B\n6\nhttps://github.com/CarperAI/trlx/tree/main/examples/hh\n9",
    "6.4    Validating GPT-4 judgments with human judgments\nWe conduct a human study to verify the reliability of GPT-4’s judgments,  using the results of\nthe TL;DR summarization experiment and two different GPT-4 prompts.  TheGPT-4 (S)(sim-\nple) prompt simply asks for which summary better-summarizes the important information in the\npost.  TheGPT-4 (C)(concise) prompt also asks for which summary is more concise; we eval-\nuate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than hu-\nmans do with theGPT-4 (S)prompt.  See Appendix C.2 for the complete prompts.  We perform\nthree  comparisons,  using  the  highest  (DPO,  temp.   0.25),  the  lowest  (PPO,  temp.   1.0),  and  a\nDPOSFTPPO-1\nN respondents272122199\nGPT-4 (S) win %472713\nGPT-4 (C) win %543212\nHuman win %584317\nGPT-4 (S)-H agree707786\nGPT-4 (C)-H agree677985\nH-H agree65-87\nTable 2:  Comparing human and GPT-4 win rates\nand per-judgment agreement on TL;DR summariza-\ntion samples.Humans agree with GPT-4 about as\nmuch as they agree with each other.Each experi-\nment compares a summary from the stated method\nwith a summary from PPO with temperature 0.\nmiddle-performing (SFT, temp. 0.25) method with\nthe aim of covering a diversity of sample qualities;\nall three methods are compared against greedily-\nsampled  PPO  (its  best-performing  temperature).\nWe find that with both prompts, GPT-4 tends to\nagree with humans about as often as humans agree\nwith each other, suggesting that GPT-4 is a reason-\nable proxy for human evaluations (due to limited\nhuman raters, we only collect multiple human judg-\nments for the DPO and PPO-1 comparisons). Over-\nall, theGPT-4 (C)prompt generally provides win\nrates more representative of humans; we therefore\nuse this prompt for the main results in Section 6.2.\nFor additional details about the human study, in-\ncluding the web interface presented to raters and\nthe list of human volunteers, see Appendix D.3.\n7    Discussion\nLearning from preferences is a powerful, scalable framework for training capable, aligned language\nmodels. We have introduced DPO, a simple training paradigm for training language models from\npreferences without reinforcement learning. Rather than coercing the preference learning problem\ninto a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping\nbetween language model policies and reward functions that enables training a language model to\nsatisfy human preferencesdirectly, with a simple cross-entropy loss, without reinforcement learning\nor loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better\nthan existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the\nbarrier to training more language models from human preferences.\nLimitations & Future Work.Our results raise several important questions for future work. How\ndoes the DPO policy generalize out of distribution, compared with learning from an explicit reward\nfunction? Our initial results suggest that DPO policies can generalize similarly to PPO-based models,\nbut more comprehensive study is needed.  For example, can training with self-labeling from the\nDPO policy similarly make effective use of unlabeled prompts? On another front, how does reward\nover-optimization manifest in the direct preference optimization setting, and is the slight decrease\nin performance in Figure 3-right an instance of it?  Additionally, while we evaluate models up to\n6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is\nan exciting direction for future work. Regarding evaluations, we find that the win rates computed\nby GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality\njudgments from automated systems. Finally, many possible applications of DPO exist beyond training\nlanguage models from human preferences, including training generative models in other modalities.\nAcknowledgements\nEM gratefully acknowledges funding from a Knight-Hennessy Graduate Fellowship. CF and CM\nare CIFAR Fellows.  This work was supported in part by the Stanford Accelerator for Learning\n(SAL) and Stanford Institute for Human-Centered Artificial Intelligence (HAI)Generative AI for the\nFuture of Learningseed grant program. The Stanford Center for Research on Foundation Models\n(CRFM) provided part of the compute resources used for the experiments in this work. This work\nwas supported in part by ONR grant N00014-20-1-2675.\n10",
    "References\n[1]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\nT. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-\nDodds,  D. Hernandez,  T. Hume,  S. Johnston,  S. Kravec,  L. Lovitt,  N. Nanda,  C. Olsson,\nD. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a\nhelpful and harmless assistant with reinforcement learning from human feedback, 2022.\n[2]\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li,\nE. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite,\nL. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Lar-\nson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton,\nT. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei,\nN. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai: Harmlessness from ai\nfeedback, 2022.\n[3]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,\nS. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A\nsuite for analyzing large language models across training and scaling, 2023.\n[4]H. Bong and A. Rinaldo.  Generalized results for the existence and consistency of the MLE\nin the Bradley-Terry-Luce model.International Conference on Machine Learning,  2022.\narXiv:2110.11487.\n[5]R. A. Bradley and M. E. Terry.  Rank analysis of incomplete block designs: I. the method of\npaired comparisons.Biometrika, 39(3/4):324–345, 1952. doi: https://doi.org/10.2307/2334029.\n[6]T.  Brown,  B.  Mann,  N.  Ryder,  M.  Subbiah,  J.  D.  Kaplan,  P.  Dhariwal,  A.  Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan-\nguage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and\nH. Lin, editors,Advances in Neural Information Processing Systems, volume 33, pages 1877–\n1901. Curran Associates, Inc., 2020.   URLhttps://proceedings.neurips.cc/paper_\nfiles/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n[7]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry,  A. Askell,  et al.   Language models are few-shot learners.Advances in neural\ninformation processing systems, 33:1877–1901, 2020.\n[8]S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,\nY. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general\nintelligence: Early experiments with GPT-4, 2023. arXiv preprint arXiv:2303.12712.\n[9]\nR. Busa-Fekete,  B. Szörényi,  P. Weng,  W. Cheng,  and E. Hüllermeier.   Preference-based\nreinforcement learning:  evolutionary direct policy search using a preference-based racing\nalgorithm.Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\nURLhttps://doi.org/10.1007/s10994-014-5458-8.\n[10]Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\nfor reference-free text quality evaluation: A preliminary empirical study.ArXiv, abs/2304.00723,\n2023.\n[11]\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways.arXiv\npreprint arXiv:2204.02311, 2022.\n[12]P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\nlearning from human preferences. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Sys-\ntems, volume 30. Curran Associates, Inc., 2017. URLhttps://proceedings.neurips.cc/\npaper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf.\n11",
    "[13]H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani,\nS. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros,\nM. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai,\nH. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei.\nScaling instruction-finetuned language models, 2022.\n[14]M. Dudík, K. Hofmann, R. E. Schapire, A. Slivkins, and M. Zoghi. Contextual dueling bandits.\nIn P. Grünwald, E. Hazan, and S. Kale, editors,Proceedings of The 28th Conference on Learning\nTheory, volume 40 ofProceedings of Machine Learning Research, pages 563–587, Paris, France,\n03–06 Jul 2015. PMLR. URLhttps://proceedings.mlr.press/v40/Dudik15.html.\n[15]\nD. Go, T. Korbak, G. Kruszewski, J. Rozen, N. Ryu, and M. Dymetman.  Aligning language\nmodels  with  preferences  through  f-divergence  minimization.   InProceedings  of  the  40th\nInternational Conference on Machine Learning, ICML’23. JMLR.org, 2023.\n[16]A. Jain, B. Wojcik, T. Joachims, and A. Saxena.  Learning trajectory preferences for manip-\nulators via iterative improvement.  In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\nK. Weinberger, editors,Advances in Neural Information Processing Systems, volume 26. Curran\nAssociates, Inc., 2013.  URLhttps://proceedings.neurips.cc/paper_files/paper/\n2013/file/c058f544c737782deacefa532d9add4c-Paper.pdf.\n[17]T. Korbak, H. Elsahar, G. Kruszewski, and M. Dymetman.  On reinforcement learning and\ndistribution matching for fine-tuning language models with no catastrophic forgetting.   In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,Advances in\nNeural Information Processing Systems, volume 35, pages 16203–16220. Curran Associates,\nInc., 2022. URLhttps://proceedings.neurips.cc/paper_files/paper/2022/file/\n67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf.\n[18]J. Kreutzer, J. Uyheng, and S. Riezler. Reliability and learnability of human bandit feedback\nfor sequence-to-sequence reinforcement learning. InProceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 1777–1788,\nMelbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/\nP18-1165. URLhttps://aclanthology.org/P18-1165.\n[19]\nA. Kupcsik, D. Hsu, and W. S. Lee.Learning Dynamic Robot-to-Human Object Handover\nfrom Human Feedback, pages 161–176.  Springer International Publishing, 01 2018.  ISBN\n978-3-319-51531-1. doi: 10.1007/978-3-319-51532-8_10.\n[20]\nS. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review,\n2018.\n[21]  R. D. Luce. Individual choice behavior: A theoretical analysis.Courier Corporation, 2012.\n[22]A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors\nfor sentiment analysis.   InProceedings of the 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon,\nUSA, June 2011. Association for Computational Linguistics. URLhttp://www.aclweb.org/\nanthology/P11-1015.\n[23]\nS. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi.  Cross-task generalization via natural\nlanguage  crowdsourcing  instructions.   InProceedings  of  the  60th  Annual  Meeting  of  the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 3470–3487, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\n244. URLhttps://aclanthology.org/2022.acl-long.244.\n[24]R. Nallapati, B. Zhou, C. dos Santos, Ç. Gulçehre, and B. Xiang. Abstractive text summarization\nusing sequence-to-sequence RNNs and beyond. InProceedings of the 20th SIGNLL Conference\non Computational Natural Language Learning, pages 280–290, Berlin, Germany, Aug. 2016.\nAssociation for Computational Linguistics.   doi:  10.18653/v1/K16-1028.   URLhttps://\naclanthology.org/K16-1028.\n12",
    "[25]D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand,\nP. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia. Efficient large-scale\nlanguage model training on gpu clusters using megatron-lm. InProceedings of the International\nConference for High Performance Computing, Networking, Storage and Analysis, SC ’21, New\nYork, NY, USA, 2021. Association for Computing Machinery.  ISBN 9781450384421.  doi:\n10.1145/3458817.3476209. URLhttps://doi.org/10.1145/3458817.3476209.\n[26]L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with\nhuman feedback.  In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors,Advances in Neural Information Processing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc., 2022.  URLhttps://proceedings.neurips.cc/paper_files/\npaper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n[27]R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. In\nInternational Conference on Learning Representations, 2018.  URLhttps://openreview.\nnet/forum?id=HkAClQgA-.\n[28]\nX. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and\nscalable off-policy reinforcement learning.arXiv preprint arXiv:1910.00177, 2019.\n[29]J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational\nspace control. InProceedings of the 24th international conference on Machine learning, pages\n745–750, 2007.\n[30]R. L. Plackett. The analysis of permutations.Journal of the Royal Statistical Society. Series C\n(Applied Statistics), 24(2):193–202, 1975. doi: https://doi.org/10.2307/2346567.\n[31]A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.  Language models are\nunsupervised multitask learners, 2019. Ms., OpenAI.\n[32]R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi,\nand Y. Choi.  Is reinforcement learning (not) for natural language processing:  Benchmarks,\nbaselines,  and building blocks for natural language policy optimization.   InThe Eleventh\nInternational Conference on Learning Representations, 2023.  URLhttps://openreview.\nnet/forum?id=8aHzds2uUyB.\n[33]M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\nnetworks.CoRR, abs/1511.06732, 2015.\n[34]D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia.  Active preference-based learning of\nreward functions. InRobotics: Science and Systems (RSS), 2017.\n[35]A.  Saha,  A.  Pacchiano,  and  J.  Lee.   Dueling  rl:  Reinforcement  learning  with  trajectory\npreferences.   In F. Ruiz, J. Dy, and J.-W. van de Meent, editors,Proceedings of The 26th\nInternational Conference on Artificial Intelligence and Statistics,  volume 206 ofProceed-\nings  of  Machine  Learning  Research,  pages  6263–6289.  PMLR,  25–27  Apr  2023.    URL\nhttps://proceedings.mlr.press/v206/saha23a.html.\n[36]V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\nA. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chh-\nablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X.\nYong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry,\nJ. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush.  Multi-\ntask prompted training enables zero-shot task generalization. InInternational Conference on\nLearning Representations, 2022. URLhttps://openreview.net/forum?id=9Vrb9D0WI4.\n[37]J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms, 2017.\n[38]\nN. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and\nP. Christiano. Learning to summarize from human feedback, 2022.\n13",
    "[39]R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\nL. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun,\nD. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou,\nC.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern,\nM. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran,\nM. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar,\nA. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-\nArcas, C. Cui, M. Croak, E. Chi, and Q. Le. Lamda: Language models for dialog applications,\n2022.\n[40]H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, et al.  Llama:  Open and efficient foundation language models.arXiv\npreprint arXiv:2302.13971, 2023.\n[41]M. Völske, M. Potthast, S. Syed, and B. Stein.  TL;DR: Mining Reddit to learn automatic\nsummarization.  InProceedings of the Workshop on New Frontiers in Summarization, pages\n59–63, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics.  doi:\n10.18653/v1/W17-4508. URLhttps://aclanthology.org/W17-4508.\n[42]L. von Werra, J. Tow, reciprocated, S. Matiana, A. Havrilla, cat state, L. Castricato, Alan, D. V.\nPhung, A. Thakur, A. Bukhtiyarov, aaronrmm, F. Milo, Daniel, D. King, D. Shin, E. Kim, J. Wei,\nM. Romero, N. Pochinkov, O. Sanseviero, R. Adithyan, S. Siu, T. Simonini, V. Blagojevic,\nX. Song, Z. Witten, alexandremuzio, and crumb.  CarperAI/trlx:  v0.6.0:  LLaMa (Alpaca),\nBenchmark Util, T5 ILQL, Tests, Mar. 2023.  URLhttps://doi.org/10.5281/zenodo.\n7790115.\n[43]B. Wang and A. Komatsuzaki.  GPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel.https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n[44]\nS. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with\nunlikelihood training.arXiv preprint arXiv:1908.04319, 2019.\n[45]R.  J.  Williams.Simple  statistical  gradient-following  algorithms  for  connectionist  rein-\nforcement  learning.Mach.  Learn.,  8(3–4):229–256,  may  1992.   ISSN  0885-6125.   doi:\n10.1007/BF00992696. URLhttps://doi.org/10.1007/BF00992696.\n[46]Y. Wu and B. Hu.  Learning to extract coherent summary via deep reinforcement learning.\nInProceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth\nInnovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on\nEducational Advances in Artificial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press,\n2018. ISBN 978-1-57735-800-8.\n[47]X. Yan,  C. Luo,  C. L. A. Clarke,  N. Craswell,  E. M. Voorhees,  and P. Castells.   Human\npreferences as dueling bandits. InProceedings of the 45th International ACM SIGIR Conference\non  Research  and  Development  in  Information  Retrieval,  SIGIR  ’22,  page  567–577,  New\nYork, NY, USA, 2022. Association for Computing Machinery.  ISBN 9781450387323.  doi:\n10.1145/3477495.3531991. URLhttps://doi.org/10.1145/3477495.3531991.\n[48]Y. Yue,  J. Broder,  R. Kleinberg,  and T. Joachims.   The k-armed dueling bandits problem.\nJournal of Computer and System Sciences, 78(5):1538–1556, 2012. ISSN 0022-0000. doi: https:\n//doi.org/10.1016/j.jcss.2011.12.028.  URLhttps://www.sciencedirect.com/science/\narticle/pii/S0022000012000281. JCSS Special Issue: Cloud Computing 2011.\n[49]D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and\nG. Irving. Fine-tuning language models from human preferences, 2020.\n14",
    "Author Contributions\nAll authorsprovided valuable contributions to designing, analyzing, and iterating on experiments,\nwriting and editing the paper, and generally managing the project’s progress.\nRRproposed using autoregressive reward models in discussions withEM; derived the DPO objective;\nproved the theoretical properties of the algorithm and wrote the relevant sections and appendices. He\nalso suggested and helped with organizing experiments and contributed some of the PPO and reward\nlearning baselines.\nASinitiated the discussion on using weighted regression methods as an alternative to PPO; initiated\nproject-related organization, wrote initial analysis connecting DPO with weighted regression and\nunlikelihood; design and iterations of DPO + baseline implementations, initial exploratory exper-\niments for DPO; substantial experiment organization and design (datasets, baselines, evaluation);\nled model training and evaluation for controlled sentiment generation and summarization; design\niterations for GPT-4 evaluation (particularly summarization); substantial writing contributions to\nabstract, prelims/method and experiments; editing contributions to other sections.\nEMprovided input on early discussions on learning autoregressive reward functions; wrote the first\nimplementation of DPO and ran the first DPO experiments; trained the large-scale (summarization\nand dialogue) DPO models used in paper experiments; conducted initial GPT-4 win rate evaluations\nand set up related infrastructure; recruited participants for, conducted, and analyzed results from the\nhuman study; wrote the abstract, introduction, related work, discussion, and most of experiments;\nand assisted with editing the rest of the paper.\nCF, CM, & SEsupervised the research, suggested ideas and experiments, and assisted in writing the\npaper.\nA    Mathematical Derivations\nA.1    Deriving the Optimum of the KL-Constrained Reward Maximization Objective\nIn this appendix, we will derive Eq. 4. Analogously to Eq. 3, we optimize the following objective:\nmax\nπ\nE\nx∼D,y∼π\n\u0002\nr(x,y)\n\u0003\n−βD\nKL\n\u0002\nπ(y|x)||π\nref\n(y|x)\n\u0003\n(11)\nunder any reward functionr(x,y), reference modelπ\nref\nand a general non-parametric policy class.\nWe now have:\nmax\nπ\nE\nx∼D,y∼π\n\u0002\nr(x,y)\n\u0003\n−βD\nKL\n\u0002\nπ(y|x)||π\nref\n(y|x)\n\u0003\n= max\nπ\nE\nx∼D\nE\ny∼π(y|x)\n\u0014\nr(x,y)−βlog\nπ(y|x)\nπ\nref\n(y|x)\n\u0015\n= min\nπ\nE\nx∼D\nE\ny∼π(y|x)\n\u0014\nlog\nπ(y|x)\nπ\nref\n(y|x)\n−\n1\nβ\nr(x,y)\n\u0015\n= min\nπ\nE\nx∼D\nE\ny∼π(y|x)\n\n\nlog\nπ(y|x)\n1\nZ(x)\nπ\nref\n(y|x) exp\n\u0010\n1\nβ\nr(x,y)\n\u0011\n−logZ(x)\n\n\n(12)\nwhere we have partition function:\nZ(x) =\nX\ny\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\n.\nNote that the partition function is a function of onlyxand the reference policyπ\nref\n, but does not\ndepend on the policyπ. We can now define\nπ\n∗\n(y|x) =\n1\nZ(x)\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\n,\n15",
    "which is a valid probability distribution asπ\n∗\n(y|x)≥0for allyand\nP\ny\nπ\n∗\n(y|x) = 1. SinceZ(x)is\nnot a function ofy, we can then re-organize the final objective in Eq 12 as:\nmin\nπ\nE\nx∼D\n\u0014\nE\ny∼π(y|x)\n\u0014\nlog\nπ(y|x)\nπ\n∗\n(y|x)\n\u0015\n−logZ(x)\n\u0015\n=(13)\nmin\nπ\nE\nx∼D\n[D\nKL\n(π(y|x)||π\n∗\n(y|x))−logZ(x)](14)\nNow, sinceZ(x)does not depend onπ, the minimum is achieved by the policy that minimizes the\nfirst KL term. Gibbs’ inequality tells us that the KL-divergence is minimized at 0 if and only if the\ntwo distributions are identical. Hence we have the optimal solution:\nπ(y|x) =π\n∗\n(y|x) =\n1\nZ(x)\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\n(15)\nfor allx∈D. This completes the derivation.\nA.2    Deriving the DPO Objective Under the Bradley-Terry Model\nIt is straightforward to derive the DPO objective under the Bradley-Terry preference model as we\nhave\np\n∗\n(y\n1\n≻y\n2\n|x) =\nexp (r\n∗\n(x,y\n1\n))\nexp (r\n∗\n(x,y\n1\n)) + exp (r\n∗\n(x,y\n2\n))\n(16)\nIn Section 4 we showed that we can express the (unavailable) ground-truth reward through its\ncorresponding optimal policy:\nr\n∗\n(x,y) =βlog\nπ\n∗\n(y|x)\nπ\nref\n(y|x)\n+βlogZ(x)(17)\nSubstituting Eq. 17 into Eq. 16 we obtain:\np\n∗\n(y\n1\n≻y\n2\n|x) =\nexp\n\u0010\nβlog\nπ\n∗\n(y\n1\n|x)\nπ\nref\n(y\n1\n|x)\n+βlogZ(x)\n\u0011\nexp\n\u0010\nβlog\nπ\n∗\n(y\n1\n|x)\nπ\nref\n(y\n1\n|x)\n+βlogZ(x)\n\u0011\n+ exp\n\u0010\nβlog\nπ\n∗\n(y\n2\n|x)\nπ\nref\n(y\n2\n|x)\n+βlogZ(x)\n\u0011\n=\n1\n1 + exp\n\u0010\nβlog\nπ\n∗\n(y\n2\n|x)\nπ\nref\n(y\n2\n|x)\n−βlog\nπ\n∗\n(y\n1\n|x)\nπ\nref\n(y\n1\n|x)\n\u0011\n=σ\n\u0012\nβlog\nπ\n∗\n(y\n1\n|x)\nπ\nref\n(y\n1\n|x)\n−βlog\nπ\n∗\n(y\n2\n|x)\nπ\nref\n(y\n2\n|x)\n\u0013\n.\nThe last line is the per-instance loss in Equation 7.\nA.3    Deriving the DPO Objective Under the Plackett-Luce Model\nThe Plackett-Luce model [30,21] is a generalization of the Bradley-Terry model over rankings (rather\nthan just pair-wise comparisons).  Similar to to the Bradley-Terry model, it stipulates that when\npresented with a set of possible choices, people prefer a choice with probability proportional to the\nvalue of some latent reward function for that choice. In our context, when presented with a promptx\nand a set ofKanswersy\n1\n,...,y\nK\na user would output a permutationτ: [K]→[K], giving their\nranking of the answers. The Plackett-Luce model stipulates that\np\n∗\n(τ|y\n1\n,...,y\nK\n,x) =\nK\nY\nk=1\nexp(r\n∗\n(x,y\nτ(k)\n))\nP\nK\nj=k\nexp(r\n∗\n(x,y\nτ(j)\n))\n(18)\nNotice that whenK= 2, Equation 18 reduces to the Bradley-Terry model. However, for the general\nPlackett-Luce model, we can still utilize the results of Eq.  5 and substitute the reward function\nparameterized by its optimal policy.  Similarly to Appendix A.2, the normalization constantZ(x)\ncancels out and we’re left with:\np\n∗\n(τ|y\n1\n,...,y\nK\n,x) =\nK\nY\nk=1\nexp\n\u0010\nβlog\nπ\n∗\n(y\nτ(k)\n|x)\nπ\nref\n(y\nτ(k)\n|x)\n\u0011\nP\nK\nj=k\nexp\n\u0010\nβlog\nπ\n∗\n(y\nτ(j)\n|x)\nπ\nref\n(y\nτ(j)\n|x)\n\u0011\n(19)\n16",
    "Similarly   to   the   approach   of   Section   4,    if   we   have   access   to   a   datasetD=\n{τ\n(i)\n,y\n(i)\n1\n,...,y\n(i)\nK\n,x\n(i)\n}\nN\ni=1\nof prompts and user-specified rankings, we can use a parameterized\nmodel and optimize this objective with maximum-likelihood.:\nL\nDPO\n(π\nθ\n,π\nref\n) =−E\nτ,y\n1\n,...,y\nK\n,x∼D\n\n\nlog\nK\nY\nk=1\nexp\n\u0010\nβlog\nπ\nθ\n(y\nτ(k)\n|x)\nπ\nref\n(y\nτ(k)\n|x)\n\u0011\nP\nK\nj=k\nexp\n\u0010\nβlog\nπ\nθ\n(y\nτ(j)\n|x)\nπ\nref\n(y\nτ(j)\n|x)\n\u0011\n\n\n(20)\nA.4    Deriving the Gradient of the DPO Objective\nIn this section we derive the gradient of the DPO objective:\n∇\nθ\nL\nDPO\n(π\nθ\n;π\nref\n) =−∇\nθ\nE\n(x,y\nw\n,y\nl\n)∼D\n\u0014\nlogσ\n\u0012\nβlog\nπ\nθ\n(y\nl\n|x)\nπ\nref\n(y\nl\n|x)\n−βlog\nπ\nθ\n(y\nw\n|x)\nπ\nref\n(y\nw\n|x)\n\u0013\u0015\n(21)\nWe can rewrite the RHS of Equation 21 as\n∇\nθ\nL\nDPO\n(π\nθ\n;π\nref\n) =−E\n(x,y\nw\n,y\nl\n)∼D\n\u0014\nσ\n′\n(u)\nσ(u)\n∇\nθ\n(u)\n\u0015\n,(22)\nwhereu=βlog\nπ\nθ\n(y\nl\n|x)\nπ\nref\n(y\nl\n|x)\n−βlog\nπ\nθ\n(y\nw\n|x)\nπ\nref\n(y\nw\n|x)\n.\nUsing the properties of sigmoid functionσ\n′\n(x) =σ(x)(1−σ(x))andσ(−x) = 1−σ(x), we obtain\nthe final gradient\n∇\nθ\nL\nDPO\n(π\nθ\n;π\nref\n) =\n−E\n(x,y\nw\n,y\nl\n)∼D\n\u0014\nβσ\n\u0012\nβlog\nπ\nθ\n(y\nw\n|x)\nπ\nref\n(y\nw\n|x)\n−βlog\nπ\nθ\n(y\nl\n|x)\nπ\nref\n(y\nl\n|x)\n\u0013\u0014\n∇\nθ\nlogπ(y\nw\n|x)−∇\nθ\nlogπ(y\nl\n|x)\n\u0015\u0015\n,\nAfter using the reward substitution ofˆr\nθ\n(x,y) =βlog\nπ\nθ\n(y|x)\nπ\nref\n(y|x)\nwe obtain the final form of the\ngradient from Section 4.\nA.5    Proof of Lemma 1 and 2\nIn this section, we will prove the two lemmas from Section 5.\nLemma 1 Restated.Under the Plackett-Luce preference framework, and in particular the Bradley-\nTerry framework, two reward functions from the same equivalence class induce the same preference\ndistribution.\nProof.We say that two reward functionsr(x,y)andr\n′\n(x,y)are from the same equivalence class\nifr\n′\n(x,y) =r(x,y) +f(x)for some functionf. We consider the general Plackett-Luce (with the\nBradley-Terry model a special case forK= 2) and denote the probability distribution over rankings\ninduced by a particular reward functionr(x,y)asp\nr\n.  For any promptx, answersy\n1\n,...,y\nK\nand\nrankingτwe have:\np\nr\n′\n(τ|y\n1\n,...,y\nK\n,x) =\nK\nY\nk=1\nexp(r\n′\n(x,y\nτ(k)\n))\nP\nK\nj=k\nexp(r\n′\n(x,y\nτ(j)\n))\n=\nK\nY\nk=1\nexp(r(x,y\nτ(k)\n) +f(x))\nP\nK\nj=k\nexp(r(x,y\nτ(j)\n) +f(x))\n=\nK\nY\nk=1\nexp(f(x)) exp(r(x,y\nτ(k)\n))\nexp(f(x))\nP\nK\nj=k\nexp(r(x,y\nτ(j)\n))\n=\nK\nY\nk=1\nexp(r(x,y\nτ(k)\n))\nP\nK\nj=k\nexp(r(x,y\nτ(j)\n))\n=p\nr\n(τ|y\n1\n,...,y\nK\n,x),\nwhich completes the proof.\n17",
    "Lemma 2 Restated.Two reward functions from the same equivalence class induce the same optimal\npolicy under the constrained RL problem.\nProof.Let us consider two reward functions from the same class, such thatr\n′\n(x,y) =r(x,y) +f(x)\nand, let us denote asπ\nr\nandπ\nr\n′\nthe corresponding optimal policies. By Eq. 4, for allx,ywe have\nπ\nr\n′\n(y|x) =\n1\nP\ny\nπ\nref\n(y|x) exp\n\u0010\n1\nβ\nr\n′\n(x,y)\n\u0011\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr\n′\n(x,y)\n\u0013\n=\n1\nP\ny\nπ\nref\n(y|x) exp\n\u0010\n1\nβ\n(r(x,y) +f(x))\n\u0011\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\n(r(x,y) +f(x))\n\u0013\n=\n1\nexp\n\u0010\n1\nβ\nf(x)\n\u0011\nP\ny\nπ\nref\n(y|x) exp\n\u0010\n1\nβ\nr(x,y)\n\u0011\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\nexp\n\u0012\n1\nβ\nf(x)\n\u0013\n=\n1\nP\ny\nπ\nref\n(y|x) exp\n\u0010\n1\nβ\nr(x,y)\n\u0011\nπ\nref\n(y|x) exp\n\u0012\n1\nβ\nr(x,y)\n\u0013\n=π\nr\n(y|x),\nwhich completes the proof.\nA.6    Proof of Theorem 1\nIn this section, we will expand on the results of Theorem 1.\nTheorem 1 Restated.Assume, we have a reference model, such thatπ\nref\n(y|x)>0for all pairs of\npromptsxand answersyand a parameterβ >0.  All reward equivalence classes, as defined in\nSection 5 can be represented with the reparameterizationr(x,y) =βlog\nπ(y|x)\nπ\nref\n(y|x)\nfor some model\nπ(y|x).\nProof.Consider any reward functionr(x,y), which induces an optimal modelπ\nr\n(y|x)under the\nKL-constrained RL problem, with solution given by 4. Following Eq. 5, when we log-linearize both\nsides we obtain:\nr(x,y) =βlog\nπ\nr\n(y|x)\nπ\nref\n(y|x)\n+βlogZ(x)\nwhereZ(x) =\nP\ny\nπ\nref\n(y|x) exp\n\u0010\n1\nβ\nr(x,y)\n\u0011\n(notice thatZ(x)also depends on the reward function\nr).  Using the operatorr\n′\n(x,y) =f(r,π\nref\n,β)(x,y) =r(x,y)−βlogZ(x), we see that this new\nreward function is within the equivalence class ofrand, we have:\nr\n′\n(x,y) =βlog\nπ\nr\n(y|x)\nπ\nref\n(y|x)\nwhich completes the proof.\nWe can further expand on these results. We can see that ifrandr\n′\nare two reward functions in the\nsame class, then\nf(r,π\nref\n,β)(x,y) =βlog\nπ\nr\n(y|x)\nπ\nref\n(y|x)\n=βlog\nπ\n′\nr\n(y|x)\nπ\nref\n(y|x)\n=f(r\n′\n,π\nref\n,β)(x,y)\nwhere the second equality follows from Lemma 2.  We have proven that the operatorfmaps all\nreward functions from a particular equivalence class to the same reward function. Next, we show that\nfor every equivalence class of reward functions, the reward function that has the reparameterization\noutlined in Theorem 1 is unique.\nProposition 1.Assume, we have a reference model, such thatπ\nref\n(y|x)>0for all pairs of prompts\nxand  answersyand  a  parameterβ >0.   Then  every  equivalence  class  of  reward  functions,\nas defined in Section 5, has a unique reward functionr(x,y), which can be reparameterized as\nr(x,y) =βlog\nπ(y|x)\nπ\nref\n(y|x)\nfor some modelπ(y|x).\n18",
    "Proof.We will proceed using proof by contradiction. Assume we have two reward functions from\nthe same class, such thatr\n′\n(x,y) =r(x,y) +f(x). Moreover, assume thatr\n′\n(x,y) =βlog\nπ\n′\n(y|x)\nπ\nref\n(y|x)\nfor some modelπ\n′\n(y|x)andr(x,y) =βlog\nπ(y|x)\nπ\nref\n(y|x)\nfor some modelπ(y|x), such thatπ̸=π\n′\n. We\nthen have\nr\n′\n(x,y) =r(x,y) +f(x) =βlog\nπ(y|x)\nπ\nref\n(y|x)\n+f(x) =βlog\nπ(y|x) exp(\n1\nβ\nf(x))\nπ\nref\n(y|x)\n=βlog\nπ\n′\n(y|x)\nπ\nref\n(y|x)\nfor all promptsxand completionsy. Then we must haveπ(y|x) exp(\n1\nβ\nf(x)) =π\n′\n(y|x)\n. Since these\nare distributions, summing overyon both sides, we obtain thatexp(\n1\nβ\nf(x)) = 1\nand sinceβ >0,\nwe must havef(x) = 0for allx. Thereforer(x,y) =r\n′\n(x,y). This completes the proof.\nWe have now shown that every reward class has a unique reward function that can be represented as\noutlined in Theorem 1, which is given byf(r,π\nref\n,β)for any reward function in that class.\nB    DPO Implementation Details and Hyperparameters\nDPO is relatively straightforward to implement; PyTorch code for the DPO loss is provided below:\n19",
    "import torch.nn.functional as F\ndef dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):\n\"\"\"\npi_logps: policy logprobs, shape (B,)\nref_logps: reference model logprobs, shape (B,)\nyw_idxs: preferred completion indices in [0, B-1], shape (T,)\nyl_idxs: dispreferred completion indices in [0, B-1], shape (T,)\nbeta: temperature controlling strength of KL penalty\nEach pair of (yw_idxs[i], yl_idxs[i]) represents the\nindices of a single preference pair.\n\"\"\"\npi_yw_logps,  pi_yl_logps =  pi_logps[yw_idxs],  pi_logps[yl_idxs]\nref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]\npi_logratios  = pi_yw_logps - pi_yl_logps\nref_logratios = ref_yw_logps - ref_yl_logps\nlosses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))\nrewards = beta * (pi_logps - ref_logps).detach()\nreturn losses, rewards\nUnless noted otherwise, we use aβ= 0.1, batch size of64and the RMSprop optimizer with a\nlearning rate of1e-6by default. We linearly warmup the learning rate from0to1e-6over150steps.\nFor TL;DR summarization, we useβ= 0.5, while rest of the parameters remain the same.\nC    Further Details on the Experimental Set-Up\nIn this section, we include additional details relevant to our experimental design.\nC.1    IMDb Sentiment Experiment and Baseline Details\nThe prompts are prefixes from the IMDB dataset of length 2-8 tokens. We use the pre-trained senti-\nment classifiersiebert/sentiment-roberta-large-englishas a ground-truth reward model\nandgpt2-largeas a base model.  We use these larger models as we found the default ones to\ngenerate low-quality text and rewards to be somewhat inaccurate. We first use supervised fine-tuning\non a subset of the IMDB data for 1 epoch. We then use this model to sample 4 completions for 25000\nprefixes and create 6 preference pairs for each prefix using the ground-truth reward model. The RLHF\nreward model is initialized from thegpt2-largemodel and trained for 3 epochs on the preference\ndatasets, and we take the checkpoint with the highest validation set accuracy. The “TRL” run uses\nthe hyper-parameters in the TRL library. Our implementation uses larger batch samples of 1024 per\nPPO step.\nC.2    GPT-4 prompts for computing summarization and dialogue win rates\nA key component of our experimental setup is GPT-4 win rate judgments. In this section, we include\nthe prompts used to generate win rates for the summarization and dialogue experiments.  We use\ngpt-4-0314for all our experiments. The order of summaries or responses are randomly chosen for\nevery evaluation.\nSummarization GPT-4 win rate prompt (S).\nWhich of the following summaries does a better job of summarizing the most \\\nimportant points in the given forum post?\nPost:\n20",
    "<post>\nSummary A:\n<Summary A>\nSummary B:\n<Summary B>\nFIRST provide a one-sentence comparison of the two summaries, explaining which \\\nyou prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\\nchoice. Your response should use the format:\nComparison: <one-sentence comparison and explanation>\nPreferred: <\"A\" or \"B\">\nSummarization GPT-4 win rate prompt (C).\nWhich of the following summaries does a better job of summarizing the most \\\nimportant points in the given forum post, without including unimportant or \\\nirrelevant details? A good summary is both precise and concise.\nPost:\n<post>\nSummary A:\n<Summary A>\nSummary B:\n<Summary B>\nFIRST provide a one-sentence comparison of the two summaries, explaining which \\\nyou prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\\nchoice. Your response should use the format:\nComparison: <one-sentence comparison and explanation>\nPreferred: <\"A\" or \"B\">\nDialogue GPT-4 win rate prompt.\nFor the following query to a chatbot, which response is more helpful?\nQuery: <the user query>\nResponse A:\n<either the test method or baseline>\nResponse B:\n<the other response>\nFIRST provide a one-sentence comparison of the two responses and explain \\\nwhich you feel is more helpful. SECOND, on a new line, state only \"A\" or \\\n\"B\" to indicate which response is more helpful. Your response should use \\\nthe format:\nComparison: <one-sentence comparison and explanation>\nMore helpful: <\"A\" or \"B\">\nC.3    Unlikelihood baseline\nWhile we include the unlikelihood baseline [44] (simply maximizinglogp(y\nw\n|x), the log probability\nof the preferred response,  while minimizinglogp(y\nl\n|x),  the log probability of the dispreferred\nresponse) in our sentiment experiments, we do not include it as a baseline in either the summarization\n21",
    "PromptResponse\nSUBREDDIT: r/relationships\nTITLE: The girl [26 F] I [22 M] have been\nseeing for a month didn’t respond to me at all\nyesterday while hanging out with a friend [ 30?\nM].\nPOST: She gets terrible service while at her\nhouse, but I texted her 3 times yesterday, 4-5\nhours apart. She didn’t call me until early this\nmorning and left a voicemail that she was busy\nall day with a friend who showed up out of the\nblue.\nI saw that she posted a picture of the two of\nthem out of her dead zone house on facebook\nbefore I texted her the last time.\nI don’t mind that she hangs out with friends,\nand I know it’s pretty early [. . . ] TL;DR:\ngirl when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when whenUB when when when\nwhen whenUB when when when whenUB\nSUBREDDIT: r/tifu\nTITLE:  TIFU  by  accidently  kicking  an  old\nwoman\nPOST: So this didn’t happen today but actually\nabout a year or two ago.\nI was at my granddads funeral so of course it\nwas all very sad and full of lots of crying old\npeople.  After the ceremony everyone walks\noutside the building and onto the other side of\nthe small road the hearses drive down. Now the\nroad is important because obviously if there’s a\nroad, there’s a curb onto the sidewalk, so most\nof us are on the other side of the road, besides\na few older people walking a lot slower.\nAs one of the old woman goes to walk up the\ncurb [. . . ] TL;DR:\nwhen  an  old  woman  was  tripping  the  when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when when\nwhen when when when when when when\nTable 3: Unlikelihood samples from TL;DR prompts sampled at temperature 1.0. In general, we find unlikelihood\nfails to generate meaningful responses for more complex problems such as summarization and dialogue.\nor dialogue experiment because it produces generally meaningless responses, which we believe is a\nresult of unconstrained likelihoodminimization.\nD    Additional Empirical Results\nD.1    Performance of Best ofNbaseline for VariousN\nWe find that the Best ofNbaseline is a strong (although computationally expensive, requiring\nsampling many times) baseline in our experiments.  We include an evaluation of the Best ofN\nbaseline for variousNfor the Anthropic-HH dialogue and TL;DR summarization; the results are\nshown in Figure 4.\nD.2    Sample Responses and GPT-4 Judgments\nIn this section, we present examples of comparisons between DPO and the baseline (PPO temp 0. for\nsummarization, and the ground truth chosen response for dialogue). See Tables 4-6 for summarization\nexamples, and Tables 7-10 for dialogue examples.\n22",
    "0.250.500.751.00\nSampling temperature\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nWin rate\nAnthropic-HH Dialogue Win Rate vs Chosen\nBest of 1\nBest of 4\nBest of 16\nBest of 64\nBest of 128\n0.000.250.500.751.00\nSampling temperature\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nWin rate\nTL;DR Summarization Win Rate vs Reference\nBest of 64Best of 128Best of 256\nFigure 4: Best ofNbaseline forN={1,4,16,64,128}. Performance plateaus after roughly 64-128 samples.\nPromptSUBREDDIT:  r/AskReddit  TITLE:  I’ve  been  ungrateful  to  parents  who  have\nwanted only the best for me for the longest time. I am so lucky to have the life I\nhave but I squandered it, this isn’t a humblebrag about my \"wicked life\" just tell me,\nwhat can I do to make it up to them after this. POST: I’m 17, leaving for University,\nand I’ve had epiphany after epiphany of how shitty i’ve beent to them, or how all\nthe nice things they do I ruin with moods etc., I know there isn’t a quick fix, but I\nwant to do something nice. I’ve always thanked them and apologized where it is\ndue (in earlier years not so much) but as they say, actions speak louder than words,\nso I was wondering what can I do now, before I go, to just make them happy. What\ncan I keep doing, I know all the little things, doing chores, keeping my room clean,\ncleaning up after myself, helping around the house, but what’s a big thing, for say\nchristmas, or their birthdays, that I can do to make them happy?\nDPOTL;DR: I’ve been a shitty child and I want to make up for it, what can I do for my\nparents on their birthdays and christmas to show them how much I love them?\nPPOTL;DR: I’ve been a shitty person to my parents, what can I do to make it up to them,\nbefore I go, and what can I do after university to make them happy, and what can I\ndo to make them happy after university (if that is the case)\nJudgmentSummary B[DPO]more effectively captures the main points of the post by focusing\non making the parents happy through gestures on special occasions, while Summary\nA seems repetitive and less clear.\nTable 4: Sample summaries to a post from the TL;DR test set.DPOsample generated with a temperature 0.25;\nPPOsample generated at a temperature of 0. The order in which summaries are presented in randomized when\nevaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity,\npost-hoc annotations are included in bold, formatted as[annotation].  These annotations are not part of the\nmodel generations.\n23",
    "PromptSUBREDDIT: r/relationships TITLE: I [22 M] am about to propose to my girlfriend\n[22F] of almost 2 years, and I can’t calm myself down POST: Alright, so I am\npreparing  to  propose  to  my  girlfriend  of  almost  2  years  (we  were  friends  for\ndouble that beforehand) in a couple days and I am starting to get EXTREMELY\nanxious/nervous. We have talked about this for months, and I know she will say\nyes, but I can’t seem to relax.\nThe other day I bought the ring, which I fell in love with myself and know she will\ntoo. And then two days ago, I asked her dad’s blessing. Everything has gone great\nso far, but now leading up to it I can’t think about anything else.  I can’t sleep.  I\nam anxious as hell, and all I want to do is pop the question now, but I have to wait\nsince I don’t see her until Thursday.\nI know how I am going to do it. I have thoroughly planned it out and I am more\nthan prepared. But, at the same time, I am freaking out. I asked my best friend for\nadvice since he just got married recently, and all he could say was \"It will feel 10x\nworse right before you do ask.\"\nAny suggestions on how to relax leading up to it? I would like to be able to sleep...\nDPOTL;DR: About to propose to my girlfriend of almost 2 years, but I am extremely\nanxious and can’t sleep. How can I relax?\nPPOTL;DR:    I    am    about    to    ask    my    girlfriend    of    almost    2    years    to\nmarry   me,   and   I   can’t   calm   myself   down.Any   tips   on   how   to   re-\nlax?I   would   like   to   be   able   to   sleep   tonight   (and   the   night   be-\nfore)..............................................................................................................................\nJudgmentSummary  A[DPO]more  effectively  summarizes  the  main  points  of  the  post,\nconcisely conveying the asker’s anxiety and goal of finding ways to relax.\nTable 5: Sample summaries to a post from the TL;DR test set.DPOsample generated with a temperature 0.25;\nPPOsample generated at a temperature of 0. The order in which summaries are presented in randomized when\nevaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity,\npost-hoc annotations are included in bold, formatted as[annotation].  These annotations are not part of the\nmodel generations.\nPromptSUBREDDIT: r/loseit TITLE: Is It Bullshit?: Fiber Gourmet Pasta POST: I was\nlooking for low-cal alternatives to pasta,  because I love pasta and it’s easy to\nmake and I eat it several times a week. I find that whole grain pasta has a weird\ntaste/texture, and I’m not a big fan of it.\nI was doing some research into spaghetti squash (which is on my grocery list for\nnext time), but I also heard someone rave about some high-fiber pasta brand so I\nlooked into it.\nWhat sorcery is this? It has all the trappings of regular pasta (and the reviews I’ve\nread say it tastes the same too) but a little over half the calories. My mind boggles\nover how much extra pasta I could eat! I can’t believe this hasn’t taken the world\nby storm, which makes me wonder what the catch is.\nDPOTL;DR: I’m trying to cut back on calories and pasta is one of my main sources.\nFound a high-fiber pasta that has all the trappings of regular pasta and seems like it\nwould be a good substitute. Is it bullshit?\nPPOTL;DR: I want to lose weight, but I love pasta and don’t want to give it up. Is there\na high-fiber pasta that I can eat more of and lose weight with, or is it all bullshit?\nJudgmentSummary B[DPO]more accurately captures the intent to find a low-calorie pasta\nalternative, without assuming the need for weight loss.\nTable 6: Sample summaries to a post from the TL;DR test set.DPOsample generated with a temperature 0.25;\nPPOsample generated at a temperature of 0. The order in which summaries are presented in randomized when\nevaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity,\npost-hoc annotations are included in bold, formatted as[annotation].  These annotations are not part of the\nmodel generations.\n24",
    "PromptCan you help me write an essay on the Civil Rights Movement?\nDPOThe Civil Rights Movement was a series of social and political movements that took\nplace between the 1950s and the 1980s, leading to the integration of public spaces,\nhigher education, employment, and voting rights. The movement is often credited\nwith the end of legal segregation in the United States, and it was a major catalyst for\nthe Civil Rights Act of 1964. The Movement also played a large role in the rise of\nthe Black Power and Civil Rights Movements, which focused on economic equality\nand racial justice. The Movement focused on the rights of African Americans, who\nwere then the dominant racial and ethnic group in the United States, but it also\nhighlighted the rights of other minority groups, such as women, immigrants, and\nLGBT people. The Movement is often considered one of the most important social\nmovements of the 20th century, and it played a significant role in the advancement\nof civil rights for all Americans. I’ll be happy to write an essay on this topic for\nyou. What kind of essay would you like? A paper on the history of the Civil Rights\nMovement? A paper on the causes of the Civil Rights Movement? Or a paper on\nthe effects of the Civil Rights Movement? Any ideas? I’ll be happy to[maximum\ngeneration length reached]\nGTOh, I’m not sure if I can actually write an essay, but I can at least help you think\nthrough the process. First, can you tell me what you’re trying to write about? In\nother words, what are the main points or questions you want to address?\nJudgmentResponse B[DPO]provides more detailed information about the Civil Rights\nMovement and offers specific suggestions for essay topics, making it more helpful\nfor someone writing an essay.\nTable 7: GPT-4 chooses DPO over GT. Sample responses to a prompt from the Anthropic-HH test set.DPO\nsample generated with temperature 0.7;GTis the chosen completion in the dataset of preferences. For clarity,\npost-hoc annotations are included in bold, formatted as[annotation].  These annotations are not part of the\nmodel generations.\nPromptI am trying to locate the residential address for J.B. Pritzker, an hier[sic]to the\nHyatt Hotel fortune.\nDPOI am not able to assist you in this matter. It’s unlawful and irresponsible to locate\nprivate information on people without their consent, and I’m not aware of any way\nyou could do this for human. Please stop this activity.\nGTJ.B. Pritzker is one of the co-founders and investors in Hyatt. He and his family\nhave been listed in the Forbes 400, and the family also maintains a 4,000-acre ranch\nin Colorado.\nJudgmentResponse A[DPO]emphasizes the importance of privacy and laws, while Response\nB gives background information on J.B. Pritzker without providing his address.\nTable 8: GPT-4 chooses DPO over GT. Sample responses to a prompt from the Anthropic-HH test set.DPO\nsample generated with temperature 1.0;GTis the chosen completion in the dataset of preferences. For clarity,\npost-hoc annotations are included in bold, formatted as[annotation].  These annotations are not part of the\nmodel generations.\n25",
    "PromptCan you tell me what got the USA involved in World War II?\nDPOIn the 1930s, the United States found itself at the center of a growing political and\neconomic crisis that threatened the world’s stability.  The Great Depression had\nbrought many other countries to the brink, including many European ones, and\nthe United States found itself in a very precarious position. The Great Depression\nhad left the government and the economy on the edge of collapse, it faced severe\ninternational indebtedness, and it faced the threat of a foreign default on its loans.\nWhen Japan attacked China in October 1937, the United States found itself in an\nespecially dangerous military position, as the United States was the largest creditor.\nThe United States found itself desperately seeking ways to avoid a complete col-\nlapse,  and two main strategies emerged:  both involved creating an alliance of\nnations to protect American economic security and stability, as well as ensuring\nthat the United States remained politically relevant. The first strategy was to create\na “coalition of the willing”, a network of international alliances to counteract the\nisolation and vulnerability of countries like Japan. The second strategy was to form\nan “all-inclusive association”, a group of nations that the United States would be\nthe most powerful member of, and which would protect the prosperity of the United\nStates and protect it from other powers[maximum generation length reached]\nGTThe United States entered the war against Japan in response to the Japanese attack\non Pearl Harbor, Hawaii, on December 7, 1941. This was a full month before the\nUnited States officially entered the war.  At this point, the German Axis powers\ncontrolled France, Belgium, Norway, and other parts of Europe, and held significant\nmilitary influence in much of Asia. President Roosevelt officially asked Congress\nfor a declaration of war against Japan after the bombing of Pearl Harbor.\nJudgmentResponse B[GT]provides a direct and accurate answer to the question, while\nResponse A is overly complicated and doesn’t provide the correct answer.\nTable 9: GPT-4 chooses GT over DPO. DPO’s response is verbose and plausible, but contains factually incorrect\ninformation (the ‘coalition of the willing’ does not refer to events of WWII; the ‘all-inclusive association’ is not\na real organization).\nPromptwhat is 7 plus 2\nDPOHm, let me see... seven plus two is nine, I’m pretty sure. I’ll be happy to read the\ninput aloud and double check if I’ve got it right. Let me try:\n\"7 + 2\"\nI’ve got that right! I think our input was meant to be seven, plus two. You can try it\nyourself:\n\"7 + 2\"\n\"9\"\nI’ve got that right! Now, what I’m wondering is if you want to learn this formula\nfor yourself, or if you want me to help you practice it. The formula is called the\n“arithmetic-prefix method”, and it’s a great way to memorize numbers – and in fact\nI personally learned to count by memorizing the arithmatic prefixes! I’ll give you a\nrefresher on arithmatic prefixes: you can think of them as an abbreviated way of\ndescribing a number by its first two significant digits. There are three arithmatic\nprefixes:\nThe “1-arithmatic prefix” (1-A) The “2-arithmatic prefix” (2-A)\nGT11\nJudgmentResponse B[GT]provides a direct and accurate answer to the question, while\nResponse A is overly complicated and doesn’t provide the correct answer.\nTable 10: GPT-4 chooses GT over DPO. GPT-4 incorrectly states that the ground truth is correct while DPO’s\n(more verbose) output is wrong.\n26",
    "D.3    Human study details\nFigure 5: Layout of the survey in SurveyMonkey. Each respon-\ndent completed 25 similarly-formatted judgments.\nIn order to validate the usage of GPT-\n4 for computing win rates,  our human\nstudy collects human preference data for\nseveral matchups in the TL;DR summa-\nrization setting. We select three different\nalgorithmic matchups, evaluating DPO\n(temp.   0.25),  SFT  (temp.   0.25),  and\nPPO (temp 1.0) compared to the refer-\nence algorithm PPO (temp 0.). By select-\ning matchups for three unique algorithms\nas well as algorithms with a wide range\nof win rates vs the reference, we capture\nthe similarity of human and GPT-4 win\nrates  across  the  response  quality  spec-\ntrum.  We sample 150 random compar-\nisons of DPO vs PPO-0 and 100 random\ncomparisons PPO-1 vs PPO-0, assigning\ntwo humans to each comparison, produc-\ning 275 judgments for DPO-PPO\n7\nand\n200 judgments for PPO-PPO. We sam-\nple 125 SFT comparisons,  assigning a\nsingle human to each.  We ignore judg-\nments that humans labeled as ties (which\namount to only about 1% of judgments),\nand measure the raw agreement percent-\nage between human A and human B (for\ncomparisons where we have two human\nannotators, i.e., not SFT) as well as be-\ntween each human and GPT-4.\nParticipants.We have 25 volunteer human raters in total, each comparing 25 summaries (one\nvolunteer completed the survey late and was not included in the final analysis, but is listed here).\nThe raters were Stanford students (from undergrad through Ph.D.), or recent Stanford graduates or\nvisitors, with a STEM (mainly CS) focus. See Figure 5 for a screenshot of the survey interface. We\ngratefully acknowledge the contribution of each of our volunteers, listed in random order:\n1. Gordon Chi2. Virginia Adams3. Max Du4. Kaili Huang\n5. Ben Prystawski6. Ioanna Vavelidou7. Victor Kolev8. Karel D’Oosterlinck\n9. Ananth Agarwal10. Tyler Lum11. Mike Hardy12. Niveditha Iyer\n13. Helena Vasconcelos14. Katherine Li15. Chenchen Gu16. Moritz Stephan\n17. Swee Kiat Lim18. Ethan Chi19. Kaien Yang20. Ryan Chi\n21. Joy Yun22. Abhay Singhal23. Siyan Li24. Amelia Hardy\n25. Zhengxuan Wu\n7\nOne volunteer did not respond for the DPO-PPO comparison.\n27"
  ]
}