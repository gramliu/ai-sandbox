{
  "key": "852URF45",
  "url": "http://arxiv.org/pdf/2307.13854",
  "metadata": {
    "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
    "abstract": "  With advances in generative AI, there is now potential for autonomous agents\nto manage daily tasks via natural language commands. However, current agents\nare primarily created and tested in simplified synthetic environments, leading\nto a disconnect with real-world scenarios. In this paper, we build an\nenvironment for language-guided agents that is highly realistic and\nreproducible. Specifically, we focus on agents that perform tasks on the web,\nand create an environment with fully functional websites from four common\ndomains: e-commerce, social forum discussions, collaborative software\ndevelopment, and content management. Our environment is enriched with tools\n(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage\nhuman-like task-solving. Building upon our environment, we release a set of\nbenchmark tasks focusing on evaluating the functional correctness of task\ncompletions. The tasks in our benchmark are diverse, long-horizon, and designed\nto emulate tasks that humans routinely perform on the internet. We experiment\nwith several baseline agents, integrating recent techniques such as reasoning\nbefore acting. The results demonstrate that solving complex tasks is\nchallenging: our best GPT-4-based agent only achieves an end-to-end task\nsuccess rate of 14.41%, significantly lower than the human performance of\n78.24%. These results highlight the need for further development of robust\nagents, that current state-of-the-art large language models are far from\nperfect performance in these real-life tasks, and that WebArena can be used to\nmeasure such progress.\n",
    "published": "2023-07-25T22:59:32Z"
  },
  "text": [
    "Published as a conference paper at ICLR 2024\nWEBARENA:  A  REALISTICWEBENVIRONMENT  FOR\nBUILDINGAUTONOMOUSAGENTS\nShuyan Zhou\n∗\nFrank F. Xu\n∗\nHao Zhu\n†\nXuhui Zhou\n†\nRobert Lo\n†\nAbishek Sridhar\n†\nXianyi Cheng\nTianyue OuYonatan BiskDaniel FriedUri AlonGraham Neubig\nCarnegie Mellon University\n{shuyanzh, fangzhex, gneubig}@cs.cmu.edu\nABSTRACT\nWith advances in generative AI, there is now potential for autonomous agents\nto manage daily tasks via natural language commands. However, current agents\nare primarily created and tested in simplified synthetic environments, leading to\na disconnect with real-world scenarios.  In this paper, we build an environment\nfor language-guided agents that ishighly realisticandreproducible. Specifically,\nwe focus on agents that perform tasks on the web, and create an environment\nwith fully functional websites from four common domains: e-commerce, social\nforum discussions, collaborative software development, and content management.\nOur environment is enriched with tools (e.g.,a map) and external knowledge\nbases (e.g.,user manuals) to encourage human-like task-solving. Building upon\nour environment, we release a set of benchmark tasks focusing on evaluating the\nfunctional correctnessof task completions. The tasks in our benchmark are diverse,\nlong-horizon, and designed to emulate tasks that humans routinely perform on the\ninternet. We experiment with several baseline agents, integrating recent techniques\nsuch as reasoning before acting.  The results demonstrate that solving complex\ntasks is challenging:  our bestGPT-4-based agent only achieves an end-to-end\ntask success rate of 14.41%, significantly lower than the human performance of\n78.24%. These results highlight the need for further development of robust agents,\nthat current state-of-the-art large language models are far from perfect performance\nin these real-life tasks, and thatWebArenacan be used to measure such progress.\nOur code, data, environment reproduction resources, and video demonstrations are\npublicly available athttps://webarena.dev/.\n1INTRODUCTION\nAutonomous agents that perform everyday tasks via human natural language commands could\nsignificantly augment human capabilities, improve efficiency, and increase accessibility. Nonetheless,\nto fully leverage the power of autonomous agents, it is crucial to understand their behavior within an\nenvironment that is bothauthenticandreproducible. This will allow measurement of the ability of\nagents on tasks that human users care about in a fair and consistent manner.\nCurrent environments for evaluate agents tend toover-simplifyreal-world situations. As a result, the\nfunctionality of many environments is a limited version of their real-world counterparts, leading to\na lack of task diversity (Shi et al., 2017; Anderson et al., 2018; Gordon et al., 2018; Misra et al.,\n2016; Shridhar et al., 2020; 2021; Yao et al., 2022a). In addition, these simplifications often lower\nthe complexity of tasks as compared to their execution in the real world (Puig et al., 2018; Shridhar\net al., 2020; Yao et al., 2022a). Finally, some environments are presented as a static resource (Shi\net al., 2017; Deng et al., 2023) where agents are confined to accessing only those states that were\npreviously cached during data collection, thus limiting the breadth and diversity of exploration.\nFor evaluation, many environments focus on comparing the textualsurface formof the predicted\n∗\nLead contributors.\n†\nEqual contribution.\n1\narXiv:2307.13854v4  [cs.AI]  16 Apr 2024",
    "Published as a conference paper at ICLR 2024\nTell me how much I spent on \nfood purchase in March 2023\n15\nWebArena Environment\n”\n”\n“\n“\nCreate a ‘NolanFans' repo, \nlisting Nolan's Oscar-winning \nfilms in a README file\nAction\nFeedback\nAI Agent\nTool Sites\nKnowledge resources\nWeb applications from popular domains\ncheck_repo \ncheck_readme \ncheck_answer\nFunctional \nSuccess\nFunctional \nFailure\nFigure 1:WebArenais a standalone, self-hostable web environment for building autonomous agents.\nWebArenacreates websites from four popular categories with functionality and data mimicking\ntheir real-world equivalents. To emulate human problem-solving,WebArenaalso embeds tools and\nknowledge resources as independent websites.WebArenaintroduces a benchmark on interpreting\nhigh-level realisticnatural language command to concrete web-based interactions.  We provide\nvalidators to programmatically validate the functional correctness of each task.\naction sequences with reference action sequences, disregarding thefunctional correctnessof the\nexecutions and possible alternative solutions (Puig et al., 2018; Jernite et al., 2019; Xu et al., 2021; Li\net al., 2020; Deng et al., 2023). These limitations often result in a discrepancy between simulated\nenvironments and the real world, and can potentially impact the generalizability of AI agents to\nsuccessfully understand, adapt, and operate within complex real-world situations.\nWe introduceWebArena, arealisticandreproducibleweb environment designed to facilitate the\ndevelopment of autonomous agents capable of executing tasks (§2).  An overview ofWebArena\nis in Figure 1.  Our environment comprises four fully operational, self-hosted web applications,\neach representing a distinct domain prevalent on the internet: online shopping, discussion forums,\ncollaborative development, and business content management. Furthermore,WebArenaincorporates\nseveral utility tools, such as map, calculator, and scratchpad, to best support possible human-like task\nexecutions. Lastly,WebArenais complemented by an extensive collection of documentation and\nknowledge bases that vary from general resources like English Wikipedia to more domain-specific\nreferences, such as manuals for using the integrated development tool (Fan et al., 2022). The content\npopulating these websites is extracted from their real-world counterparts, preserving the authenticity\nof the content served on each platform. We deliver the hosting services using Docker containers with\ngym-APIs (Brockman et al., 2016), ensuring both the usability and the reproducibility ofWebArena.\nAlong withWebArena, we release a ready-to-use benchmark with 812 long-horizon web-based\ntasks (§3).  Each task is described as a high-level natural language intent, emulating the abstract\nlanguage usage patterns typically employed by humans (Bisk et al., 2019).  Two example intents\nare shown in the upper left of Figure 1. We focus on evaluating thefunctional correctnessof these\ntasks,i.e.,does the result of the execution actually achieve the desired goal (§3.2).  For instance,\nto evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the\ndesignated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021;\nWang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng\net al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is\na ubiquitous phenomenon in sufficiently complex tasks.\nWe use this benchmark to evaluate several agents that can follow NL command and perform web-\nbased tasks (§4).  These agents are implemented in a few-shot in-context learning fashion with\npowerful large language models (LLMs) such asGPT-4andPALM-2. Experiment results show that\nthe bestGPT-4agent performance is somewhat limited, with an end-to-end task success rate of only\n14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of\ncurrent LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery\nto successfully perform complex tasks  (§5.1). These outcomes underscore the necessity for further\ndevelopment towards robust and effective agents (LeCun, 2022) inWebArena.\n2WEBARENA\n: WEBSITES  AS  ANENVIRONMENT  FORAUTONOMOUSAGENTS\nOur goal is to create arealisticandreproducibleweb environment. We achieve reproducibility by\nmaking the environment standalone, without relying on live websites. This circumvents technical\n2",
    "Published as a conference paper at ICLR 2024\nSearch for museums  \nin Pittsburgh\nwebarena.wikipedia.com\nSearch for each art \nmuseum on the Map\nwebarena.openstreetmap.com\nRecord the optimized \nresults to the repo\nwebarena.gitlab.com\n...\nCreate an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance \nstarting from Schenley Park. Log the order in my “awesome-northeast-us-travel” repository\n“\n”\nFigure 2: A high-level task that can be fully executed inWebArena. Success requires sophisticated,\nlong-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh\nart museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and\n(3) update the README file in the appropriate repository with the planned route.\nchallenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and\nconfiguration changes, which obstruct a fair comparison across different systems over time.  We\nachieve realism by using open-source libraries that underlie many in-use sites from several popular\ncategories and importing data to our environment from their real-world counterparts.\n2.1CONTROLLINGAGENTS THROUGHHIGH-LEVELNATURALLANGUAGE\nTheWebArenaenvironment is denoted asE=⟨S,A,O,T⟩with state spaceS, action spaceA(§2.4)\nand observation spaceO(§2.3).  The transition functionT:S ×A−→ Sis deterministic, and\nit is defined by the underlying implementation of each website in the environment.  Given a task\ndescribed as a natural language intenti, an agent issues an actiona\nt\n∈ Abased on intenti, the\ncurrent observationo\nt\n∈O, the action historya\nt−1\n1\nand the observation historyo\nt−1\n1\n. Consequently,\nthe action results in a new states\nt+1\n∈Sand its corresponding observationo\nt+1\n∈O. We propose\na reward functionr(a\nT\n1\n,s\nT\n1\n)to measure the success of a task execution, wherea\nT\n1\nrepresents the\nsequence of actions from start to the end time stepT, ands\nT\n1\ndenotes all intermediate states. This\nreward function assesses if state transitions align with the expectations of the intents. For example,\nwith an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates\nthe accuracy of the agent’s actions, such as checking the correctness of the predicted answer.\n2.2WEBSITESELECTION\nTo decide which categories of websites to use, we first analyzed approximately 200 examples from the\nauthors’ actual web browser histories. Each author delved into their browsing histories, summarizing\nthe goal of particular segments of their browser session.  Based on this, we classified the visited\nwebsites into abstract categories. We then identified the four most salient categories and implemented\none instance per category based on this analysis:  (1) E-commerce platforms supporting online\nshopping activities (e.g.,Amazon, eBay), (2) social forum platforms for opinion exchanges  (e.g.,\nReddit, StackExchange), (3) collaborative development platforms for software development (e.g.,\nGitLab), and (4) content management systems (CMS) that manage the creation and revision of the\ndigital content (e.g.,online store management).\nIn addition to these platforms, we selected three utility-style tools that are frequently used in web-\nbased tasks: (1) a map for navigation and searching for information about points of interest (POIs)\nsuch as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information-\nseeking and knowledge acquisition are critical in web-based tasks, we also incorporated various\nknowledge resources intoWebArena. These resources range from general information hubs, such\nas the English Wikipedia, to more specialized knowledge bases, such as the website user manuals.\nImplementationWe leveraged open-source libraries relevant to each category to build our own\nversions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage-\nment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their\n3",
    "Published as a conference paper at ICLR 2024\n<li> \n  <div> \n     <a href=\"...\"><img src=\"...\"></a> \n     <div class> \n        <a href=\"...\">Outdoor Patio ...\n</a> \n        <div> \n              <span>Rating:</span> \n              <div> \n                 <span>82%</span> \n              </div> \n              <a href=“...#reviews\">12 \n<span>Reviews</span></a> \nwebarena.onestopshop.com\nwebarena.onestopshop.com\nRootWebArea ‘Patio, Lawn ..’ \n  link 'Image' \n     img 'Image' \n  link 'Outdoor Patio..’ \n  LayoutTable '' \n      StaticText 'Rating:' \n      generic '82%' \n      link '12 Reviews' \n  StaticText ‘$49.99' \n  button 'Add to Cart’ focusable: True \n  button 'Wish List’ focusable: ... \n  button 'Compare’ focusable: ...\nwebarena.onestopshop.com\nFigure 3: We design the observation to be the URL and the content of a web page, with options to\nrepresent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right).\nThe content of the middle and right figures are trimmed to save space.\nreal-world counterparts. As an example, our version of GitLab was developed based on the actual\nGitLab project.\n1\nWe carefully emulated the features of a typical code repository by including both\npopular projects with many issues and pull requests and smaller, personal projects.  Details of all\nwebsites inWebArenacan be found in Appendix A.1. We deliver the environment as dockers and\nprovide scripts to reset the environment to a deterministic initial state (See Appendix A.2).\n2.3OBSERVATIONSPACE\nWe design the observation space to roughly mimic the web browser experience: a web page URL, the\nopened tabs , and the web page content of the focused tab.WebArenais the first web environment\nto consider multi-tab web-based tasks to promote tool usage, direct comparisons and references\nacross tabs, and other functionalities. The multi-tab functionality offers a more authentic replication\nof human web browsing habits compared to maintaining everything in a single tab.  We provide\nflexible configuration to render the page content in many modes: (see Figure 3 for an example): (1)\nthe raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used\nin past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based\nrepresentation that represents the current web page as an RGB array and (3) the accessibility tree of\nthe web page.\n2\nThe accessibility tree is a subset of the DOM tree with elements that arerelevantand\nusefulfor displaying the contents of a web page. Every element is represented as its role (e.g.,a link),\nits text content, and its properties (e.g.,whether it is focusable). Accessibility trees largely retain the\nstructuredinformation of a web page while being more compact than the DOM representation.\nWe provide an option to limit the content to the contents within a viewport for all modes.  This\nensures that the observation can be input into a text-based model with limited context length or an\nimage-based model with image size or resolution requirements.\n2.4ACTIONSPACE\nFollowing previous work on navigation and operation in web and embodied environments (Shi et al.,\n2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse\noperations available on web pages.  Figure 4 lists all the available actions categorized into three\ndistinct groups. The first group includes element operations such as clicking, hovering, typing, and\nkey combination pressing. The second comprises tab-related actions such as opening, closing, and\nswitching between tabs. The third category consists of URL navigation actions, such as visiting a\nspecific URL or navigating forward and backward in the browsing history.\nBuilding on these actions,WebArenaprovides agents with the flexibility to refer to elements for\noperation in different ways. An element can be selected by its on-screen coordinates,(x, y), or by\na unique element ID that is prepended to each element.  This ID is generated when traversing the\nDocument Object Model (DOM) or accessibility tree. With element IDs, the element selection is\ntransformed into ann-way classification problem, thereby eliminating any disambiguation efforts\nrequired from the agent or the underlying implementation. For example, issuing the actionclick\n[1582]clicks the button given the observation of[1582] Add to Cart. This flexible element\nselection allowsWebArenato support agents designed in various ways (e.g.,accepting input from\ndifferent modalities) without compromising fair comparison metrics such as step count.\n1\nhttps://gitlab.com/gitlab-org/gitlab\n2\nhttps://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree\n4",
    "Published as a conference paper at ICLR 2024\nAction TypeDescription\nnoopDo nothing\nclick(elem)Click at an element\nhover(elem)Hover on an element\ntype(elem, text)Type to an element\npress(key_comb)\nPress a key comb\nscroll(dir)Scroll up and down\ntab_focus(index)focus oni-th tab\nnew_tabOpen a new tab\ntab_closeClose current tab\ngo_backVisit the last URL\ngo_forwardUndogo_back\ngoto(URL)\nGo to URL\nFigure 4: Action Space ofWebArena\nCategoryExample\nInformation\nSeeking\nWhen was the last time I bought shampoo\nCompare walking and driving time\nfrom AMC Waterfront to Randyland\nSite\nNavigation\nCheckout merge requests assigned to me\nShow me the ergonomic chair\nwith the best rating\nContent\n&\nConfig\nPost to ask “whether I need a car in NYC”\nDelete the reviews from the scammer Yoke\nFigure 5: Example intents from three categories.\nUser Role SimulationUsers of the same website often have disparate experiences due to their\ndistinctroles,permissions, andinteraction histories. We emulate this scenario by generating unique\nuser profiles on each platform. The details can be found in Appendix A.3.\n3BENCHMARKSUITE OFWEB-BASEDTASKS\nWe provide a benchmark with 812 test examples on grounding high-level natural language instructions\nto interactions inWebArena. Each example has a metric to evaluate the functional correctness of\nthe task execution.  In this section, we first formally define the task of controlling an autonomous\nagent through natural language. Then we introduce the annotation process of our benchmark.\n3.1INTENTCOLLECTION\nWe focus on curatingrealisticintents to carry outcomplexandcreativetasks withinWebArena. To\nstart with, our annotators were guided to spend a few minutes exploring the websites to familiarize\nthemselves with the websites’ content and functionalities.  As most of our websites are virtually\nidentical to their open-web counterparts, despite having sampled data, most annotators can quickly\ncomprehend the websites.\nNext, we instructed the annotators to formulate intents based on the following criteria:\n(1)The intent should beabstractandhigh-level, implying that the task cannot be fulfilled with\nmerely one or two actions.  As an example, instead of “click thesciencesubreddit”, we\nencouraged annotators to come up with something more complex like “post a greeting message\nonsciencesubreddit”, which involves performing multiple actions.\n(2)The intent should becreative. Common tasks such as account creation can be easily thought of.\nWe encouraged the annotators to add constraints (e.g.,“create a Reddit accountidentical to my\nGitLab one”) to make the intents more unique.\n(3)The intent should be formulated as atemplateby making replaceable elements as variables.\nThe annotators were also responsible for developing several instantiations for each variable.\nFor example, the intent “create a Reddit account identical to my GitLab one” can be converted\ninto “create a {{site1}} account identical to my {{site2}} one”, with an instantiation like “{site1:\nReddit, site2: GitLab}” and another like “{site1: GitLab, site2: OneStopShopping}”. Notably,\ntasks derived from the same template can have distinct execution traces. The similarity resides\nprimarily in the high-level semantics rather than the specific implementation.\nWe also provided a prompt for the annotators to use with ChatGPT\n3\nfor inspiration, that contains an\noverview of each website and instructs the model to describe potential tasks to be performed on these\nsites. Furthermore, we offered a curated list of examples for annotators to reference.\nIntent AnalysisIn total, we curated 241 templates and 812 instantiated intents. On average, each\ntemplate is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. Furthermore,\nwe classify the intents into three primary categories with examples shown in Figure 5:\n3\nhttps://chat.openai.com/\n5",
    "Published as a conference paper at ICLR 2024\n(1)Information-seekingtasks expect a textual response. Importantly, these tasks inWebArena\noften require navigation across multiple pages or focus onuser-centriccontent.  This makes\nthem distinct from open-domain question-answering  (Yang et al., 2018; Kwiatkowski et al.,\n2019), which focuses on querying general knowledge with a simple retrieval step. For instance,\nto answer “When was the last time I bought the shampoo”, an agent traverses the user’s purchase\nhistory, checking order details to identify the most recent shampoo purchase.\n(2)Site navigation: This category is composed of tasks that require navigating through web pages\nusing a variety of interactive elements such as search functions and links. The objective is often\nto locate specific information or navigate to a particular section of a site.\n(3)\nContent and configuration operation: This category encapsulates tasks that require operating\nin the web environment to create, revise, or configure content or settings. This includes adjusting\nsettings, managing accounts, performing online transactions, generating new web content, and\nmodifying existing content. Examples range from updating a social media status or README\nfile to conducting online purchases and configuring privacy settings.\n3.2EVALUATIONANNOTATION\nEvaluating Information Seeking TasksTo measure the correctness of information-seeking tasks\nwhere a textual answer is expected, we provide the annotated answera\n∗\nfor each intent. Thea\n∗\nis\nfurther compared with the predicted answerˆawith one of the following scoring functionsr\ninfo\n(ˆa, a\n∗\n).\nFirst, we defineexact_matchwhere onlyˆathat is identical witha\n∗\nreceives a score of one. This\nfunction is primarily applicable to intent types whose responses follow a more standardized format,\nsimilar to the evaluation on question answering literature (Rajpurkar et al., 2016; Yang et al., 2018).\nSecond, we createmust_includewhere anyˆacontaininga\n∗\nreceives a score of one. This function\nis primarily used in when an unordered list of text is expected or where the emphasis of evaluation is\non certain key concepts. In the second example in Table 1, we expect both the correct name and the\nemail address to be presented, irrespective of the precise wording used to convey the answer.\nFinally, we introducefuzzy_matchwhere we utilize a language model to assess whetherˆais\nsemantically equivalent toa\n∗\n.  Specifically, in this work, we usegpt-4-0613to perform this\nevaluation. The corresponding prompt details are provided in Appendix A.7. Thefuzzy_match\nfunction applies to situations where the format of the answer is diverse. For instance, in responding\nto “Compare the time for walking and driving route from AMC Waterfront to Randyland”, it is\nessential to ensure that driving time and walking time are accurately linked with the correct terms.\nThefuzzy_matchfunction could also flexibly match the time “2h58min” with different forms\nsuch as “2 hour 58 minutes”, “2:58” and others. We demonstrate a language model can achieve nearly\nperfect performance on this task in §A.8.\nEvaluating Site Navigation and Content & Config TasksThe tasks in these categories require\naccessing web pages that meet certain conditions or performing operations that modify the underlying\ndata storage of the respective websites.   To assess these,  we establish reward functionsr\nprog\n(s)\nthat programmatically examine the intermediate statesswithin an execution trajectory to ascertain\nwhether the outcome aligns with the intended result. These intermediate states are often the underlying\ndatabases of the websites, the status, and the content of a web page at each step of the execution.\nEvaluating each instance involves two components.  First, we provide alocator, tasked with\nretrieving the critical content pertinent to each intent. The implementation of this locator varies from\na database query, a website-supported API call, to a JavaScript element selection on the relevant web\npage, depending on implementation feasibility. For example, the evaluation process for the intent of\nthe fifth example in Table 1, first obtains the URL of the latest post by examining the last state in the\nstate sequences. Then it navigates to the corresponding post page and obtains the post’s content by\nrunning the Javascript“document.querySelector(‘.submission__inner’).outerText”.\nSubsequently, we annotatekeywordsthat need to exist within the located content. For example,\nthe evaluation verifies if the post is correctly posted in the “nyc” subreddit by examining the URL of\nthe post and if the post contains the requested content by examining the post content. We reuse the\nexact_matchandmust_includefunctions from information-seeking tasks for this purpose.\n6",
    "Published as a conference paper at ICLR 2024\nFunctionIDIntentEval Implementation\nr\ninfo\n(a\n∗\n,ˆa)\n1\nTell me the name of the customer who\nhas the most cancellations in the history\nexact_match(ˆa, “Samantha Jones”)\n2\nFind the customer name and\nemail with phone number 8015551212\nmust_include(ˆa, “Sean Miller”)\nmust_include(ˆa, “sean@gmail.com”)\n3\nCompare walking and driving time\nfrom AMC Waterfront to Randyland\nfuzzy_match(ˆa, “walking: 2h58min”)\nfuzzy_match(ˆa, “driving: 21min”)\nr\nprog\n(s)\n4\nCheckout merge requests\nassigned to me\nurl=locate_current_url(s)\nexact_match(URL, “gitlab.com/merge_\nrequests?assignee_username=byteblaze”)\n5\nPost to ask “whether I\nneed a car in NYC”\nurl=locate_latest_post_url(s)\nbody=locate_latest_post_body(s)\nmust_include(URL, “/f/nyc”)\nmust_include(body,“a car in NYC”)\nTable 1: We introduce two evaluation approaches.r\ninfo\n(top) measures the correctness of performing\ninformation-seeking tasks.  It compares the predicted answerˆawith the annotated referencea\n∗\nwith three implementations.r\nprog\n(bottom) programmatically checks whether the intermediate states\nduring the executions possess the anticipated properties specified by the intent.\nUnachievable TasksDue to constraints such as inadequate evidence, user permissions (§A.3),\nor the absence of necessary functional support on the website, humans may ask for tasks that are\nnot possible to complete. Inspired by previous work on evaluating question-answering models on\nunanswerable questions (Rajpurkar et al., 2018), we design unachievable tasks inWebArena. For\ninstance, fulfilling an intent like “Tell me the contact number of OneStopShop” is impracticable\ninWebArena, given that the website does not provide such contact information.  We label such\ninstances as \"N/A\" and expect an agent to produce an equivalent response. These examples allow us\nto assess an agent’s ability to avoid making unfounded claims and its adherence to factual accuracy.\nAnnotation ProcessThe intents were contributed by the authors following the annotation guideline\nin §3.1.  Every author has extensive experience with web-based tasks.  The reference answers to\nthe information-seeking tasks were curated by the authors and an external annotator.  To ensure\nconsistency and accuracy, each question was annotated twice.  If the two annotators disagreed, a\nthird annotator finalized the annotation.  The programs to evaluate the remaining examples were\ncontributed by three of the authors who are proficient in JavaScript programming.  Difficult tasks\nwere often discussed collectively to ensure the correctness of the annotation. The annotation required\nthe annotator to undertake the full execution and scrutinize the intermediate states.\nAvg. Time110s\nSuccess Rate\ninfo\n74.68%\nSuccess Rate\nothers\n81.32%\nSuccess Rate\nall\n78.24%\nHuman PerformanceWe sample one task from each of the 170 tem-\nplates and ask five computer science graduate students to perform these\ntasks.   The  human  performance  is  on  the  right.   Overall,  the  human\nannotators complete 78.24% of the tasks, with lower performance on\ninformation-seeking tasks. Through examining the recorded trajectories,\nwe found that 50% of the failures are due to misinterpreting the intent (e.g.,providing travel distance\nwhen asked for travel time), incomplete answers (e.g.,providing only name when asked for name and\nemail), and incomplete executions (e.g.,partially filling the product information), while the remaining\ninstances have more severe failures, where the executions are off-target. More discussions on human\nannotations can be found in §A.5.\n4BASELINEWEBAGENTS\nWe experiment with three LLMs using two prompting strategies, both with two examples in the\ncontext.  In the first setting, we ask the LLM to directly predict the next action given the current\nobservation, the intent and the previously performed action.  In the second setting, with the same\ninformation, the model first performs chain-of-thought reasoning steps in the text before the action\nprediction (CoT, Wei et al. (2022); Yao et al. (2022b)). Before the examples, we provide a detailed\noverview of the browser environment, the allowed actions, and many rules. To make the model aware\nof the unachievable tasks, the instruction explicitly asks the agent to stop if it believes the task is\nimpossible to perform. We refer to this directive as Unachievable hint, orUA hint. This introduction\n7",
    "Published as a conference paper at ICLR 2024\nis largely identical to the guidelines we presented to human annotators to ensure a fair comparison.\nWe use an accessibility tree with element IDs as the observation space. The agent can identify which\nelement to interact with by the ID of the element. For instance, the agent can issueclick [1582]\nto click the “Add to Cart” button with the ID of 1582. The full prompts can be found in Appendix\nA.9. The detailed configurations of each model can be found in Appendix A.6.\n5RESULTS\nCoT UA HintModelSRSR\nAC\nSR\nUA\n✓✓TEXT-BISON-0015.054.0027.78\n✗✓GPT-3.56.414.9038.89\n✓✓GPT-3.58.756.4458.33\n✓✓GPT-411.708.6377.78\n✗✗GPT-3.55.104.908.33\n✓✗GPT-3.56.166.068.33\n✓✗GPT-414.4113.0244.44\n-✓Human78.2477.30100.00\nTable 2: The end-to-end task success rate (SR %) on\nWebArenawith different prompting strategies.CoT:\nthe model performs step-by-step reasoning before is-\nsuing the action.UA hint: ask the model to stop when\nencountering unachievable questions.\nThe  main  results  are  shown  on  the  top  of\nTable 2.GPT-4(OpenAI, 2023) with CoT\nprompting achieves a modest end-to-end task\nsuccess  rate  of  11.70%,  which  is  signifi-\ncantly  lower  than  the  human  performance\nof 78.24%.GPT-3.5(OpenAI, 2022) with\nCoT prompting is only able to successfully\nperform  8.75%  of  the  tasks.   The  explicit\nreasoning procedure is somewhat helpful, it\nbrings 2.34% improvement over the version\nwithout it. Further,TEXT-BISON-001(Anil\net al., 2023) underperformsGPT-3.5, with\na success rate of 5.05%. These results under-\nline the inherent challenges and complexities\nof executing tasks that span long horizons,\nparticularly in realistic environments such as\nWebArena.\n5.1ANALYSIS\nDo models know when to stop?In our error analysis of the execution trajectories, we observe\na prevalent error pattern of early stopping due to the model’s conclusion of unachievability.  For\ninstance,GPT-4erroneously identifies 54.9% of feasible tasks as impossible. This issue primarily\nstems from the UA hint in the instruction, while this hint allows models to identify unachievable\ntasks, it also hinders performance on achievable tasks. To address this, we conduct an ablation study\nwhere we remove this hint. We then break down the success rate for both achievable and unachievable\ntasks.  As shown in Table 2, eliminating this instruction led to a performance boost in achievable\ntasks, enhancing the overall task success rate ofGPT-4to 14.41%.  Despite an overall decline in\nidentifying unachievable tasks,GPT-4retains the capacity to recognize 44.44% of such tasks. It does\nso by generatingreasons of non-achievability, even without explicit instructions. On the other hand,\nGPT-3.5rarely exhibits this level of reasoning. Instead, it tends to follow problematic patterns such\nas hallucinating incorrect answers, repeating invalid actions, or exceeding the step limits. This result\nsuggests that even subtle differences in instruction design can significantly influence the behavior of\na model in performing interactive tasks in complex environments.\n0.10.20.30.40.50.60.81.0\nSuccess rate (%) within a template\n0\n5\n10\n15\n20\n25\n# Template\ngpt-3.5-direct\ngpt-3.5-cot\ngpt-4-cot\nTable 3: Distribution of suc-\ncess rates on templates with\n≥1successful executions on\nGPTmodels (no UA hint).\nCan  a  model  maintain  consistent  performance  across  similar\ntasks?Tasks that originate from the same template usually follow\nsimilar reasoning and planning processes, even though their observa-\ntions and executions will differ. We plot a histogram of per-template\nsuccess rates for our models in Table 3. Of the 61 templates,GPT-4\nmanages to achieve a 100% task success rate on only four templates,\nwhileGPT-3.5fails to achieve full task completion for any of the\ntemplates. In many cases, the models are only able to complete one\ntask variation with a template. These observations indicate that even\nwhen tasks are derived from the same template, they can present\ndistinct challenges.  For instance, while “Fork metaseq” can be a\nstraightforward task, “Fork all repos from Facebook” derived from the same template requires more\nrepetitive operations, hence increasing its complexity. Therefore,WebArenaprovide a testbed to\nevaluate more sophisticated methods.  In particular, those that incorporate memory components,\n8",
    "Published as a conference paper at ICLR 2024\nBenchmark\nDynamic\nInteraction?\nRealistic\nEnvironment?\nDiverse\nHuman Tasks?\nFunctional\nCorrectness?\nMind2Web(Deng et al., 2023)✗✓✓✗\nForm/QAWoB (Shi et al., 2017)✗✓✓✗\nMiniWoB++(Liu et al., 2018)✓✗✗✓\nWebshop(Yao et al., 2022a)✓✗✗✓\nALFRED(Shridhar et al., 2020)✓✗✗✓\nVirtualHome    (Puig et al., 2018)✗✗✓✗\nAndroidEnv(Toyama et al., 2021)✓✓✗✗\nWebArena✓✓✓✓\nTable 4: The comparison between our benchmark and existing benchmarks on grounding natural\nlanguage instructions to concrete executions. Our benchmark is implemented in our fully interactable\nhighly-realistic  environment. It features diverse tasks humans may encounter in their daily routines.\nWe design evaluation metrics to assess the functional correctness of task executions.\nenabling thereuseof successful strategies from past experiments Zhou et al. (2022a); Wang et al.\n(2023). More error analysis with examples can be found in Appendix A.10.\n6RELATEDWORK\nBenchmarks for Controlling Agents through Natural LanguageControlling agents via natural\nlanguage in the digital world have been studied in the literature (Branavan et al., 2009; Shi et al., 2017;\nLiu et al., 2018; Toyama et al., 2021; Deng et al., 2023; Li et al., 2020; Xu et al., 2021). However,\nthe balance betweenfunctionality,authenticity, andsupport for environmental dynamicsremains a\nchallenge. Existing benchmarks often compromise these aspects, as shown in Table 4. Some works\nrely on static states, limiting agents’ explorations and functional correctness evaluation (Shi et al.,\n2017; Deng et al., 2023), while others simplify real-world complexities, restricting task variety (Yao\net al., 2022a; Liu et al., 2018). While AndroidEnv (Toyama et al., 2021) replicates an Android setup,\nit does not guarantee the reproducibility since live Android applications are used.  (Kolve et al., 2017;\nShridhar et al., 2020; Puig et al., 2018) and extends to gaming environments (Fan et al., 2022; Küttler\net al., 2020), where the environment mechanisms often diverge from human objectives.\nInteractive Decision-Making AgentsNakano et al. (2021) introduce WebGPT which searches\nthe web and reads the search results to answer questions. Gur et al. (2023) propose a web agent that\nsynthesizes Javascript code for the task executions. Adding a multi-modal dimension, Lee et al. (2023)\nand Shaw et al. (2023) develop agents that predict actions based on screenshots of web pages rather\nthan relying on the text-based DOM trees. Performing tasks in interactive environments requires the\nagents to exhibit several capabilities including hierarchical planning, state tracking, and error recovery.\nExisting works (Huang et al., 2022; Madaan et al., 2022; Li et al., 2023) observe LLMs could break\na task into more manageable sub-tasks (Zhou et al., 2022b).  This process can be further refined\nby representing task executions as programs, a technique that aids sub-task management and skill\nreuse (Zhou et al., 2022a; Liang et al., 2023; Wang et al., 2023; Gao et al., 2023). Meanwhile, search\nand backtracking methods introduce a more structured approach to planning while also allowing\nfor decision reconsideration (Yao et al., 2023; Long, 2023). Existing works also incorporate failure\nrecovery, self-correction (Shinn et al., 2023; Kim et al., 2023), observation summarization (Sridhar\net al., 2023) to improve execution robustness.  The complexity ofWebArenapresents a unique\nchallenge and opportunity for further testing and improvement of these methods.\n7CONCLUSION\nWe presentWebArena, a highly-realistic, standalone, and reproducible web environment designed\nfor the development and testing of autonomous agents.WebArenaincludes fully functional web\napplications and organic data from popular domains.  Additionally, we curate a comprehensive\nbenchmark consisting of 812 examples that focus on mapping high-level natural language intents into\nconcrete web interactions. We also offer outcome-based evaluation that programmatically validate\nthe tasks success. Our experiments show that evenGPT-4only achieves a limited end-to-end task\nsuccess rate of 14.41%, significantly lagging behind the human performance of 78.24%.  These\nfindings underscore the need for future research to focus on enhancing the robustness and efficacy of\nautonomous agents withinWebArenaenvironment.\n9",
    "Published as a conference paper at ICLR 2024\nACKNOWLEDGEMENT\nWe would like to thank Emmy Liu, Zhiruo Wang, Zhitong Guo for examining our annotations,\nShunyu Yao for providing the raw Amazon product data in Webshop, Pengfei Liu, Zaid Sheikh\nand Aman Madaan for the helpful discussions. We are also grateful to the Center for AI Safety for\nproviding computational resources. This material is partly based on research sponsored in part by the\nAir Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government\nis authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any\ncopyright notation thereon.  The views and conclusions contained herein are those of the authors\nand should not be interpreted as necessarily representing the official policies or endorsements, either\nexpressed or implied, of the Air Force Research Laboratory or the U.S. Government. This project\nwas also partially supported by a gift from AWS AI.\nREFERENCES\nPeter  Anderson,  Qi  Wu,  Damien  Teney,  Jake  Bruce,  Mark  Johnson,  Niko  Sünderhauf,  Ian  D.\nReid, Stephen Gould, and Anton van den Hengel.  Vision-and-language navigation:  Interpret-\ning visually-grounded navigation instructions in real environments.  In2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-\n22, 2018,  pp.  3674–3683.  IEEE  Computer  Society,  2018.   doi:  10.1109/CVPR.2018.00387.\nURLhttp://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_\nVision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,\nSiddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A.\nChoquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa\nDev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad\nFienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,\nSteven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,\nMichael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang\nLi, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,\nAroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John\nNham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,\nBrennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,\nDaniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny\nZhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\nYonatan Bisk, Jan Buys, Karl Pichotta, and Yejin Choi. Benchmarking hierarchical script knowledge.\nInProceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npp. 4077–4085, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1412. URLhttps://aclanthology.org/N19-1412.\nS.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. Reinforcement learning for\nmapping instructions to actions. InProceedings of the Joint Conference of the 47th Annual Meeting\nof the ACL and the 4th International Joint Conference on Natural Language Processing of the\nAFNLP, pp. 82–90, Suntec, Singapore, 2009. Association for Computational Linguistics.  URL\nhttps://aclanthology.org/P09-1010.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym, 2016. URLhttps://arxiv.org/abs/1606.01540.\n10",
    "Published as a conference paper at ICLR 2024\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards,\nYura Burda, Nicholas Joseph, Greg Brockman, et al.  Evaluating large language models trained\non code.ArXiv preprint, abs/2107.03374, 2021.  URLhttps://arxiv.org/abs/2107.\n03374.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. InThirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022. URLhttps://openreview.net/forum?\nid=rc8o_j8I8PX.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. InInternational Conference on Machine\nLearning, pp. 10764–10799. PMLR, 2023.\nDaniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and\nAli  Farhadi.    IQA:  visual  question  answering  in  interactive  environments.    In2018 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT,\nUSA, June 18-22, 2018, pp. 4089–4098. IEEE Computer Society, 2018.  doi:  10.1109/CVPR.\n2018.00430. URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/\nGordon_IQA_Visual_Question_CVPR_2018_paper.html.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and\nAleksandra Faust. A real-world webagent with planning, long context understanding, and program\nsynthesis.arXiv preprint arXiv:2307.12856, 2023.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.  Language models as zero-\nshot planners: Extracting actionable knowledge for embodied agents.  In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.),International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 ofProceedings of Machine Learning Research, pp. 9118–9147. PMLR, 2022. URL\nhttps://proceedings.mlr.press/v162/huang22a.html.\nYacine Jernite, Kavya Srinet, Jonathan Gray, and Arthur Szlam.  CraftAssist Instruction Parsing:\nSemantic Parsing for a Minecraft Assistant.ArXiv preprint, abs/1905.01978, 2019. URLhttps:\n//arxiv.org/abs/1905.01978.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.  Language models can solve computer tasks.\nArXiv preprint, abs/2303.17491, 2023. URLhttps://arxiv.org/abs/2303.17491.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel\nGordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment\nfor Visual AI.arXiv, 2017.\nHeinrich  Küttler,  Nantas  Nardelli,  Alexander  H.  Miller,  Roberta  Raileanu,  Marco  Selvatici,\nEdward  Grefenstette,   and  Tim  Rocktäschel.The  nethack  learning  environment.In\nHugo  Larochelle,  Marc’Aurelio  Ranzato,  Raia  Hadsell,  Maria-Florina  Balcan,  and  Hsuan-\nTien  Lin  (eds.),Advances in Neural Information Processing Systems 33:  Annual Con-\nference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URLhttps://proceedings.neurips.cc/paper/2020/hash/\n569ff987c643b4bedf504efda8f786c2-Abstract.html.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov.  Natural questions:  A benchmark for question answering research.Transactions of the\nAssociation for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL\nhttps://aclanthology.org/Q19-1026.\n11",
    "Published as a conference paper at ICLR 2024\nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.Open\nReview, 62, 2022.\nKenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos,\nUrvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot\nparsing as pretraining for visual language understanding. InInternational Conference on Machine\nLearning, pp. 18893–18912. PMLR, 2023.\nXinze Li,  Yixin Cao,  Muhao Chen,  and Aixin Sun.   Take a break in the middle:  Investigating\nsubgoals towards hierarchical script generation.ArXiv preprint, abs/2305.10907, 2023.  URL\nhttps://arxiv.org/abs/2305.10907.\nYang  Li,  Jiacong  He,  Xin  Zhou,  Yuan  Zhang,  and  Jason  Baldridge.Mapping  natural  lan-\nguage instructions to mobile UI action sequences.   InProceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,  pp.  8198–8210,  Online,  2020.  Asso-\nciation for Computational Linguistics.   doi:  10.18653/v1/2020.acl-main.729.   URLhttps:\n//aclanthology.org/2020.acl-main.729.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng.  Code as policies: Language model programs for embodied control.  In2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 9493–9500. IEEE, 2023.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang.  Reinforcement\nlearning on web interfaces using workflow-guided exploration. In6th International Conference on\nLearning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net, 2018. URLhttps://openreview.net/forum?id=\nryTp3f-0-.\nJieyi Long.  Large language model guided tree-of-thought.ArXiv preprint, abs/2305.08291, 2023.\nURLhttps://arxiv.org/abs/2305.08291.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of\ncode are few-shot commonsense learners. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 1384–1403, Abu Dhabi, United Arab Emirates,\n2022. Association for Computational Linguistics.   URLhttps://aclanthology.org/\n2022.emnlp-main.90.\nDipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. Tell me dave: Context-sensitive\ngrounding of natural language to manipulation instructions.The International Journal of Robotics\nResearch, 35(1-3):281–300, 2016.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse,  Shantanu Jain,  Vineet Kosaraju,  William Saunders,  et al.   Webgpt:  Browser-assisted\nquestion-answering with human feedback.arXiv preprint arXiv:2112.09332, 2021.\nOpenAI. Chatgpt: Optimizing language models for dialogue. 2022.\nOpenAI. Gpt-4 technical report.arXiv, pp. 2303–08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.Advances in Neural Information Processing Systems,  35:\n27730–27744, 2022.\nXavier  Puig,  Kevin  Ra,  Marko  Boben,  Jiaman  Li,  Tingwu  Wang,  Sanja  Fidler,  and  Antonio\nTorralba.   Virtualhome:  Simulating  household  activities  via  programs.   In2018 IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018,  pp.  8494–8502.  IEEE  Computer  Society,  2018.doi:   10.1109/CVPR.\n2018.00886. URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/\nPuig_VirtualHome_Simulating_Household_CVPR_2018_paper.html.\n12",
    "Published as a conference paper at ICLR 2024\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for\nmachine comprehension of text. InProceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, pp. 2383–2392, Austin, Texas, 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/D16-1264. URLhttps://aclanthology.org/D16-1264.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor SQuAD.  InProceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/P18-2124. URLhttps://aclanthology.\norg/P18-2124.\nPeter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi\nKhandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow\ninstructions via graphical user interfaces.arXiv preprint arXiv:2306.00245, 2023.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang.  World of bits:\nAn open-domain platform for web-based agents.  In Doina Precup and Yee Whye Teh (eds.),\nProceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,\nAustralia, 6-11 August 2017, volume 70 ofProceedings of Machine Learning Research, pp. 3135–\n3144. PMLR, 2017. URLhttp://proceedings.mlr.press/v70/shi17a.html.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection.ArXiv preprint, abs/2303.11366, 2023.  URLhttps://arxiv.\norg/abs/2303.11366.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded instructions\nfor everyday tasks. In2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition,\nCVPR 2020, Seattle, WA, USA, June 13-19, 2020,  pp.  10737–10746.  IEEE,  2020.   doi:  10.\n1109/CVPR42600.2020.01075.  URLhttps://doi.org/10.1109/CVPR42600.2020.\n01075.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J.\nHausknecht.  Alfworld:  Aligning text and embodied environments for interactive learning.  In\n9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net,  2021.   URLhttps://openreview.net/forum?id=\n0IOX0YcCdTn.\nAbishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and Shuyan Zhou. Hierarchical prompting assists\nlarge language model on web navigation.arXiv preprint arXiv:2305.14257, 2023.\nDaniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed,\nTyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform\nfor android.ArXiv preprint, abs/2105.13231, 2021. URLhttps://arxiv.org/abs/2105.\n13231.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models.ArXiv\npreprint, abs/2305.16291, 2023. URLhttps://arxiv.org/abs/2305.16291.\nZhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig.  Execution-based evaluation for\nopen-domain code generation.ArXiv preprint, abs/2212.10481, 2022. URLhttps://arxiv.\norg/abs/2212.10481.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in\nNeural Information Processing Systems, 35:24824–24837, 2022.\nNancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James Landay, and Monica\nLam. Grounding open-domain instructions to automate web support tasks. InProceedings of the\n2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 1022–1032, Online, 2021. Association for Computational\nLinguistics.  doi:  10.18653/v1/2021.naacl-main.80.  URLhttps://aclanthology.org/\n2021.naacl-main.80.\n13",
    "Published as a conference paper at ICLR 2024\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. InProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2369–2380, Brussels, Belgium, 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/D18-1259. URLhttps://aclanthology.org/D18-1259.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.   Webshop:  Towards scalable\nreal-world web interaction with grounded language agents. volume abs/2207.01206, 2022a. URL\nhttps://arxiv.org/abs/2207.01206.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models.ArXiv preprint, abs/2210.03629,\n2022b. URLhttps://arxiv.org/abs/2210.03629.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan.  Tree of thoughts: Deliberate problem solving with large language models.ArXiv\npreprint, abs/2305.10601, 2023. URLhttps://arxiv.org/abs/2305.10601.\nVictor Zhong, Caiming Xiong, and Richard Socher.  Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arxiv 2017.ArXiv preprint, abs/1709.00103, 2017.\nURLhttps://arxiv.org/abs/1709.00103.\nShuyan Zhou, Pengcheng Yin, and Graham Neubig. Hierarchical control of situated agents through\nnatural language.  InProceedings of the Workshop on Structured and Unstructured Knowledge\nIntegration (SUKI), pp. 67–84, Seattle, USA, 2022a. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.suki-1.8. URLhttps://aclanthology.org/2022.suki-1.8.\nShuyan Zhou, Li Zhang, Yue Yang, Qing Lyu, Pengcheng Yin, Chris Callison-Burch, and Graham\nNeubig. Show me more details: Discovering hierarchies of procedures from semi-structured web\ndata. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 2998–3012, Dublin, Ireland, 2022b. Association for Computational\nLinguistics.   doi:  10.18653/v1/2022.acl-long.214.   URLhttps://aclanthology.org/\n2022.acl-long.214.\n14",
    "Published as a conference paper at ICLR 2024\nAAPPENDIX\nA.1WEBSITEIMPLEMENTATION\nGiven the selected websites described in §2.2, we make the best attempt to reproduce the functionality\nof commonly used sites in a reproducible way. To achieve this, we utilized open-source frameworks\nfor the development of the websites across various categories and imported data from their real-world\ncounterparts. For the E-commerce category, we constructed a shopping website with approximately\n90kproducts, including the prices, options, detailed product descriptions, images, and reviews,\nspanning over 300 product categories. This website is developed using Adobe Magento, an open-\nsource e-commerce platform\n4\n. Data resources were obtained from data from actual online sites, such\nas that included in the Webshop data dumpYao et al. (2022a). As for the social forum platform, we\ndeployed an open-source software Postmill\n5\n, the open-sourced counterpart of Reddit\n6\n. We sampled\nfrom the top 50 subreddits\n7\n.  We then manually selected many subreddit for northeast US cities\nas well as subreddit for machine learning and deep learning-related topics. This manual selection\nencourages cross-website tasks such as seeking information related to the northeast US on both\nReddit and the map.  In total, we have 95 subreddits, 127390 posts, and 661781 users.  For the\ncollaborative software development platform, we choose GitLab\n8\n. We heuristically simulate the code\nrepository characteristics by sampling at least ten repositories for every programming language:80%\nof them are sampled from the set of top90percentile wrt stars repos using a discrete probability\ndistribution weighted proportional to their number of stars; the remaining are sampled from the\nbottom ten percentile set using similar weighted distribution. This is done to ensure fair representation\nof repos of all kinds, from popular projects with many issues and pull requests to small personal\nprojects. In total, we have 300 repositories and more than 1000 accounts with at least one commit\nto a repository.  For the content management system, we adapted Adobe Magento’s admin portal,\ndeploying the sample data provided in the official guide. We employ OpenStreetMap\n9\nfor map service\nimplementation, confining our focus to the northeast US region due to data storage constraints. We\nimplement a calculator and a scratchpad ourselves.\nLastly, we configure the knowledge resources as individual websites, complemented with search\nfunctionality for efficient information retrieval.  Specifically, we utilize Kiwix\n10\nto host an offline\nversion of English Wikipedia with a knowledge cutoff of May 2023. The user manuals for GitLab\nand Adobe Commerce Merchant documentation are scraped from the official websites.\nA.2ENVIRONMENTDELIVERY ANDRESET\nOne goal for our evaluation environment is ease of use and reproducibility. As a result, we deploy our\nwebsites in separate Docker images\n11\n, one per website. The Docker images are fully self-contained\nwith all the code of the website, database, as well as any other software dependencies.  They also\ndo not rely on external volume mounts to function, as the data of the websites are also part of the\ndocker image. This way, the image is easy to distribution containing all the pre-populated websites\nfor reproducible evaluation. End users can download our packaged Docker images and run them on\ntheir systems and re-deploy the exact websites together with the data used in our benchmarks for\ntheir local benchmarking.\nSince some evaluation cases may require the agent to modify the data contained in the website,\ne.g.,creating a new user, deleting a post, etc., it is crucial to be able to easily reset the website\nenvironment to its initial state. With Docker images, the users could stop and delete the currently\nrunning containers for that website and start the container from our original image again to fully\nreset the environment to the initial state. Depending on the website, this process may take from a\nfew seconds to one minute. However, not all evaluation cases would require an environment reset, as\n4\nhttps://github.com/magento/magento2\n5\nhttps://postmill.xyz/\n6\nhttps://www.reddit.com/\n7\nhttps://redditlist.com/sfw.html\n8\nhttps://gitlab.com/gitlab-org/gitlab\n9\nhttps://www.openstreetmap.org/\n10\nhttps://www.kiwix.org/en/\n11\nhttps://www.docker.com/\n15",
    "Published as a conference paper at ICLR 2024\nCMS\n22.4%\nMap\n13.4%\nE-commerce\n23.0%\nReddit\n13.1%\nGitlab\n22.2%\nCross Site\n5.9%\nFigure 6: The intent distribution across different websites. Cross-site intents necessitate interacting\nwith multiple websites. Notably, regardless of the website, all user intents require interactions with\nmultiple web pages.\nmany of the intents are information gathering and are read-only for the website data. Also, combined\nwith the inference time cost for the agent LLMs, we argue that this environment reset method, through\nrestarting Docker containers from the original images, will have a non-negligible but small impact on\nevaluation time.\nA.3USERROLESSIMULATION\nUsers of the same website often have disparate experiences due to their distinctroles,permissions,\nandinteraction histories. For instance, within an E-commerce CMS, a shop owner might possess\nfull read and write permissions across all content, whereas an employee might only be granted write\npermissions for products but not for customer data. We aim to emulate this scenario by generating\nunique user profiles on each platform.\nOn the shopping site, we created a customer profile that has over 35 orders within a span of two years.\nOn GitLab, we selected a user who maintains several popular open-source projects with numerous\nmerge requests and issues.  This user also manages a handful of personal projects privately.  On\nReddit, our chosen profile was a user who actively participates in discussions, with many posts and\ncomments. Lastly, on our E-commerce CMS, we set up a user profile for a shop owner who has full\nread-and-write access to all system contents.\nAll users are automatically logged into their accounts using a pre-cached cookie.   To our best\nknowledge, this is the first publicly available agent evaluation environment to implement such a\ncharacteristic. Existing literature typically operates under the assumption of universally identical user\nroles Shi et al. (2017); Liu et al. (2018); Deng et al. (2023).\nA.4INTENTDISTRIBUTION\nThe distribution of intents across the websites are shown in Figure 6.\nA.5HUMANPERFORMANCE\nWe acknowledge that there may be a difference in human performance when annotators with different\ndemographics are involved. In fact, many tasks in our dataset require domain-specific knowledge.\nFor instance, an average user may not know what a git merge request is; or how to create a product in\na complex content management system. We aim to design tasks that have easy-to-imagine outcomes\n(e.g.,a new product page is created) rather than those that are easily performed by an average user\nwithout significant domain knowledge.\n16",
    "Published as a conference paper at ICLR 2024\nCoT UA Hint  Model   SR\n✓✗GPT-3.56.28\nTable 5: The task success rate (SR %) of GPT-3.5-TURBO-16K-0613 with temperature 0.0.\nDatasetgpt-4-0613  gpt-4-1106-preview\nDate (900 examples)100100\nTime duration (900 examples)100100\nTable 6: The accuracy (%) of two versions of GPT-4 on judging if dates and time duration of different\nformats are equivalent.\nA.6EXPERIMENTCONFIGURATIONS\nWe experiment withGPT-3.5-TURBO-16K-0613,  GPT-4-0613, andTEXT-BISON-001with a\ntemperature of1.0and a top-pparameter of0.9. The maximum number of state transitions is set to\n30. We halt execution if the same action is repeated more than three times on the same observation\nor if the agent generates three consecutive invalid actions.  These situations typically indicate a\nhigh likelihood of execution failure and hence warrant early termination. ForTEXT-BISON-001, we\nadditionally allow ten retries until it generates a valid action.\nPrimarily, we use a high temperature of 1.0 to encourage theexploration.  To aid replicating the\nresults, we provide the results ofGPT-3.5-TURBO-16K-0613with temperature 0.0 in Table 5 and\nthe execution trajectories in our code repository.\nA.7PROMPT FORFUZZY_MATCH\nHelp a teacher to grade the answer of a student given a question. Keep in mind that the student may\nuse different phrasing or wording to answer the question. The goal is to evaluate whether the answer\nis semantically equivalent to the reference answer.\nquestion: {{intent}}\nreference answer: {{reference answer}}\nall the string ’N/A’ that you see is a special sequence that means ’not achievable’\nstudent answer: {{prediction}}\nConclude the judgement by correct/incorrect/partially correct.\nPredictions that are judged as “correct” will receive a score of one, while all other predictions will\nreceive a score of zero.\nA.8THEACCURACY  OFFUZZYMATCHFUNCTION\nTo evaluate this, we manually checked 40 examples and found that 39 of them are identical to our\nhuman judgment. In addition, among the 82 examples that require using GPT-4 for evaluation, the\nanswer of 49 (60%) examples is a date (e.g.,10/23/2022) or time duration (e.g.,15 minutes). In these\ncases, GPT-4 is only used to judge the differentformatof the answers. We quantitatively evaluate\nthe correctness of GPT-4 in this case by generating different formats of a date and time duration\nprogrammatically. We randomly sample negative examples. For instance, Nov 3, 2022, November\n3, 2022, 3rd November 2022, 3 Nov 2022, 2022-11-03, and 3rd of November, 2022 are all correct\nvariances of 2022/11/03. The accuracy of GPT-4 is shown in Table 6. We can see that two versions of\nGPT-4 are extremely accurate, both achieving 100% accuracy.\nA.9THEPROMPTS OF THEBASELINEWEBAGENTS\nThe system message of the reasoning agent for both GPT-3.5 andGPT-4is in Figure 7, and two\nexamples are in Figure 8.  The system message of the direct agent for GPT-3.5 is in Figure 9 and\nthe two examples are in Figure 10.UA hintrefers to the instruction of “ If you believe the task is\n17",
    "Published as a conference paper at ICLR 2024\nYou are an autonomous intelligent agent tasked with navigating a web browser. You will be given\nweb-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\nHere’s the information you’ll have:\nThe user’s objective: This is the task you’re trying to complete.\nThe current web page’s accessibility tree: This is a simplified representation of the webpage, providing\nkey information.\nThe current web page’s URL: This is the page you’re currently navigating.\nThe open tabs: These are the tabs you have open.\nThe previous action: This is the action you just performed. It may be helpful to track your progress.\nThe actions you can perform fall into several categories:\nPage Operation Actions\n`click [id]`: This action clicks on an element with a specific id on the webpage.\n`\ntype [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By\ndefault, the \"Enter\" key is pressed after typing unless press_enter_after is set to 0.\n`hover [id]`: Hover over an element with id.\n`press [key_comb]`: Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).\n`scroll [direction=down|up]`: Scroll the page up or down.\nTab Management Actions:\n`new_tab`: Open a new, empty browser tab.\n`tab_focus [tab_index]`: Switch the browser’s focus to a specific tab using its index.\n`close_tab`: Close the currently active tab.\nURL Navigation Actions:\n`goto [url]`: Navigate to a specific URL.\n`go_back`: Navigate to the previously viewed page.\n`go_forward`: Navigate to the next page (if a previous\n`go_back`action was performed).\nCompletion Action:\n`\nstop [answer]`: Issue this action when you believe the task is complete. If the objective is to find\na text-based answer, provide the answer in the bracket.  If you believe the task is impossible to\ncomplete, provide the answer as \"N/A\" in the bracket.\nHomepage:\nIf you want to visit other websites, check out the homepage at http://homepage.com. It has a list of\nwebsites you can visit.\nhttp://homepage.com/password.html lists all the account names and passwords for the websites. You\ncan use them to log in to the websites.\nTo be successful, it is very important to follow the following rules:\n1. You should only issue an action that is valid given the current observation\n2. You should only issue one action at a time.\n3. You should follow the examples to reason step by step and then issue the next action.\n4. Generate the action in the correct format. Start with a \"In summary, the next action I will perform\nis“ phrase, followed by action inside``````.  For example, \"In summary, the next action I will\nperform is```click [1234]```”.\n5. Issue stop action when you think you have achieved the objective. Don’t generate anything after\nstop.\"\"\"\nFigure 7: Thesystemmessage of the reasoning agent. This message has a general explanation of\nthe task, the available actions, and some notes on avoiding common failures.\nimpossible to complete, provide the answer as \"N/A\" in the bracket.”. We remove this sentence in our\nablation studies.\n18",
    "Published as a conference paper at ICLR 2024\nexample_user\nOBSERVATION:\n[1744] link ’HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)’\n[1749] StaticText ’$279.49’\n[1757] button ’Add to Cart’\n[1760] button ’Add to Wish List’\n[1761] button ’Add to Compare’\nURL: http://onestopmarket.com/office-products/office-electronics.html\nOBJECTIVE: What is the price of HP Inkjet Fax Machine\nPREVIOUS ACTION: None\nexample_assistant\nLet’s think step-by-step.  This page lists the information of HP Inkjet Fax Machine, which is the\nproduct identified in the objective.  Its price is $279.49.  I think I have achieved the objective.  I\nwill issue the stop action with the answer.  In summary, the next action I will perform is```stop\n[$279.49]```\nexample_user\nOBSERVATION:\n[164] textbox ’Search’ focused: True required: False\n[171] button ’Go’\n[174] link ’Find directions between two points’\n[212] heading ’Search Results’\n[216] button ’Close’\nURL: http://openstreetmap.org\nOBJECTIVE: Show me the restaurants near ABC\nPREVIOUS ACTION: None\nexample_assistant\nLet’s think step-by-step. This page has a search box whose ID is [164]. According to the nominatim\nrule of openstreetmap, I can search for the restaurants near a location by\n ̈\nrestaurants near\n ̈\n.  I can\nsubmit my typing by pressing the Enter afterwards. In summary, the next action I will perform is\n```type [164] [restaurants near ABC] [1]```\nFigure 8:  The two examples provided asexample_userandexample_assistantfor the\nreasoning agent. Before issuing the action, the agent first perform reasoning.\n19",
    "Published as a conference paper at ICLR 2024\nYou are an autonomous intelligent agent tasked with navigating a web browser. You will be given\nweb-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\nHere’s the information you’ll have:\nThe user’s objective: This is the task you’re trying to complete.\nThe current web page’s accessibility tree: This is a simplified representation of the webpage, providing\nkey information.\nThe current web page’s URL: This is the page you’re currently navigating.\nThe open tabs: These are the tabs you have open.\nThe previous action: This is the action you just performed. It may be helpful to track your progress.\nThe actions you can perform fall into several categories:\nPage Operation Actions\n`click [id]`: This action clicks on an element with a specific id on the webpage.\n`\ntype [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By\ndefault, the \"Enter\" key is pressed after typing unless press_enter_after is set to 0.\n`hover [id]`: Hover over an element with id.\n`press [key_comb]`: Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).\n`scroll [direction=down|up]`: Scroll the page up or down.\nTab Management Actions:\n`new_tab`: Open a new, empty browser tab.\n`tab_focus [tab_index]`: Switch the browser’s focus to a specific tab using its index.\n`close_tab`: Close the currently active tab.\nURL Navigation Actions:\n`goto [url]`: Navigate to a specific URL.\n`go_back`: Navigate to the previously viewed page.\n`go_forward`: Navigate to the next page (if a previous\n`go_back`action was performed).\nCompletion Action:\n`\nstop [answer]`: Issue this action when you believe the task is complete. If the objective is to find\na text-based answer, provide the answer in the bracket.  If you believe the task is impossible to\ncomplete, provide the answer as \"N/A\" in the bracket.\nHomepage:\nIf you want to visit other websites, check out the homepage at http://homepage.com. It has a list of\nwebsites you can visit.\nhttp://homepage.com/password.html lists all the account name and password for the websites. You\ncan use them to log in to the websites.\nTo be successful, it is very important to follow the following rules:\nTo be successful, it is very important to follow the following rules:\n1. You should only issue an action that is valid given the current observation\n2. You should only issue one action at a time.\n3. Generate the action in the correct format. Always put the action inside a pair of```. For example,\n```click [1234]```\n4. Issue stop action when you think you have achieved the objective. Don’t generate anything after\nstop.\"\"\"\nFigure 9: Thesystemmessage of the direct agent. This message has the general explanation of the\ntask, the available actions and some notes on avoiding common failures.\n20",
    "Published as a conference paper at ICLR 2024\nexample_user\nOBSERVATION:\n[1744] link ’HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)’\n[1749] StaticText ’$279.49’\n[1757] button ’Add to Cart’\n[1760] button ’Add to Wish List’\n[1761] button ’Add to Compare’\nURL: http://onestopmarket.com/office-products/office-electronics.html\nOBJECTIVE: What is the price of HP Inkjet Fax Machine\nPREVIOUS ACTION: None\nexample_assistant\n```stop [$279.49]```\nexample_user\nOBSERVATION:\n[164] textbox ’Search’ focused: True required: False\n[171] button ’Go’\n[174] link ’Find directions between two points’\n[212] heading ’Search Results’\n[216] button ’Close’\nURL: http://openstreetmap.org\nOBJECTIVE: Show me the restaurants near ABC\nPREVIOUS ACTION: None\nexample_assistant\n```type [164] [restaurants near ABC] [1]```\nFigure 10: The two examples provided asexample_userandexample_assistantfor the\ndirect agent. The agent directly emits the next action given the observation.\n21",
    "Published as a conference paper at ICLR 2024\n[2430] searchbox 'Search query' \n    [5172] StaticText 'DMV area'\n[2361] link 'Projects 0' \n[2365] link 'Users 1' \n[2070] heading \" We couldn't \nfind any projects matching \nFacebook\"\nFigure  11:  Two  examples  where  theGPT-4agent  failed,  along  with  their  screenshot  and  the\naccessibility tree of the relevant sections (grey). On the left, the agent fails to proceed to the “Users”\nsection to accomplish the task of “Fork all Facebook repos”; on the right, the agent repeats entering\nthe same search query even though the observation indicates the input box is filled.\nA.10ADDITIONALERRORANALYSIS\nObservation BiasRealistic websites frequently present information on similar topics across various\nsections to ensure optimal user accessibility. However, aGPT-4agent often demonstrates a tendency\nto latch onto the first related piece of information it encounters without sufficiently verifying its\nrelevance or accuracy. For instance, the homepage of the E-Commerce CMS displays the best-selling\nitems based onrecent purchases, while historical best-seller data is typically accessed via a separate\nreport. Presented with the task of “What is the top-1 best-selling product in 2022”, theGPT-4agent\ndefaults to leveraging the readily available information on the homepage, bypassing the necessary\nstep of generating the report to obtain the accurate data.\nFailures in Observation InterpretationInterestingly, whileGPT-4is capable of summarizing the\nobservations, it occasionally overlooks more granular information, such as the previously entered\ninput.   As  in  the  right-hand  example  of  Figure  11,[5172] StaticTextindicates  that  the\nsearch term “DMV area” has already been entered.  However, the agent disregards this detail and\ncontinuously issues the commandtype [2430] [DMV area]until it reaches the maximum\nstep limit.  Furthermore, the agent often neglects the previous action information that is provided\nalongside the observation.\nWe hypothesize that these observed failures are related to the current pretraining and supervised\nfine-tuning on dialogues employed in GPT models Ouyang et al. (2022). These models are primarily\ntrained to execute instructions givenimmediateobservations (i.e.,, the dialogue history); thereby,\nthey may exhibit a lack of explorations.  Furthermore, in dialogue scenarios, subtle differences in\nNL expressions often have less impact on the overall conversation. As a result, models may tend to\noverlook minor variations in their observations.\n22"
  ]
}