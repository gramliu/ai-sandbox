{
  "key": "HUDEX4SZ",
  "url": "https://arxiv.org/pdf/2309.07906v1",
  "metadata": {
    "title": "Generative Image Dynamics",
    "abstract": "  We present an approach to modeling an image-space prior on scene dynamics.\nOur prior is learned from a collection of motion trajectories extracted from\nreal video sequences containing natural, oscillating motion such as trees,\nflowers, candles, and clothes blowing in the wind. Given a single image, our\ntrained model uses a frequency-coordinated diffusion sampling process to\npredict a per-pixel long-term motion representation in the Fourier domain,\nwhich we call a neural stochastic motion texture. This representation can be\nconverted into dense motion trajectories that span an entire video. Along with\nan image-based rendering module, these trajectories can be used for a number of\ndownstream applications, such as turning still images into seamlessly looping\ndynamic videos, or allowing users to realistically interact with objects in\nreal pictures.\n",
    "published": "2023-09-14T17:54:01Z"
  },
  "text": [
    "Generative Image Dynamics\nZhengqi LiRichard TuckerNoah SnavelyAleksander Holynski\nGoogle Research\n...\nInput Picture\nX coefficients\nStochastic Motion Texture\n0.2Hz\n0.4Hz\n3.0Hz\nLooping \nvideo\nInteractive\ndynamics\n...\nYcoefficients\nFigure 1.  Our approach models a generative image-space prior on scene dynamics: from a single RGB image, our model generates a neural\nstochastic motion texture, a motion representation that models dense long-term motion trajectories in the Fourier domain. We show that\nour motion priors enable applications such as turning a single picture into a seamlessly looping video, or simulating object dynamics in\nresponse to an interactive user excitation (e.g., dragging and releasing a point on the object). On the right, we visualize the output videos\nusing space-timeX-tslices through 10 seconds of video (along the scanline shown in the input picture).\nAbstract\nWe  present  an  approach  to  modeling  an  image-space\nprior on scene dynamics. Our prior is learned from a col-\nlection  of  motion  trajectories  extracted  from  real  video\nsequences containing natural, oscillating motion such as\ntrees,  flowers,  candles,  and  clothes  blowing  in  the  wind.\nGiven a single image, our trained model uses a frequency-\ncoordinated  diffusion  sampling  process  to  predict  a  per-\npixel long-term motion representation in the Fourier domain,\nwhich we call aneural stochastic motion texture. This rep-\nresentation can be converted into dense motion trajectories\nthat span an entire video. Along with an image-based ren-\ndering module, these trajectories can be used for a number\nof downstream applications, such as turning still images into\nseamlessly looping dynamic videos, or allowing users to real-\nistically interact with objects in real pictures. See our project\npage for more results: generative-dynamics.github.io.\n1. Introduction\nThe natural world is always in motion, with even seem-\ningly static scenes containing subtle oscillations as a result\nof factors such as wind, water currents, respiration, or other\nnatural rhythms. Motion is one of the most salient visual\nsignals, and humans are particularly sensitive to it: captured\nimagery without motion (or even with slightly unrealistic\nmotion) will often seem uncanny or unreal.\nWhile it is easy for humans to interpret or imagine motion\nin scenes, training a model to learn realistic scene motion is\nfar from trivial. The motion we observe in the world is the\nresult of a scene’s underlying physical dynamics, i.e., forces\napplied to objects that respond according to their unique\nphysical properties — their mass, elasticity, etc. These prop-\nerties and forces are hard to measure and capture at scale,\nbut fortunately, in many cases measuring them is unneces-\nsary: we can instead capture and learn from the resulting\nobserved motion. This observed motion is multi-modal and\ngrounded in complex physical effects, but it is nevertheless\narXiv:2309.07906v1  [cs.CV]  14 Sep 2023",
    "often predictable: candles will flicker in certain ways, trees\nwill sway, and their leaves will rustle. This predictability is\ningrained in our human perception of real scenes: by viewing\na still image, we can imagine plausible motions that might\nhave been ongoing as the picture was captured — or, if there\nmight have been many possible such motions, adistribution\nof natural motions conditioned on that image. Given the fa-\ncility with which humans are able to imagine these possible\nmotions, a natural research problem is to model this same\ndistribution computationally.\nRecent advances in generative models, and in particu-\nlar, conditional diffusion models [40, 78, 80], have enabled\nus to model highly rich and complex distributions, includ-\ning distributions of real images conditioned on text [68–70].\nThis capability has enabled a number of previously impos-\nsible applications, such as text-conditioned generation of\narbitrary, diverse, and realistic image content. Following the\nsuccess of these image models, recent work has shown that\nmodeling other domains, such as videos [7,39] and 3D geom-\netry [72, 90, 91, 93], can be similarly useful for downstream\napplications.\nIn this paper, we explore modeling a generative prior\nforimage-space scene motion, i.e., the motion of all pix-\nels in a single image. This model is trained on automati-\ncally extracted motion trajectories from a large collection\nof real video sequences. Conditioned on an input image,\nthe trained model predicts aneural stochastic motion tex-\nture: a set of coefficients of a motion basis that characterize\neach pixel’s trajectory into the future. We limit our scope to\nreal-world scenes with natural, oscillating dynamics such as\ntrees and flowers moving in the wind, and therefore choose\nthe Fourier series as our basis functions. We predict a neu-\nral stochastic motion texture using a diffusion model that\ngenerates coefficients for a single frequency at a time, but\ncoordinates these predictions across frequency bands. The\nresulting frequency-space textures can then be transformed\ninto dense, long-range pixel motion trajectories, which can\n(along with an image-based rendering diffusion model) be\nused to synthesize future frames, turning still images into\nrealistic animations, as illustrated in Fig. 1.\nCompared with priors over raw RGB pixels, priors over\nmotion capture more fundamental, lower-dimensional under-\nlying structure that efficiently explains variations in pixel\nvalues. Hence, our motion representation leads to more co-\nherent long-term generation and more fine-grained control\nover animations compared with prior methods that perform\nimage animation via raw video synthesis. We also demon-\nstrate that our generated motion representation is convenient\nfor a number of downstream applications, such as creating\nseamless looping videos, editing the generated motion, and\nenabling interactive dynamic images, i.e., simulating the\nresponse of object dynamics to user-applied forces.\n2. Related Work\nGenerative synthesis.Recent advances in generative mod-\nels have enabled photorealistic synthesis of images condi-\ntioned on text prompts [16, 17, 23, 68–70]. These generative\ntext-to-image models can be augmented to synthesize video\nsequences by extending the generated image tensors along a\ntime dimension [7,9,39,58,77,96,96,101]. While these meth-\nods are effective at producing plausible video sequences that\ncapture the spatiotemporal statistics of real footage, the re-\nsulting videos can suffer from a number of common artifacts,\nsuch as incoherent motion, unrealistic temporal variation in\ntextures, and violations of physical constraints like preserva-\ntion of mass.\nAnimating images.Instead of generating videos entirely\nfrom text, other techniques take as input a still picture and\nanimate  it.  Many  recent  deep  learning  methods  adopt  a\n3D-Unet  architecture  to  produce  video  volumes  directly\nfrom an input image [26, 33, 37, 43, 49, 83]. Because these\nmodels are effectively the same video generation models\n(but  conditioned  on  image  information  instead  of  text),\nthey  exhibit  similar  artifacts  to  those  mentioned  above.\nOne way to overcome these limitations is to not directly\ngenerate  the  video  content  itself,  but  instead  animate  an\ninput  source  image  through  explicit  or  implicit  image-\nbased  rendering,  i.e.,  moving  the  image  content  around\naccording  to  motion  derived  from  external  sources  such\nas a driving video [47, 74–76, 89], motion or 3D geome-\ntry priors [8, 28, 42, 60, 61, 87, 91, 92, 94, 99], user annota-\ntions [6,18,31,35,88,95,98] or a physical simulation [20,22].\nThese methods demonstrate greater temporal coherence and\nrealism, but require additional guidance signals or user input,\nor otherwise rely on limited motion representations (e.g.,\noptical flow fields, as opposed to full-video dense motion\ntrajectories).\nMotion models and motion priors.A number of other\nworks leverage representations of motion beyond two-frame\nflow fields, both in Eulerian and Lagrangian domains. For in-\nstance, Fourier or phase-based motion representations (like\nours) have been used for magnifying and visualizing mo-\ntion [63, 85], or for video editing applications [59]. These\nrepresentations can also be used in motionprediction—\nwhere an image or video is used to inform a deterministic\nfuture motion estimate [32, 66], or a more richdistribution\nof possible motions (which can be modeled explicitly or by\npredicting the pixel values that would be induced by some\nimplicit motion estimate) [84,86,94]. Our work can similarly\nbe thought of as learning priors for motion induced by under-\nlying scene dynamics, where our prior is in the form of an\nimage-conditioned distribution over long-range dense trajec-\ntories. Other recent work has demonstrated the advantages\nof modeling and predicting motion using generative models\nin a number of closed-domain settings such as humans and",
    "animals [2, 19, 27, 67, 81, 97].\nVideos as textures.Certain moving scenes can be thought\nof as a kind of texture—termeddynamic texturesby Doretto\net al. [25]—that model videos as space-time samples of a\nstochastic process. Dynamic textures can represent smooth,\nnatural motions such as waves, flames, or moving trees, and\nhave been widely used for video classification, segmentation\nor encoding [12–15, 71]. A related kind of texture, called a\nvideo texture, represents a moving scene as a set of input\nvideo frames along with transition probabilities between\nany pair of frames [73]. A large body of work exists for\nestimating and producing dynamic or video textures through\nanalysis of scene motion and pixel statistics, with the aim of\ngenerating seamlessly looping or infinitely varying output\nvideos  [1, 21, 30, 54, 55, 73].  In  contrast  to  much  of  this\nprevious work, our method learns priors in advance that can\nthen be applied to single images.\n3. Overview\nGiven a single pictureI\n0\n, our goal is to generate a video\n{\nˆ\nI\n1\n,\nˆ\nI\n2\n.,...,\nˆ\nI\nT\n}of  lengthTfeaturing  oscillation  dynam-\nics such as those of trees, flowers, or candle flames mov-\ning in the breeze. Our system consists of two modules, a\nmotion prediction module and an image-based rendering\nmodule.  Our  pipeline  begins  by  using  a  latent  diffusion\nmodel (LDM) to predict aneural stochastic motion texture\nS=\n\u0000\nS\nf\n0\n,S\nf\n1\n,...,S\nf\nK−1\n\u0001\nfor the input imageI\n0\n. A stochas-\ntic motion texture is a frequency representation of per-pixel\nmotion trajectories in an input image (Sec. 4). The predicted\nstochastic motion texture is then transformed to a sequence\nof motion displacement fieldsF= (F\n1\n,F\n2\n,...,F\nT\n)using\nan inverse discrete Fourier transform. These motion fields,\nin turn, are used to determine the position of each input pixel\nat each future time step. Given these predicted motion fields,\nour rendering module animates the input RGB image us-\ning an image-based rendering technique that splats encoded\nfeatures from the input image and decodes these splatted\nfeatures into an output frame with an image synthesis net-\nwork (Sec. 5). Because our method explicitly estimates a\nrepresentation of motion from a single picture, it enables\nseveral downstream applications, such as the animation of a\nsingle still picture with varying speed and motion magnitude,\nthe generation of seamless looping video, and the simulation\nof object dynamics response to an external user excitation\n(i.e., interactive dynamics) (Sec. 6).\n4. Neural stochastic motion textures\n4.1. Motion textures\nAs  proposed  by  Chuanget  al.  [20],  a  motion  texture\ndefines a sequence of time-varying 2D displacement maps\nF={F\nt\n|t= 1,...,T}, where the 2D displacement vector\nF\nt\n(p)at each pixel coordinatepfrom input imageI\n0\ndefines\nthe position of that pixel at a future timet. To generate a\nfuture frame at timet, one can splat pixels fromI\n0\nusing the\ncorresponding displacement mapD\nt\n, resulting in a forward-\nwarped imageI\n′\nt\n:\nI\n′\nt\n(p+F\nt\n(p)) =I\n0\n(p)(1)\n4.2. Stochastic motion textures\nAs demonstrated by prior work in computer graphics [20,\n24, 46, 64], many natural motions, especially the oscillating\nmotions we focus on, can be described as a superposition\nof a small number of harmonic oscillators represented with\ndifferent frequencies, amplitude and phases. One way to\nintroduce stochasticity to the motions is to integrate noise\nfields, but as observed by prior work [20], directly adding\nrandom noise into the spatial and temporal domain of the\nestimated motion fields often leads to unrealistic or erratic\nanimations.\nMoreover, adopting motion textures in the temporal do-\nmain, as defined above, implies predictingT2D displace-\nment fields in order to generate a video withTframes. To\navoid predicting such a large output representation for long\noutput videos, many prior animation methods either generate\nvideo frames autoregressively [7, 28, 53, 56, 83], or predict\neach future output frame independently via an extra time\nembedding [4]. However, neither strategy ensures long-term\ntemporal consistency of generated video frames, and both\ncan produce videos that drift or diverge over time.\nTo address the above issues, we represent per-pixel mo-\ntion textures (i.e., full motion trajectories for all pixels) for\nthe input scene in thefrequency domainand formulate the\nmotion prediction problem as a multi-modal image-to-image\ntranslation task. We adopt the latent diffusion model (LDM)\nto generate a stochastic motion texture, comprised of a4K-\nchannel 2D motion spectrum map, whereK << Tis the\nnumber of frequencies modeled, and where at each frequency\nwe need four scalars to represent the complex Fourier coef-\nficients for thexandydimensions. Fig. 1 illustrates these\nneural stochastic motion textures.\nThe  motion  trajectory  of  a  pixel  at  future  time  steps\nF(p) ={F\nt\n(p)|t= 1,2,...T}and its representation in\nthe  frequency  domain  as  the  motion  spectrumS(p) =\n{S\nf\nk\n(p)|k= 0,1,..\nT\n2\n−1}are related by the Fast Fourier\ntransform (FFT):\nS(p) =FFT(F(p)).(2)\nHow should we select theKoutput frequencies for our\nrepresentation? Prior work in real-time animation has ob-\nserved that most natural oscillation motions are composed\nprimarily of low-frequency components [24, 64]. To validate\nthis hypothesis, we computed the average power spectrum of\nthe motion extracted from 1,000 randomly sampled 5 second\nreal video clips. As shown in the left plot of Fig. 2, the power",
    "0.02.55.07.510.012.515.0\nFrequency (Hz)\n0\n20\n40\n60\n80\nAmplitude\nX-axis\nY-axis\n0.00.51.01.5\nAmplitude of Fourier coefficent at 3.0 Hz\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrequency\nScaling w/ resolution\nAdaptive normalization\nFigure 2.Left:We visualize the average motion power spectrum\nfor thexandYmotion components extracted from a dataset of\nreal videos, shown as the blue and green curves. Natural oscillation\nmotions are composed primarily of low-frequency components,\nand so we use the firstK= 16terms as marked by red dots.\nRight:we show a histogram of the amplitude of Fourier terms at\n3Hz (K= 16) after (1) scaling amplitude by image width and\nheight (blue), or (2) frequency adaptive normalization (red). Our\nadaptive normalization prevents the coefficients from concentrating\nat extreme values.\nspectrum of the motion decreases exponentially with increas-\ning frequency. This suggests that most natural oscillation\nmotions can indeed be well represented by low-frequency\nterms. In practice, we found that the firstK= 16Fourier co-\nefficients are sufficient to realistically reproduce the original\nnatural motion in a range of real videos and scenes.\n4.3. Predicting motion with a diffusion model\nWe choose a latent diffusion model (LDM) [69] as the\nbackbone for our motion prediction module, as LDMs are\nmore computationally efficient than pixel-space diffusion\nmodels,  while  preserving  generation  quality.  A  standard\nLDM consists of two main modules: (1) a Variational Au-\ntoencoder (VAE) that compresses the input image to a latent\nspace through an encoderz=E(I), then reconstructs the\ninput from the latent features via a decoderI=D(z), and\n(2) a U-Net based diffusion model that learns to iteratively\ndenoise latent features starting from Gaussian random noise.\nOur training applies this not to an input image but to stochas-\ntic motion textures from a real video sequence, which are\nencoded and then diffused fornsteps with a pre-defined\nvariance schedule to produce noisy latentsz\nn\n. The 2D U-\nNets are trained to denoise the noisy latents by iteratively\nestimating the noiseε\nθ\n(z\nn\n;n,c)used to update the latent\nfeature at each stepn∈(1,2,...,N). The training loss for\nthe LDM is written as\nL\nLDM\n=E\nn∈U[1,N],ε\nn\n∈N(0,1)\n\u0002\n||ε\nn\n−ε\nθ\n(z\nn\n;n,c)||\n2\n\u0003\n(3)\nwherecis the embedding of any conditional signal, such as\ntext, semantic labels, or, in our case, the first frame of the\ntraining video sequence,I\n0\n. The clean latent featuresz\n0\nare\nthen passed through the decoder to recover the stochastic\nmotion textures.\nFrequency adaptive normalization.One issue we ob-\nserved is that stochastic motion textures have particular dis-\ntribution characteristics across frequencies. As visualized in\nthe left plot of Fig. 2, the amplitude of our motion textures\nspans a range of 0 to 100 and decays approximately exponen-\ntially with increasing frequency. As diffusion models require\nthat output values lie between 0 and 1 for stable training and\ndenoising, we must normalize the coefficients ofSextracted\nfrom real videos before using them for training. If we scale\nthe magnitudes ofScoefficients to [0,1] based on image\nwidth and height as in prior work [28, 72], almost all the\ncoefficients at higher frequencies will end up close to zero,\nas shown in Fig. 2 (right-hand side). Models trained on such\ndata can produce inaccurate motions, since during inference,\neven small prediction errors can lead to large relative errors\nafter denormalization when the magnitude of the normalized\nScoefficients are very close to zero.\nTo address this issue, we employ a simple but effective\nfrequency adaptive normalization technique. In particular,\nwe first independently normalize Fourier coefficients at each\nfrequency based on statistics computed from the training set.\nNamely, at each individual frequencyf\nj\n, we compute the\n97\nth\npercentile of the Fourier coefficient magnitudes over all\ninput samples and use that value as a per-frequency scaling\nfactors\nf\nj\n. Furthermore, we apply a power transformation to\neach scaled Fourier coefficient to pull it away from extremely\nsmall or large values. In practice, we found that a square root\ntransform performs better than other transformations, such\nas log or reciprocal. In summary, the final coefficient values\nof stochastic motion textureS(p)at frequencyf\nj\n(used for\ntraining our LDM) are computed as\nS\n′\nf\nj\n(p) =sign(S\nf\nj\n)\ns\n\f\n\f\n\f\n\f\nS\nf\nj\n(p)\ns\nf\nj\n\f\n\f\n\f\n\f\n.(4)\nAs shown on the right plot of Fig. 2, after applying frequency\nadaptive normalization the stochastic motion texture coeffi-\ncients no longer concentrate in a range of extremely small\nvalues.\nFrequency-coordinated denoising.The straightforward\nway to to predict a stochastic motion textureSwithKfre-\nquency bands is to output a tensor of4Kchannels from a\nstandard diffusion U-Net. However, as in prior work [7], we\nobserve that training a model to produce such a large number\nof channels tends to produce over-smoothed and inaccurate\noutput. An alternative would be to independently predict a\nmotion spectrum map at each individual frequency by in-\njecting an extra frequency embedding to the LDM, but this\nresults in uncorrelated predictions in the frequency domain,\nleading to unrealistic motion.\nTherefore, we propose a frequency-coordinated denois-\ning strategy as shown in Fig. 3. In particular, given an input\nimageI\n0\n, we first train an LDMε\nθ\nto predict a stochastic\nmotion texture mapS\nf\nj\nwith four channels to represent each\nindividual frequencyf\nj\n, where we inject extra frequency\nembedding along with time-step embedding to the LDM\nnetwork. We then freeze the parameters of this LDM model",
    "...\n...\nIterative denoising \nReshape\nSpatial layer\nFrequency Attention\nReshape\nTrain\nInference\nNoisy latent\nFigure 3.Motion prediction module.We predict a neural stochastic motion textureSthrough a frequency-coordinated denoising model.\nEach block of the diffusion networkε\nθ\ninterleaves 2D spatial layers with frequency cross-attention layers (red box, right), and iteratively\ndenoises latent featuresz\nn\n. The denoised features are fed to a VAE decoderDto produceS. During training, we concatenate the downsampled\ninputI\n0\nwith noisy latent features encoded from a real motion texture via a VAE encoderE, and replace the noisy features with Gaussian\nnoisez\nN\nduring inference (left).\nε\nθ\nand introduce attention layers and interleave them with\n2D spatial layers ofε\nθ\nacrossKfrequency bands. Specif-\nically, for a batch sizeBof input images, the 2D spatial\nlayers ofε\nθ\ntreat the correspondingB·Knoisy latent fea-\ntures of channel sizeCas independent samples with shape\nR\n(B·K)×C×H×W\n. The cross-attention layer then interprets\nthese as consecutive features spanning the frequency axis,\nand we reshape the latent features from previous 2D spatial\nlayers toR\nB×K×C×H×W\nbefore feeding them to the atten-\ntion layers. In other words, the frequency attention layers\nare used to coordinate the pre-trained motion latent features\nacross all frequency channels in order to produce coherent\nstochastic motion textures. In our experiments, we observed\nthat the average VAE reconstruction error improves from\n0.024to0.018when we switch from a standard 2D U-Net\nto a frequency-coordinated denoising module, suggesting\nan improved upper bound on LDM prediction accuracy; in\nour ablation study in Sec. 7.6, we also demonstrate that this\ndesign choice improves video generation quality compared\nwith simpler configurations mentioned above.\n5. Image-based rendering\nWe now describe how we take a stochastic motion tex-\ntureSpredicted for a given input imageI\n0\nand render a\nfuture frame\nˆ\nI\nt\nat timet. We first derive motion trajectory\nfields in the time domain using the inverse temporal FFT\napplied at each pixelF(p) =FFT\n−1\n(S(p)). The motion\ntrajectory fields determine the position of every input pixel\nat every future time step. To produce a future frame\nˆ\nI\nt\n, we\nadopt a deep image-based rendering technique and perform\nsplatting with the predicted motion fieldF\nt\nto forward warp\nthe encodedI\n0\n, as shown in Fig. 4. Since forward warp-\ning can lead to holes, and multiple source pixels can map\nto the same output 2D location, we adopt the feature pyra-\nmid softmax splatting strategy proposed in prior work on\nframe interpolation [62]. Specifically, we encodeI\n0\nthrough\na feature extractor network to produce a multi-scale feature\nmapM={M\nj\n|j= 0,...,J}. For each individual feature\nmapM\nj\nat scalej, we resize and scale the predicted 2D\nmotion fieldF\nt\naccording to the resolution ofM\nj\n. We use\nflow magnitude, as a proxy for geometry, to determine the\ncontributing weight of each source pixel mapped to its desti-\nnation location. In particular, we compute a per-pixel weight,\nW(p) =\n1\nT\nP\nt\n||F\nt\n(p)||\n2\nas the average magnitude of the\npredicted motion trajectory fields. In other words, we assume\nlarge motions correspond to moving foreground objects, and\nsmall or zero motions correspond to background objects.\nWe use motion-derived weights instead of learnable ones\nbecause we observe that in the single-view case, learnable\nweights are not effective for addressing disocclusion ambi-\nguities, as shown in the second column of Fig. 5.\nWith the motion fieldF\nt\nand weightsW, we apply soft-\nmax  splatting  to  warp  feature  map  at  each  scale  to  pro-\nduce a warped featureM\n′\nj,t\n=W\nsoftmax\n(M\nj\n,F\nt\n,W), where\nW\nsoftmax\nis the softmax splatting operation. The warped fea-\nturesM\n′\nj,t\nare then injected into intermediate blocks of an\nimage synthesis decoder network to produce a final rendered\nimage\nˆ\nI\nt\n.\nWe jointly train the feature extractor and synthesis net-\nworks with a start and target frames(I\n0\n,I\nt\n)randomly sam-\npled from real videos, where we use the estimated flow field\nfromI\n0\ntoI\nt\nto warp encoded features fromI\n0\n, and supervise\npredictions\nˆ\nI\nt\nagainstI\nt\nwith a VGG perceptual loss [45].\nAs shown in Fig. 5, compared to direct average splatting\nand a baseline deep warping method [42], our motion-aware\nfeature splatting produces a frame without holes or artifacts",
    "Feature extractor\nSoftmax\nsplatting\n(Subject to \nW\n)\nSynthesis network\nFigure 4.Rendering module.We fill in missing content and refine\nthe warped input image using a motion-aware deep image-based\nrendering module, where multi-scale features are extracted from the\ninput imageI\n0\n. Softmax splatting is then applied over the features\nwith a motion fieldF\nt\nfrom time0tot(subject to the weightsW\nderived from motion). The warped features are fed to an image\nsynthesis network to produce the refined image\nˆ\nI\nt\n.\naround disocclusions.\n6. Applications\nWe demonstrate applications that add dynamics to single\nstill images using our proposed motion representations and\nanimation pipeline.\nImage-to-video.Our system enables the animation of a\nsingle  still  picture  by  first  predicting  a  neural  stochastic\nmotion texture from the input image and generating an an-\nimation by applying our image-based rendering module to\nthe motion displacement fields derived from the stochastic\nmotion texture. Since we explicitly model scene motions,\nthis allows us to produce slow-motion videos by linear in-\nterpolating the motion displacement fields and to magnify\n(or minify) animated motions by adjusting the amplitude of\npredicted stochastic motion texture coefficients.\nSeamless  looping.It  is  sometimes  useful  to  generate\nvideos with motion that loops seamlessly, meaning that there\nis no appearance or motion discontinuity between the start\nand end of the video. Unfortunately, it is hard to find a large\ncollection of seamlessly looping videos for training diffusion\nmodels. Instead, we devise a method to use our motion dif-\nfusion model, trained on regular non-looping video clips, to\nproduce seamless looping video. Inspired by recent work on\nguidance for image editing [3, 29], our method is amotion\nself-guidancetechnique that guides the motion denoising\nsampling processing using explicit looping constraints. In\nparticular, at each iterative denoising step during the infer-\nence stage, we incorporate an additional motion guidance\nsignal alongside standard classifier-free guidance [41], where\nwe enforce each pixel’s position and velocity at the start and\n(a) Average-splat    (b) Baseline-splat(c) Ours\nFigure 5. From left to right, we show a rendered future frame with\n(a) average splatting in RGB pixel space, (b) softmax splatting with\nlearnable weights [42], and (c) our motion-aware feature splatting.\nend frames to be as similar as possible:\nˆε\nn\n= (1 +w)ε\nθ\n(z\nn\n;n,c)−wε\nθ\n(z\nn\n;n,∅) +uσ\nn\n∇\nz\nn\nL\nn\ng\nL\nn\ng\n=||F\nn\nT\n−F\nn\n1\n||\n1\n+||∇F\nn\nT\n−∇F\nn\n1\n||\n1\n(5)\nwhereF\nn\nt\nis the predicted 2D motion displacement field at\ntimetand denosing stepn.wis the classifier-free guidance\nweight, anduis the motion self-guidance weight. In the\nsupplemental material, we apply baseline appearance-based\nlooping algorithm [54] to generate looping video from our\noutput non-looping example, and show that our motion self-\nguidance technique produces seamless looping videos with\nless distortion and fewer artifacts.\nInteractive dynamics from a single image.As shown in\nDaviset al. [22], the image-space motion spectrum from\nan observed video of an oscillating object is approximately\nproportional to the physical vibration modal basis of that\nobject. The modal shapes capture the oscillation dynamics\nof the object at different frequencies, and hence the image-\nspace projections of an object’s vibration modes can be used\nto simulate the object’s response to a user-defined force such\nas poking or pulling. Therefore, we adopt the modal analysis\ntechnique from prior work [22, 65], which assumes that the\nmotion of an object can be explained by the superposition of\na set of harmonic oscillators. This allows us to write image-\nspace 2D motion displacement field for the object’s physical\nresponse as a weighted sum of Fourier spectrum coefficients\nS\nf\nj\nmodulated by the state of complex modal coordinates\nq\nf\nj\n,t\nat each simulated time stept:\nF\nt\n(p) =\nX\nf\nj\nS\nf\nj\n(p)q\nf\nj\n,t\n(6)\nWe simulate the state of the modal coordinatesq\nf\nj\n,t\nvia\na  forward  Euler  method  applied  to  the  equations  of  mo-\ntion for a decoupled mass-spring-damper system (in modal\nspace) [22, 65]. We refer readers to our supplementary mate-\nrial and the original work for a full derivation. Note that our\nmethod produces an interactive scene from asingle image,\nwhereas these prior methods required a video as input.",
    "7. Experiments\n7.1. Implementation details\nWe  use  an  LDM  [69]  as  the  backbone  for  predicting\nstochastic motion textures, for which we use a variational\nauto-encoder (VAE) with a continuous latent space of di-\nmension4. We train the VAE with anL\n1\nreconstruction loss,\na multi-scale gradient consistency loss [50–52], and a KL-\ndivergence regularization with weight10\n−6\n. We adopt the\nsame 2D U-Net and variance schedule used in the original\nLDM work to iteratively denoise encoded features with a\nMSE loss [40]. For quantitative evaluation, we train the VAE\nand LDM on images of size256×160, which takes around\n6 days to converge using 16 Nvidia A100 GPUs. For our\nmain quantitative and qualitative results, we run the motion\ndiffusion model with DDIM [79] for 500 steps and setη= 1\nto generate stochastic motion textures. For our ablation study,\nwe run DDIM for 200 steps and setη= 0for all the configu-\nrations. We also show generated videos of up to a resolution\nof512×288, created by fine-tuning our models on higher\nresolution data.\nWe adopt ResNet-34 [36] as our multi-scale feature ex-\ntractor.  Our  image  synthesis  network  is  based  on  a  co-\nmodulation StyleGAN architecture, which is a prior con-\nditional image generation and inpainting model [53, 100].\nOur rendering module runs in real-time at 25FPS on a single\nNvidia V100 GPU during inference.\nWe adopt the universal guidance technique [3] to generate\nseamless looping videos, where we set weightsw= 1.5,u=\n200and the number of self-recurrence iterations to2. We\nrefer reader to supplementary material for full details of\nnetwork architectures and hyper-parameter settings.\n7.2. Data and baselines\nData.Since our focus is on natural scenes exhibiting oscil-\nlatory motion such as trees, flowers, and candles moving in\nthe wind, we collect and process a set of 2,631 videos of such\nphenomena from online sources as well as from our own\ncaptures, where we withhold 10% of the videos for testing\nand use the remainder for training. To generate ground truth\nstochastic motion textures for training our motion predic-\ntion module, we apply a coarse-to-fine image pyramid-based\noptical flow algorithm [10, 57] between selected starting\nframes and every future frame within a video sequence. Note\nthat we found the choice of optical flow method to be cru-\ncial. We observed that deep-learning based flow estimators\ntend to produce over-smoothed flow fields, leading to blobby\nor unrealistic animations. We treat every 10th frame from\neach training video as a starting image and generate corre-\nsponding ground truth stochastic motion textures using the\nfollowing 149 frames. We filter out samples with incorrect\nmotion estimates or significant camera motions by removing\nexamples with an average flow motion magnitude>8 pixels,\nImage SynthesisVideo Synthesis\nMethodFID↓FID\nsw\n↓KID↓FVD↓DT-FVD↓\nStochastic I2V [26]   57.962.22.78160.011.6\nMCVD [83]56.360.52.43215.635.5\nLFDM [61]42.346.81.82112.59.49\nDMVFN [44]28.536.31.02104.78.22\nEndoet al. [28]14.317.30.19109.95.35\nOurs3.234.230.0427.411.54\nTable 1.Quantitative comparisons on the test set.We report both\nimage synthesis and video synthesis quality. Here, KID is scaled\nby 100. See Sec. 7.4 for descriptions of baselines and error metrics.\n20406080100120140\nFrame index\n0\n20\n40\n60\n80\n100\nFID\nSling Window FID\nDMVFN\nMCVD\nStochastic I2V\nLFDM\nEndo et al.\nOurs\n20406080100120\nFrame index\n0\n5\n10\n15\n20\n25\n30\nDT-FVD\nSling Window DT-FVD\nFigure 6.Sliding Window FID and DT-FVD.We show sliding\nwindow FID of window size 30 frames, and DT-FVD of size 16\nframes, for videos generated by different methods.\nor where all pixels have an average motion magnitude larger\nthan one pixel. In total, our data consists of more than 130K\nsamples of image-motion pairs.\nBaselines.We  compare  our  approach  to  several  recent\nsingle-image animation and video prediction methods. Both\nEndoet al. [28] and DMVFN [44] predict instantaneous\n2D motion fields and future frames in an auto-regressive\nmanner.  Other  recent  work  such  as  Stochastic  Image-to-\nVideo (I2V) [26] and MCVD [83] adopt either VAEs or\ndiffusion models to predict video frames directly from a\nsingle picture. LFDM [61] predicts flow fields in latent space\nwith a diffusion model, then uses those flow fields to warp the\nencoded input image, generating future frames via a decoder.\nWe apply these models autoregressively to generate longer\nvideos by taking the last output frame and using it as the\ninput to another round of generation until the video reaches\na length of 150 frames. We train all the above methods on\nour data using their respective open-source implementations.\n7.3. Metrics\nWe evaluate the quality of the videos generated by our\napproach and by prior baselines in two main ways. First, we\nevaluate the quality of individual synthesized frames using\nmetrics designed for image synthesis tasks. We adopt the\nFr\n ́\nechet Inception Distance (FID) [38] and Kernel Inception\nDistance (KID) [5] to measure the average distance between\nthe  distribution  of  generated  frames  and  the  distribution",
    "Input imageReferenceStochastic I2V [26]MCVD [83]Endoet al. [28]Ours\nFigure 7.X-tslices of videos generated by different approaches.From left to right: input image and correspondingX-tvideo slices from\nthe ground truth video, from videos generated by three baselines [26, 28, 83], and finally videos generated by our approach.\nof ground truth frames. We further use a sliding window\nFIDFID\nsw\n) with a window size of 30 frames, as proposed\nby [53,56], to measure how generated frame quality degrades\nover time.\nSecond, to evaluate the quality and temporal coherence\nof synthesized videos in both the spatial and temporal do-\nmains, we adopt the Fr\n ́\nechet Video Distance (FVD) [82],\nwhich is based on an I3D model [11] trained on the Human\nKinetics datasets [48]. To more faithfully reflect synthesis\nquality for the natural oscillation motions we seek to gen-\nerate, we also adopt the Dynamic Texture Frechet Video\nDistance (DT-FVD) proposed by Dorkenwaldet al. [26],\nwhich measures FVD with a I3D model trained on the Dy-\nnamic Textures Database [34], a dataset consisting primarily\nof natural motion textures. Similarly, we introduce a sliding\nwindow FVD with window size16to measure how gener-\nated video quality degrades over time. For all the methods,\nwe evaluate each error metric on a256×128central crop\nof the predicted videos with 150 frames generated without\nperforming temporal interpolation, at256×128resolution.\n7.4. Quantitative results\nTable 1 shows quantitative comparisons between our ap-\nproach and baselines on our test set of unseen video clips.\nOur approach significantly outperforms prior single-image\nanimation baselines in terms of both image and video synthe-\nsis quality. Specifically, our much lower FVD and DT-FVD\ndistances suggest that the videos generated by our approach\nare more realistic and more temporally coherent. Further,\nFig. 6 shows the sliding window FID and sliding window DT-\nImage SynthesisVideo Synthesis\nMethodFID↓FID\nsw\n↓KID↓FVD↓DT-FVD↓\nK= 43.20    4.150.0330.181.98\nK= 83.25    4.300.0428.811.85\nK= 243.26    4.250.0427.501.58\nScale w/ resolution  3.75    4.340.0535.051.93\nIndependent pred.   3.20    4.210.0436.301.80\nVolume pred.3.56    4.610.0430.671.80\nAverage splat4.22    5.140.0728.621.76\nBaseline splat [42]  3.69    4.730.0527.981.68\nFull (K= 16)3.21    4.210.0427.631.60\nTable 2.Ablation study.We run all configurations using a DDIM\nwith 200 steps. Please see Sec. 7.6 for the details of the different\nconfigurations.\nFVD distances of generated videos from different methods.\nThanks to our global stochastic motion textures representa-\ntion, videos generated by our approach are more temporally\nconsistent and do not suffer from drift or degradation over\ntime.\n7.5. Qualitative results\nWe visualize qualitative comparisons between videos gen-\nerated by our approach and by baselines in two ways. First,\nwe show spatio-temporalX-tslices of the generated videos,\na standard way of visualizing small or subtle motions in a\nvideo [85]. As shown in Fig. 7, our generated video dynam-",
    "Input imageReferenceEndoet al. [28]DMVFN [44]LFDM [61]Ours\nFigure 8.Visual comparisons of generated future frames and corresponding motion fields.By inspecting differences with a reference\nimage from the ground truth video, we observe that our approach produces more realistic textures and motions compared with baselines. We\nrefer readers to the supplementary video for full results.\nics more strongly resemble the motion patterns observed in\nthe corresponding real reference videos (second column),\ncompared to other methods. Baselines such as Stochastic\nI2V [26] and MCVD [83] fail to model both appearance\nand motion realistically over time. Endoet al. [28] produces\nvideo frames with fewer artifacts but exhibits over-smooth\nor non-oscillation motions.\nWe also qualitatively compare the quality of individual\ngenerated frames and motions across different methods by\nvisualizing the predicted image\nˆ\nI\nt\nand its corresponding mo-\ntion displacement field at timet= 128. Fig. 8 shows that the\nframes generated by our approach exhibit fewer artifacts and\ndistortions compared to other methods, and our correspond-\ning 2D motion fields most resemble the reference displace-\nment fields estimated from the corresponding real videos. In\ncontrast, the background content generated by other methods\ntend to drift, as shown in the flow visualizations in the even-\nnumbered rows. Moreover, the video frames generated by\nother methods exhibit significant color distortion or ghosting\nartifacts, suggesting that the baselines are less stable when\ngenerating videos with long time duration.\n7.6. Ablation study\nWe conduct an ablation study to validate the major de-\nsign choices in our motion prediction and rendering mod-\nules, comparing our full configuration with different variants.\nSpecifically, we evaluate results using different numbers of\nfrequency bandsK= 4,8,16, and24. We observe that\nincreasing the number of frequency bands improves video\nprediction quality, but the improvement is marginal when\nusing more than 16 frequencies. Next, we remove adaptive\nfrequency normalization from the ground truth stochastic\nmotion textures, and instead just scale them based on input\nimage width and height (Scale w/ resolution). Additionally,\nwe remove the frequency coordinated-denoising module (In-\ndependent pred.), or replace it with a simpler module where\na tensor volume of4Kchannel stochastic motion textures\nare predicted jointly via a standard 2D U-net diffusion model\n(Volume pred.). Finally, we compare results where we render\nvideo frames using average splatting (Average splat), or use\na baseline rendering method that applies softmax splatting\nover single-scale features subject to learnable weights used\nin Holynskiet al. [42] (Baseline splat). From Table 2, we\nobserve that all simpler or alternative configurations lead to\nworse performance compared with our full model.\n8. Discussion and conclusion\nLimitations.Since our approach only predicts stochastic\nmotion textures at low frequencies, it might fail to model\ngeneral non-oscillating motions or high-frequency vibrations\nsuch as those of musical instruments. Furthermore, the qual-\nity of our generated videos relies on the quality of the motion\ntrajectories estimated from the real video sequences. Thus,\nwe observed that animation quality can degrade if observed\nmotions in the real videos consists of large displacements.\nMoreover, since our approach is based on image-based ren-\ndering  from  input  pixels,  the  animation  quality  can  also\ndegrade if the generated videos require the creation of large",
    "amounts of content unseen in the input frame.\nConclusion.We  present  a  new  approach  for  modeling\nnatural  oscillation  dynamics  from  a  single  still  picture.\nOur image-space motion prior is represented with a neu-\nral stochastic motion texture, a frequency representation of\nper-pixel motion trajectories, which is learned from collec-\ntions of real world videos. Our stochastic motion textures are\npredicted using our frequency-coordinated latent diffusion\nmodel and are used to animate future video frames using\na neural image-based rendering module. We show that our\napproach produces photo-realistic animations from a single\npicture and significantly outperforms prior baseline methods,\nand that it can enable other downstream applications such as\ncreating interactive animations.\nAcknowledgements.We thank Rick Szeliski, Andrew Liu,\nBoyang Deng, Qianqian Wang, Xuan Luo, and Lucy Chai\nfor fruitful discussions and helpful comments.\nReferences\n[1]Aseem  Agarwala,  Ke  Colin  Zheng,  Chris  Pal,  Maneesh\nAgrawala, Michael Cohen, Brian Curless, David Salesin,\nand Richard Szeliski.  Panoramic video textures.  InACM\nSIGGRAPH 2005 Papers, pages 821–827. 2005.\n[2]Hyemin  Ahn,  Esteve  Valls  Mascaro,  and  Dongheui  Lee.\nCan we use diffusion probabilistic models for 3d motion\nprediction?arXiv preprint arXiv:2302.14503, 2023.\n[3]Arpit    Bansal,    Hong-Min    Chu,    Avi    Schwarzschild,\nSoumyadip Sengupta, Micah Goldblum, Jonas Geiping, and\nTom Goldstein.  Universal guidance for diffusion models.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 843–852, 2023.\n[4]Hugo  Bertiche,  Niloy  J  Mitra,  Kuldeep  Kulkarni,  Chun-\nHao P Huang, Tuanfeng Y Wang, Meysam Madadi, Sergio\nEscalera, and Duygu Ceylan. Blowing in the wind: Cyclenet\nfor human cinemagraphs from still images.   InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 459–468, 2023.\n[5]\nMikołaj Bi\n ́\nnkowski, Danica J Sutherland, Michael Arbel,\nand Arthur Gretton.   Demystifying MMD GANs.arXiv\npreprint arXiv:1801.01401, 2018.\n[6]Andreas Blattmann, Timo Milbich, Michael Dorkenwald,\nand Bj\n ̈\norn Ommer. ipoke: Poking a still image for controlled\nstochastic video synthesis. InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 14707–\n14717, 2021.\n[7]Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with\nlatent diffusion models.  InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 22563–22575, 2023.\n[8]Richard Strong Bowen, Richard Tucker, Ramin Zabih, and\nNoah Snavely. Dimensions of motion: Monocular prediction\nthrough flow subspaces. In2022 International Conference\non 3D Vision (3DV), pages 454–464. IEEE, 2022.\n[9]Tim  Brooks,  Janne  Hellsten,  Miika  Aittala,  Ting-Chun\nWang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei\nEfros, and Tero Karras. Generating long videos of dynamic\nscenes.Advances in Neural Information Processing Systems,\n35:31769–31781, 2022.\n[10]Thomas Brox, Andr\n ́\nes Bruhn, Nils Papenberg, and Joachim\nWeickert.  High accuracy optical flow estimation based on\na theory for warping. InComputer Vision-ECCV 2004: 8th\nEuropean Conference on Computer Vision, Prague, Czech\nRepublic, May 11-14, 2004. Proceedings, Part IV 8, pages\n25–36. Springer, 2004.\n[11]Joao Carreira and Andrew Zisserman.  Quo vadis, action\nrecognition? a new model and the kinetics dataset.  Inpro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299–6308, 2017.\n[12]\nDan Casas, Marco Volino, John Collomosse, and Adrian\nHilton.  4d video textures for interactive character appear-\nance.   InComputer  Graphics  Forum,  volume  33,  pages\n371–380. Wiley Online Library, 2014.\n[13]Antoni B Chan and Nuno Vasconcelos. Mixtures of dynamic\ntextures. InTenth IEEE International Conference on Com-\nputer Vision (ICCV’05) Volume 1, volume 1, pages 641–647.\nIEEE, 2005.\n[14]Antoni B Chan and Nuno Vasconcelos.  Classifying video\nwith kernel dynamic textures. In2007 IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1–6. IEEE,\n2007.\n[15]\nAntoni B Chan and Nuno Vasconcelos. Modeling, clustering,\nand segmenting video with mixtures of dynamic textures.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 30(5):909–926, 2008.\n[16]Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\nWilliam T Freeman, Michael Rubinstein, et al. Muse: Text-\nto-image generation via masked generative transformers.\narXiv preprint arXiv:2301.00704, 2023.\n[17]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman.  Maskgit: Masked generative image transformer.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315–11325, 2022.\n[18]Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-\nYi Lin, and Ming-Hsuan Yang.  Motion-conditioned diffu-\nsion model for controllable video synthesis.arXiv preprint\narXiv:2304.14404, 2023.\n[19]\nXin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18000–18010, 2023.\n[20]Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian\nCurless, David H Salesin, and Richard Szeliski.  Animat-\ning pictures with stochastic motion textures. InACM SIG-\nGRAPH 2005 Papers, pages 853–860. 2005.\n[21]\nVincent C Couture, Michael S Langer, and Sebastien Roy.\nOmnistereo video textures without ghosting. In2013 Inter-\nnational Conference on 3D Vision-3DV 2013, pages 64–70.\nIEEE, 2013.",
    "[22]Abe Davis, Justin G Chen, and Fr\n ́\nedo Durand. Image-space\nmodal bases for plausible manipulation of objects in video.\nACM Transactions on Graphics (TOG), 34(6):1–7, 2015.\n[23]Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis.Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021.\n[24]Julien Diener, Mathieu Rodriguez, Lionel Baboud, and Li-\nonel Reveret. Wind projection basis for real-time animation\nof trees.  InComputer graphics forum, volume 28, pages\n533–540. Wiley Online Library, 2009.\n[25]Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu, and\nStefano Soatto. Dynamic textures.International journal of\ncomputer vision, 51:91–109, 2003.\n[26]Michael Dorkenwald, Timo Milbich, Andreas Blattmann,\nRobin Rombach, Konstantinos G. Derpanis, and Bjorn Om-\nmer.  Stochastic image-to-video synthesis using cinns.  In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 3742–3753,\nJune 2021.\n[27]\nYuming Du, Robin Kips, Albert Pumarola, Sebastian Starke,\nAli Thabet, and Artsiom Sanakoyeu.   Avatars grow legs:\nGenerating smooth human motion from sparse tracking in-\nputs with diffusion model. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 481–490, 2023.\n[28]Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama.\nAnimating landscape: Self-supervised learning of decoupled\nmotion and appearance for single-image video synthesis.\nACM Transactions on Graphics (Proceedings of ACM SIG-\nGRAPH Asia 2019), 38(6):175:1–175:19, 2019.\n[29]\nDave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation.arXiv preprint arXiv:2306.00986,\n2023.\n[30]Matthew  Flagg,  Atsushi  Nakazawa,  Qiushuang  Zhang,\nSing Bing Kang, Young Kee Ryu, Irfan Essa, and James M\nRehg.  Human video textures.  InProceedings of the 2009\nsymposium on Interactive 3D graphics and games, pages\n199–206, 2009.\n[31]Jean-Yves Franceschi, Edouard Delasalles, Micka\n ̈\nel Chen,\nSylvain Lamprier, and Patrick Gallinari.  Stochastic latent\nresidual video prediction.  InInternational Conference on\nMachine Learning, pages 3233–3246. PMLR, 2020.\n[32]Ruohan Gao, Bo Xiong, and Kristen Grauman.  Im2Flow:\nMotion hallucination from static images for action recog-\nnition. InProc. Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[33]\nYuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning.arXiv preprint arXiv:2307.04725, 2023.\n[34]Isma Hadji and Richard P Wildes.  A new large scale dy-\nnamic  texture  dataset  with  application  to  convnet  under-\nstanding.  InProceedings of the European Conference on\nComputer Vision (ECCV), pages 320–335, 2018.\n[35]Zekun Hao, Xun Huang, and Serge Belongie. Controllable\nvideo generation with sparse trajectories.  InProceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 7854–7863, 2018.\n[36]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.  InProceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[37]Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng,  Ying  Shan,  et  al.    Animate-a-story:  Storytelling\nwith retrieval-augmented video generation.arXiv preprint\narXiv:2307.06940, 2023.\n[38]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter.  Gans trained by a two\ntime-scale  update  rule  converge  to  a  local  nash  equilib-\nrium.Advances in neural information processing systems,\n30, 2017.\n[39]\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi  Gao,  Alexey  Gritsenko,  Diederik  P  Kingma,  Ben\nPoole, Mohammad Norouzi, David J Fleet, et al.  Imagen\nvideo: High definition video generation with diffusion mod-\nels.arXiv preprint arXiv:2210.02303, 2022.\n[40]\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models.Advances in neural information\nprocessing systems, 33:6840–6851, 2020.\n[41]\nJonathan Ho and Tim Salimans.  Classifier-free diffusion\nguidance.arXiv preprint arXiv:2207.12598, 2022.\n[42]Aleksander Holynski, Brian L Curless, Steven M Seitz, and\nRichard Szeliski. Animating pictures with Eulerian motion\nfields.   InProceedings  of  the  IEEE/CVF  Conference  on\nComputer Vision and Pattern Recognition, pages 5810–5819,\n2021.\n[43]\nTobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,\nand Andrea Dittadi. Diffusion models for video prediction\nand infilling.Trans. Mach. Learn. Res., 2022, 2022.\n[44]Xiaotao  Hu,  Zhewei  Huang,  Ailin  Huang,  Jun  Xu,  and\nShuchang Zhou. A dynamic multi-scale voxel flow network\nfor video prediction.ArXiv, abs/2303.09875, 2023.\n[45]\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei.  Percep-\ntual losses for real-time style transfer and super-resolution.\nInComputer Vision–ECCV 2016: 14th European Confer-\nence, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part II 14, pages 694–711. Springer, 2016.\n[46]Hitoshi Kanda and Jun Ohya.  Efficient, realistic method\nfor animating dynamic behaviors of 3d botanical trees.  In\n2003 International Conference on Multimedia and Expo.\nICME’03.  Proceedings  (Cat.  No.  03TH8698),  volume  2,\npages II–89. IEEE, 2003.\n[47]Johanna Karras, Aleksander Holynski, Ting-Chun Wang,\nand  Ira  Kemelmacher-Shlizerman.   Dreampose:  Fashion\nimage-to-video synthesis via stable diffusion.arXiv preprint\narXiv:2304.06025, 2023.\n[48]\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset.arXiv preprint arXiv:1705.06950,\n2017.",
    "[49]Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel,\nChelsea Finn, and Sergey Levine.  Stochastic adversarial\nvideo prediction.arXiv preprint arXiv:1804.01523, 2018.\n[50]Zhengqi  Li,  Tali  Dekel,  Forrester  Cole,  Richard  Tucker,\nNoah Snavely, Ce Liu, and William T Freeman.   Learn-\ning the depths of moving people by watching frozen people.\nInProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 4521–4530, 2019.\n[51]Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. InProceedings\nof  the  IEEE  conference  on  computer  vision  and  pattern\nrecognition, pages 2041–2050, 2018.\n[52]Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. InProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 4273–4284,\n2023.\n[53]\nZhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo\nKanazawa. Infinitenature-zero: Learning perpetual view gen-\neration of natural scenes from single images. InEuropean\nConference on Computer Vision, pages 515–534. Springer,\n2022.\n[54]Jing Liao, Mark Finch, and Hugues Hoppe. Fast computation\nof seamless video loops.ACM Transactions on Graphics\n(TOG), 34(6):1–10, 2015.\n[55]Zicheng Liao, Neel Joshi, and Hugues Hoppe. Automated\nvideo looping with progressive dynamism.ACM Transac-\ntions on Graphics (TOG), 32(4):1–10, 2013.\n[56]Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Maka-\ndia, Noah Snavely, and Angjoo Kanazawa. Infinite nature:\nPerpetual view generation of natural scenes from a single\nimage. InProceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 14458–14467, 2021.\n[57]\nCe Liu.Beyond pixels: exploring new representations and\napplications for motion analysis. PhD thesis, Massachusetts\nInstitute of Technology, 2009.\n[58]\nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.   Videofusion: Decomposed diffusion models\nfor high-quality video generation.   InProceedings of the\nIEEE/CVF  Conference  on  Computer  Vision  and  Pattern\nRecognition, pages 10209–10218, 2023.\n[59]Long Mai and Feng Liu. Motion-adjustable neural implicit\nvideo representation. InProceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10738–10747, 2022.\n[60]Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Implicit\nwarping for animation with image sets.Advances in Neural\nInformation Processing Systems, 35:22438–22450, 2022.\n[61]Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and\nMartin Renqiang Min.  Conditional image-to-video gener-\nation with latent flow diffusion models. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18444–18455, 2023.\n[62]\nSimon Niklaus and Feng Liu. Softmax splatting for video\nframe interpolation. InProceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5437–5446, 2020.\n[63]Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed\nElgharib, Fr’edo Durand, William T Freeman, and Wojciech\nMatusik.  Learning-based video motion magnification.  In\nProceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 633–648, 2018.\n[64]Shin Ota, Machiko Tamura, Kunihiko Fujita, T Fujimoto,\nK Muraoka, and Norishige Chiba.  1/f/sup/spl beta//noise-\nbased real-time animation of trees swaying in wind fields. In\nProceedings Computer Graphics International 2003, pages\n52–59. IEEE, 2003.\n[65]\nAutomne  Petitjean,  Yohan  Poirier-Ginter,  Ayush  Tewari,\nGuillaume Cordonnier, and George Drettakis.  Modalnerf:\nNeural modal analysis and synthesis for free-viewpoint navi-\ngation in dynamically vibrating scenes. InComputer Graph-\nics Forum, volume 42, 2023.\n[66]\nSilvia  L.  Pintea,  Jan  C.  van  Gemert,  and  Arnold  W.  M.\nSmeulders. D\n ́\nej\n`\na vu: Motion prediction in static images. In\nProc. European Conf. on Computer Vision (ECCV), 2014.\n[67]\nSigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H\nBermano, and Daniel Cohen-Or.  Single motion diffusion.\narXiv preprint arXiv:2302.05905, 2023.\n[68]Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen.  Hierarchical text-conditional image gen-\neration with clip latents.arXiv preprint arXiv:2204.06125,\n1(2):3, 2022.\n[69]Robin  Rombach,  Andreas  Blattmann,  Dominik  Lorenz,\nPatrick Esser, and Bj\n ̈\norn Ommer.  High-resolution image\nsynthesis with latent diffusion models.  InProceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022.\n[70]Chitwan  Saharia,  William  Chan,  Saurabh  Saxena,  Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al.   Photorealistic text-to-image diffusion models with\ndeep language understanding.Advances in Neural Informa-\ntion Processing Systems, 35:36479–36494, 2022.\n[71]Payam Saisan, Gianfranco Doretto, Ying Nian Wu, and Ste-\nfano Soatto. Dynamic texture recognition. InProceedings\nof the 2001 IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition. CVPR 2001, volume 2,\npages II–II. IEEE, 2001.\n[72]Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek\nKar, Mohammad Norouzi, Deqing Sun, and David J. Fleet.\nThe surprising effectiveness of diffusion models for optical\nflow and monocular depth estimation, 2023.\n[73]Arno Sch\n ̈\nodl, Richard Szeliski, David H Salesin, and Irfan\nEssa. Video textures. InProceedings of the 27th annual con-\nference on Computer graphics and interactive techniques,\npages 489–498, 2000.\n[74]Aliaksandr Siarohin, St\n ́\nephane Lathuili\n`\nere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe.   Animating arbitrary objects\nvia deep motion transfer. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2377–2386, 2019.\n[75]Aliaksandr Siarohin, St\n ́\nephane Lathuili\n`\nere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for im-\nage animation.Advances in neural information processing\nsystems, 32, 2019.",
    "[76]Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai,  and  Sergey  Tulyakov.   Motion  representations  for\narticulated animation.   InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13653–13662, 2021.\n[77]Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny.  Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2.  InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3626–3636, 2022.\n[78]Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand  Surya  Ganguli.    Deep  unsupervised  learning  using\nnonequilibrium thermodynamics.  InInternational confer-\nence on machine learning, pages 2256–2265. PMLR, 2015.\n[79]Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models.arXiv:2010.02502, October\n2020.\n[80]Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions.arXiv preprint arXiv:2011.13456, 2020.\n[81]\nGuy  Tevet,  Sigal  Raab,  Brian  Gordon,  Yonatan  Shafir,\nDaniel Cohen-Or, and Amit H Bermano.  Human motion\ndiffusion model.arXiv preprint arXiv:2209.14916, 2022.\n[82]\nThomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael  Marinier,  Marcin  Michalski,  and  Sylvain  Gelly.\nTowards accurate generative models of video: A new metric\n& challenges.arXiv preprint arXiv:1812.01717, 2018.\n[83]Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher\nPal. Mcvd: Masked conditional video diffusion for predic-\ntion, generation, and interpolation. In(NeurIPS) Advances\nin Neural Information Processing Systems, 2022.\n[84]Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. InNeural Informa-\ntion Processing Systems, 2016.\n[85]Neal  Wadhwa,  Michael  Rubinstein,  Fr\n ́\nedo  Durand,  and\nWilliam T Freeman.   Phase-based video motion process-\ning.ACM Transactions on Graphics (ToG), 32(4):1–10,\n2013.\n[86]\nJacob Walker, Carl Doersch, Abhinav Gupta, and Martial\nHebert. An uncertain future: Forecasting from static images\nusing variational autoencoders. InProc. European Conf. on\nComputer Vision (ECCV), 2016.\n[87]Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense\noptical flow prediction from a static image. InProceedings\nof the IEEE International Conference on Computer Vision,\npages 2443–2451, 2015.\n[88]Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu  Wang,  Yingya  Zhang,  Yujun  Shen,  Deli  Zhao,\nand Jingren Zhou.  Videocomposer: Compositional video\nsynthesis  with  motion  controllability.arXiv  preprint\narXiv:2306.02018, 2023.\n[89]Yaohui  Wang,  Di  Yang,  Francois  Bremond,  and  Antitza\nDantcheva.Latent  image  animator:  Learning  to  ani-\nmate images via latent space navigation.arXiv preprint\narXiv:2203.09043, 2022.\n[90]Frederik  Warburg,  Ethan  Weber,  Matthew  Tancik,  Alek-\nsander Holynski, and Angjoo Kanazawa. Nerfbusters: Re-\nmoving ghostly artifacts from casually captured nerfs.arXiv\npreprint arXiv:2304.10532, 2023.\n[91]\nDaniel  Watson,  William  Chan,  Ricardo  Martin-Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.\nNovel view synthesis with diffusion models.arXiv preprint\narXiv:2210.04628, 2022.\n[92]\nChung-Yi  Weng,  Brian  Curless,  and  Ira  Kemelmacher-\nShlizerman. Photo wake-up: 3d character animation from a\nsingle photo. InProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5908–5917,\n2019.\n[93]Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF:\nRegularizing Neural Radiance Fields with Denoising Diffu-\nsion Models. InCVPR, 2023.\n[94]\nTianfan   Xue,   Jiajun   Wu,   Katherine   L   Bouman,   and\nWilliam T Freeman. Visual dynamics: Stochastic future gen-\neration via layered cross convolutional networks.Trans. Pat-\ntern Analysis and Machine Intelligence, 41(9):2236–2250,\n2019.\n[95]\nShengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan.  Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory.arXiv preprint arXiv:2308.08089, 2023.\n[96]Sihyun  Yu,  Kihyuk  Sohn,  Subin  Kim,  and  Jinwoo  Shin.\nVideo  probabilistic  diffusion  models  in  projected  latent\nspace. InProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18456–18466,\n2023.\n[97]Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,\nFangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re-\nmodiffuse:  Retrieval-augmented  motion  diffusion  model.\narXiv preprint arXiv:2304.01116, 2023.\n[98]\nYabo  Zhang,  Yuxiang  Wei,  Dongsheng  Jiang,  Xiaopeng\nZhang,  Wangmeng  Zuo,  and  Qi  Tian.Controlvideo:\nTraining-free controllable text-to-video generation.arXiv\npreprint arXiv:2305.13077, 2023.\n[99]\nJian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. InProceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657–3666, 2022.\n[100]Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image com-\npletion via co-modulated generative adversarial networks.\nInInternational Conference on Learning Representations\n(ICLR), 2021.\n[101]Daquan  Zhou,  Weimin  Wang,  Hanshu  Yan,  Weiwei  Lv,\nYizhe Zhu, and Jiashi Feng.  Magicvideo: Efficient video\ngeneration  with  latent  diffusion  models.arXiv  preprint\narXiv:2211.11018, 2022."
  ]
}