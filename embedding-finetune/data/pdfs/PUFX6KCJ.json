{
  "key": "PUFX6KCJ",
  "url": "http://arxiv.org/pdf/2310.07704",
  "metadata": {
    "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
    "abstract": "  We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of\nunderstanding spatial referring of any shape or granularity within an image and\naccurately grounding open-vocabulary descriptions. To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid\nregion representation that integrates discrete coordinates and continuous\nfeatures jointly to represent a region in the image. To extract the continuous\nfeatures of versatile regions, we propose a spatial-aware visual sampler, adept\nat handling varying sparsity across different shapes. Consequently, Ferret can\naccept diverse region inputs, such as points, bounding boxes, and free-form\nshapes. To bolster the desired capability of Ferret, we curate GRIT, a\ncomprehensive refer-and-ground instruction tuning dataset including 1.1M\nsamples that contain rich hierarchical spatial knowledge, with 95K hard\nnegative data to promote model robustness. The resulting model not only\nachieves superior performance in classical referring and grounding tasks, but\nalso greatly outperforms existing MLLMs in region-based and\nlocalization-demanded multimodal chatting. Our evaluations also reveal a\nsignificantly improved capability of describing image details and a remarkable\nalleviation in object hallucination. Code and data will be available at\nhttps://github.com/apple/ml-ferret\n",
    "published": "2023-10-11T17:55:15Z"
  },
  "text": [
    "Preprint\nFERRET:REFER   ANDGROUNDANYTHINGANY-\nWHERE ATANYGRANULARITY\n\nHaoxuan You\n1†\n, Haotian Zhang\n2†\n, Zhe Gan\n2\n, Xianzhi Du\n2\n, Bowen Zhang\n2\n, Zirui Wang\n2\n,\nLiangliang Cao\n2\n, Shih-Fu Chang\n1\n, Yinfei Yang\n2\n1\nColumbia University,\n2\nApple AI/ML\nhaoxuan.you@cs.columbia.edu,{haotian\nzhang2,zhe.gan,yinfeiy}@apple.com\nWhat animal is in area [region0]?\nIt is a ferret[box0].\nCool, what is the animal [region1]? \nAre they the same one?\nNo, the animal [box1] is a ferret, \nwhich is a different one from the \none [box0]. They are not the same \nferret.\nHow to make the food[region0]\nwith available ingredients?\nTomakeasandwich[box0]withavailableingredients,\n1.Youcanpreparethesandwichesbyplacingthe\nslicedturkey[box1]ononebread,andthesliced\ncheese[box2]onanother.\n2.Toassemblethesandwiches,pressthetwohalves\nofthesandwichtogether,andyoucanusetheknife\n[box3]oryourhands[box4]toholdthemtogetherif\nneeded.Onceyouhaveacompletesandwich,youcan\nusetheknifetocutitintoquartersorhalves,andserve\nitonaplate[box5].\nregion0\nbox0\nbox1  \nbox2  \nbox3  \nbox4  \nbox5  \nInput Referring\nOutput Grounding\nFerret\nregion1\nregion0\nbox0 \nbox1 \nFigure 1: Ferret enablesreferringandgroundingcapabilities for multimodal large language model\n(LLM). In terms of referring, a user can refer to a region or an object in point, box, or any free-form\nshape. TheregionNin the input will be replaced by the proposed hybrid representation before being\nfed into the LLM. In terms of grounding, Ferret is able to accurately ground any open-vocabulary\ndescriptions. TheboxNin the output denotes the predicted bounding box coordinates.\nABSTRACT\nWe introduce Ferret, a new Multimodal Large Language Model (MLLM) capa-\nble of understanding spatial referring of any shape or granularity within an image\nand accurately grounding open-vocabulary descriptions.  To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid re-\ngion  representation  that  integrates  discrete  coordinates  and  continuous  features\njointly  to  represent  a  region  in  the  image.   To  extract  the  continuous  features\nof  versatile  regions,  we  propose  a  spatial-aware  visual  sampler,  adept  at  han-\ndling varying sparsity across different shapes.   Consequently,  Ferret can accept\ndiverse region inputs, such as points, bounding boxes, and free-form shapes.  To\nbolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-\nand-ground instruction tuning dataset including 1.1M samples that contain rich\nhierarchical spatial knowledge,  with 95K hard negative data to promote model\nrobustness.  The resulting model not only achieves superior performance in clas-\nsical referring and grounding tasks, but also greatly outperforms existing MLLMs\nin region-based and localization-demanded multimodal chatting. Our evaluations\nalso reveal a significantly improved capability of describing image details and a\nremarkable alleviation in object hallucination.  Code and data will be available at\nhttps://github.com/apple/ml-ferret.\n\nWork done during an internship at Apple.\n†\nEqual contribution.\n1\narXiv:2310.07704v1  [cs.CV]  11 Oct 2023",
    "Preprint\n1INTRODUCTION\nIn vision-language learning,  how to enable spatial understanding of models is a fundamental re-\nsearch problem.  Two desired capabilities stem from this problem:referringandgrounding.  Re-\nferring  demands  that  the  model  can  accurately  comprehend  the  semantics  of  specific  given  re-\ngions (Krahmer & Van Deemter, 2012; Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016;\nZellers et al., 2019), whereas grounding necessitates the model to localize the region in accordance\nwith the given semantic description (Luo & Shakhnarovich, 2017; Nagaraja et al., 2016; Yu et al.,\n2017; Kamath et al., 2021).\nEssentially,referringandgroundingdemand  the  same  type  of  knowledge:  alignment  of  spatial\ninformation and semantics. Despite that, existing works mostly learn referring and grounding indi-\nvidually (Li et al., 2022; Wu et al., 2022; Yu et al., 2017). In comparison, humans can learn from one\ntask and generalize the shared knowledge to the other task effortlessly, and are able to seamlessly\nintegrate referring/grounding capabilities with daily dialogue and reasoning (Zellers et al., 2019).\nInspired by the above gap, in this paper, we study three main questions: (i) How to unify referring\nand grounding in one framework, and will they benefit each other?  (ii) How to represent versatile\ntypes of regions that humans usually use for referring, such as point, box, scribble, and even free-\nform shapes?  (iii) How to make referring and grounding open-vocabulary, instruction-following,\nand robust, which are crucial for practical applications?\nTargeting these three questions, we introduceFerret, a novel refer-and-ground Multimodal Large\nLanguage Model (MLLM). First of all, we choose MLLM as the bedrock of Ferret due to their pow-\nerful vision-language global understanding capability (Zhu et al., 2023a; Liu et al., 2023b; Li et al.,\n2023c). To unify referring and grounding, Ferret first represents the coordinates of regions in natural\nlanguage numerical form,\n1\nas illustrated in Figure 3.  However, it is inefficient to use single point\nor box coordinates to represent versatile shapes of regions, such as strokes, scribbles, or complex\npolygons.  These shapes are essential for more universal and precise human-model interaction.  To\nsolve this problem, we further propose a spatial-aware visual sampler to acquire the visual features\nfor regions in any shape,  taking care of the varying sparsity in those shapes.   Then,  the discrete\ncoordinates and the continuous visual features are combined together to represent the visual regions\nin the input, composing a hybrid region representation in Ferret.  Equipped with above methods,\nFerret can deal with input that mixes referred regions with free-form text, and is able to ground the\nmentioned objects in its output by seamlessly generating the coordinates for each groundable object\nalong with generating text.  To our best knowledge, Ferret is the first work that is able to process\nfree-formed region inputs in MLLMs.\nIn order to make the refer-and-ground capability in Ferret open-vocabulary, instruction-following,\nand robust, we collectGRIT, aGround-and-ReferInstruction-Tuning dataset with 1.1M samples.\nGRIT contains multiple levels of spatial knowledge, covering objects, relationships, region descrip-\ntions, and complex reasoning. It includes both text-in location-out (grounding) and location-in text-\nout (referring) data, as well as data that mixes location and text in both input and output.  The ma-\njority of the dataset is converted from existing vision(-language) tasks like object detection (Krishna\net al., 2017) and phrase grounding (Yu et al., 2016; Plummer et al., 2015) with carefully designed\ntemplates to make it instruction-following.  Additionally, 34K refer-and-ground instruction-tuning\nconversations are collected via the help of ChatGPT/GPT-4 (OpenAI, 2023b) to facilitate training\nan instruction-following and open-vocabulary refer-and-ground generalist.  Moreover, we conduct\nspatial-aware negative data mining, which further promotes model robustness.\nFerret  subsumes  strong  open-vocabulary  capabilities  of  spatial  understanding  and  localization.\nWhen evaluated on conventional referring and grounding tasks, it achieves superior performance.\nMore than that, we believe refer-and-ground capabilities should be integrated into daily conversa-\ntions of humans,e.g., people refer to something they don’t know and ask what it is used for (like\nFigure 1). To evaluate this new capability, we introduceFerret-Bench, covering three new types of\ntasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark\nexisting MLLMs and observe that Ferret can outperform the best of them by 20.4% on average.\nMoreover, Ferret demonstrates an intriguing property of alleviating object hallucinations.\n1\nNote that there is no additional vocabulary or position encoders introduced in Ferret model.\n2",
    "Preprint\nTable 1: Comparison of Ferretv.s.recent MLLMs integrating spatial awareness. ‘Convention’ refers\nto a comprehensive collection of publicly available data that has been transformed using templates,\n‘GPT-Generate’ signifies the generated refer/ground datasets employing GPT, and ‘Robustness’ de-\nnotes datasets aimed at mitigating hallucination and improving robustness. Section 4 explains more\ndetails about each.\nModel\nInput TypesOutput\nGrounding\nData ConstructionQuantatitive Eval.\nof Refer/Ground\nw. Chat\nPoint   Box   Free-formConvention   GPT-Generate   Robustness\nBuboGPT✗✗✗✔✔✗✗✗\nVision-LLM✗✗✗✔✔✗✗✗\nKosmos-2✗✔✗✔✔✗✗✗\nShikra✔✔✗✔✔✔✗✗\nGPT4-ROI✗✔✗✗✔✗✗✗\nPVIT✗✔✗✗✔✔✗✔\nFerret✔✔✔✔✔✔✔✔\nIn summary, our contributions are threefold.  (i) We propose Ferret, that uses a hybrid region rep-\nresentation equipped with a novel spatial-aware visual sampler,  to enable fine-grained and open-\nvocabulary referring and grounding in MLLM. (ii) We construct GRIT, a large-scale ground-and-\nrefer instruction tuning dataset, for model training. It also contains additional spatial negative sam-\nples to enhance model robustness.   (iii) We introduce Ferret-Bench,  to evaluate tasks jointly re-\nquiring  referring/grounding,  semantics,  knowledge,  and  reasoning.   Our  model  exhibits  superior\nperformance in a wide range of tasks and reduces object hallucination.\n2RELATEDWORK\nMultimodal  large  language  models  (MLLMs).Large  Language  Models  (LLMs),  including\nGPTs (Brown et al., 2020; OpenAI, 2023a), PaLM (Chowdhery et al., 2022), BLOOM (Scao et al.,\n2022), and LLaMA (Touvron et al., 2023a;b), have revolutionized research in NLP, spurring sig-\nnificant  advances  in  multimodal  language  models  as  well.   Early  models  primarily  focused  on\nlarge-scale  image-text  pre-training.    Notable  examples  include  SimVLM  (Wang  et  al.,  2022c),\nGIT  (Wang  et  al.,  2022a),  PaLI  (Chen  et  al.,  2022b),  PaLI-X  (Chen  et  al.,  2023c),  BLIP-2  (Li\net al., 2023c), Flamingo (Alayrac et al., 2022), PaLM-E (Driess et al., 2023), CM3 (Aghajanyan\net al., 2022),  and CM3Leon (Yu et al., 2023).  Flamingo,  in particular,  pioneered the integration\nof a pre-trained CLIP image encoder with LLMs through gated cross-attention blocks, showcasing\nemergent multimodal in-context few-shot learning capabilities.  Its open-sourced variants, such as\nOpenFlamingo (Awadalla et al., 2023) and IDEFICS (Laurenc ̧on et al., 2023), have garnered sig-\nnificant attention.  Typically, these models undergo pre-training using millions or even billions of\nimage-text pairs and interleaved image-text datasets (Zhu et al., 2023b).\nOn the other hand, recent research has increasingly focused on using pre-trained LLMs for visual\ninstruction tuning. Prominent examples include LLaVA (Liu et al., 2023b), MiniGPT-4 (Zhu et al.,\n2023a),  mPLUG-Owl (Ye et al., 2023),  Otter (Li et al., 2023a),  InstructBLIP (Dai et al., 2023),\nto name a few.  In addition to text generation, recent models like FROMAGe (Koh et al., 2023b),\nGILL (Koh et al., 2023a), Emu (Sun et al., 2023), have also enabled MLLMs for image retrieval and\nimage generation. Please refer to Chapter 5 of Li et al. (2023b) for a detailed review.\nMLLMs for referring and grounding.In the realm of existing literature, works such as Kosmos-\n2 (Peng et al., 2023) and Shikra (Chen et al., 2023b),  closely resemble ours as they also enable\nMLLMs  for  fine-grained  image  comprehension  and  open-world  referring  and  grounding.   Addi-\ntional works in this direction include GPT4ROI (Zhang et al., 2023), PVIT (Chen et al., 2023a),\nBuboGPT (Zhao et al., 2023), VisionLLM (Wang et al., 2023), and ContextDET (Zang et al., 2023).\nNevertheless, pivotal distinctions set our model apart. First, prior endeavors supported only bound-\ning boxes (and points in Shikra) as input. Conversely, due to Ferret’s innovative hybrid region repre-\nsentation, we accommodate a broader range of free-form shapes for referring, encompassing points,\nboxes, sketches, scribbles, polygons, and more. Second, we meticulously curate an extensive refer-\nand-ground instruction tuning dataset.  Third, we introduce Ferret-Bench to facilitate forthcoming\n3",
    "Preprint\nresearch and enhance evaluation benchmarks in this direction.  Lastly, our model exhibits superior\nperformance compared to previous works, notably mitigating object hallucination to a significant\nextent. A more straightforward side-by-side comparison is shown in Tab. 1.\nUnifying grounding and VL understanding.Our work is also related to previous work that aims\nto unify text and bounding box output for vision-language (VL) models,  such as UniTAB (Yang\net  al.,  2022),  OFA  (Wang  et  al.,  2022b),  and  Unified-IO  (Lu  et  al.,  2022),  which  also  represent\nbounding boxes using a set of additional discrete tokens as proposed in Pix2Seq (Chen et al., 2021;\n2022a). Ferret is unique in that (i) our model is built upon LLMs, marrying the power of LLMs and\ngrounding, thus unlocking new capabilities such as grounded instruction tuning, and (ii) we handle\nbounding  box  coordinates  as  regular  text  tokens,  avoiding  the  need  for  extra  specialized  tokens\ndedicated to representing boxes.\n3METHOD\nWe start with detailing the proposed hybrid region representation to depict regions of various shapes\nand formats. Then, we present the model architecture of Ferret.\n3.1HYBRIDREGIONREPRESENTATION\nThe object [obj0] is a pistol, and \nthe object [obj1] is a knife\nWhat is the object [region0] \nand object [region1]? \nregion0\nregion1\nFigure  2:  Bounding  box\nv.s.Free-from   Shape.\nThese  two  objects  have\nalmost  the  same  bound-\ning box, causing ambigu-\nity  when  relying  on  the\nbox to refer to. Equipped\nwith hybrid region repre-\nsentation, Ferret can sep-\narate them.\nWhen  referring  to  specific  regions,  three  primary  formats  are  generally\nused:  point,  box,  and  free-form  shapes.   While  the  point  and  box  for-\nmats can be succinctly represented by coordinates (e.g., [x,y] for a point,\nand [x\nmin\n,y\nmin\n,x\nmax\n,y\nmax\n] for a box) as in Peng et al. (2023); Chen et al.\n(2023b),  the  free-form  shape  is  more  versatile,  encompassing  a  variety\nof region types such as scribbles,  polygons,  and masks.   The advantage\nof  free-form  shape  is  straightforwardly  illustrated  in  Figure  2.   Depict-\ning free-form shapes through coordinates is computationally expensive and\nobscure, and its complexity hinders the model learning to establish a clear\ncorrelation  between  the  provided  coordinates  and  the  corresponding  re-\ngions.\nTo generalize across all three distinct formats, we propose a hybrid region\nrepresentation that synergizes discrete coordinates with continuous visual\nfeatures to refer to a particular region, which is shown in the top-left of\nFigure 3. For coordinates, following Chen et al. (2021); Yang et al. (2022),\nwe quantize each coordinate into one of then\nbins\ndiscrete bins.\n2\nRegarding\ncontinuous visual features, for a given regionR, we first construct a 2D\nbinary maskMof the same size as the image, marking a value of 1 inside\nthe targeted region and 0 outside of the region. Then, the binary maskM,\njointly with the extracted image feature mapZ, is sent into our proposed\nspatial-aware visual samplers(·), which will be detailed in Section 3.2, to\nextract the visual continuous featuref=s(M,Z).\nFinally, we represent a point with{x,y,f\nR\np\n}, where the regionR\np\nis a circle centered in{x,y}with\na fixed radius.\n3\nA box or a free-form shape can both be represented by{x\nmin\n,y\nmin\n,x\nmax\n,y\nmax\n,f\nR\nbox\n},\nwherex\nmin\n/x\nmax\ndenotes the minimum/maximumx-axis coordinate of the region, and so forth for\ny-axis.R\nbox\ndenotes the input region.\n3.2MODELARCHITECTURE\nAs  illustrated  in  Figure  3,  Ferret  is  mainly  composed  of  (i)  an  image  encoder  to  extract  image\nembeddings, (ii) the proposed spatial-aware visual sampler to extract regional continuous features,\nand (iii) an LLM to jointly model image, text, and region features.\n2\nn\nbins\n= 1000by default.  The value is input invariant, which means for any input image size, the original\ncoordinate will be mapped to the new coordinates. This makes the model robust to different input resolutions.\n3\nRadius is set to 5 by default.\n4",
    "Preprint\nLarge Language Model \nImage Encoder\nEmbedding\nImage Input\nText w/ references\nWhat’s in region [100, 600, 500, 900] <SPE> ?\nSpatial-Aware\nVisual Sampler\nIt’s acat tail [80, 590, 450, 920]\nFerret Model\nPointBox\nFree-form Shape\n(Sketch, Scribble, polygons)\nRegion Name+[Coordinates]+<feature>\nHybridRegion Representation\nFlatten &\nProjection\nSampling\nKNN\nFusion\nw/ Neighbor\nPooling\nSampled Points\nw/ neighbors\nInput Points\nSampled Points\nas output\nSpatial-Aware Visual Sampler\nFeature Map \n& Mask\nRegion \nFeatures\nBlock 1Block 2\nFigure 3:  Overview of the proposed Ferret model architecture.  (Left) The proposed hybrid region\nrepresentation and spatial-aware visual sampler. (Right) Overall model architecture. All parameters\nbesides the image encoder are trainable.\nInput.We feed the image into a pre-trained visual encoder, CLIP-ViT-L/14  (Radford et al., 2021),\nto extract the image embeddingsZ∈R\nH×W×C\n. For text, we tokenize the text sequence using the\npre-trained LLM’s tokenizer and project them into text embeddingsT∈R\nL×D\n.  As for referred\nregions, we append the coordinates and a special token as a placeholder for continuous features after\nthe name of the region:  “⟨region\nname⟩⟨coordinates⟩⟨SPE⟩”.  For example, “a cat [100, 50, 200,\n300]⟨SPE⟩”. If the name is unknown or hard to describe because multiple objects are included, we\njust use “region” or “area” as the “⟨regionname⟩”. In this way, referred regions can be well mixed\nwith ordinary texts to form complete sentences.\nSpatial-aware visual sampler.The shape of the referred regions can be quite varied, not limited\nto just points or rectangle boxes.  Grid-based processing like convolution or patch attention cannot\nhandle irregular shapes.  Similar to our cases, 3D point clouds are also in irregular shape and show\nvaried sparsity in the 3D space.  Inspired by existing works in 3D point cloud learning (Qi et al.,\n2017a; Ma et al., 2022; Wang et al., 2019), we propose a spatial-aware visual sampler.\nGiven extracted image feature mapZ∈R\nH×W×C\nand the binary region maskM, we first randomly\nsampleNpositive points insideM. For each point, its feature is obtained by bilinear interpolation.\nTheNpoints are fed into a cascade of blocks, where each of them includes three steps: sampling,\ngathering, pooling. (1) Sampling:\nN\nr\npoints are sampled fromNpoints via farthest point sampling\n(FPS) algorithm (Qi et al., 2017b),\n4\nwhich can guarantee sufficient coverage.  (2) Gathering:  For\neach of the sampled pointsx\ni\n, we search itsknearest neighbors from the pool of previousNpoints,\nand obtain a group of points{x\ni1\n,x\ni2\n,...,x\nik\n}.  Then, inspired by PointMLP (Ma et al., 2022), for\neach group, we fuse the features of sampled pointx\ni\nand it neighbor points by:\nh\nik\n=σ([θ([Z(x\nik\n)−Z(x\ni\n);C(x\nik\n)−C(x\ni\n)]);Z(x\ni\n);C(x\ni\n)]),\n(1)\nwherex\nik\nis  one  of  the  neighbors  ofx\ni\n,Z(x)denotes  the  pointx’s  feature  (in  the  first  block,\nit is interpolated from feature mapZ;  in the succeeding blocks,  it is the output feature from the\nprevious block),C(x)denotes the 2D coordinates of pointx,[; ]means channel-wise concatenation\nof multiple vectors,θis implemented by a linear layer to adapt the relative local features, andσis\nalso a linear layer to fuse each local feature from neighbors with sampled point feature. (3) Pooling:\nA max pooling is conducted to fusekneighbor features into one feature as the representation of the\nsampled point:\nh\ni\n=max\nk:(x\nik\n)∈KNNs ofx\ni\nh\nik\n.(2)\nAfter the three steps, we obtain fewer points but a more dense feature space since it incorporates the\nlocal neighbor features as well as their relative positions.  In experiments, we setN=512,r=4 and\nk=24, and cascade two such blocks, which in the end outputs32points with their features. Similar\n4\nFPS starts from a random single point sampled fromNpoints. In each iteration, it samples one point from\nthe rest points such that it is the farthest from the set of already sampled points. See detail in Qi et al. (2017b).\n5",
    "Preprint\nDetail +\nReasoning:\nGPT Generated data\nRegion:\nVisual Genome\nWhere is the corgi!\ncorgi [box0].\nWhat is the class of object \n[region0]!\nIt’s a corgi.\nCan you figure out \nthe geometric relation of \nobject [region1] and\nobject[region2]!\nBox on the floor.\nDescribe the area [region3] in \nshort phrase.\nIn short, a dog sit near a sofa.\nDo you know when the object\n[region4]was invented!\nThe invention of TV [box1] is \ncredit to John Logie Baird. He \ninvented in the early 1920s.\nObject:\nVisual Genome\nRefCOCOs\nFlickr30k\nRelationship:\nVisual Genome\nRobustness:\nSpatial Negative Mining\nIs there a cat in the image!\nNo, but there is a dog [box0] in \nthe image.\nInput\nOutput\nOutput\nInput\nInput\nOutput\nOutput\nInput\nOutput\nInput\nOutput\nregion0\nbox0\nregion1\nregion2\nregion3\nregion4\nbox1\nInput\nFigure 4:Overview of the GRIT dataset for Ferret model training.  It contains three types of data:  (i) public\ndatasets that are converted into an instruction-following format (the top-3 rows); (ii) data generated via prompt-\ning ChatGPT and GPT-4 (the 4th row); and (iii) negative data to enhance model robustness (the last row).\nto ROIAlign (He et al., 2017), we flatten the point features into a single vector and project it to the\ndimension of LLM embeddings. The final feature is used to replace the⟨SPE⟩token in the input.\nOutput.The  above  region  denotations  are  used  in  Ferret  input  to  refer  to  specific  regions.   In\nFerret output, to achieve grounding, we generate the box coordinates right after the corresponding\nregions/nouns in the text response. For instance, “There is a dog [100, 150, 300, 200] in the figure.”\nWith this data format, our model is expected to implicitly learn what is groundable in the current\nimage and what their locations are.\nLLM.We  consider  Vicuna  (Chiang  et  al.,  2023)  as  our  language  model,   a  decoder-only\nLLM (Brown et al., 2020) that is instruction-tuned on top of LLaMA (Touvron et al., 2023a). Prior\nto being fed into the LLM, the image embeddings undergo transformation via an additional linear\nlayer to match the embedding dimension of the text tokens.\n4GRIT: GROUND-AND-REFERINSTRUCTION-TUNINGDATASET\nIn this section, we present GRIT, aGround-and-ReferInstruction-Tuning dataset containing around\n1.1M multimodal dialogues for model training.   GRIT consists of three types of data:  (i) public\ndatasets that are converted into an instruction-following format (Section 4.1); (ii) instruction-tuning\ndata generated via ChatGPT and GPT-4 (Section 4.2); and (iii) additional data from spatial negative\nmining for enhancing model robustness (Section 4.3).\n4.1HIERARCHY\nSpatial understanding can be characterized by varying levels of granularity and task formats. During\nour dataset creation, we look into the following categories based on two dimensions:\n•  In terms ofgranularity, we identify four main categories: (i) individual objects, (ii) relationships\namong objects, (iii) descriptions of specific regions, and (iv) region-based complex reasoning.\n•  In terms oftask format, we further divide the data into three distinct types: (i) Region-in Text-out\ndata, (ii) Text-in Region-out data, and (iii) Text-Region combined data.\n5\nWe compiled an extensive set of public data focusing on the aforementioned dimensions and con-\nverted them into an instruction-following format using carefully designed templates.  A more in-\ndepth view of these templates is available in Appendix A.1.\n5\nFor Region-in Text-out data, the input highlights a specific region, prompting queries about it. For Text-in\nRegion-out data, the input comprises textual descriptions, and the task is to pinpoint or ground the relevant\nregion in its response. The combined Text-Region data integrates both text and region within a single sequence,\nwhich can be present in the input, output, or both.\n6",
    "Preprint\nTable  2:  One  example  used  in  in-context  learning  to  construct  GPT-Assisted  Refer-and-Ground\nInstructon-Tuning.\nObjects\nObject 0: chair at [0.596, 0.637, 0.698, 0.997].\n...\nRelationships\nObject 11 : frame→with→Object 9 : photo\n...\nRegion Descriptions\nRegion Description at [0.560, 0.466, 0.600, 0.529] :\na white picture frame with a black and white photo on it.\n...\nGlobal Caption\nWhite chairs sit around a polished wood dining table while\na sectional soft sits in the background.\n...\nConversation\nQuestion:\nWhat is the object [0.212, 0.539, 0.718, 0.820] made of?\n===\nAnswer:\nThe dining table over there is crafted from a tree trunk.\n===\nQuestion:\nWhere would be a good place to have my dinner? Point out the locations in image.\n===\nAnswer:\nI suggest having dinner at the table [0.214, 0.541, 0.720, 0.997], possibly sitting on the white chair [0.264, 0.717, 0.524, 0.995].\nAlso, enjoy the wine [0.242, 0.644, 0.288, 0.682].\n===\nQuestion:\nIn terms of comfort for resting, which object is better - the one [0.830, 0.431, 0.978, 0.596] or the one [0.264, 0.717, 0.524,\n0.995]?\n===\nAnswer:\nAmong the two, the couch located [0.830, 0.431, 0.978, 0.596] would be more comfortable for resting. The object [0.264, 0.717,\n0.524, 0.995] is a dining chair, which is typically less comfortable for extended relaxation.\n.........\nIndividual objects.To achieve visual understanding at the object level, we select object detection\ndatasets such as Visual Genome (Krishna et al., 2017), Object365 (Shao et al., 2019), and visual\ngrounding datasets including RefCOCOs (Yu et al., 2016; Lin et al., 2014; Nagaraja et al., 2016)\nand Flickr30k-Entities (Plummer et al., 2015).  The converted Visual Genome object data follow a\nRegion-in Text-outformat.  Additionally, to enable Ferret to understand free-form shapes, we apply\nSAM (Kirillov et al., 2023) to Visual Genome object data to obtain a segmentation mask for each\nobject, which is fed into the spatial-aware visual sampler to extract continuous region feature during\ntraining.  The visual grounding datasets and Object365 data adhere to aText-in Region-outformat.\nThis section has in total 678k data.\nRelationships among objects & descriptions of regions.We selected data pertaining to object\nrelationships and region captions from Visual Genome (Krishna et al., 2017) to address these two\nfacets, respectively.  Both datasets employ aRegion-in Text-outformat and 177k data are obtained.\nSimilar  to  Visual  Genome  object  data,  we  also  extract  segmentation  masks  of  objects  in  Visual\nGenome relationship data via SAM.\nRegion-based complex reasoning.Regarding complex reasoning centered on specific regions, we\nconstructed a novel dataset with the help of ChatGPT/GPT-4.  It adopts a combined Text-Region\nformat, and is detailed in the subsequent section.\n4.2GPT-ASSISTEDVISUALINSTRUCTIONDATAGENERATION\nBesides converting existing datasets by templates, dialogue instruction tuning data is proved to be\ncritical for MLLM to understand human intention and generate fluent, natural, and long-form re-\nsponses (Liu et al., 2023b; Zhu et al., 2023a; Li et al., 2023d).  Few-shot prompting is widely used\n7",
    "Preprint\nto  obtain  visual  instruction  tuning  data,  where  textual  scene  descriptions  of  images  and  human-\nannotated dialogues are provided as few-shot demonstrations, and ChatGPT/GPT4 are prompted to\ngenerate new dialogue based on the new image’s textual scene descriptions.\nHowever, previous instruction tuning data mainly focus on describing the entire image without ex-\nplicitly specifying spatial-related information.  To collect refer-and-ground instruction tuning data,\nwe emphasize region-based spatial knowledge in the following three steps.  (i) Besides objects and\nglobal captions usually used as before, our symbolic scene description additionally includes physical\nrelationships between objects and region captions along with coordinates of them.  (ii) In human-\nannotated dialogues, we add coordinates after the groundable regions or objects either in input or\noutput  or  both,  and  the  dialogues  are  typically  focused  on  specific  regions.   It  helps  to  implic-\nitly prompt ChatGPT/GPT4 to follow similar patterns when generating new dialogues.  A few-shot\nprompt used in our data is shown in Table 2. (iii) The generated dialogues sometimes cannot follow\nthe rules and patterns we wrote in system prompts and few-shot examples, which might be due to\nthat the context of LLM input is too long to handle all the details. To alleviate it, we propose to use\nChatGPT/GPT-4 again to refine the initially generated dialogues, whose context length is only 10%\nof the data generated from the first round on average.  To save cost, we use ChatGPT in the first\nround of generation and GPT-4 for refining. 34k dialogues in total are collected.\nAdditionally, to exploit existing instruction-tuning data such as those in LLaVA (Liu et al., 2023b),\nwe apply an open-vocabulary object detector, GLIPv2 (Zhang et al., 2022), on LLaVA-158k data to\nlocalize groundable nouns in the text. Then, we append the bounding boxes after the corresponding\nnouns, forming a pseudo-grounded LLaVA instruction data that are also used for training Ferret.\n4.3SPATIALNEGATIVEMINING\nAs highlighted in prior studies (Li et al., 2023e; Liu et al., 2023a), MLLM exhibits a propensity to\nhallucinate in response to yes/no questions. We observed a similar occurrence when inquiring about\ndetailed regions. To address this, we also conduct negative sample mining by following two ways: (i)\nImage-conditioned Category Localization, and (ii)Semantics-conditioned Category Localization.\nThey both ask the model to localize specific object categories, thereby enabling the model’s ability\nto discern and potentially recognize the absence of certain objects.  They differ in how to select the\nnegative category.  For (i), Object365 data are employed and we randomly select the object class\nfrom the vocabulary that is not shown in the given image.  For (ii),  Flickr30k data are used and\nnegative categories are sourced by utilizing ChatGPT/GPT4 to find entities that are most analogous\nto the original class, attribute, or quantity,e.g., ‘man’ vs.  ‘woman’, ‘blue’ vs.  ‘yellow’, ‘two’ vs.\n‘three’.\nWe curate the data to maintain an equilibrium between positive and negative samples for each of the\ntwo types.\n6\n95k data are collected. A more comprehensive elaboration is provided in Appendix A.2.\n5EXPERIMENTS\nFirst of all, we illustrate the training details of Ferret.  Then in evaluation, we start with evaluating\nFerret on conventional referring and grounding benchmarks (Sec.  5.1 and 5.2).  Then, we demon-\nstrate the power of Ferret in more complex multimodal chatting with refer-and-ground capability\nin Sec.  5.3.  For a detailed visualization of each, kindly check Appendix C. We further ablate key\ncomponents in Ferret (Sec.  5.4), analyze the object hallucination of Ferret (Sec.  5.5) and discuss\nFerretv.s.GPT-4V (Sec. 5.6).\nTraining Details.We initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vi-\ncuna,  and  the  projection  layer  with  LLaVA’s  first-stage  weights,  leaving  the  visual  sampler  ran-\ndomly initialized.  After the initialization, Ferret is trained on the aforementioned GRIT data for\nthree epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of2e−5and a batch\nsize of 128.  The training takes∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B. During training,\nwhen input refers to regions, we randomly choose either the center points or the bounding boxes\n6\nWe observed that even though we don’t collect other data specifically for training, Ferret demonstrates the\ncapability to generalize robustness across diverse categories like relationships, events,etc.  We attribute this\nversatility to the potent compositional capabilities inherent to LLM.\n8",
    "Preprint\nTable  3:Results  of  referring  object  classi-\nfication on three different referring types,  in-\ncluding point, box, and free-form shape.  ‘✕’\nmeans no such capability.\nModels\nLVIS (%)\nPoint    Box    Free-form\nRandom Guess505050\nLLaVA50.150.3✕\nKosmos-2 (Peng et al., 2023)✕60.25✕\nShikra-7B (Chen et al., 2023b)   57.82  67.71✕\nGPT4-ROI (Zhang et al., 2023)✕61.76✕\nFerret-7B67.94  79.4269.77\nFerret-13B68.35  80.4670.98\nTable 4:Results of grounded image captioning on the test\nset of Flickr30k Entities.  BLEU@4, METEOR, CIDEr, and\nSPICE are used for the caption evaluation.F1\nall\nandF1\nloc\nare used for grounding evaluation. ‘–’ means not reported.\nModels\nCaption Eval.Grounding Eval.\nB@4MCS\nF1\nall\nF1\nloc\nGVD (Zhou et al., 2019)27.3   22.5  62.3  16.57.5522.2\nCyclical (Ma et al., 2020)26.8   22.4  61.1  16.88.4422.78\nPOS-SCAN (Zhou et al., 2020)30.1   22.6  69.3  16.87.1717.49\nUniTAB (Yang et al., 2022)30.1   23.7  69.7  17.412.9534.79\nShikra-13B (Chen et al., 2023b)––73.9–––\nFerret-7B35.1   24.6  74.8  18.015.0237.62\nFerret-13B37.0   25.5  76.1  18.315.1238.03\n(or segmentation masks if available) to represent the regions. We perform de-duplication in training\ndata to remove the samples that are in downstream evaluations.\n5.1INPUTREFERRING\nThe model’s capability of understanding referring is reflected in that, given a referred region in the\nquestion, how accurately the model can understand the semantics of the referred region. To measure\nit, we start with the most basic semantics,object, as it is fundamental and clear to define. To be more\nspecific, the task we evaluate on isReferring Object Classification: the question refers to a specific\nregion  in  the  image,  and  the  model  needs  to  classify  the  object  in  the  region.   Since  Ferret  and\nMLLMs usually generate free-form text responses, it is inaccurate to match the predicted class with\nthe ground-truth class if directly asking the model to classify without constraints.   Alternatively,\nwe  make  it  a  binary-choice  question  in  the  format  of  “Is  the  object⟨location⟩a⟨classA⟩or  a\n⟨classB⟩?”. We feed the binary-choice question and image into the MLLMs to obtain the response,\nand then detect if the response matches the ground-truth (GT) class by some rule.\n7\nTo  prepare  the  data,  we  used  the  validation  split  of  LVIS  dataset  (Gupta  et  al.,  2019)  covering\nover  1000  object  categories,  and  sampled  2667  objects  as  the  GT  objects.   Then,  we  randomly\nchoose a different object category in the same image whose central point is close to the GT object\nas the negative object, and replace⟨class\nA⟩and⟨classB⟩with those two randomly to form 2667\nquestions. Additionally, to mimic the versatility of referring in human life, we replace the⟨location⟩\nwith three different types: point, box, and free-form shape.  For point, we randomly sample a point\ninside the GT object that is also near the GT object’s boundary.  For box, we use the GT bounding\nbox provided by LVIS. For the free-form shape, we randomly generate some strokes inside the GT\nobject to simulate that. Results on all three types of referring are summarized in Table 3. Ferret can\nsignificantly outperform previous models (Peng et al., 2023; Chen et al., 2023b) and handle all types\nof referring, a capability notably absent in previous works. We visualize some examples in Figure 5.\n5.2OUTPUTGROUNDING\nFerret performs well in referential dialogue, allowing for its integration into various VL tasks, no-\ntably those with grounding outputs.  To rigorously assess the grounding capability, we first subject\nFerret to benchmark visual grounding tasks in a generative paradigm.  Then, to measure the align-\nments between words and regions, we further evaluate Ferret on grounded captioning task.\nVisual grounding.Visual grounding aims to ground language queries into aligned image regions.\nWe experiment on the sub-tasks of referring expression comprehension (REC) with three renowned\nbenchmarks:  RefCOCO (Lin et al., 2014), RefCOCO+ (Yu et al., 2016), and RefCOCOg   (Mao\net al., 2016), and phrase grounding with Flickr30k Entities dataset (Plummer et al., 2015).  REC\ntask involves a question or description about a specific area in an image, with the model expected\nto predict just one bounding box.   Phrase grounding,  conversely,  seeks to associate all the noun\nphrases in the input sentence with corresponding boxes, requiring the model to predict these boxes\n7\nSometimes both GT class and negative class appear in the answer,e.g.,  “The object is⟨classGT⟩,  not\n⟨classNeg⟩”. Our rule removes the substring in-between “not” and comma/period, and then detects GT class.\n9",
    "Preprint\nTable 5:Performance comparison (Acc@0.5) on the referring expression comprehension (RefCOCO, Ref-\nCOCO+, RefCOCOg) and phrase grounding (Flickr30k Entities) tasks.∗indicates that the method is specifi-\ncally fine-tuned in the second stage.\nModels\nRefCOCORefCOCO+RefCOCOgFlickr30k Entities\nvaltestA   testBvaltestA   testBvaltestvaltest\nMAttNet (Yu et al., 2018)76.40   80.43   69.2864.93   70.26   56.0066.67   67.01––\nOFA-L (Wang et al., 2022b)79.96   83.67   76.3968.29   76.00   61.7567.57   67.58––\nTransVG (Deng et al., 2021)81.02   82.72   78.3564.82   70.70   56.9468.67   67.73–79.10\nUNITER (Chen et al., 2020)81.41   87.04   74.17\n75.90   81.45   66.7074.02   68.67––\nVILLA (Gan et al., 2020)82.39   87.48   74.8476.17   81.54   66.8476.18   76.71––\nUniTAB (Yang et al., 2022)86.32   88.84   80.61\n78.70   83.22   69.4879.96   79.9778.7679.58\nMDETR (Kamath et al., 2021)86.75   89.58   81.4179.52   84.09   70.6281.64   80.8982.3*83.8*\nShikra-7B (Chen et al., 2023b)87.01   90.61   80.2481.6087.36   72.1282.27   82.1975.8476.54\nFerret-7B87.49   91.35   82.4580.7887.38   73.1483.93   84.7680.3982.21\nShikra-13B (Chen et al., 2023b)   87.83   91.11   81.8182.8987.79   74.4182.64   83.1677.4178.44\nFerret-13B89.48   92.41   84.36\n82.8188.14   75.1785.83   86.3481.1384.76\nIstheobject[region0]aheadband\noranecklace!\naheadband[box0].\nIstheobject[region0]a lamp or a \nlampshade!\nItisalampshade[box0].\nReferringObjectClassification(LVIS) –boxReferringObjectClassification(LVIS)–freeformshape\nPhraseGrounding(Flickr30kEntities)\nWhat are the locations of a man, a \ntrail, a young girl, some boards of \nwood!\naman[box0].atrail[box1].a\nyounggirl[box2].some boards of \nwood[box3].\nWhat is the location of almost \nhidden bus that is the third one in \ncounting from the bottom up in the \nimage!\nalmost hidden bus that is the third \none in counting from the bottom \nup[box0].\nReferringExpressionComprehension(RefCOCOg)\nFigure 5: Some examples demonstrating Ferret’s referring and grounding capabilities. More visual-\nizations are shown in Appendix C.\nand the word-box connections. For both tasks, we utilize uniform prompts, represented as “What are\nthe locations of<query>/<phrases>?”, where<query>denotes the textual referring expression,\nwhile<phrases>stands for a “comma-delimited” aggregation of the given phrases.  The model is\ntrained to output in “<query>[box].” format. The generated bounding box is considered correct if\nits intersection over union (IoU) with the GT box is greater than 0.5.  As shown in Table 5, Ferret\nachieves an outstanding performance on all metrics, and is comparable to specialized fine-tuning\napproaches (Kamath et al., 2021). Some results are visualized in Figure 5.\nGrounded captioning.The grounded captioning task requires the model to generate a caption and\nground all generated noun phrases to image regions. The final predictions generally consist of three\nparts,i.e., the text caption, visual regions as boxes, and the grounding alignments between words\nand boxes.  Following the established benchmarks on the Flickr30k Entities dataset,  we evaluate\ncaptioning and grounding separately with the captioning metrics and grounding F1 scores, respec-\ntively.F1\nall\nevaluates grounding as a multi-label classification problem. We also reportF1\nloc\nthat\nonly computes the grounding score on correctly predicted object words. Results are summarized in\nTable  4, and Ferret achieves state-of-the-art.\n5.3FERRET-BENCH: MULTIMODALCHATTING WITHREFERRING ANDGROUNDING\nMultimodal chatting has been an emergent ability of MLLMs.   Previous benchmarks (Liu et al.,\n2023b)  mainly  evaluate  conversation,  detailed  description,  and  complex  reasoning  via  GPT-4  as\na judge.  Yet, a gap exists as no dataset currently evaluates multimodal chatting that necessitates\nreferring or grounding actions,e.g., instances where individuals reference an unfamiliar object and\ninquire about its purpose. To benchmark this intriguing and practical capability, we introduce Ferret-\nBench that covers three kinds of region-based questions evaluating referring and grounding capa-\n10",
    "Preprint\nTable  6:Visualization  results  of  Referring  Reasoning  in  Ferret-Bench  to  illustrate  the  difference  between\nvarious models (LLaVA vs. Kosmos-2 vs. Shikra vs. Ferret (Ours)). For clarity, we have omitted the generated\nbounding box in the model’s output. More visualizations can be found in Appendix C\nQuestion:\nWhat is the purpose of the object [region0] on the bike?\nGround Truth:\nThe object is a shock absorber on the bike.\nIts purpose is to absorb or dampen shock impulses.\nIt does this by converting the kinetic energy of\nthe shock into another form of energy which is then\ndissipated. In the context of the motorcycle,\nit is particularly useful in providing comfort and\nstability for the rider, especially when travelling\nover uneven or rough terrains.\nLLaVA:\nA black and silver Kawasaki motorcycle which is likely used for transportation, commuting, leisure\nrides, or long-distance travel.\nKosmos-2:\nThe purpose of the object on the motorcycle is to store and protect the motorcycle’s fuel tank.\nShikra:\nThe answer is leg.\nFerret (Ours):\nThe object is a shock absorber.  This component is essential for the bike’s stability,  providing a\nsmoother ride by soaking up the impact from bumps and uneven surfaces. It significantly improves\nthe overall performance and comfort of the bike, enabling the rider to maintain control and stability\neven on rough terrains.\nbility:  (i)Referring Description:  models are asked to describe a referred region based onits in-\nteraction with surrounding objects.  (ii)Referring Reasoning:  models need to reason on top of\none or more referred regions correctly.  (iii)Grounding in Conversation:  models are required to\nreason correctly and accurately ground/localize the objects/regions necessary for the reasoning. For\nthe ease of benchmarking other methods, we represent the regions with boxes instead of points or\nfree-form shapes.\nSpecifically, we randomly sample 40 images from the COCO validation set for each type of question,\nand generate the questions and GPT-4’s answers following the instruction generation pipeline in\nSec.   4.2.   Following Liu et al. (2023b),  we feed the question and image into MLLMs to obtain\nthe predicted answer, and then prompt GPT-4 to rate the predicted answer and pseudo answer from\nGPT-4  based  on  the  ground-truth  textual  scene  description  (object,  relationship,  region  caption,\nglobal caption).  GPT-4 evaluates both the precision of referring understanding, object grounding,\nand correctness of semantics.  The rating score ranges from 1 to 10, in which higher means better.\nWe calculate the ratio of the predicted answer’s score and the GPT-4 answer’s score, which is then\npresented as a percentage to measure the performance of MLLMs.  We also asked GPT-4 to give a\ncomprehensive review for the rating and found that GPT-4 is good at measuring the degree of spatial\nprecision, such as how much the predicted bounding box diverges from the GT box coordinate. We\nrefer the readers to Appendix B for further elaboration.\nWe use LLaVA-Bench (Liu et al., 2023b) and the proposed Ferret-Bench to compare Ferret with\nprevious models, including LLaVA (Liu et al., 2023b), Shikra (Chen et al., 2023b), and Kosmos-\n2 (Peng et al., 2023).  Results are summarized in Table 7.  Ferret achieves superior performance in\nall types of tasks, boosting the score for the detailed description category from 68.3 to 80.9, and\nespecially excels at the three new tasks demanding referring and grounding abilities. One visualiza-\ntion comparison is shown in Table 6, in which Ferret demonstrates strong spatial understanding and\ncommonsense reasoning capability.\n11",
    "Preprint\nTable 7: Results on LLaVA-Bench and the proposed Ferret-Bench via GPT4-as-a-Judge evaluation.\nLLaVA-BenchFerret-Bench\nConversation\nDetailComplex\nAvg.\nReferringReferring    Grounding in\nAvg.\nDescription   ReasoningDescription   Reasoning   Conversation\nLLaVA\n8\n85.468.392.181.941.431.728.834.0\nKosmos-271.763.474.970.051.833.748.444.6\nShikra-7B80.670.788.179.946.041.650.145.9\nFerret-7B84.479.496.386.768.767.357.564.5\nFerret-13B85.280.996.487.570.668.759.766.3\nTable 8: Ablation study on the mutual benefit\nof grounding data and referring data.\nModel\nReferring (LVIS)Grounding\nPointBoxFlickr30k\nFerret67.979.480.4\nw/o Grounding data   65.475.6\n✕\nw/o Referring data✕✕\n79.8\nTable 9: Ablation study on the effectiveness of the\nproposed spatial-aware visual sampler.\nModule\nReferring (LVIS)\nPoint   Box   Free-form\nSpatial-aware Visual Sampler67.9   79.469.8\nVisual Sampler in SEEM67.1   77.268.9\n5.4ABLATION\nIn the ablation studies below, in default, we ablate Ferret-7B and mainly evaluate in referring object\nclassification and grounding tasks on Flickr30k Entities validation set.\nMutual benefits of grounding and referring.As shown in Table 8, grounding and referring, as\ntwo main capabilities emphasized in this paper, can actually benefit each other.  Particularly, when\nadding grounding data into training, the referring performance gets improved, and vice versa.\nSpatial-aware Visual Sampler.We ablate the effectiveness of the spatial-aware visual sampler\nby replacing it with the visual sampler in SEEM (Zou et al., 2023), where they average the features\nof all the sampled points as the region feature.  As we can see in Table 9, ours can outperform the\nprevious visual sampler in all three referring tasks.\nLLM model size.We study how much LLM model size influences the performance of referring\nand grounding. As seen in Table 3-7, having a larger LM backbone can generally help.\n5.5OBJECTHALLUCINATION\nAttribute to the incorporation of fine-grained spatial knowledge and negative mining,  Ferret also\nexhibits strong power against the hallucination problem.  We evaluate object hallucinations on the\nPOPE  benchmark  (Li  et  al.,  2023e).   Results  are  summarized  in  Table  10.   Ferret  has  exhibited\nperformance comparable to Shikra (Chen et al., 2023b), and far surpasses recent popular MLLMs.\n9\n5.6FERRETv.s.GPT-4V(ISION): A QUICKGLANCE ATREFERRING& GROUNDING\nRecently, GPT-4 released its multimodal version to the public, which is named GPT-4V. In a follow-\nup technical report (Yang et al., 2023), GPT-4V’s grounding ability is briefly touched. In this section,\nwe use some examples to probe GPT-4V’s referring and grounding capabilities, and compare with\nFerret.  For referring, GPT-4V is prompted with the following two ways:  (i) referred regions are\nmarked by red circle/outline in the image and the question asks about the region in red circle/outline.\n(ii) image is still but instead,  we provide the image size and coordinates in question to refer to\n8\nThe result on LLaVA-Bench is obtained by evaluating LLaVA released checkpoint. The slight discrepancy\nmight be due to evolving GPT4 APIs. For Ferret-Bench, we employ the same conversation template as Ferret,\nproviding LLaVA with a predefined input size, resizing all coordinates accordingly, and generating a response.\n9\nUnlike other methods, Ferret refrains from relying on VQA. This decision stems from our observation that\nVQA answers tend to be concise, and this brevity can restrict the conversational capabilities of LLMs.\n12",
    "Preprint\nTable 10:  Results on the object hallucination benchmark using the POPE evaluation pipeline (Li\net al., 2023e).\nDatasetsMetricsFerret   Shikra   InstructBLIP   MiniGPT4   LLaVA   MM-GPT   mPLUG-Owl\nRandom\nAccuracy (↑)90.2486.9088.5779.6750.3750.1053.97\nPrecision (↑)97.72    94.4084.0978.2450.1950.0552.07\nRecall (↑)83.00    79.2695.1382.2099.13100.0099.60\nF1 Score (↑)89.76    86.1989.2780.1766.6466.7168.39\nYes43.78    43.2656.5752.5398.7799.9095.63\nPopular\nAccuracy (↑)84.9083.9782.7769.7349.8750.0050.90\nPrecision (↑)88.24    87.5576.2765.8649.9350.0050.46\nRecall (↑)80.53    79.2095.1381.9399.27100.0099.40\nF1 Score (↑)84.21    83.1684.6673.0266.4466.6766.94\nYes45.63    45.2362.3762.2099.40100.0098.57\nAdversarial\nAccuracy (↑)82.3683.1072.1065.1749.7050.0050.67\nPrecision (↑)83.60    85.6065.1361.1949.8550.0050.34\nRecall (↑)80.53    79.6095.1382.9399.07100.0099.33\nF1 Score (↑)82.00    82.4977.3270.4266.3266.6766.82\nYes48.18    46.5073.0367.7799.37100.0098.67\nWhat is the object in red circle used for?\nThe object circled in red on the \nmotorcycle is the exhaust pipe or muffler. \nIt is used to channel and expel exhaust \ngases from the engine after combustion. \nThe muffler also helps reduce the noise \nproduced by the engine.\nThe object in the red circle appears to be a \ndisc brake on the motorcycle. Disc brakes \nare used to slow down or stop the \nmotorcycle when the brake lever is applied. \nWhat is the object in red circle used for?\nThe object is a pipe, typically used for \ntransporting exhaust gases from a \nmotorcycle.\nThe object is a shock absorber, an \nintegral part of a motorcycle, designed \nto smooth out rough surfaces and \nimprove stability during rides.\nWhat is the object [region0] used for?\nWhat is the object [region1] used for?\nThe size of the input image is 775 × 797 pixels Can you \ntell me what is the object in the bounding box (x1, \ny1)=(10, 520), (x2, y2)=(190, 600) used for?\nThe object within the specified bounding box appears to \nbe the exhaust or muffler of the motorcycle. It is used to \ndirect and expel exhaust gases from the engine after \ncombustion, reduce the noise produced by the engine.\nThe size of the input image is 775 × 797 pixels Can you \ntell me what is the object in the bounding box (x1, y1) \n= (570, 420), (x2, y2) = (620, 480) used for?\nThe object within this area seems to be an engine cover \nor a badge on the engine. It's commonly used for brand \nidentification and decorative purposes.\nregion0\nregion1\nFerret:\nGPT-4V w/ Colored Region:\nGPT-4V w/ Coordinate in Text:\nReferring:\nGrounding:\ntraffic light [obj0], traffic light [obj1], \ntraffic light [obj2], traffic light [obj3].\nDetect all the objects among \n{traffic light}.\nFerret:\nLocalize each traffic light in the image \nusing bounding box. The image size is \n(width, height) = (545, 813)\nHere are the bounding boxes for the traffic lights in \nthe image:\n(x_min, y_min, x_max, y_max) = (255, 94, 324, 183)\n(x_min, y_min, x_max, y_max) = (321, 95, 390, 183)\n(x_min, y_min, x_max, y_max) = (388, 97, 457, 185)\nNote: These coordinates represent the top-left and \nbottom-right corners of the bounding boxes around \nthe traffic lights.\nInput:\nVisualized Output:\nVisualized Output:\nGPT-4V:\nFigure 6:  Ferretv.s.GPT-4V in Referring & Grounding.  In GPT-4V, we refer to regions by either\nmarking them with red outlines in the image or adding their coordinates in the text.  In terms of\nreferring, we found that GPT-4V falls short in understanding relatively small regions. Similarly, for\ngrounding, GPT-4V fails to localize relatively small objects in complex scenes.\nspecific regions. As for grounding, we follow  Yang et al. (2023)’s prompt,i.e., “Localize⟨class⟩in\nthe image using bounding boxes. The image size is (width, height)”.\nAs we observed, GPT-4V is able to understand the referring to a certain extent via either colored\nregion in the image or coordinates in text.  However, compared with Ferret, GPT-4V falls short in\nprecise understanding when referring to small regions,e.g., the ‘shock absorber‘ in the motorcycle\n(see the upper example in Figure 6). On the other hand, GPT-4V is more knowledgable in common-\nsense,e.g., it can further highlight that the exhaust pipe can reduce the noise, a nuance potentially\nattributable to GPT-4’s enhanced linguistic capabilities.  In regard to grounding, we tested GPT-4V\nwith CAPTCHAs, a task which is also mentioned in  Yang et al. (2023). In the traffic light example,\nFerret excels at accurately identifying most traffic lights even in cluttered scenes, as demonstrated\nin the bottom example of Figure 6.\n13",
    "Preprint\nThat  being  said,  in  the  realm  of  general  question  answering,  GPT-4V  is  notably  impressive.   It\nskillfully manages not only initial questions but also follow-up inquiries linked to specific regions,\ndelivering in-depth responses.  Nevertheless, Ferret shines especially when precise bounding boxes\nfor grounding are needed, and catering to those applications that require pinpoint accuracy in smaller\nregions. This is precisely where Ferret steps in to fill the gap.\n6CONCLUSION\nWe present Ferret, a new multimodal large language model adept at referring and grounding.  Fer-\nret can refer image regions in any free-form shape, and automatically establish grounding for text\ndeemed groundable by the model.  We have curated the GRIT dataset for model training, and the\nFerret-Bench dataset for evaluation.  Ferret, like most MLLMs, may produce harmful and counter-\nfactual responses. For future work, inspired by LISA (Lai et al., 2023), we plan to enhance Ferret to\nbe able to output segmentation masks in addition to bounding boxes.\nREFERENCES\nArmen  Aghajanyan,  Bernie  Huang,  Candace  Ross,  Vladimir  Karpukhin,  Hu  Xu,  Naman  Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multi-\nmodal model of the internet.arXiv preprint arXiv:2201.07520, 2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al.   Flamingo:  a visual language\nmodel for few-shot learning.arXiv preprint arXiv:2204.14198, 2022.\nAnas  Awadalla,  Irena  Gao,  Joshua  Gardner,  Jack  Hessel,  Yusuf  Hanafy,  Wanrong  Zhu,  Kalyani\nMarathe, Yonatan Bitton, Samir Gadre,  Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt.  Openflamingo, March 2023.  URLhttps:\n//doi.org/10.5281/zenodo.7733589.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.  Language models are\nfew-shot learners.Advances in neural information processing systems, 33:1877–1901, 2020.\nChi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu.  Position-\nenhanced  visual  instruction  tuning  for  multimodal  large  language  models.arXiv preprint\narXiv:2308.13437, 2023a.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm’s referential dialogue magic.arXiv preprint arXiv:2306.15195, 2023b.\nTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton.   Pix2seq:  A language\nmodeling framework for object detection.arXiv preprint arXiv:2109.10852, 2021.\nTing Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton.  A unified\nsequence interface for vision tasks.arXiv preprint arXiv:2206.07669, 2022a.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al.  Pali: A jointly-scaled multilingual\nlanguage-image model.arXiv preprint arXiv:2209.06794, 2022b.\nXi  Chen,  Josip  Djolonga,  Piotr  Padlewski,  Basil  Mustafa,  Soravit  Changpinyo,  Jialin  Wu,  Car-\nlos  Riquelme  Ruiz,  Sebastian  Goodman,  Xiao  Wang,  Yi  Tay,  et  al.   Pali-x:  On  scaling  up  a\nmultilingual vision and language model.arXiv preprint arXiv:2305.18565, 2023c.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representation learning. InECCV, 2020.\nWei-Lin  Chiang,  Zhuohan  Li,  Zi  Lin,  Ying  Sheng,  Zhanghao  Wu,  Hao  Zhang,  Lianmin  Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.   Vicuna:  An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality.See https://vicuna. lmsys. org (accessed 14 April\n2023), 2023.\n14",
    "Preprint\nAakanksha  Chowdhery,  Sharan  Narang,  Jacob  Devlin,  Maarten  Bosma,  Gaurav  Mishra,  Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.   Palm:\nScaling language modeling with pathways.arXiv preprint arXiv:2204.02311, 2022.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning.arXiv preprint arXiv:2305.06500, 2023.\nJiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-\nend visual grounding with transformers. InICCV, 2021.\nDanny  Driess,  Fei  Xia,  Mehdi  SM  Sajjadi,  Corey  Lynch,  Aakanksha  Chowdhery,  Brian  Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multi-\nmodal language model.arXiv preprint arXiv:2303.03378, 2023.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\ntraining for vision-and-language representation learning.NeurIPS, 2020.\nAgrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmen-\ntation.  InProceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npp. 5356–5364, 2019.\nKaiming He, Georgia Gkioxari, Piotr Doll\n ́\nar, and Ross Girshick. Mask r-cnn. InProceedings of the\nIEEE international conference on computer vision, pp. 2961–2969, 2017.\nAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Car-\nion. Mdetr-modulated detection for end-to-end multi-modal understanding. InProceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 1780–1790, 2021.\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.  Referitgame:  Referring to\nobjects in photographs of natural scenes.   InProceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP), pp. 787–798, 2014.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao,  Spencer  Whitehead,  Alexander  C  Berg,  Wan-Yen  Lo,  et  al.   Segment  anything.arXiv\npreprint arXiv:2304.02643, 2023.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels.arXiv preprint arXiv:2305.17216, 2023a.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.  Grounding language models to images for\nmultimodal generation.arXiv preprint arXiv:2301.13823, 2023b.\nEmiel  Krahmer  and  Kees  Van  Deemter.   Computational  generation  of  referring  expressions:  A\nsurvey.Computational Linguistics, 38(1):173–218, 2012.\nRanjay Krishna,  Yuke Zhu,  Oliver Groth,  Justin Johnson,  Kenji Hata,  Joshua Kravitz,  Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.   Visual genome:  Connecting lan-\nguage and vision using crowdsourced dense image annotations.International journal of computer\nvision, 123:32–73, 2017.\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.  Lisa:  Rea-\nsoning segmentation via large language model.arXiv preprint arXiv:2308.00692, 2023.\nHugo Laurenc ̧on, Lucile Saulnier, L\n ́\neo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,\nThomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open\nweb-scale filtered dataset of interleaved image-text documents.arXiv preprint arXiv:2306.16527,\n2023.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.   Otter:  A\nmulti-modal model with in-context instruction tuning.arXiv preprint arXiv:2305.03726, 2023a.\nChunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao.\nMultimodal foundation models:  From specialists to general-purpose assistants.arXiv preprint\narXiv:2309.10020, 2023b.\n15",
    "Preprint\nJunnan  Li,  Dongxu  Li,  Silvio  Savarese,  and  Steven  Hoi.Blip-2:   Bootstrapping  language-\nimage  pre-training  with  frozen  image  encoders  and  large  language  models.arXiv preprint\narXiv:2301.12597, 2023c.\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, et al. M\n3\nit: A large-scale dataset towards multi-modal multilingual instruc-\ntion tuning.arXiv preprint arXiv:2306.04387, 2023d.\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Li-\njuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n10965–10975, 2022.\nYifan Li,  Yifan Du,  Kun Zhou,  Jinpeng Wang,  Wayne Xin Zhao,  and Ji-Rong Wen.   Evaluating\nobject hallucination in large vision-language models.arXiv preprint arXiv:2305.10355, 2023e.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\n ́\nar,  and C Lawrence Zitnick.   Microsoft coco:  Common objects in context.   InComputer\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pp. 740–755. Springer, 2014.\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.  Aligning large\nmulti-modal model with robust instruction tuning.arXiv preprint arXiv:2306.14565, 2023a.\nHaotian Liu,  Chunyuan Li,  Qingyang Wu,  and Yong Jae Lee.   Visual instruction tuning.arXiv\npreprint arXiv:2304.08485, 2023b.\nIlya  Loshchilov  and  Frank  Hutter.Decoupled  weight  decay  regularization.arXiv preprint\narXiv:1711.05101, 2017.\nJiasen  Lu,  Christopher  Clark,  Rowan  Zellers,  Roozbeh  Mottaghi,  and  Aniruddha  Kembhavi.\nUnified-io:   A  unified  model  for  vision,  language,  and  multi-modal  tasks.arXiv preprint\narXiv:2206.08916, 2022.\nRuotian Luo and Gregory Shakhnarovich.  Comprehension-guided referring expressions.  InPro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7102–7111,\n2017.\nChih-Yao Ma, Yannis Kalantidis, Ghassan AlRegib, Peter Vajda, Marcus Rohrbach, and Zsolt Kira.\nLearning to generate grounded visual captions without localization supervision. InECCV, 2020.\nXu Ma,  Can Qin,  Haoxuan You,  Haoxi Ran,  and Yun Fu.   Rethinking network design and local\ngeometry in point cloud:  A simple residual mlp framework.arXiv preprint arXiv:2202.07123,\n2022.\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\nGeneration and comprehension of unambiguous object descriptions.  InProceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 11–20, 2016.\nVarun  K  Nagaraja,  Vlad  I  Morariu,  and  Larry  S  Davis.   Modeling  context  between  objects  for\nreferring expression understanding.   InComputer Vision–ECCV 2016: 14th European Confer-\nence, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pp. 792–807.\nSpringer, 2016.\nOpenAI. GPT-4 technical report.https://arxiv.org/abs/2303.08774, 2023a.\nOpenAI. Gpt-4 technical report.arXiv, 2023b.\nZhiliang  Peng,  Wenhui  Wang,  Li  Dong,  Yaru  Hao,  Shaohan  Huang,  Shuming  Ma,  and  Furu\nWei.   Kosmos-2:  Grounding  multimodal  large  language  models  to  the  world.arXiv preprint\narXiv:2306.14824, 2023.\n16",
    "Preprint\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svet-\nlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-\nto-sentence models. InProceedings of the IEEE international conference on computer vision, pp.\n2641–2649, 2015.\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.  Pointnet: Deep learning on point sets\nfor 3d classification and segmentation. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 652–660, 2017a.\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.  Pointnet++: Deep hierarchical fea-\nture learning on point sets in a metric space.Advances in neural information processing systems,\n30, 2017b.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.  Learning transferable visual\nmodels from natural language supervision.  InInternational conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\n ́\nc, Daniel Hesslow, Roman\nCastagn\n ́\ne,  Alexandra Sasha Luccioni,  Franc ̧ois Yvon,  Matthias Gall\n ́\ne,  et al.   Bloom:  A 176b-\nparameter open-access multilingual language model.arXiv preprint arXiv:2211.05100, 2022.\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian\nSun.  Objects365: A large-scale, high-quality dataset for object detection.  InProceedings of the\nIEEE/CVF international conference on computer vision, pp. 8430–8439, 2019.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang.  Generative pretraining in multimodality.arXiv\npreprint arXiv:2307.05222, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\n ́\nee\nLacroix, Baptiste Rozi\n`\nere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.  Llama:  Open and\nefficient foundation language models.arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.  Llama 2: Open founda-\ntion and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023b.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang.  Git:  A generative image-to-text transformer for vision and language.arXiv\npreprint arXiv:2205.14100, 2022a.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,\nJingren Zhou, and Hongxia Yang.  Ofa:  Unifying architectures, tasks, and modalities through a\nsimple sequence-to-sequence learning framework. InICML, 2022b.\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong\nLu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for\nvision-centric tasks.arXiv preprint arXiv:2305.11175, 2023.\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.\nDynamic graph cnn for learning on point clouds.ACM Transactions on Graphics (tog), 38(5):\n1–12, 2019.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple\nvisual language model pretraining with weak supervision. InICLR, 2022c.\nJialian Wu,  Jianfeng Wang,  Zhengyuan Yang,  Zhe Gan,  Zicheng Liu,  Junsong Yuan,  and Lijuan\nWang.   Grit:  A generative region-to-text transformer for object understanding.arXiv preprint\narXiv:2212.00280, 2022.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu,\nand Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling.\nInEuropean Conference on Computer Vision, pp. 521–539. Springer, 2022.\n17",
    "Preprint\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan\nWang. The dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen\nHu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models\nwith multimodality.arXiv preprint arXiv:2304.14178, 2023.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context\nin referring expressions.  InComputer Vision–ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 69–85. Springer, 2016.\nLicheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg.  A joint speaker-listener-reinforcer model\nfor referring expressions. InProceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 7282–7290, 2017.\nLicheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg.  Mat-\ntnet: Modular attention network for referring expression comprehension. InCVPR, 2018.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning.arXiv preprint arXiv:2309.02591, 2023.\nYuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy.  Contextual object detection\nwith multimodal large language models.arXiv preprint arXiv:2305.18279, 2023.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.  From recognition to cognition:  Visual\ncommonsense reasoning.  InProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 6720–6731, 2019.\nHaotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan\nWang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-\nlanguage understanding.Advances in Neural Information Processing Systems, 35:36067–36080,\n2022.\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and\nPing Luo. Gpt4roi: Instruction tuning large language model on region-of-interest.arXiv preprint\narXiv:2307.03601, 2023.\nYang  Zhao,  Zhijie  Lin,  Daquan  Zhou,  Zilong  Huang,  Jiashi  Feng,  and  Bingyi  Kang.   Bubogpt:\nEnabling visual grounding in multi-modal llms.arXiv preprint arXiv:2307.08581, 2023.\nLuowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J Corso, and Marcus Rohrbach.   Grounded\nvideo description. InCVPR, 2019.\nYuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, and Hanwang Zhang. More grounded image\ncaptioning by distilling image-text matching model. InCVPR, 2020.\nDeyao  Zhu,  Jun  Chen,  Xiaoqian  Shen,  Xiang  Li,  and  Mohamed  Elhoseiny.Minigpt-4:   En-\nhancing  vision-language  understanding  with  advanced  large  language  models.arXiv preprint\narXiv:2304.10592, 2023a.\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Young-\njae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-\nscale corpus of images interleaved with text.arXiv preprint arXiv:2304.06939, 2023b.\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee.  Seg-\nment everything everywhere all at once.arXiv preprint arXiv:2304.06718, 2023.\n18",
    "Preprint\nADETAILS OFDATASET\nA.1TASKTEMPLATES FORPUBLICDATASETS\nIn Section 4.1, we mentioned using carefully designed task templates to convert public datasets such\nas Visual Genome into instruction-following format.  The task templates we used are provided in\nTable 11. For simplicity, we only list three examples for each task.\nTable 11:  Examples of task templates Ferret used to transfer different public data types into the\ninstruction-following format.\nTaskThree randomly chosen examples from many.\nReferring-Object\nWhat is the class of the object<location>within the image?\nClassify object<location>in the image.\nIdentify the object<location>in the image.\nReferring-Relation\nWhat does<object1> <location1>do to<object2> <location2>of the image?\nWhat is the physical relation between<object1> <location1>and<object2> <location2>?\nCan you figure out the geometric relation of the<object1> <location1>and<object2> <location2>?\nReferring-Region\nDescribe the region<location>in a short phrase.\nWhat is in the region<location>? Describe in a phrase.\nCapture in a phrase: what’s near region<location>in the picture?\nREC.\nWhere is<object>in the image?\nWhat are the coordinates for the given<object>in the image?\nGiven the image, could you please tell me where is<object>\nPhrase Grounding\nWhat are the locations of<objects>?\nCould you provide me with the exact locations of<objects>?\nPlease indicate the positions of<objects>in the image?\nObject Detection (O365)\nDetect all objects among<class>in the image.\nPerform object detection given the image within<class>.\nGiven the image and set<class>, identify all the objects that belong to the set.\nGrounded Captioning\nWhat is this photo about? Use concise language.\nDescribe the overall picture in just a few words.\nWhat do you see happening in this image? Provide the answer in short.\nObject Hallucination\nIs there a<object>in the image?\nAre there<object>in the image?\nPlease tell me whether<object>exists in the image?\nA.2DETAILS ONSPATIALNEGATIVEMINING\nIn Section 4.3, we conducted negative sample mining for two aspects: (i)Image-conditioned Cate-\ngory Localization, and (ii)Semantics-conditioned Category Localization.  They use the same tem-\nplate to convert the original data, which falls into the task of object hallucination in Table 11. Specifi-\ncally, for the negative categories in (ii), we prompt ChatGPT/GPT-4 to generate entities that are most\nanalogous to the original class, attribute, or quantity,e.g., ‘man’ vs.  ‘woman’, ‘blue’ vs.  ‘yellow’,\n‘two’ vs. ‘three’. The prompt feed into ChatGPT/GPT-4 encompasses all the entities extracted from\n5 captions associated with one single image. We show the exact prompt template in Table 12.\nA.3EXAMPLES FORGENERATINGREFER-AND-GROUND DATASETS\nWe provide some example prompts to generate refer-and-ground from ChatGPT/GPT-4.   Prompt\nand the in-context example of multiple-round visual conversation data are shown in Table 13 and\nTable 14.  Prompt and the in-context example of one-round reasoning data are shown in Table 15\nand Table 16.\n19",
    "Preprint\nTable 12: In this example, we provide the prompt to generate the spatial negative sets.\nmessages= [{\"role\":\"system\", \"content\":f”’You are an AI visual assistant that can\nanalyze a single image.  You receiveseveral entitiesgiven by a list, each describing the objects in\nthe image you are observing.\nFor  each  entity  mentioned,  change  them  with  the  most  misleading  entity  name  (may  belong  to\nthe same category but are actually different) (nonexistent objects:  man→woman,nonexistent\nattributes:  brown→yellow,nonexistent quantities:  two→three, etc.).  The instructions should\ncontain interrogative and declarative sentences.\nThe output format needs to be a list only which contains the misleading entity names. Please follow\nthe instructions carefully.\n1. The length of the output list needs to be exactly equal to the input list.\n2. Do not explain the reasons.\n3. Do not mention the input entities, at least the output name and input name needs to be different.\n4. Do not mention something abstract, like\n ̈\nalien\n ̈\n.\n5. When dealing with quantities, focus solely on increasing the numbers during revision.\n6.   When  dealing  with  words  like  ”a  few”,  ”a  group”,  ”several”,  ”some”,  etc.,  try  changing  the\nobjects (A few men→A few women).\n7. Ensure that inclusive words are not substituted with their specific subsets. For example, if the word\nis ”people,” avoid replacing it with genders like ”man” or ”woman.” Instead, consider modifying\nthem to different categories, such as ”people”→”animals.”.”’}]\nBEXAMPLES ANDPROMPTSFORFERRET-BENCH\nWe  leverage  GPT-4  to  generate  three  kinds  of  region-based  questions  evaluating  referring  and\ngrounding capability:  (i) Referring Description,  (ii) Referring Reasoning,  and (iii) Grounding in\nConversation. Here, we only provide the prompt in Table 17 used to generate the referring descrip-\ntion response. One example of GPT-4 answers is shown in Table 18. We recommend readers check\nout more examples in Appendix C.\n20",
    "Preprint\nTable 13:  In this example, we provide the prompt used to generate the conversation response for\nrefer-and-ground instruction tuning, following the practice of LLaVA (Liu et al., 2023b).\nmessages=  [{\"role\":\"system\", \"content\":f”’You  are  an  AI  visual  assistant  that\ncan analyze a single image.  You receive five global captions, each describing the same image you\nare observing.  In addition, specific object locations within the image are given, along with detailed\ncoordinates.  These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2)\nwith floating numbers ranging from 0 to 1.  These values correspond to the top left x, top left y,\nbottom right x, and bottom right y.  Also, the relationships between pairs of objects are provided in\nthe format of object→relationship→subject, where the object/subject are indexed by object id\nfrom previous object lists as well as the object names.  Also, several region descriptions are given,\neach describing a box region of the image, with detailed coordinates.\nDesign a conversation between you and a person asking about this photo. Ask diverse questions and\ngive corresponding answers. The answers should be in a tone that a visual AI assistant is seeing the\nimage and answering the question.\nHere are some additional requirements about generated questions and answers:\n1. Only include questions that have definite answers:\n(1) one can see the content in the image that the question asks about and can answer confidently;\n(2)  one  can  determine  confidently  from  the  image  that  it  is  not  in  the  image.   Do  not  ask  any\nquestions that cannot be answered confidently.\n2. Also include complex questions that are relevant to the content in the image, for example, asking\nabout background knowledge of the objects in the image, asking to discuss events happening in the\nimage,  asking about object actions in the context of entire images,  etc.  Again,  do not ask about\nuncertain details.\n3.    Provide  detailed  answers  when  answering  complex  questions.    For  example,  give  detailed\nexamples or reasoning steps to make the content more convincing and well-organized.   You can\ninclude multiple paragraphs if necessary.\n4. In all samples, either in question or answer, you must mention bounding box coordinates to refer\nto the object or regions instead of directly saying the object name or describing the regions in text.\nIn answer, explain the region in the context of the scene.\n5.  Do not mention that the information source is provided in the text/caption/region description.\nAlways answer as if you are directly looking at the image.\n6.  Make the question as diverse as possible.  Include questions asking about the visual content of\nthe image, including the object types, counting the objects, object actions, object locations, relative\npositions between objects, object selection, object functions, etc. Make the question challenging by\nless including the visual content details in the question.”’}\n]\nforsampleinfewshot\nsamples:\nmessages.append({\"role\":\"user\", \"content\":sample[‘context’]})\nmessages.append({\"role\":\"assistant\", \"content\":sample[‘response’]}\n)\nmessages.append({\"role\":\"user\", \"content\":‘\\n’.join(query)})\n21",
    "Preprint\nTable 14:  One example used in in-context learning to construct GPT-Assisted Refer-and-Ground\nInstructon-Tuning. We encourage readers to refer to the codebase for the prompts for other types.\nObjects\nObject 0: chair at [0.596, 0.637, 0.698, 0.997].\n...\nRelationships\nObject 11 : frame→with→Object 9 : photo\n...\nRegion Descriptions\nRegion Description at [0.560, 0.466, 0.600, 0.529] :\na white picture frame with a black and white photo on it.\n...\nGlobal Caption\nWhite chairs sit around a polished wood dining table while\na sectional soft sits in the background.\n...\nConversation\nQuestion:\nWhat is the object [0.212, 0.539, 0.718, 0.820] made of?\n===\nAnswer:\nThe dining table over there is crafted from a tree trunk.\n===\nQuestion:\nWhere would be a good place to have my dinner? Point out the locations in image.\n===\nAnswer:\nI suggest having dinner at the table [0.214, 0.541, 0.720, 0.997], possibly sitting on the white chair [0.264, 0.717, 0.524, 0.995].\nAlso, enjoy the wine [0.242, 0.644, 0.288, 0.682].\n===\nQuestion:\nIn terms of comfort for resting, which object is better - the one [0.830, 0.431, 0.978, 0.596] or the one [0.264, 0.717, 0.524,\n0.995]?\n===\nAnswer:\nAmong the two, the couch [0.830, 0.431, 0.978, 0.596] would be more comfortable for resting. The object [0.264, 0.717, 0.524,\n0.995] is a dining chair, which is typically less comfortable for extended relaxation.\n===\nQuestion:\nIs the item [0.182, 0.238, 0.342, 0.461] real or is it a mirror reflection?\n===\nAnswer:\nThe television and the fireplace [0.182, 0.238, 0.342, 0.461] are not real but are actually reflections in the mirror.\n===\nQuestion:\nCan you describe the decorating style of the living room shown in this image?\n===\nAnswer:\nThe living room is decorated in a modern chic style. The image shows white chairs and lamps, a gray couch, and a wooden table\nand floor, all contributing to a modern aesthetic. The large mirror and built-in wine rack are also characteristic of modern design.\nThus, the overall decor creates an atmosphere of elegance and sophistication, indicating a modern chic style.\n22",
    "Preprint\nTable 15: In this example, we provide the prompt used to generate the reasoning response for refer-\nand-ground instruction tuning, following the practice of LLaVA (Liu et al., 2023b).\nmessages=  [{\"role\":\"system\", \"content\":f”’You  are  an  AI  visual  assistant  that\ncan analyze a single image.  You receive five global captions, each describing the same image you\nare observing.  In addition, specific object locations within the image are given, along with detailed\ncoordinates.  These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2)\nwith floating numbers ranging from 0 to 1.  These values correspond to the top left x, top left y,\nbottom right x, and bottom right y. Also, the relationships between pairs of objects are provided, in\nthe format of object→relationship→subject, where the object/subject are indexed by object id\nfrom previous object lists as well as the object names.  Also, several region descriptions are given,\neach describing a box region of the image, with detailed coordinates.\nThe task is to use the provided image information (objects, attribute, relationship, region description,\ncaptions), create a plausible and challenging question about the image, and provide the answer in\ndetail.\nCreate complex questions that mention specific regions of the image, but the question should require\nsome knowledge-aware or high-level commonsense reasoning beyond describing the scene.\nTo  answer  such  questions,  one  should  first  understand  the  visual  content,  then  based  on  the\nbackground  knowledge  or  reasoning,  either  explain  why  the  things  are  happening  that  way  or\nprovide guides and help to the user’s request.  Make the question challenging by not including the\nvisual content details in the question so that the user needs to reason about that first.\nHere are some additional requirements about generated questions and answers:\n1.   In  question  or  answer,  you  must  mention  bounding  box  coordinates  to  refer  to  the  object  or\nregions, instead of directly say the object name or describing the regions in text. In answers, explain\nthe region in the context of scene. Include details like object counts, position of the objects, relative\nposition between the objects.\n2.  Don’t ask the question you are not confident to answer.  Only include question that have definite\nanswer.\n3. Do not mention that the information source is provided in text/catpion/region description. Always\nanswer as if you are directly looking at the image.\n4. Make the question as diverse as possible and as complex-reasoning required as possible.”’}\n]\nforsampleinfewshot\nsamples:\nmessages.append({\"role\":\"user\", \"content\":sample[‘context’]})\nmessages.append({\"role\":\"assistant\", \"content\":sample[‘response’]}\n)\nmessages.append({\"role\":\"user\", \"content\":‘\\n’.join(query)})\n23",
    "Preprint\nTable 16:  One example used in in-context learning to construct GPT-Assisted Refer-and-Ground\nInstructon-Tuning. We encourage readers to refer to the codebase for the prompts for other types.\nObjects\nObject 0: badge at [0.802, 0.505, 0.872, 0.581].\n...\nRelationships\nObject 5 : fringe→on→Object 4 : flag\n...\nRegion Descriptions\nRegion Description at [0.038, 0.029, 0.508, 0.995] :\nA Grinning gentlemen about to get his tie out.\n...\nGlobal Caption\nA female in military uniform cutting a businessman’s neck tie.\n...\nConversation\nQuestion:\nWhat is unusual about the region [0.102, 0.230, 0.578, 0.761]?\n===\nAnswer:\nIn the specified region, there is an intriguing sight:  a pair of oversized scissors positioned near a man’s tie, accompanied by\na woman in military attire.  The two of them share a hearty laugh, creating an unexpected contrast.  The atmosphere appears\ncelebratory,  resembling a ribbon-cutting ceremony with flags in the background.   However,  rather than cutting a ribbon,  the\nscissors are directed toward the man’s tie, adding an unexpected twist to the situation.\n===\nCMOREVISUALIZATION\nWe provide more quantitative results of the predictions under various tasks from Ferret to indicate\nthe model’s strength and capability.\n•  Please refer to Figure 7 for Referring Object Classification on LVIS with different referring for-\nmats (point/box/).\n•  Please refer to Figure 8 for Visual Grounding on Flickr30k Entities and Referring Expression\nComprehension on RefCOCO/RefCOCO+/RefCOCOg.\n•  Please refer to Figure 9 for Grounded Captioning on Flickr30k Karpathy split.\n•  Please refer to Figure 10 for Evaluating Object Hallucination (POPE) on COCO val split.\n•  Please refer to Table 19 for Referring Description in Ferret-Bench.\n•  Please refer to Table 20 for Referring Resoning in Ferret-Bench.\n•  Please refer to Table 21 for Grounding in Conversation in Ferret-Bench.\nIstheobject[region0]a\nrefrigeratororachair!\nItisarefrigerator[box0].\nIstheobject[region0]aheadband\noranecklace!\naheadband[box0].\nReferringObjectClassification(LVIS) –point\nIstheobject[region0]apipeora\nclocktower!\naclocktower[box0].\nIstheobject[region0]a lamp or a \nlampshade!\nItisalampshade[box0].\nReferringObjectClassification(LVIS)–point\nReferringObjectClassification(LVIS) –boxReferringObjectClassification(LVIS)–freeformshape\nFigure 7:Referring Object Classification on LVIS. The task aims to classify specific region(s) in\nan image given by point/box/segmentation inputs.\n24",
    "Preprint\nTable 17: In this example, we provide the prompt used to generate the referring description response.\nmessages=  [{\"role\":\"system\", \"content\":f”’You  are  an  AI  visual  assistant  that\ncan analyze a single image.  You receive five global captions, each describing the same image you\nare observing.  In addition, specific object locations within the image are given, along with detailed\ncoordinates.  These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2)\nwith floating numbers ranging from 0 to 1.  These values correspond to the top left x, top left y,\nbottom right x, and bottom right y. Also, the relationships between pairs of objects are provided, in\nthe format of object→relationship→subject, where the object/subject are indexed by object id\nfrom previous object lists as well as the object names.  Also, several region description are given,\neach describing a box region of image, with detailed coordinates.\nThe task is to use the provided image information (objects, attribute, relationship, region description,\ncaptions), create a plausible and challenging question about the image, and provide the answer in\ndetail.\nCreate questions that refer to coordinates of some objects or regions without describing it, and ask\nabout its interaction with surrounding/nearby objects.\nTo answer such questions, one should require first understanding the visual content, then based on\nthe spatial information provided.\nHere are some additional requirements about generated questions and answers:\n1. In question, you must mention bounding box coordinates to refer to the object or regions, instead\nof directly say the object name or describing the regions in text.  In answers, explain the region in\nthe context of scene.   Include details like object counts,  position of the objects,  relative position\nbetween the objects.\n2.  Don’t ask the question you are not confident to answer.  Only include question that have definite\nanswer.\n3. Do not mention that the information source is provided in text/catpion/region description. Always\nanswer as if you are directly looking at the image.\n4. Don’t mention additional coordinates in the answer.\n5. Question should be explicitly ask about context/surrounding/nearby information/interaction.”’}\n]\nforsampleinfewshot\nsamples:\nmessages.append({\"role\":\"user\", \"content\":sample[‘context’]})\nmessages.append({\"role\":\"assistant\", \"content\":sample[‘response’]}\n)\nmessages.append({\"role\":\"user\", \"content\":‘\\n’.join(query)})\n25",
    "Preprint\nTable 18: One example of GPT4’s answers about referring description.\nConversation\nQuestion:\nDescribe the region [0.102, 0.230, 0.578, 0.761] in the context of surrounding objects.\nAnswer:\nIt’s a man in the region, who is standing on top of a skateboard, performing a rail grind over a metal hand rail. There is a bench\nand a tree near him. Also, he is in front of some buildings.\nWhat are the locations of a man, a \ntrail, a young girl, some boards of \nwood!\naman[box0].atrail[box1].a\nyounggirl[box2].some boards of \nwood[box3].\nWhereareseveral video cameras,\nanda concrete wall!\nseveral video cameras[box0].a \nconcrete wall[box1].\nCouldyoupleasetellmethe\nlocationsof dogs, a canal, his \nreflection, andthe water!\ndogs[box0].a canal[box1].his \nreflection[box2].the water[box3].\nPhraseGrounding(Flickr30kEntities)\nWhat is the location of bowl of \ncarrots in the image!\nbowl of carrots [box0].\nReferringExpressionComprehension(RefCOCO)\nLocatethepartiallyvisible zebra in \nthe image!\npartiallyvisible zebra[box0].\nFindthetop right bowlin the \nimage!\ntop right bowl[box0].\nWhat is the location of donut with \na hole nearest coffee in the image!\ndonut with a hole nearest coffee\n[box0].\nHelpmefindcar with visible \nlicense plate looks like a BMW!\ncar with visible license plate looks \nlike a BMW[box0].\nTellmethepreciselocationof \nnumber 56!\nnumber 56[box0].\nWhat is the location of almost \nhidden bus that is the third one in \ncounting from the bottom up in the \nimage!\nalmost hidden bus that is the third \none in counting from the bottom \nup[box0].\nReferringExpressionComprehension(RefCOCOg)\nLocatea horse with a white stripe \non it ' s nose in the image!\na horse with a white stripe on it ‘s \nnose[box0].\nWhereisa large teddy bear with a \nsmaller one in its lap in the image!\na large teddy bear with a smaller \none in its lap[box0].\nReferringExpressionComprehension(RefCOCO+)\nFigure 8:Phrase Groundingon Flickr30k Entities andReferring Expression Comprehension\non  RefCOCO/RefCOCO+/RefCOCOg.   The  tasks  aim  to  localize  specific  object(s)  in  an  image\ndescribed by a referring expression/given entity.\n26",
    "Preprint\nWrite a brief description of the \ngiven image.\nAwomen[box0]wearingablack\nandwhiteswimsuit[box1]plays\nvolleyball[box2].\nWhat is this photo about! Use \nconcise language.\nAwetGermanShepherddog\n[box0]carryingastick[box1]inits\nmouth[box2].\nDescribe the overall picture in just \na few words!\nAcowboy[box0]inablackshirt\n[box1]andwhitehat[box2]riding\nahorse[box3]isabouttolasso.\nGroundedCaptioning(Flickr30kKarpathysplit)\nCan you briefly describe the main \nfeatures of this image for me!\nAyoungwoman[box0]ina\nburgundysweater[box1]is\nwalkingwithaguitar[box3].\nWhat do you see happening in this \nimage! Provide the answer in \nshort.\nAfootballplayer[box0] inared\njersey[box1]istackledbyaplayer\n[box2]inabluejersey[box3].\nWhat are the key elements in this \npicture! Explain briefly.\nAchild[box0] isstandinginapark\n[box1]witharedball[box2]anda\nbackpack[box3].\nFigure 9:Grounded Captioning on Flickr30k. The task aims to generate a caption about the image\nand ground all generated noun phrases to image regions.\nIs there a microwavein the image!\nYes,thereisamicrowave[box0]\nintheimage.\nIs there a personin the image!\nYes,thereisaperson[box0]inthe\nimage.\nPOPE(COCO adversarial)\nIs there a tvin the image!\nNo,thereisnotv,butthereisa\ntoaster[box0]intheimage.\nIs there a chairin the image!\nNo,thereisnochair,butthereisa\ncar[box0]intheimage.\nPOPE(COCO popular)\nPOPE(COCO random)\nIs there a tennis racketin the \nimage!\nYes,thereisatennis racket[box0]\nintheimage.\nIs there a baseball glove in the \nimage!\nNo,thereisnobaseball glove,but\nthereisatennisball[box0]inthe\nimage.\nIs there a skisin the image!\nYes,thereisaskis[box0]inthe\nimage.\nIs there a dining tablein the image!\nNo,thereisnodining table,but\nthereisaskateboard[box0]in\ntheimage.\nIs there a broccoliin the image!\nYes,thereisabroccoli[box0]in\ntheimage.\nIs there a bowlin the image!\nNo,thereisnobowl,butthereisa\nplate[box0]intheimage.\nIs there a truckin the image!\nYes,thereisatruck[box0]inthe\nimage.\nIs there a personin the image!\nNo,thereisnoperson,butthereis\natruck[box0]intheimage.\nFigure 10:Object Hallucination Evaluation (POPE) on COCO. The task aims to input a query\ninquiring about the existence of an object, and the model is expected to generate a response in the\nform of either “yes/no”.\n27",
    "Preprint\nTable 19:Referring Description in Ferret-Bench.Qualitative examples to illustrate the difference\nbetween various models (LLaVA vs.  Kosmos-2 vs.  Shikra vs.  Ferret (Ours)).  For clarity, we have\nomitted the bounding box outputs in the textual representations.\nQuestion:\nWhat is the interaction between the object [region0]\nand its surrounding objects?\nGround Truth:\nIn the region, there is a metal support column.\nThis column is providing support for a bridge above it,\nwhich a train is passing over. The column also features\na red line on it. This region is part of a larger scene that\nincludes a train track on an elevated bridge.\nLLaVA:\nThe object is a bridge which appears to be a part of a train station, as there are traffic lights present in\nthe scene. The traffic lights are likely used to control the train’s movement and ensure safe passage\nover the bridge.\nKosmos-2:\nThe interaction between the object and its surroundings is that of a train passing over a bridge.\nShikra:\nThe metal pole is standing tall and straight, supporting the train that is traveling on the elevated\ntrack.\nFerret (Ours):\nThe object is a metal post that is part of a bridge. This post is essential because it supports the bridge\nstructure, ensuring its stability for the train to safely traverse the bridge.  Other elements such as\nthe train, and the overhead guard rail, depend on this post for support and stability.  Without this\npost, the bridge wouldn’t be able to withstand the train’s weight and motion, potentially leading to\na structural collapse.  Therefore, this post plays a critical role in maintaining the bridge’s integrity\nand stability.\nQuestion:\nWhat is the relationship between the object [region0]\nand object [region1] and its surrounding objects?\nGround Truth:\nThe object is a vase, and the object is a flower.\nThe flower is in the vase, suggesting it is a\ndecorative element within the room.\nLLaVA:\nA wooden table with a green bowl placed on it.\nKosmos-2:\nThe object is a vase with a green base.\nShikra:\nThe answer is flowers.\nFerret (Ours):\nThe object and object are a vase and a flower, respectively. The vase is holding the flower, suggest-\ning it’s displaying or supporting it.\n28",
    "Preprint\nTable 20:Referring Reasoning in Ferret-Bench.Qualitative examples to illustrate the difference\nbetween various models (LLaVA vs.  Kosmos-2 vs.  Shikra vs.  Ferret (Ours)).  For clarity, we have\nomitted the bounding box outputs in the textual representations.\nQuestion:\nWhat is the purpose of the object [region0]?\nGround Truth:\nThe object is a bottle. Bottles are typically used to\nhold and store different types of liquids. In this context,\nit might be used to store a beverage for the meal.\nLLaVA:\nA knife and a fork, indicating that it is ready for a meal.\nKosmos-2:\nThe purpose of the object is to attract the birds to the table.\nShikra:\nThe answer is keep birds away.\nFerret (Ours):\nThe object is a bottle, generally used to store and dispense liquids like water, juice, or other bever-\nages.\nQuestion:\nWhat is the purpose of the object [region0] on the bike?\nGround Truth:\nThe object is a shock absorber on the bike.\nIts purpose is to absorb or dampen shock impulses.\nIt does this by converting the kinetic energy of\nthe shock into another form of energy which is then\ndissipated. In the context of the motorcycle,\nit is particularly useful in providing comfort and\nstability for the rider, especially when travelling\nover uneven or rough terrains.\nLLaVA:\nA black and silver Kawasaki motorcycle which is likely used for transportation, commuting, leisure\nrides, or long-distance travel.\nKosmos-2:\nThe purpose of the object on the motorcycle is to store and protect the motorcycle’s fuel tank.\nShikra:\nThe answer is leg.\nFerret (Ours):\nThe object is a shock absorber.  This component is essential for the bike’s stability,  providing a\nsmoother ride by soaking up the impact from bumps and uneven surfaces. It significantly improves\nthe overall performance and comfort of the bike, enabling the rider to maintain control and stability\neven on rough terrains.\n29",
    "Preprint\nTable  21:Grounding  in  Conversation  in  Ferret-Bench.Qualitative  examples  to  illustrate  the\ndifference between various models (Kosmos-2 vs.  Shikra vs.  Ferret (Ours)).  For clarity, we only\ndisplay the bounding box outputs in Ferret ’s responses.\nQuestion:\nWhat is happening in the field and\nprovide the coordinates of mentioned objects?\nGround Truth:\nIn the image, there is a group of sheep [box0] [box1]\ngrazing in a field [box2]. The field is filled with\nplant life [box3]. There is also a bush [box4] and\nsome trees [box5] present in the field.\nKosmos-2:\nIn the field, a herd of sheep [box0] is grazing on the\nlush green grass [box1].\nShikra:\nA group of sheep [box0] [box1] [box2] [box3] [box4] [box5]\ngrazing on lush green grass.\nFerret (Ours):\nIn the field, there are sheep [box0] [box1] [box2] [box3] [box4] [box5] grazing on the grass [box6].\nQuestion:\nWhat is the man doing and what objects are in front of him?\nPlease provide the coordinates of these objects.\nGround Truth:\nThe man [box0] is sitting at a table [box1] and looking\nat a plate of food [box2] . There is a glass [box3]\nand a bottle [box4] on the table in front of him.\nKosmos-2:\nThe man is sitting at a dining table [box0] smiling and\nlooking at the plate of food in front him [box1].\nShikra:\nThe man [box0] is sitting at a table [box1] with a plate\nof food [box2] in front of him. He is looking up and\nsmiling, possibly staring at a person [box0] in the restaurant\nFerret (Ours):\nThe man [box0] is having a meal at a table [box1].  In front of him, there is a plate [box2] with a\nvariety of food items such as fruit and eggs.  There is also a glass [box3] filled with water and a\nbottle [box4].\n30"
  ]
}