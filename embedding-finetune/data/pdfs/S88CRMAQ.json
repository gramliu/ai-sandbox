{
  "key": "S88CRMAQ",
  "url": "http://arxiv.org/pdf/2405.06640",
  "metadata": {
    "title": "Linearizing Large Language Models",
    "abstract": "  Linear transformers have emerged as a subquadratic-time alternative to\nsoftmax attention and have garnered significant interest due to their\nfixed-size recurrent state that lowers inference cost. However, their original\nformulation suffers from poor scaling and underperforms compute-matched\ntransformers. Recent linear models such as RWKV and Mamba have attempted to\naddress these shortcomings by proposing novel time-mixing and gating\narchitectures, but pre-training large language models requires significant data\nand compute investments. Thus, the search for subquadratic architectures is\nlimited by the availability of compute and quality pre-training datasets. As a\ncost-effective alternative to pre-training linear transformers, we propose\nScalable UPtraining for Recurrent Attention (SUPRA). We present a method to\nuptrain existing large pre-trained transformers into Recurrent Neural Networks\n(RNNs) with a modest compute budget. This allows us to leverage the strong\npre-training data and performance of existing transformer LLMs, while requiring\n5% of the training cost. We find that our linearization technique leads to\ncompetitive performance on standard benchmarks, but we identify persistent\nin-context learning and long-context modeling shortfalls for even the largest\nlinear models. Our code and models can be found at\nhttps://github.com/TRI-ML/linear_open_lm.\n",
    "published": "2024-05-10T17:59:08Z"
  },
  "text": [
    "Linearizing Large Language Models\nJean Mercat\n∗\nIgor Vasiljevic\n∗\nSedrick Keh\n∗\nKushal AroraAchal DaveAdrien GaidonThomas Kollar\nToyota Research Institute\n{firstname.lastname}@tri.global\nFigure 1: We reuse pre-trained LLMs (gray) and convert them to RNNs with minimal uptraining (SUPRA),\noutperforming linear attention models (RWKV) on natural language tasks like HellaSwag, attaining the same\nmemory advantages (center), but also inheriting the limitations of RNNs on tasks like MMLU (right).\nAbstract\nLinear transformers have emerged as a subquadratic-time alternative to softmax\nattention and have garnered significant interest due to their fixed-size recurrent\nstate that lowers inference cost. However, their original formulation suffers from\npoor scaling and underperforms compute-matched transformers.  Recent linear\nmodels such as RWKV and Mamba have attempted to address these shortcomings\nby proposing novel time-mixing and gating architectures, but pre-training large\nlanguage models requires significant data and compute investments.  Thus, the\nsearch for subquadratic architectures is limited by the availability of compute and\nquality pre-training datasets. As a cost-effective alternative to pre-training linear\ntransformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA).\n1\nWe present a method touptrainexisting large pre-trained transformers into Re-\ncurrent Neural Networks (RNNs) with a modest compute budget.  This allows\nus to leverage the strong pre-training data and performance of existing trans-\nformer  LLMs,  while  requiring  5%  of  the  training  cost.   We  find  that  our  lin-\nearization technique leads to competitive performance on standard benchmarks,\nbut we identify persistent in-context learning and long-context modeling short-\nfalls for even the largest linear models.  Our code and models can be found at\nhttps://github.com/TRI-ML/linear_open_lm.\n∗\nEqual contribution.\n1\nWe borrow the term “uptraining” from Ainslie et al. (2023) to refer to continued training with a modified\narchitecture, as opposed to fine-tuning, which usually refers to continued training on a different dataset.\n1\narXiv:2405.06640v1  [cs.CL]  10 May 2024",
    "1  Introduction\nOver the last few years, Transformers (Vaswani et al., 2017) have displaced Recurrent Neural\nNetworks (RNNs) in sequence modeling tasks, owing to their highly parallel training efficiency and\nunmatched scaling performance (Kaplan et al., 2020). However, this training efficiency comes at\nthe cost of inference cost that scales linearly with the number of tokens, compared to the fixed-cost\ninference of RNNs. The memory-intensive nature of transformers has led to renewed interest in\nrecurrence—the fixed-size hidden state remains an attractive modeling proposition to reduce the\ncost of inference for language and multimodal models.\nSeveral recent works, starting withLinear Transformers(Katharopoulos et al., 2020), have observed a\nrelationship between a linearized form of attention and recurrence, leading to a duality between\ntransformers and RNNs: models can be trained with sequence parallelism (i.e.  as transformers,\navoiding backpropagation through time), but can operate as RNNs at inference time. Although\nthis architecture allows efficient training of RNNs, softmax transformers continue to outperform\nlinear transformers across natural language understanding benchmarks. A number of novel RNN\narchitectures have attempted to bridge this performance gap. These include RWKV (Peng et al.,\n2023a), Retentive Networks (Sun et al., 2023), TransNormer (Qin et al., 2022a), and more recently,\nGriffin (De et al., 2024) and RecurrentGemma (Griffin Team et al., 2024).  These models are pre-\ntrained on the same pre-training datasets as transformers and show promising results.\nState-space models (Gu et al., 2021) (SSMs) are another recurrent alternative to softmax transformers,\ncombining RNNs and convolutional networks to efficiently model long sequences. The Mamba (Gu\n& Dao, 2023) architecture is a SSM that shows impressive performance at smaller scales, matching or\nexceeding the performance of softmax transformers on a number of natural language understanding\n(NLU) benchmarks.  However, a gap remains for long-context NLU tasks, showing a persistent\nadvantage of softmax attention.\nArchitecture search at the scale of large language models is expensive. Rather than pre-training\nlinear models, another approach is toconvertan existing transformer into an RNN; Kasai et al. (2021)\nproposed to uptrain encoder-decoder transformers into RNNs by introducing an approximating\nMLP attention module. Zhang et al. (2024) improved on this method by adding a loss to match\nsoftmax attention to approximate more closely the base transformer.\nWhile approximating attention is an intriguing approach to re-using pre-trained transformers, it\nleads to instability and poor performance when uptraining large-scale models. We instead take\na different approach: rather thanapproximatesoftmax attention, wereplaceit with a linear kernel\nand a normalization strategy to uptrain the most performant LLMs into RNNs (see Figure 2). We\ntake advantage of models trained on high-quality, proprietary datasets for trillions of tokens (e.g.\nMistral (Jiang et al., 2023) and Llama2 (Touvron et al., 2023)). Fine-tuning these models on publicly\navailable data for a small fraction of pre-training tokens (see Figure 1), we obtain linear models\nthat are competitive with the best linear transformers for a fraction of the compute. We call our\napproach Scalable UPtraining for Recurrent Attention (SUPRA).\nOur contributions are as follows:\n•\nWe propose Scalable UPtraining for Recurrent Attention (SUPRA), a linearization strategy\nto uptrain state-of-the-art LLMs into performant RNNs.\n•We show that this simple uptraining technique is competitive with the strongest pre-trained\nrecurrent LLMs.\n•We investigate the limitations of recurrent LLMs, comparing pre-trained and uptrained\nRNNs to transformers, revealing a persistent gap for in-context learning and long-context\ntasks.\n2",
    "Figure 2: Our linearization strategy: we replace the softmax normalization with GroupNorm (GN)\nand introduce a small MLP to project the queries and keys, converting a pre-trained attention block\n(left) to a linear attention (right). The model can be be trained in parallel as a transformer and used\nrecurrently at inference time with a mathematically equivalent reformulation.\n2  Methodology\nIn this section we review linear transformers, and describe the linearization technique of  Kasai\net al. (2021) as it lays the groundwork for our approach. Finally, we present SUPRA, our method\nfor uptraining large transformers into RNNs.\n2.1    Background: Linear Attention\nLinear Transformers(Katharopoulos et al., 2020) establish a connection between transformers and\nRNNs, generalizing the definition of attention by replacing the softmax dot-product attentionv\n′\nwith a more generic similarity function sim(q,k)between the queriesqand keysk:\nv\n′\ni\n=\n∑\ni\nj=1\nsim(q\ni\n,k\nj\n)v\nj\n∑\ni\nj=1\nsim(q\ni\n,k\nj\n)\n.(1)\nStandard softmax attention is a special case, using sim(q,k) =exp\n\u0010\nq\nT\nk\n√\nd\n\u0011\n.\nThe authors explore several alternative functions forsim(q,k), including a linear kernel.  Their\nmain architecture uses the similarity functionsim(q,k) =φ(q)·φ(k)with a fixed exponential\nlinear unit kernelφ(x) =elu(x) +1. They show the computational benefits of linear attention and,\nmore importantly for this work, they demonstrate how such a model can be expressed as an RNN\nin the case of attention with causal masking.\nRecurrent InferenceLinear attention can be expressed as an RNN that updates a states\ni\nand a\nnormalization factorz\ni\nat each time step. Katharopoulos et al. (2020) call these terms theattention\nmemoryandnormalized memory.  This RNN formulation is mathematically equivalent to linear\n3",
    "attention, allowing the user to choose the most efficient one for a given task and hardware. Consider\na stream of tokens we want to generateX= [x\n1\n,x\n2\n,x\n3\n,. . .]. At inference time, we use the following\nupdate rule, where subscripts denote timestep in the recurrence (callingk\ni\n=W\nK\nx\ni\n, etc):\ns\n0\n=0z\n0\n=0(2)\ns\ni\n=s\ni−1\n+φ(k\ni\n)v\nT\ni\n(3)\nz\ni\n=z\ni−1\n+φ(k\ni\n)(4)\nv\n′\ni\n=\nφ(q\ni\n)\nT\ns\ni\nφ(q\ni\n)\nT\nz\ni\n(5)\nThe states\ni\nacts as a constant-size KV cache. Instead of appending new values to the cache, the state\nis updated. This allows for inference cost that is constant in the number of generated tokens.\n2.2    Finetuning a Transformer into an RNN\nKasai et al. (2021) introduced a linear transformer uptraining procedure that converts a pre-trained\nsoftmax transformer into an RNN byapproximatingthe attention computation with multi-layer\nperceptrons (MLPs). The method (T2R) starts with a softmax attention model, and linearizes the\nsoftmax operation. Recall the kernel linear attention similarity function:\nsim(x,y) =φ(x)·φ(y).(6)\nInstead of choosingφas a simple non-linearity, the authors use a trainable layer:\nφ(x) =relu(Wx+b).(7)\nThe weights are shared between keys and queries for a given attention head.  By usingφand\nrearranging the operations, attention can be written as:\nv\n′\ni\n=\nφ(q\ni\n)\nT\n∑\ni\nj=1\nφ(k\nj\n)v\nT\nj\nφ(q\ni\n)\nT\n∑\ni\nj=1\nφ(k\nj\n)\n.(8)\nThis allows the recurrent inference described in Section 2.1. However, this formulation has a number\nof drawbacks. First, it requires a significant re-training of the model, using approximately 20% of\npre-training tokens for conversion, while suffering a 5−10% drop in performance on language\nbenchmarks. Furthermore, this approach was tested on relatively small models (≈100Mscale).\nBecause it mimics the attention formulation closely, it suffers from stability issues at larger scales.\nTo address these issues, we modify the approach to adapt it to large-scale model uptraining.\n2.3    SUPRA: Scalable UPtraining for Recurrent Attention\nRather than pre-training linear models from scratch, we choose to insteaduptrainstate-of-the-art\ntransformers. Leveraging models that take advantage of high-quality (but proprietary) pre-training\ndatasets, we linearize them using a modest fraction of pre-training data (see Figure 1). We build on\nT2R, identifying two major issues and proposing SUPRA, an approach to fine-tuning very large\ntransformers into RNNs.\nWe first follow the literature in linear transformers and identify thenormalization factorin linear\nattention as unstable (e.g. TransNormer (Qin et al., 2022a)). In Section 3.3 we show that uptraining\na 1B model following the procedure in T2R causes a large drop in performance.   We instead\nfollow Retentive Networks (Sun et al., 2023) and replace the normalization with a GroupNorm\noperation.\n4",
    "Next we note that linear attention suffers more with absolute positional encoding than softmax\nattention, and a modern relative positional encoding scheme like RoPE (Su et al., 2021) is crucial\nfor competitive performance. Rather than training a linear transformer from scratch incorporating\nthese findings (RetNet, TransNormer) we use MLP kernels toconvertlarge language models into\nRNNs.\nStarting with the pre-trained model, we add weights shared between keys and queriesW,φ(x) =\nrelu(W\nx+b)and  use  the  rotary  positional  embedding  (RoPE  (Su  et  al.,  2021))  such  that  the\nsimilarity function becomes\nsim(q\ni\n,k\nj\n) =RoPE(φ(q\ni\n))·RoPE(φ(k\nj\n)).(9)\nWe normalize the output with a GroupNorm (Wu & He, 2018) instead of dividing by the sum of\nsim(q\ni\n,k\nj\n)(as in T2R). We use a fixed decay vectorγ∈(0, 1)\nh\n, withhheads, as in Sun et al. (2023).\nThis leads to the following attention formulation (see Figure 2 for a graphical representation):\nv\n′\ni\n=GroupNorm\n \ni\n∑\nj=1\nγ\ni−j\nsim(q\ni\n,k\nj\n)v\nj\n!\n.(10)\nThese new parameters are trained jointly with the rest of the network; at test time, we use the\nrecurrent formulation for inference.\n3  Experiments\nWe uptrain a variety of models from the 1B to 7B range into RNNs (Llama2 (Touvron et al., 2023)\nand Mistral (Jiang et al., 2023)),  and evaluate our models in two settings:  standard language\nunderstanding benchmarks and long-context evaluations.  We compare the results of different\narchitectural choices and training strategies, and then show the limitations of linear models on\nvarious benchmarks, describing the persistent gap between vanilla attention and recurrence. We\nchoose Llama2-7B and Mistral-7B as our base models for uptraining, but our recipe is general to\nany transformer model.\nWe compare our procedure to a variety of pre-trained recurrent models.  Given that the largest\navailable state-space models are at the 2.8B scale, we also train a Mamba model on the Refined-\nWeb (Penedo et al., 2023) dataset from scratch for 1.2T tokens, to serve as a strong baseline for a\npre-trained recurrent model\n2\n.\nWe use a fork of OpenLM (Gururangan et al., 2023) for all training and fine-tuning.  Please see\nSection 7 for hyperparameters and further details on reproducibility.\nLanguage Modeling.In Table 1 we report results on standard NLU evaluations using the Eleuther\nevaluation harness (Gao et al., 2023). We primarily compare to transformers and linear models at\nthe 7B scale, and we train a Mamba model at 7B for comparison with RWKV-5. As our model is ini-\ntialized from strong pre-trained transformers (Llama2 and Mistral-7B), it preserves performance on\nmost benchmarks (except MMLU; see Section 4 for a discussion below). Our technique outperforms\nRWKV-5 with minimal uptraining and is competitive with our 7B Mamba trained from scratch on\n1.2T tokens.\n2\nThe Mistral-SUPRA and Mamba-7B models are released along with the code.\n5",
    "ModelSize   Tokens   HellaSwag   PIQA   WG   ARC-E   ARC-C   MMLU   Average\nStableLM21.6B200069.076.763.668.638.938.459.2\nStableLM3B100073.879.365.872.140.044.262.5\nGemma2B200071.478.664.474.041.541.261.9\nMamba1.4B60059.073.961.465.532.925.253.0\nRWKV-51.5B110053.171.659.062.232.726.250.8\nMamba2.8B60066.275.863.469.736.326.356.3\nLlama27B200076.079.169.176.346.345.965.4\nGemma7B600080.781.973.781.153.262.972.2\nMistral7B8000(?)81.082.174.080.953.862.472.4\nRetNet6.7B20060.775.458.1––––\nRWKV-57B110070.977.267.471.843.631.060.3\nRWKV-5-1.7T7B170073.078.672.975.845.634.963.5\nMamba (ours)7B120077.981.071.877.546.733.364.7\nLlama2-SUPRA7B+2071.878.665.871.139.524.958.6\nMistral-SUPRA7B+2074.880.167.474.642.328.061.2\nMistral-SUPRA7B+10077.180.470.375.945.834.264.0\nTable 1: Linear models (RNNs and SSMs) highlighted in gray. 5-shot results are used for MMLU.\nNorm results are used for PIQA, HellaSwag, ARC-C. RetNet results taken from RetNet paper.\nModelSize    TrainQasper (2-shot)NarrativeQA (0-shot)\nContext2048409681921638420484096819216384\nLlama1-7B7B204824.437.235.084.8821.441.030.00.0\nLlama2-7B7B409623.2627.266.205.4921.3222.610.00.0\nLlama2-7B*7B409623.2627.2631.4625.5221.3222.6123.014.27\nMistral-7B7B819621.5325.5033.616.8824.9426.9025.930.63\nRecurrentGemma-2B2.7B819222.4413.1613.4212.6619.8011.5912.9312.95\nRWKV-5-1.7T7B204822.2823.8722.3020.3517.7718.6517.8116.00\nMamba (ours)7B204819.685.585.906.3219.700.280.00.0\nMistral-SUPRA7B204819.4417.1317.1117.2218.9917.7617.7517.74\nTable 2: Long context evaluations. Performance at various context size cutoffs for Qasper (2-shot)\nand NarrativeQA (0-shot). * denotes linear RoPE scaling with YaRN (Peng et al., 2023b).\nLong Context.Recurrent models were thought to perform well on long-context tasks because\nof their ability to preserve performance beyond their training sequence size.   However,  their\ndownstream performance on long-context tasks has not been well-documented. Prior studies either\ndo not conduct long-context evaluations (Katharopoulos et al., 2020; Kasai et al., 2021), evaluate only\non perplexity (Sun et al., 2023; De et al., 2024; Gu & Dao, 2023), or evaluate on datasets which require\ntask-specific training (Peng et al., 2023a). Instead, we consider downstreamnatural languagetasks\nfrom the SCROLLS benchmark (Shaham et al., 2022a). Specifically, in Table 2 we present two tasks –\nQasper (Dasigi et al., 2021) and NarrativeQA (Ko\nˇ\ncisk\n ́\ny et al., 2018) – from the set of tasks evaluated\nin the Llama2-Long report (Xiong et al., 2023).  We evaluate both tasks with an input context\ncut-off at different lengths. A strong long-context model should perform better given more context.\nHowever, the training context lengths for these models do not go beyond 8k tokens. Transformer\nmodels show the strongest results up to the context length they were trained for but degrade beyond\nthat. Interestingly, applying the YaRN trick (Peng et al., 2023b) enables transformers to scale beyond\ntheir training context quite well. RWKV shows a strong ability to handle much longer context than\nits training. Our Mamba model on the contrary is not able to generalize beyond its training context\nlength.  Surprisingly, the RecurrentGemma model (Griffin Team et al., 2024) shows degrading\n6",
    "performances even within its training context length. Finally, our Mistral-SUPRA  model preserves\nsome performance at larger context lengths but we believe it to result from the decay along the\ncontext length that shortens the effective context. This is discussed in more details below. We find a\nsignificant gap in performance between transformers and available linear models, including models\nuptrained from strong long-context transformers. We speculate that more sophisticated recurrent\nstate update rules may be required to perform well at this task. Ideas such as gating strategies (De\net al., 2024), higher order linear attention (Mercat, 2020), or associative binding (Munkhdalai et al.,\n2024) could be explored.\nDecay factors.The default decay factors proposed in Qin et al. (2024) gives better results than\nno decay on short context benchmarks but at a long range, the decay cancels out the influence\nof the context (max(γ)\n2048\n=3.35e\n−4\n). This can be related to a smooth version of window atten-\ntion (Beltagy et al., 2020). However, as more context is given to the model, the long-range evaluation\nperformances plateau.  When using the values proposed in Sun et al. (2023), that allow longer\nrange attention, we observe a performance drop on short-context benchmarks and no substantial\nimprovement on long-context evaluation.\nModelSizeTokensHellaSwag   ARC-E   ARC-C\nMamba1B100B58.362.729.1\nSUPRA (from scratch)1B100B54.759.627.8\nT2R (from scratch)1B100B55.261.028.4\nTransformer1B100B55.960.429.3\nTransformer*1B1.6T62.166.234.3\nFine-tune only new weights1B(1.6T)+10B33.237.822.6\n2-step fine-tune1B(1.6T)+10B+10B56.162.230.4\nT2R1B(1.6T)+10B40.653.827.9\nSUPRA1B(1.6T)+10B57.062.431.6\nSUPRA1B(100B)+10B51.757.527.4\nTable 3: Ablating different choices for linear uptraining: note the importance of normalization. For\nthe second half of the table, we uptrain a transformer trained on 1.6T tokens on 10B further tokens.\n*This model was trained on a different mix of data.\nAblations.Table 3 compares transformers pre-trained on 100B tokens to Mamba (Gu & Dao, 2023),\nT2R (Kasai et al., 2021), and our approach. At this scale, with 100B tokens of training, the Mamba\nmodel performs best and other models show similar performance. The second half of Table 3 shows\nresults for uptraining from a pre-trained transformer. TheT2R (Kasai et al., 2021) uptraining was\nunstable, yielding poor results compapred to SUPRA. This confirms that normalization is key for\nmaintaining performance of the base LLM when uptraining.\nTo test the hypothesis that linear attention approximates the softmax attention, we experimented\nwith a 2-step approach. The first step trains only the new parameters such that the model could\nlearn to approximate the softmax.  The second step fine-tunes all the weights.  The results show\nno benefit from the two steps approach and indicates that the softmax is not approximated. See\nAppendix A for a different approach to compare softmax attention and linear attention.\nFinally, we compare the results of SUPRA uptrainings from two pre-trained softmax models.  It\nappears that pre-training a linear model for a 100B token yields better results than fine-tuning a\nsoftmax model that was trained with the same budget. These results also shows, along with the\ncomparison of LLama2-SUPRA and Mistral-SUPRA in Table 1, that SUPRA benefits significantly\n7",
    "from a stronger pre-trained transformer. Thus, given a limited training budget, using SUPRA from\na strong pre-trained model is the best option.\n4  Discussion\nComparison to pre-training SSMs/RNNs.With only 20B tokens of training, which represents\n2−10% of RWKV and RetNet training cost, we obtain a model that outperforms both on HellaSwag\nand that is competitive on other benchmarks (see Table 1).  Given the existing performance gap\nbetween the strongest transformer models and the most performant linear models, SUPRA is a\nsimple recipe for conversion, allowing the study of strong RNNs with limited uptraining.\nComparison to Transformers on Short-Context Tasks.Our approach does not explicitly approxi-\nmate attention from the base transformer model (see Appendix A), we do see a modest drop in\nperformance across all benchmarks compared to softmax transformers.  This could be partially\nexplained by the lower quality of our data compared to the pre-training mix used to train models\nlike Mistral-7B. It is also likely that linear transformers are inherently less expressive. However,\nthe performance drop is relatively modest on most benchmarks, and significantly smaller than the\ndrop from T2R uptraining, which shows the relevance of our approach.\nLong Context Comparisons.Prior work on linear attention showcased similar or better validation\nset perplexity to transformer models over long context (e.g. Sun et al. (2023)) but did not evaluate\nlinear models onnatural languagelong-context evaluations like SCROLLS (Shaham et al., 2022b).\nThe results in Table 2 show that recurrent models generally maintain performance beyond their\ntraining context (except for Mamba-7b) while transformers (without modification) do not. However,\nTable 2 also demonstrates that simple linear scaling of the rotary positional embedding (Peng et al.,\n2023b; emozilla, 2023) can allow for context scaling beyond the window used for training a given\ntransformer model, effectively nullifying the performance edge of these linear models. Furthermore,\ntransformers generally outperform linear models at their maximum training context length. Further\nresearch is needed into extending linear models to long-context inference to take full advantage of\nthe lower inference cost relative to vanilla transformers.\nLimitations.Since our method relies on initializing with strong pre-trained transformers, our\nmodels inherit any of the biases and weaknesses of their base models.   Additionally,  models\nthat are already instruct-tuned do not linearize as well as base models. Our models suffer from\npoor performance on MMLU which requires in-context learning (5-shot), a weakness of linear\nmodels (Aky\n ̈\nurek et al., 2024). We leave the investigation of these weaknesses of linear models to\nfuture work and hope that our proposed uptraining approach can help facilitate and accelerate the\nresearch in this area.\n5  Related Work\nLinear Transformers.The linear transformers introduced in Katharopoulos et al. (2020) lagged\nbehind vanilla transformers in downstream performance, and subsequent architectures such as\nTransNormer (Qin et al., 2022a) and RetNet (Sun et al., 2023) narrow the gap, but do not demonstrate\ncompetitive results with modern transformers at scale. RWKV (Peng et al., 2023a), a linear trans-\nformer that takes inspiration from LSTM (Hochreiter & Schmidhuber, 1997), is competitive with\ncompute-matched transformer-based models, but lags behind on a number of NLU benchmarks.\nGriffin (De et al., 2024) is a concurrent model that takes a hybrid approach, combining a sliding\nwindow with linear attention shows impressive performance relative to vanilla transformers, but is\ntrained on a high-quality proprietary dataset.\nAnother thread in the literature focuses on efficient attention alternatives (Performers (Choromanski\net al., 2020), Cosformer (Qin et al., 2022b), LUNA (Ma et al., 2021), RFA (Peng et al., 2021), Attention-\n8",
    "free Transformer (Zhai et al., 2021)). All of these approaches sacrifice performances for efficiency.\nEfficiency improvements for vanilla transformers have narrowed the capabilities gap between\nvanilla and linear transformers.  The KV cache Pope et al. (2023)greatly narrows the inference\nefficiency gap between linear and vanilla transformers. RingAttention Liu et al. (2023) allows for\nvery long context scaling of vanilla attention without approximation.\nState Space Models.State-space models (SSMs) such as H3 (Dao et al., 2022), Hyena (Poli et al.,\n2023), and Mamba (Gu & Dao, 2023) are recent alternatives to vanilla transformers, combining the\nstrengths of convolutional and recurrent models with efficient hardware implementations. Instead\nof parallelizing training over the sequence, they produce an efficient way to train the sequential\nRNN. While these models are competitive with vanilla transformers on some tasks, we show that\nSSMs share the limitations of linear transformers on several in-context learning and long-context\ntasks.\nUptraining Linear Transformers.Hedgehog (Zhang et al., 2024) builds on the work of Kasai et al.\n(2021), identifying three different ways of training linear transformers – from scratch, uptraining\nquadratic transformers for a specific task, and uptraining generally.  The authors focus on the\nfirst two, and we focus on the third. Moreover, they aim at approximating the softmax attention\nmatrices with linear alternatives. In this work, we do not aim to approximate softmax attention,\nwe replace it with a linear alternative (see ablation above and appendix A). Their method is only\ntested for smaller scale models and with parameter-efficient fine-tuning for larger models, but\npresents challenges for scaling for two reasons: (1) their training strategy involves comparing full\nattention matrices which is computationally expensive, and not feasible for full fine-tuning of large\nmodels with long sequences and (2) their method also inherits the gradient instabilities of linear\ntransformers studied in Sun et al. (2023), while our normalization setup leads to stable uptraining\nof large models.\n6  Conclusion\nWe introduced SUPRA, a technique for converting large-scale pre-trained softmax transformers\ninto recurrent neural networks, enabling the study of the strengths and limitations of recurrent\nmodels at scale with minimal compute cost. Compared to pre-training linear models from scratch,\nthe SUPRA strategy produces competitive models comparable to the best available recurrent LLMs\n(RWKV and Mamba) at the 7B scale.\nWe identify the strengths of linear models on standard NLU benchmarks but also the enduring\nlimitations on in-context (i.e. MMLU) and long-context (NarrativeQA, Qasper) tasks, showing that\nlinearized models do not inherit these capabilities from the base softmax transformers.\nOne  possible  path  to  rectifying  these  limitations  is  explicitly  training  for  in-context  learn-\ning (Aky\n ̈\nurek et al., 2024). We leave explorations of specialized and instruct data in the context of\nlinear transformers to future work. More sophisticated gating mechanisms as in in Peng et al. (2023a)\nand De et al. (2024) are promising alternatives to our simple linearization. Using our uptraining\nmethod would greatly reduce the necessary time and cost of such experimentation.\n9",
    "7  Reproducibility\nCodebaseWe train our linear models using our fork of OpenLM (Gururangan et al., 2023) that\nwe modify to include a linear attention function (printed below).  We use Lightning Attention\n2 (Qin et al., 2024) that offers a fast Triton (Tillet et al., 2019) kernel for linear attention computation.\nEvaluations are done with the Eleuther evaluation harness (Gao et al., 2023).\nDataWe train and uptrain models on RefinedWeb (Penedo et al., 2023)(with 2 epochs for our\nMamba training), which we tokenize with the pre-trained model’s tokenizers. When training from\nscratch, we used the GPT-NeoX-20B (Black et al., 2022) tokenizer.  We tokenize with sequence\npacking and use a default sequence length of 2048.\nHyperparametersWe use square matrices with biases for the linear layers in the kernelφfunctions\nto keep the same feature dimension in the queries and keys. We use the same kernel, with the same\nweights for both queries and keys and apply a ReLU activation. We use 1000 steps of linear learning\nrate warmup and a cosine learning rate decay from 3e\n−5\nto 1e\n−5\nfor our 7B uptrainings and from\n3e\n−4\nto 1e\n−5\nfor our 1B uptrainings and for trainings from scratch.  We use the Adam optimizer\nwithβ\n1\n=0.9 andβ\n2\n=0.95. We trained our models with mini-batches totaling 2M tokens each.\nOur default RoPE frequency uses the Llama value of 10\n4\n. For longer sequence lengths, we use a\nRoPE frequency of 10\n6\n.\nTrainingDepending on the model size and the availability,  we use from 4 to 32 nodes of 8\nGPUs Nvidia H100 with Pytorch FSDP. We use a mixed precision strategy from OpenLM that\nautomatically selects between bfloat 16 and float 32 for different operations. A linear 7B parameter\nmodel uptraining throughput is around 4300 tokens per second per GPU.\nModelsOur results can be reproduced by following the same training recipe or using the model\nweights that we release: Mistral-SUPRA and Mamba-7b.\n1def  linear_attn_func(q, k, v, qk_scale: float , use_decay: bool = True ,\nnormalize: bool = False) -> torch.Tensor:\n2\"\"\"\n3Args:\n4q: queries , shape (batch_size , num_heads , seq_len , dim_qk)\n5k: keys , shape (batch_size , num_heads , seq_len , dim_qk)\n6v: values , shape (batch_size , num_heads , seq_len , dim_v)\n7qk_scale: scale  factor  for  queries  and  keys\n8\"\"\"\n9h = q.shape [1]\n10if  use_decay:\n11s = slope_tensor(h, q.device , q.dtype)\n12else:\n13s = no_slope_tensor(h, q.device , q.dtype)\n14\n15output = lightning_attn_ops(q, k * qk_scale , v, s)\n16if  normalize:\n17norm = torch.clamp_min(\n18torch.einsum(\n19\"nhld , nhld ->nhl\", q, k.cumsum (2) * qk_scale\n20), 1e-6\n21)\n22return  output / norm.unsqueeze (-1)\n23else:\n24return  output\n10",
    "References\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\n ́\non, and Sumit\nSanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints.\narXiv preprint arXiv:2305.13245, 2023.\nEkin Aky\n ̈\nurek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Arhitec-\ntures and algorithms.arXiv preprint arXiv:2401.12973, 2024.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.  Layer normalization.arXiv preprint\narXiv:1607.06450, 2016.\nIz Beltagy, Matthew E Peters, and Arman Cohan.  Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu\nPurohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach.  Gpt-neox-20b: An\nopen-source autoregressive language model, 2022. URLhttps://arxiv.org/abs/2204.06745.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention\nwith performers.arXiv preprint arXiv:2009.14794, 2020.\nTri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\n ́\ne.  Hun-\ngry  hungry  hippos:   Towards  language  modeling  with  state  space  models.arXiv preprint\narXiv:2212.14052, 2022.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset\nof information-seeking questions and answers anchored in research papers.  InProceedings of\nthe 2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 4599–4610, Online, June 2021. Association for Computational\nLinguistics.  doi:  10.18653/v1/2021.naacl-main.365.  URLhttps://aclanthology.org/2021.\nnaacl-main.365.\nSoham  De,  Samuel  L  Smith,  Anushan  Fernando,  Aleksandar  Botev,  George  Cristian-Muraru,\nAlbert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al.  Griffin:\nMixing gated linear recurrences with local attention for efficient language models.arXiv preprint\narXiv:2402.19427, 2024.\nemozilla.   Dynamically  scaled  rope  further  increases  strength  of  retaining  walls,  2023.   URL\nhttps://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_\nfurther_increases/. Reddit post.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff,\nChris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.  A framework for few-shot\nlanguage model evaluation, 12 2023. URLhttps://zenodo.org/records/10256836.\nAlexsandar  Botev  Griffin  Team,  Soham  De,  Samuel  L  Smith,  Anushan  Fernando,  George-\nChristian Muraru, Ruba Haroun, and Leonard Berrada et al.  Recurrentgemma.arXiv preprint\narXiv:2404.07839, 2024.\n11",
    "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.arXiv\npreprint arXiv:2312.00752, 2023.\nAlbert Gu, Karan Goel, and Christopher R\n ́\ne. Efficiently modeling long sequences with structured\nstate spaces.arXiv preprint arXiv:2111.00396, 2021.\nSuchin Gururangan, Mitchell Wortsman, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia\nShi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis,\nAli Farhadi, Vaishaal Shankar, and Ludwig Schmidt.  OpenLM: a minimal but performative\nlanguage modeling (lm) repository, 2023. URLhttps://github.com/mlfoundations/open_lm/.\nGitHub repository.\nSepp Hochreiter and J\n ̈\nurgen Schmidhuber.  Long short-term memory.Neural computation, 9(8):\n1735–1780, 1997.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\nMistral 7b.arXiv preprint arXiv:2310.06825, 2023.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.  Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nJungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao,\nWeizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns.arXiv preprint\narXiv:2103.13076, 2021.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ̧ois Fleuret. Transformers are\nrnns: Fast autoregressive transformers with linear attention. InInternational conference on machine\nlearning, pp. 5156–5165. PMLR, 2020.\nTom\n ́\na\nˇ\ns Ko\nˇ\ncisk\n ́\ny, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\n ́\nabor Melis,\nand Edward Grefenstette.  The NarrativeQA reading comprehension challenge.Transactions\nof the Association for Computational Linguistics, 6:317–328, 2018. doi: 10.1162/tacla00023. URL\nhttps://aclanthology.org/Q18-1023.\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-\ninfinite context.arXiv preprint arXiv:2310.01889, 2023.\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettle-\nmoyer. Luna: Linear unified nested attention.Advances in Neural Information Processing Systems,\n34:2441–2453, 2021.\nJean Mercat. Higher order linear transformer.arXiv preprint arXiv:2010.14816, 2020.\nTsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient\ninfinite context transformers with infini-attention.arXiv preprint arXiv:2404.07143, 2024.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.  The refinedweb\ndataset for falcon llm: outperforming curated corpora with web data, and web data only.arXiv\npreprint arXiv:2306.01116, 2023.\n12",
    "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin\nCheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the\ntransformer era.arXiv preprint arXiv:2305.13048, 2023a.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models.arXiv preprint arXiv:2309.00071, 2023b.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.\nRandom feature attention.arXiv preprint arXiv:2103.02143, 2021.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,\nStefano Ermon, and Christopher R\n ́\ne. Hyena hierarchy: Towards larger convolutional language\nmodels.arXiv preprint arXiv:2302.10866, 2023.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan\nHeek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.  Efficiently scaling transformer inference.\nProceedings of Machine Learning and Systems, 5, 2023.\nZhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong.\nThe devil in linear transformer.arXiv preprint arXiv:2210.10340, 2022a.\nZhen Qin,  Weixuan Sun,  Hui Deng,  Dongxu Li,  Yunshen Wei,  Baohong Lv,  Junjie Yan,  Ling-\npeng  Kong,  and  Yiran  Zhong.   cosformer:  Rethinking  softmax  in  attention.arXiv preprint\narXiv:2202.08791, 2022b.\nZhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-\n2: A free lunch for handling unlimited sequence lengths in large language models.arXiv preprint\narXiv:2401.04658, 2024.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long\nlanguage sequences. InProceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022a. Association for\nComputational Linguistics. URLhttps://aclanthology.org/2022.emnlp-main.823.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language sequences.\narXiv preprint arXiv:2201.03533, 2022b.\nJianlin  Su,  Yu  Lu,  Shengfeng  Pan,  Ahmed  Murtadha,  Bo  Wen,  and  Yunfeng  Liu.   Roformer:\nEnhanced transformer with rotary position embedding.arXiv preprint arXiv:2104.09864, 2021.\nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and\nFuru Wei.  Retentive network:  A successor to transformer for large language models.arXiv\npreprint arXiv:2307.08621, 2023.\nPhilippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled\nneural network computations. InProceedings of the 3rd ACM SIGPLAN International Workshop on\nMachine Learning and Programming Languages, MAPL 2019, pp. 10–19, New York, NY, USA, 2019.\nAssociation for Computing Machinery. ISBN 9781450367196. doi: 10.1145/3315508.3329973. URL\nhttps://doi.org/10.1145/3315508.3329973.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023.\n13",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing\nsystems, 30, 2017.\nYuxin Wu and Kaiming He.  Group normalization.  InProceedings of the European conference on\ncomputer vision (ECCV), pp. 3–19, 2018.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin,\nRashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar\nMehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis,\nSinong Wang, and Hao Ma. Effective long-context scaling of foundation models, 2023.\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and\nJosh Susskind. An attention free transformer.arXiv preprint arXiv:2105.14103, 2021.\nMichael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher R\n ́\ne.   The hedgehog & the\nporcupine: Expressive linear attentions with softmax mimicry.arXiv preprint arXiv:2402.04347,\n2024.\n14",
    "A  Attention Approximation\nIn this section we investigate whether our up-training procedure leads to linear attention that\napproximates the softmax attention from the base model, as might be expected.\nThere are many possible ways to compare attention matrices. Moreover, some architecture changes\nsuch  as  attention  decay  and  lack  of  normalization  in  the  linear  attention  make  a  meaningful\ncomparison difficult.  We represent non-normalized comparisons in Figure 3.  It represents the\ncosine similarities and singular value distances between the attention matrices at every layer and\nfor every head of the Mistral model compared with our Mistral-SUPRA. Each pixel of these images\nis a scalar similarity measure between two matrices represented by a color scale. In Figure 3, we see\nlarge differences between the matrices.\nSince we removed the attention matrix normalization and replaced it with a LayerNorm Ba et al.\n(2016), we want to compare normalized attention matrices instead.  We divide each line of the\nmatrix by the absolute value of the sum of its elements such that the softmax attention matrix is\nunaffected and the linear attention matrix is normalized. In Figure 4, we see significantly higher\nbetween most matrices with some exceptions. These observations indicate that the linear attention\nmatrices derived from SUPRA arenot an approximationof the softmax matrices.\nFigure 3: Representation of the cosine similarity and the distance between the singular values of\nthe softmax attention matrices compared to the SUPRA attention matrices.\nFigure 4: Representation of the cosine similarity and the distance between the singular values of\nthenormalizedsoftmax attention matrices compared to thenormalizedSUPRA attention matrices.\n15"
  ]
}