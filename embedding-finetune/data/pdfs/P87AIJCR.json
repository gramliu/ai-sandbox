{
  "key": "P87AIJCR",
  "url": "http://arxiv.org/pdf/2206.08853",
  "metadata": {
    "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\n  Knowledge",
    "abstract": "  Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https://minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n",
    "published": "2022-06-17T15:53:05Z"
  },
  "text": [
    "MINEDOJO: Building Open-Ended\nEmbodied Agents with Internet-Scale Knowledge\nLinxi Fan\n1\n, Guanzhi Wang\n2∗\n, Yunfan Jiang\n3∗\n, Ajay Mandlekar\n1\n, Yuncong Yang\n4\n,\nHaoyi Zhu\n5\n, Andrew Tang\n4\n, De-An Huang\n1\n, Yuke Zhu\n1 6†\n, Anima Anandkumar\n1 2†\n1\nNVIDIA,\n2\nCaltech,\n3\nStanford,\n4\nColumbia,\n5\nSJTU,\n6\nUT Austin\n∗\nEqual contribution\n†\nEqual advising\nhttps://minedojo.org\nAbstract\nAutonomous agents have made great strides in specialist domains like Atari games\nand Go. However, they typically learntabula rasain isolated environments with\nlimited and manually conceived objectives, thus failing to generalize across a wide\nspectrum of tasks and capabilities.  Inspired by how humans continually learn\nand adapt in the open world, we advocate a trinity of ingredients for building\ngeneralist agents: 1) an environment that supports a multitude of tasks and goals,\n2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable\nagent  architecture.   We  introduceMINEDOJO,  a  new  framework  built  on  the\npopularMinecraftgame that features a simulation suite with thousands of diverse\nopen-ended tasks and an internet-scale knowledge base with Minecraft videos,\ntutorials, wiki pages, and forum discussions. UsingMINEDOJO’s data, we propose\na novel agent learning algorithm that leverages large pre-trained video-language\nmodels as a learned reward function. Our agent is able to solve a variety of open-\nended tasks specified in free-form language without any manually designed dense\nshaping reward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https://minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n1    Introduction\nDeveloping autonomous embodied agents that can attain human-level performance across a wide\nspectrum of tasks has been a long-standing goal for AI research. There has been impressive progress\ntowards this goal, most notably in games [80,85,126] and robotics [68,99,146,134,107]. These\nembodied agents are typically trainedtabula rasain isolated worlds with limited complexity and\ndiversity. Although highly performant, they are specialist models that do not generalize beyond a\nnarrow set of tasks. In contrast, humans inhabit an infinitely rich reality, continuously learn from and\nadapt to a wide variety of open-ended tasks, and are able to leverage large amount of prior knowledge\nfrom their own experiences as well as others.\nWe argue thatthree main pillarsare necessary for generalist embodied agents to emerge. First, the\nenvironment in which the agent acts needs toenable an unlimited variety of open-ended goals\n[116,71,120,117]. Natural evolution is able to nurture an ever-expanding tree of diverse life forms\nthanks to the infinitely varied ecological settings that the Earth supports [117,129]. This process has\nnot stagnated for billions of years. In contrast, today’s agent training algorithms cease to make new\nprogress after convergence in narrow environments [80,146].  Second, alarge-scale database of\nprior knowledgeis necessary to facilitate learning in open-ended settings. Just as humans frequently\nlearn from the internet, agents should also be able to harvest practical knowledge encoded in large\namounts of video demos [42,77], multimedia tutorials [79], and forum discussions [127,65,54]. In a\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\narXiv:2206.08853v2  [cs.LG]  22 Nov 2022",
    "<latexit sha1_base64=\"A93yt662bGCbG73Mr+hOk0IeTLY=\">AAAB6nicdVDJSgNBEK1xjXGLevTSGARPQ08yWbxFvAheIpoFkiH0dHqSJj0L3T1CCPkELx4U8eoXefNv7CyCij4oeLxXRVU9PxFcaYw/rJXVtfWNzcxWdntnd28/d3DYVHEqKWvQWMSy7RPFBI9YQ3MtWDuRjIS+YC1/dDnzW/dMKh5Hd3qcMC8kg4gHnBJtpNuLnu7l8tgu4LJbwciQUtWpFA0purjkniPHxnPkYYl6L/fe7cc0DVmkqSBKdRycaG9CpOZUsGm2myqWEDoiA9YxNCIhU95kfuoUnRqlj4JYmoo0mqvfJyYkVGoc+qYzJHqofnsz8S+vk+qg6k14lKSaRXSxKEgF0jGa/Y36XDKqxdgQQiU3tyI6JJJQbdLJmhC+PkX/k2bBdso2vnHztetlHBk4hhM4AwcqUIMrqEMDKAzgAZ7g2RLWo/VivS5aV6zlzBH8gPX2CYbOjgA=</latexit>\nA\nt\nCraft Glass Bridge\nCombat Zombie\nFish Squid\nBuild Oak HouseMake Ice Igloo\nFarm Sugar Cane\nFind Ocean \nMonument\nTreasure Hunt\nin End City\nExplore \nDesert Temple\nOpen-ended EnvironmentsGeneralist AgentInternet-scale Knowledge Base\nWiki\nYouTube\nReddit\nFigure 1:MINEDOJOis a novel framework for developing open-ended, generally capable agents\nthat can learn and adapt continually to new goals.MINEDOJOfeatures a benchmarking suite with\nthousandsof diverse open-ended tasksspecified in natural language prompts, and also provides an\ninternet-scale, multimodal knowledge baseof YouTube videos, Wiki pages, and Reddit posts. The\ndatabase captures the collective experience and wisdom of millions of Minecraft gamers for an AI\nagent to learn from. Best viewed zoomed in.\ncomplex world, it would be extremely inefficient for an agent to learn everything from scratch through\ntrial and error. Third, theagent’s architectureneeds to be flexible enough to pursue any task in open-\nended environments, and scalable enough to convert large-scale knowledge sources into actionable\ninsights [19,96]. This motivates the design of an agent that has a unified observation/action space,\nconditions on natural language task prompts, and adopts the Transformer pre-training paradigm [27,\n91, 15] to internalize knowledge effectively.\nIn light of these three pillars, we introduceMINEDOJO, a new framework to help the community\ndevelop open-ended, generally-capable agents. It is built on the popular Minecraft game, where a\nplayer explores a procedurally generated 3D world with diverse types of terrains to roam, materials\nto mine,  tools to craft,  structures to build,  and wonders to discover.   Unlike most other games\n[80,85,126], Minecraft defines no specific reward to maximize and no fixed storyline to follow,\nmaking it well suited for developing open-ended environments for embodied AI research. We make\nthe following three major contributions:\n1.   Simulation  platform  with  thousands  of  diverse  open-ended  tasks.MINEDOJOprovides\nconvenient APIs on top of Minecraft that standardize task specification, world settings, and agent’s\nobservation/action spaces.  We introduce a benchmark suite that consists of thousands of natural\nlanguage-prompted tasks, making ittwo orders of magnitudelarger than prior Minecraft benchmarks\nlike the MineRL Challenge [48,62]. The suite includes long-horizon, open-ended tasks that cannot\nbe easily evaluated through automated procedures, such as “build an epic modern house with two\nfloors and a swimming pool”.  Inspired by the Inception score [98] and FID score [55] that are\ncommonly used to assess AI-generated image quality, we introduce a novel agent evaluation protocol\nusing a large video-language model pre-trained on Minecraft YouTube videos. This complements\nhuman scoring [104] that is precise but more expensive.  Our learned evaluation metric has good\nagreement with human judgment in a subset of the full task suite considered in the experiments.\n2. Internet-scale multimodal Minecraft knowledge base.\nMinecraft has more than 100 million\nactive players [131], who have collectively generated an enormous wealth of data.  They record\ntutorial videos, stream live play sessions, compile recipes, and discuss tips and tricks on forums.\nMINEDOJOfeatures a massive collection of 730K+ YouTube videos with time-aligned transcripts,\n6K+ free-form Wiki pages, and 340K+ Reddit posts with multimedia contents (Fig. 3). We hope that\nthis enormous knowledge base can help the agent acquire diverse skills, develop complex strategies,\ndiscover interesting objectives, and learn actionable representations automatically.\n2",
    "shear asheep\ncombat zombie \npigman\nfind a nether \nportal\nput carpets on \nthe floor\nFigure 2: Visualization of our agent’s learned behaviors on four selected tasks. Leftmost texts are the\ntask prompts used in training. Best viewed on a color display.\n3.   Novel  algorithm  for  embodied  agents  with  large-scale  pre-training.\nWe  develop  a  new\nlearning algorithm for embodied agents that makes use of the internet-scale domain knowledge we\nhave collected from the web. Using the massive volume of YouTube videos fromMINEDOJO, we\ntrain a video-text contrastive model in the spirit of CLIP [92], which associates natural language\nsubtitles with their time-aligned video segments. We demonstrate that this learned correlation score\ncan be used effectively as anopen-vocabulary, massively multi-task reward functionfor RL training.\nOur agent solves the majority of 12 tasks in our experiment using the learned reward model (Fig. 2).\nIt achieves competitive performance to agents trained with meticulously engineered dense-shaping\nrewards, and in some cases outperforms them, with up to 73% improvement in success rates. For\nopen-ended tasks that do not have a simple success criterion, our agents also perform well without\nany special modifications.\nIn summary, this paper proposes an open-ended task suite, internet-scale domain knowledge, and agent\nlearning with recent advances on large pre-trained models [13]. We have open-sourcedMINEDOJO’s\nsimulator, knowledge bases, algorithm implementations, pretrained model checkpoints, and task\ncuration tools athttps://minedojo.org/.  We hope thatMINEDOJOwill serve as an effective\nstarter framework for the community to develop new algorithms and advance towards generally\ncapable embodied agents.\n2MINEDOJOSimulator & Benchmark Suite\nMINEDOJO\noffers a set of simulator APIs help researchers develop generally capable, open-ended\nagents in Minecraft. It builds upon the open-source MineRL codebase [48] and makes the following\nupgrades:  1) We provideunified observation and action spacesacross all tasks, facilitating the\ndevelopment of multi-task and continually learning agents that can constantly adapt to new scenarios\nand novel tasks. This deviates from the MineRL Challenge design that tailors observation and action\nspaces to individual tasks; 2) Our simulation unlocks all three types of worlds in Minecraft, including\ntheOverworld, theNether, and theEnd, whichsubstantially expands the possible task space,\nwhile MineRL only supports the Overworld natively; and 3) We provide convenient APIs to configure\ninitial conditions and world settings to standardize our tasks.\nWith thisMINEDOJOsimulator, we define thousands of benchmarking tasks, which are divided into\ntwo categories: 1)Programmatic tasksthat can be automatically assessed based on the ground-truth\nsimulator states; and 2)Creative tasksthat do not have well-defined or easily-automated success\ncriteria, which motivates our novel evaluation protocol using a learned model (Sec. 4). To scale up\nthe number of Creative tasks, we mine ideas from YouTube tutorials and use OpenAI’s GPT-3 [15]\n3",
    "service to generate substantially more task definitions. Compared to Creative tasks, Programmatic\ntasks are simpler to get started, but tend to have restricted scope, limited language variations, and less\nopen-endedness in general.\n2.1    Task Suite I: Programmatic Tasks\nWe  formalize  each  programmatic  task  as  a  5-tuple:T=  (G,G,I,f\nS\n,f\nR\n).Gis  an  English\ndescription  of  the  task  goal,  such  as  “find  material  and  craft  a  gold  pickaxe”.Gis  a  natural\nlanguage  guidance  that  provides  helpful  hints,  recipes,  or  advice  to  the  agent.   We  leverage\nOpenAI’sGPT-3-davinciAPI  to  automatically  generate  detailed  guidance  for  a  subset  of\nthe  tasks.   For  the  example  goal  “bring  a  pig  into  Nether”,  GPT-3  returns:1) Find a pig\nin the overworld; 2) Right-click on the pig with a lead; 3) Right-click on\nthe Nether Portal with the lead and pig selected; 4) The pig will be pulled\nthrough the portal!Iis the initial conditions of the agent and the world, such as the initial\ninventory, spawn terrain, and weather.f\nS\n:s\nt\n→ {0,1}is the success criterion, a deterministic\nfunction that maps the current world states\nt\nto a Boolean success label.f\nR\n:s\nt\n→Ris an optional\ndense reward function.  We only providef\nR\nfor a small subset of the tasks inMINEDOJOdue\nto the high costs of meticulously crafting dense rewards.  For our current agent implementation\n(Sec. 4.1), we do not use detailed guidance. Inspired by concurrent works SayCan[3]and Socratic\nModels[143], one potential idea is to feed each step in the guidance to our learned reward model\nsequentially so that it becomes a stagewise reward function for a complex multi-stage task.\nMINEDOJOprovides 4 categories of programmatic tasks with 1,581 template-generated natural\nlanguage goals to evaluate the agent’s different capabilities systematically and comprehensively:\n1.Survival: surviving for a designated number of days.\n2.Harvest: finding, obtaining, cultivating, or manufacturing hundreds of materials and objects.\n3.Tech Tree: crafting and using a hierarchy of tools.\n4.Combat: fighting various monsters and creatures that require fast reflex and martial skills.\nEach task template has a number of variations based on the terrain, initial inventory, quantity, etc.,\nwhich form a flexible spectrum of difficulty. In comparison, the NeurIPS MineRL Diamond challenge\n[48] is a subset of our programmatic task suite, defined by the task goal “obtain 1 diamond\" in\nMINEDOJO.\n2.2    Task Suite II: Creative Tasks\nWe define each creative task as a 3-tuple,T= (G,G,I), which differs from programmatic tasks due\nto the lack of straightforward success criteria. Inspired by model-based metrics like the Inception\nscore [98] and FID score [55] for image generation, we design a novel task evaluation metric based\non a pre-trained contrastive video-language model (Sec. 4.1). In the experiments, we find that the\nlearned metric exhibits a high level of agreement with human evaluations (see Table 2).\nWe brainstorm and author 216 Creative tasks, such as “build a haunted house with zombie inside” and\n“race by riding a pig”. Nonetheless, such a manual approach is not scalable. Therefore, we develop\ntwo systematic approaches to extend the total number of task definitions to 1,560. This makes our\nCreative tasks3 orders of magnitudelarger than Minecraft BASALT challenge [104], which has 4\nCreative tasks.\nApproach 1.  Task Mining from YouTube Tutorial Videos.We identify our YouTube dataset\nas a rich source of tasks,  as many human players demonstrate and narrate creative missions in\nthe tutorial playlists. To collect high-quality tasks and accompanying videos, we design a 3-stage\npipeline that makes it easy to find and annotate interesting tasks (see Sec. C.2 for details). Through\nthis pipeline, we extract 1,042 task ideas from the common wisdom of a huge number of veteran\nMinecraft gamers, such as “make an automated mining machine” and “grow cactus up to the sky”.\nApproach 2. Task Creation by GPT-3.\nWe leverage GPT-3’s few-shot capability to generate new\ntask ideas by seeding it with the tasks we manually author or mine from YouTube. The prompt tem-\nplate is:Here are some example creative tasks in Minecraft:  {a few examples}.\n4",
    "YouTubeWikiReddit\nFigure  3:MINEDOJO’s  internet-scale,  multimodal  knowledge  base.Left,  YouTube  videos:\nMinecraft gamers showcase the impressive feats they are able to achieve.  Clockwise order:  an\narchery range, Hogwarts castle, Taj Mahal, a Nether homebase.Middle, Wiki:Wiki pages contain\nmultimodal knowledge in structured layouts, such as comprehensive catalogs of creatures and recipes\nfor crafting.  More examples in Fig. A.4 and A.5.Right, Reddit:We create a word cloud from\nReddit posts and comment threads. Gamers ask questions, share achievements, and discuss strategies\nextensively. Sample posts in Fig. A.7. Best viewed zoomed in.\nLet’s brainstorm more detailed while reasonable creative tasks in Minecraft.\nGPT-3 contributes 302 creative tasks after de-duplication, and demonstrates a surprisingly proficient\nunderstanding of Minecraft terminology.\n2.3    Collection of Starter Tasks\nWe curate a set of 64 core tasks for future researchers to get started more easily. If their agent works\nwell on these tasks, they can more confidently scale to the full benchmark.\n•32 programmatic tasks: 16 “standard” and 16 “difficult”, spanning all 4 categories (survival,\nharvesting, combat, and tech tree). We rely on our Minecraft knowledge to decide the difficulty\nlevel. “Standard” tasks require fewer steps and lower resource dependencies to complete.\n•32 creative tasks: 16 “standard” and 16 “difficult”. Similarly, tasks labeled with “standard” are\ntypically short-horizon tasks.\nWe recommend that researchers run 100 evaluation episodes for each task and report the percentage\nsuccess rate. The programmatic tasks have ground-truth success, while the creative tasks need our\nnovel evaluation protocol (Sec. 5).\n3    Internet-scale Knowledge Base\nTwo commonly used approaches [112,126,85,36] to train embodied agents include training agents\nfrom scratch using RL with well-tuned reward functions for each task, or using a large amount of\nhuman-demonstrations to bootstrap agent learning. However, crafting well-tuned reward functions is\nchallenging or infeasible for our task suite (Sec. 2.2), and employing expert gamers to provide large\namounts of demonstration data would also be costly and infeasible [126].\nInstead, we turn to the open web as an ever-growing, virtually unlimited source of learning material\nfor embodied agents. The internet provides a vast amount of domain knowledge about Minecraft,\nwhich we harvest by extensive web scraping and filtering. We collect 33 years worth of YouTube\nvideos, 6K+ Wiki pages, and millions of Reddit comment threads.  Instead of hiring a handful of\nhuman demonstrators, we capture the collective wisdom of millions of Minecraft gamers around the\nworld. Furthermore, language is a key and pervasive component of our database that takes the form\nof YouTube transcripts, textual descriptions in Wiki, and Reddit discussions. Language facilitates\nopen-vocabulary understanding, provides grounding for image and video modalities, and unlocks the\npower of large language models [27,109,15] for embodied agents. To ensure socially responsible\nmodel development, we take special measures to filter out low-quality and toxic contents [13,51]\nfrom our databases, detailed in the Appendix (Sec. D).\n5",
    "“Shear sheep to \nobtain wool”\nMineCLIP\nCorrelation = 0.95\nRGB\nVoxel\nGPS\nInventory\nObservation space\nMove\nAttack\nCam\nEquip\nAction space\nMineDojoSim\n<latexit sha1_base64=\"ca4T8nSUbwsT4Z+NGlf9Q5N3EkE=\">AAAB7XicdVDLSgMxFM3UV62vqks3wSK4GmbKdGp3RTcuK9hpoR1KJs20sZlkSDJCGfoPblwo4tb/ceffmD4EFT0kcDjnXu69J0oZVdpxPqzC2vrG5lZxu7Szu7d/UD48CpTIJCZtLJiQ3QgpwignbU01I91UEpREjHSiydXc79wTqajgt3qakjBBI05jipE2UtBPx3QQDMoVx676NfOgY3ue59erhri1huc3oGs7C1TACq1B+b0/FDhLCNeYIaV6rpPqMEdSU8zIrNTPFEkRnqAR6RnKUUJUmC+2ncEzowxhLKT5XMOF+r0jR4lS0yQylQnSY/Xbm4t/eb1MxxdhTnmaacLxclCcMagFnJ8Oh1QSrNnUEIQlNbtCPEYSYW0CKpkQvi6F/5Ogaru+7d14leblKo4iOAGn4By4oA6a4Bq0QBtgcAcewBN4toT1aL1Yr8vSgrXqOQY/YL19AvPQj2g=</latexit>\n\u0000\nV\nStack the last 16 RGB frames\n<latexit sha1_base64=\"RalrFwoax4lhGLaFvab2Gtps3C0=\">AAAB7XicdVDLSgMxFL3js9ZX1aWbYBFcDTNlOrW7ohvdVbAPaIeSSTNtbGYyJBmhlP6DGxeKuPV/3Pk3pg9BRQ8JHM65l3vvCVPOlHacD2tldW19YzO3ld/e2d3bLxwcNpXIJKENIriQ7RAryllCG5ppTtuppDgOOW2Fo8uZ37qnUjGR3OpxSoMYDxIWMYK1kZrddMh6171C0bFLftk85Nie5/mVkiFuuer5VeTazhxFWKLeK7x3+4JkMU004VipjuukOphgqRnhdJrvZoqmmIzwgHYMTXBMVTCZbztFp0bpo0hI8xON5ur3jgmOlRrHoamMsR6q395M/MvrZDo6DyYsSTNNE7IYFGUcaYFmp6M+k5RoPjYEE8nMrogMscREm4DyJoSvS9H/pFmyXd/2brxi7WIZRw6O4QTOwIUK1OAK6tAAAnfwAE/wbAnr0XqxXhelK9ay5wh+wHr7BOAcj1s=</latexit>\n\u0000\nI\nTime\n<latexit sha1_base64=\"RalrFwoax4lhGLaFvab2Gtps3C0=\">AAAB7XicdVDLSgMxFL3js9ZX1aWbYBFcDTNlOrW7ohvdVbAPaIeSSTNtbGYyJBmhlP6DGxeKuPV/3Pk3pg9BRQ8JHM65l3vvCVPOlHacD2tldW19YzO3ld/e2d3bLxwcNpXIJKENIriQ7RAryllCG5ppTtuppDgOOW2Fo8uZ37qnUjGR3OpxSoMYDxIWMYK1kZrddMh6171C0bFLftk85Nie5/mVkiFuuer5VeTazhxFWKLeK7x3+4JkMU004VipjuukOphgqRnhdJrvZoqmmIzwgHYMTXBMVTCZbztFp0bpo0hI8xON5ur3jgmOlRrHoamMsR6q395M/MvrZDo6DyYsSTNNE7IYFGUcaYFmp6M+k5RoPjYEE8nMrogMscREm4DyJoSvS9H/pFmyXd/2brxi7WIZRw6O4QTOwIUK1OAK6tAAAnfwAE/wbAnr0XqxXhelK9ay5wh+wHr7BOAcj1s=</latexit>\n\u0000\nI\n<latexit sha1_base64=\"RalrFwoax4lhGLaFvab2Gtps3C0=\">AAAB7XicdVDLSgMxFL3js9ZX1aWbYBFcDTNlOrW7ohvdVbAPaIeSSTNtbGYyJBmhlP6DGxeKuPV/3Pk3pg9BRQ8JHM65l3vvCVPOlHacD2tldW19YzO3ld/e2d3bLxwcNpXIJKENIriQ7RAryllCG5ppTtuppDgOOW2Fo8uZ37qnUjGR3OpxSoMYDxIWMYK1kZrddMh6171C0bFLftk85Nie5/mVkiFuuer5VeTazhxFWKLeK7x3+4JkMU004VipjuukOphgqRnhdJrvZoqmmIzwgHYMTXBMVTCZbztFp0bpo0hI8xON5ur3jgmOlRrHoamMsR6q395M/MvrZDo6DyYsSTNNE7IYFGUcaYFmp6M+k5RoPjYEE8nMrogMscREm4DyJoSvS9H/pFmyXd/2brxi7WIZRw6O4QTOwIUK1OAK6tAAAnfwAE/wbAnr0XqxXhelK9ay5wh+wHr7BOAcj1s=</latexit>\n\u0000\nI\n<latexit sha1_base64=\"RalrFwoax4lhGLaFvab2Gtps3C0=\">AAAB7XicdVDLSgMxFL3js9ZX1aWbYBFcDTNlOrW7ohvdVbAPaIeSSTNtbGYyJBmhlP6DGxeKuPV/3Pk3pg9BRQ8JHM65l3vvCVPOlHacD2tldW19YzO3ld/e2d3bLxwcNpXIJKENIriQ7RAryllCG5ppTtuppDgOOW2Fo8uZ37qnUjGR3OpxSoMYDxIWMYK1kZrddMh6171C0bFLftk85Nie5/mVkiFuuer5VeTazhxFWKLeK7x3+4JkMU004VipjuukOphgqRnhdJrvZoqmmIzwgHYMTXBMVTCZbztFp0bpo0hI8xON5ur3jgmOlRrHoamMsR6q395M/MvrZDo6DyYsSTNNE7IYFGUcaYFmp6M+k5RoPjYEE8nMrogMscREm4DyJoSvS9H/pFmyXd/2brxi7WIZRw6O4QTOwIUK1OAK6tAAAnfwAE/wbAnr0XqxXhelK9ay5wh+wHr7BOAcj1s=</latexit>\n\u0000\nI\nAggregate\nVideo\nFeature\nPer-frame\nFeature\nFigure 4:  Algorithm design.MINECLIPis a contrastive video-language model pre-trained on\nMINEDOJO’s massive Youtube database. It computes the correlation between an open-vocabulary\nlanguage goal string and a 16-frame video snippet. The correlation score can be used as a learned\ndense reward function to train a strong multi-task RL agent.\nYouTube Videos and Transcripts.Minecraft is among the most streamed games on YouTube [41].\nHuman players have demonstrated a stunning range of creative activities and sophisticated missions\nthat take hours to complete (examples in Fig. 3). We collect 730K+ narrated Minecraft videos, which\nadd up to 33 years of duration and 2.2B words in English transcripts. In comparison, HowTo100M [77]\nis a large-scale human instructional video dataset that includes 15 years of experience in total – about\nhalf of our volume. The time-aligned transcripts enable the agent to ground free-form natural lan-\nguage in video pixels and learn the semantics of diverse activities without laborious human labeling.\nWe operationalize this insight in our pre-trained video-language model (Sec. 4.1).\nMinecraft Wiki.The Wiki pages cover almost every aspect of the game mechanics, and supply\na rich source of unstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step\ntutorials.  We use Selenium [103] to scrape 6,735 pages that interleave text, images, tables, and\ndiagrams.  The pages are highly unstructured and do not share any common schema, as the Wiki\nis meant for human consumption rather than AI training.  To preserve the layout information, we\nadditionally save the screenshots of entire pages and extract 2.2M bounding boxes of the visual\nelements (visualization in Fig. A.4 andA.5). We do not use Wiki data in our current experiments.\nSince the Wiki contains detailed recipes for all crafted objects, they could be provided as input\nor training data for hierarchical planning methods and policy sketches[8].   Another promising\nfuture direction is to apply document understanding models such as LayoutLM [138,137] and\nDocFormer [9] to learn actionable knowledge from these unstructured Wiki data.\nReddit.\nWe scrape 340K+ posts along with 6.6M comments under the “r/Minecraft” subreddit.\nThese posts ask questions on how to solve certain tasks, showcase cool architectures and achievements\nin image/video snippets, and discuss general tips and tricks for players of all expertise levels. We\ndo not use Reddit data for training in Sec. 5, but a potential idea is to finetune large language models\n[27, 91]on our Reddit corpus to generate instructions and execution plans that are better grounded\nin the Minecraft domain.  Concurrent works[3, 56, 143]have explored similar ideas and showed\nexcellent results on robot learning, which is encouraging for more future research in MINEDOJO.\n4    Agent Learning with Large-scale Pre-training\nOne of the grand challenges of embodied AI is to build a single agent that can complete a wide range\nof open-world tasks. TheMINEDOJOframework aims to facilitate new techniques towards this goal\nby providing an open-ended task suite (Sec.  2) and large-scale internet knowledge base (Sec.  3).\nHere we take an initial step towards this goal by developing a proof of concept that demonstrates\nhow a single language-prompted agent can be trained inMINEDOJOto complete several complex\nMinecraft tasks. To this end, we propose a novel agent learning algorithm that takes advantage of the\nmassive YouTube data offered byMINEDOJO. We note that this is only one of the numerous possible\n6",
    "Table 1:  Our novelMINECLIPreward model is able to achieve competitive performance with\nmanually written dense reward function for Programmatic tasks, and significantly outperforms the\nCLIP\nOpenAI\nmethod across all Creative tasks.  Entries represent percentage success rates averaged\nover 3 seeds, each tested for 200 episodes. Success conditions are precise in Programmatic tasks, but\nestimated by MineCLIP for Creative tasks.\nGroupTasksOurs(Attn)Ours(Avg)Manual RewardSparse-onlyCLIP\nOpenAI\nMilk Cow64.5±37.16.5±3.562.8±40.10.0±0.00.0±0.0\nHunt Cow83.5±7.10.0±0.048.3±35.90.3±0.40.0±0.0\nShear Sheep12.1±9.10.6±0.252.3±33.20.0±0.00.0±0.0\nHunt Sheep8.1±4.10.0±0.041.9±33.00.3±0.40.0±0.0\nCombat Spider80.5±13.060.1±42.587.5±4.647.8±33.80.0±0.0\nCombat Zombie47.3±10.672.3±6.449.8±26.98.8±12.40.0±0.0\nCombat Pigman1.6±2.30.0±0.013.6±9.80.0±0.00.0±0.0\nCombat Enderman0.0±0.00.0±0.00.3±0.20.0±0.00.0±0.0\nFind Nether Portal37.4±40.889.8±5.7N/AN/A26.3±32.6\nFind Ocean33.4±45.654.3±40.7N/AN/A9.9±14.1\nDig Hole91.6±5.988.1±13.3N/AN/A0.0±0.0\nLay Carpet97.6±1.998.8±1.0N/AN/A0.0±0.0\nways to useMINEDOJO’s internet database — the Wiki and Reddit corpus also hold great potential\nto drive new algorithm discoveries for the community in future works.\nIn this paper, we consider a multi-task reinforcement learning (RL) setting, where an agent is tasked\nwith completing a collection ofMINEDOJOtasks specified by language instructions (Sec. 2). Solving\nthese tasks often requires the agent to interact with the Minecraft world in a prolonged fashion.\nAgents developed in popular RL benchmarks [119,146] often rely on meticulously crafted dense and\ntask-specific reward functions to guide random explorations. However, these rewards are hard or even\ninfeasible to define for our diverse and open-ended tasks inMINEDOJO. To address this challenge, our\nkey insight is to learna dense, language-conditioned reward function from in-the-wild YouTube\nvideos and their transcripts. Therefore, we introduceMINECLIP, a contrastive video-language\nmodel that learns to correlate video snippets and natural language descriptions (Fig. 4).MINECLIP\nis multi-task by design, as it is trained on open-vocabulary and diverse English transcripts.\nDuring RL training,MINECLIPprovides a high-quality reward signalwithoutany domain adaptation\ntechniques, despite the domain gap between noisy YouTube videos and clean simulator-rendered\nframes.MINECLIPeliminates the need to manually engineer reward functions for each and every\nMINEDOJOtask. For Creative tasks that lack a simple success criterion (Sec. 2.2),MINECLIPalso\nserves the dual purpose of anautomatic evaluation metricthat agrees well with human judgement\non a subset of tasks we investigate (Sec. 4.2, Table 2).  Because the learned reward model incurs\na non-trivial computational overhead, we introduce several techniques to significantly improve RL\ntraining efficiency, makingMINECLIPa practical module for open-ended agent learning in Minecraft\n(Sec. 4.2).\n4.1    Pre-Training MINECLIP on Large-scale Videos\nFormally, the learned reward function can be defined asΦ\nR\n: (G,V)→Rthat maps a language goal\nGand a video snippetVto a scalar reward. An idealΦ\nR\nshould return a high reward if the behavior\ndepicted in the video faithfully follows the language description, and a low reward otherwise. This\ncan be achieved by optimizing the InfoNCE objective [125,52,20], which learns to correlate positive\nvideo and text pairs [118, 6, 78, 4, 136].\nSimilar to the image-text CLIP model [92], MINECLIPis composed of a separate text encoderφ\nG\nthat embeds a language goal and a video encoderφ\nV\nthat embeds a moving window of 16 consecutive\nframes with160×256resolution (Fig. 4). Our neural architecture has a similar design as CLIP4Clip\n[75], whereφ\nG\nreuses OpenAI CLIP’s pretrained text encoder, andφ\nV\nis factorized into a frame-wise\nimage encoderφ\nI\nand a temporal aggregatorφ\na\nthat summarizes the sequence of 16 image features\ninto a single video embedding.  Unlike CLIP4Clip, we insert two extra layers of residual CLIP\nAdapter [38] after the aggregatorφ\na\nto produce a better video feature, and finetuneonlythe last two\nlayers of the pretrainedφ\nI\nandφ\nG\n.\n7",
    "Table 2:MINECLIPagrees well with the ground-truth human judgment on the Creative tasks we\nconsider. Numbers are F1 scores betweenMINECLIP’s binary classification of tasks success and\nhuman labels (scaled to the percentage for better readability).\nTasksFind Nether PortalFind OceanDig HoleLay Carpet\nOurs (Attn)98.7100.099.497.4\nOurs (Avg)100.0100.0100.098.4\nCLIP\nOpenAI\n48.798.480.654.1\nFrom theMINEDOJOYouTube database, we follow the procedure in VideoCLIP [136] to sample\n640K pairs of 16-second video snippets and time-aligned English transcripts, after applying a keyword\nfilter. We train twoMINECLIPvariants with different types of aggregatorφ\na\n: (1)MINECLIP[avg]\ndoes simple average pooling, which is fast but agnostic to the temporal ordering; (2)MINECLIP[attn]\nencodes the sequence by two transformer layers, which is relatively slower but captures more temporal\ninformation, and thus produces a better reward signal in general.  Details of data preprocessing,\narchitecture, and hyperparameters are listed in the Appendix (Sec. E).\n4.2    RL with MINECLIP Reward\nWe train a language-conditioned policy network that takes as input raw pixels and predicts discrete\ncontrol.  The policy is trained with PPO [102] on theMINECLIPrewards.  In each episode, the\nagent is prompted with a language goal and takes a sequence of actions to fulfill this goal.  When\ncalculating theMINECLIPrewards, we concatenate the agent’s latest 16 egocentric RGB frames in a\ntemporal window to form a video snippet.MINECLIPhandles all task promptszero-shotwithout any\nfurther finetuning. In our experiments (Sec. 5), we show thatMINECLIPprovides effective dense\nrewards out of the box, despite the domain shift between in-the-wild YouTube frames and simulator\nframes. Besides regular video data augmentation, we do not employ any special domain adaptation\nmethods during pre-training. Our finding is consistent with CLIP’s strong zero-shot performances on\nrobustness benchmarks in object recognition [92].\nCompared to hard-coded reward functions in popular benchmarks [146,119,34], theMINECLIP\nmodel has 150M parameters and is thus much more expensive to query.  We make several design\nchoices to greatly accelerate RL training with MINECLIP in the loop:\n1.The language goalGis fixed for a specific task, so thetext featuresφ\nG\ncan be precomputed\nto avoid invoking the text encoder repeatedly.\n2.\nOur agent’sRGB encoder reuses the pre-trained weights ofφ\nI\nfromMINECLIP. We do\nnot finetuneφ\nI\nduring RL training, which saves computation and endows the agent with good\nvisual representations from the beginning.\n3.MINECLIP’s video encoderφ\nV\nis factorized into an image encoderφ\nI\nand a light-weight\naggregatorφ\na\n. This design choice enablesefficient image feature caching. Consider two\noverlapping video sequences of 8 frames,V[0:8]andV[1:9].  We can cache the image\nfeatures of the 7 overlapping framesV[1]toV[7]to maximize compute savings. Ifφ\nV\nis\na monolithic model like S3D [135] in VideoCLIP [136], then the video features from every\nsliding window must be recomputed, which would incur a much higher cost per time step.\n4.\nWe leverageSelf-Imitation Learning[84] to store the trajectories with highMINECLIP\nreward values in a buffer, and alternate between PPO and self-imitation gradient steps.  It\nfurther improves sample efficiency as shown in the Appendix (Fig. A.8).\n5    Experiments\nWe evaluate our agent-learning approach (Section 4) on 8 Programmatic tasks and 4 Creative tasks\nfrom theMINEDOJObenchmarking suite.  We select these 12 tasks due to the diversity of skills\nrequired to solve them (e.g., harvesting, combat, building, navigation) and domain-specific entities\n(e.g.,  animals,  resources,  monsters,  terrains,  and  structures).   We  split  the  tasks  into  3  groups\nand train one multi-task agent for each group:Animal-Zoo(4 Programmatic tasks on hunting or\n8",
    "Table 3:MINECLIPagents have stronger zero-shot visual generalization ability to unseen terrains,\nweathers, and lighting. Numbers outside parentheses are percentage success rates averaged over 3\nseeds (each tested for 200 episodes), while those inside parentheses are relative performance changes.\nTasksOurs(Attn), trainOurs(Attn), unseen testCLIP\nOpenAI\n, trainCLIP\nOpenAI\n, unseen test\nMilk Cow64.5±37.164.8±31.3(+0.8%)90.0±0.429.2±3.7  (−67.6%)\nHunt Cow83.5±7.155.9±7.2(−32.9%)72.7±3.516.7±1.6  (−77.0%)\nCombat Spider80.5±13.062.1±29.7(−22.9%)79.5±2.554.2±9.6  (−31.8%)\nCombat Zombie47.3±10.639.9±25.3(−15.4%)50.2±7.530.8±14.4(−38.6%)\nharvesting resource from animals),Mob-Combat(Programmatic, fight 4 types of hostile monsters),\nandCreative(4 tasks).\nIn the experiments, we empirically check the quality ofMINECLIPagainst manually written reward\nfunctions, and quantify how different variants of our learned model affect the RL performance. Table 1\npresents our main results, and Fig. 2 visualizes our learned agent behavior in 4 of the considered tasks.\nPolicy networks of all methods share the same architecture and are trained by PPO + Self-Imitation\n(Sec. 4.2, training details in the Appendix, Sec. F). We compare the following methods:\n•Ours  (Attn)\n:  our agent trained with theMINECLIP[attn]reward model.   For Program-\nmatic tasks, we also add the final success condition as a binary reward. For Creative tasks,\nMINECLIP is the only source of reward.\n•Ours (Avg): the average-pooling variant of our method.\n•Manual Reward: hand-engineered dense reward using ground-truth simulator states.\n•Sparse-only: the final binary success as a single sparse reward. Note that neither sparse-only\nnor manual reward is available for Creative tasks.\n•CLIP\nOpenAI\n: pre-trained OpenAI CLIP model that hasnotbeen finetuned on anyMINEDOJO\nvideos.\nMINECLIP  is competitive with manual reward.For Programmatic tasks (first 8 rows),  RL\nagents guided byMINECLIPachieve competitive performance as those trained by manual reward.\nIn three of the tasks, they evenoutperformthe hand-engineered reward functions, which rely on privi-\nleged simulator states unavailable toMINECLIP. For a more statistically sound analysis, we conduct\nthe PairedStudent’st-test to compare the mean success rate of each task (pairing column 3 “Ours\n(Attn)” and column 5 “Manual Reward” in Table1). The test yields p-value0.3991\u001d0.05, which\nindicates that the difference between our method and manual reward is not considered statistically\nsignificant, and therefore they are comparable with each other. Because all tasks require nontrivial ex-\nploration, our approach also dominates the Sparse-only baseline. Note that the original OpenAI CLIP\nmodel fails to achieve any success. We hypothesize that the creatures in Minecraft look dramatically\ndifferent from their real-world counterparts, which causes CLIP to producemisleadingsignals worse\nthan no shaping reward at all. It implies the importance of finetuning onMINEDOJO’s YouTube data.\nMINECLIP  provides  automated  evaluation.For  Creative  tasks  (last  4  rows),  there  are  no\nprogrammatic success criteria available.  We convertMINECLIPinto a binary success classifier\nby thresholding the reward value it outputs for an episode.  To test the quality ofMINECLIPas\nan automatic evaluation metric, we ask human judges to curate a dataset of 100 successful and\n100 failed trajectories for each task. We then run bothMINECLIPvariants and CLIP\nOpenAI\non the\ndataset and report the binary F1 score of their judgement against human ground-truth in Table 2.\nThe results demonstrate that bothMINECLIP[attn]andMINECLIP[avg]attain a very high degree of\nagreement with human evaluation results on this subset of the Creative task suite that we investigate.\nCLIP\nOpenAI\nbaseline also achieves nontrivial agreement on Find Ocean and Dig Hole tasks, likely\nbecause real-world oceans and holes have similar texture. We use theattnvariant as an automated\nsuccess criterion to score the 4 Creative task results in Table 1. Our proposed method consistently\nlearns better than CLIP\nOpenAI\n-guided agents. It shows thatMINECLIPis an effective approach to\nsolving open-ended tasks when no straightforward reward signal is available.  We provide further\nanalysis beyond these 4 tasks in the Appendix (Sec. G.4).\n9",
    "Table 4: We train a single multi-task agent for all 12 tasks. All numbers represent percentage success\nrates averaged over 3 seeds, each tested for 200 episodes.\nGroupTasksSingle Agent on All TasksOriginalPerformance Change\nMilk Cow91.5±0.764.5±37.1↑\nHunt Cow46.8±3.783.5±7.1↓\nShear Sheep73.5±0.812.1±9.1↑\nHunt Sheep27.0±1.08.1±4.1↑\nCombat Spider72.1±1.380.5±13.0↓\nCombat Zombie27.1±2.747.3±10.6↓\nCombat Pigman6.5±1.21.6±2.3↑\nCombat Enderman0.0±0.00.0±0.0=\nFind Nether Portal99.1±0.437.4±40.8↑\nFind Ocean95.1±1.533.4±45.6↑\nDig Hole85.8±1.291.6±5.9↓\nLay Carpet96.5±0.897.6±1.9=\nTable 5: We test the open-vocabulary generalization ability to two unseen tasks. All numbers represent\npercentage success rates averaged over 3 seeds, each tested for 200 episodes.\nTasksOurs (zero-shot)Ours (after RL finetune)Baseline (RL from scratch)\nHunt Pig1.3±0.646.0±15.30.0±0.0\nHarvest Spider String1.6±1.336.5±16.912.5±12.7\nMINECLIP shows good zero-shot generalization to significant visual distribution shift.We\nevaluate  the  learned  policy  without  finetuning  on  a  combination  of  unseen  weather,  lighting\nconditions, and terrains — 27 scenarios in total. For the baseline, we train agents with the original\nCLIP\nOpenAI\nimage encoder (not trained on our YouTube videos) by imitation learning. The robustness\nagainst visual shift can be quantitatively measured by the relative performance degradation on\nnovel test scenarios for each task. Table3shows that while all methods incur performance drops,\nagents  withMINECLIPencoder  is  more  robust  to  visual  changes  than  the  baseline  across  all\ntasks. Pre-training on diverse in-the-wild YouTube videos is important to boosting zero-shot visual\ngeneralization capability, a finding consistent with literature [92, 82].\nLearning a Single Agent for All 12 TasksWe have also trained a single agent for all 12 tasks.\nTo reduce the computational burden without loss of generality, the agent is trained by self-imitating\nfrom successful trajectories generated from the self-imitation learning pipeline (Section F.3).  We\nsummarize the result in Table 4. Similar to our main experiments, all numbers represent percentage\nsuccess rates averaged over 3 training seeds, each tested for 200 episodes. Compared to the original\nagents, the new 12-multitask agent sees a performance boost in 6 tasks, degradation in 4 tasks,\nand roughly the same success rates in 2 tasks. This result suggests that there are both positive and\nnegative task transfers happening. To improve the multi-task performance, more advanced algorithms\n[141, 133] can be employed to mitigate the negative transfer effects.\nWe also perform Paired Student’st-test to statistically compare the performance of the 12-multitask\nagent and those separately trained on each task group. We obtain a p-value of0.3720\u001d0.05, which\nsuggests that the difference between the two training settings is not statistically significant.\nGeneralize to Novel Tasks\nTo test the ability to generalize to new open-vocabulary commands,\nwe evaluate the agent on two novel tasks:  “harvest spider string” and “hunt pig”.  Table 5 shows\nthat the agent struggles in the zero-shot setting because it has not interacted with pigs or spider\nstrings during training, and thus does not know how to interact with them effectively. However, the\nperformance improves substantially by finetuning with theMINECLIPreward. Here the baseline\nmethods are trained from scratch using RL with theMINECLIPencoders and reward. Therefore,\nthe only difference is whether the policy has been pre-trained on the 12 tasks or not.  Given the\n10",
    "same environment sampling budget (only around 5% of total samples), it significantly outperforms\nbaselines. It suggests that the multitask agent has learned transferable knowledge on hunting and\nresource collection, which enables it to quickly adapt to novel tasks.\n6    Related work\nOpen-ended   Environments   for   Decision-making   Agents.There   are   many   environments\ndeveloped with the goal of open-ended agent learning.   Prior works include maze-style worlds\n[121,129,61], purely text-based game [69], grid worlds  [21,16], browser/GUI-based environments\n[108,124], and indoor simulators for robotics   [1,107,114,34,110,99,89].  Minecraft offers\nan exciting alternative for open-ended agent learning.  It is a 3D visual world with procedurally\ngenerated landscapes and extremely flexible game mechanics that support an enormous variety\nof activities. Prior methods in open-ended agent learning[30, 57, 130, 63, 26]do not make use of\nexternal knowledge, but our approach leverages internet-scale database to learn open-vocabulary\nreward models, thanks to Minecraft’s abundance of gameplay data online.\nMinecraft for AI Research.The Malmo platform [60] is the first comprehensive release of a\nGym-style agent API [14] for Minecraft. Based on Malmo, MineRL [48] provides a codebase and\nhuman play trajectories for the annual Diamond Challenge at NeurIPS [47,49,62].MINEDOJO’s\nsimulator builds upon the pioneering work of MineRL, but greatly expands the API and benchmarking\ntask suite. Other Minecraft benchmarks exist with different focuses. For example, CraftAssist [44]\nand IGLU [66] study agents with interactive dialogues. BASALT [104] applies human evaluation to 4\nopen-ended tasks. EvoCraft [45] is designed for structure building, and Crafter [50] optimizes for fast\nexperimentation. Unlike prior works,MINEDOJO’s core mission is to facilitate the development of\ngenerally capable embodied agents using internet-scale knowledge. We include a feature comparison\ntable of different Minecraft platforms for AI research in Table A.1.\nInternet-scale Multimodal Knowledge Bases.\nBig dataset such as Common Crawl [24], the Pile\n[37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pre-\ntrained language models [27,91,15] and multimodal models [118,6,78,145,7,4,136]. While gener-\nally useful for learning representations, these datasets are not specifically targeted at embodied agents.\nTo provide agent-centric training data, RoboNet [25] collects video frames from 7 robot platforms,\nand Ego4D [43] recruits volunteers to record egocentric videos of household activities. In comparison,\nMINEDOJO’s knowledge base is constructed without human curation efforts, much larger in volume,\nmore diverse in data modalities, and comprehensively covers all aspects of the Minecraft environment.\nEmbodied Agents with Large-scale Pre-training.Inspired by the success in NLP, embodied\nagent research[29, 11, 94, 23]has seen a surge in adoption of the large-scale pre-training paradigm.\nThe  recent  advances  can  be  roughly  divided  into  4  categories.   1)Novel  agent  architecture:\nDecision Transformer [19,58,144] applies the powerful self-attention models to sequential decision\nmaking.  GATO[95]and Unified-IO[74]learn a single model to solve various decision-making\ntasks with different control interfaces.   VIMA [59] unifies a wide range of robot manipulation\ntasks with multimodal prompting. 2)Pre-training for better representations: R3M [82] trains a\ngeneral-purpose visual encoder for robot perception on Ego4D videos [43]. CLIPort [111] leverages\nthe pre-trained CLIP model [92] to enable free-form language instructions for robot manipulation.\n3)Pre-training  for  better  policies:  AlphaStar  [126]  achieves  champion-level  performance  on\nStarCraft by imitating from numerous human demos. SayCan [3] leverages large language models\n(LMs) to ground value functions in the physical world.  [72] and [96] directly reuse pre-trained\nLMs as policy backbone. VPT[10]is a concurrent work that learns an inverse dynamics model from\nhuman contractors to pseudo-label YouTube videos for behavior cloning. VPT is complementary to\nour approach, and can be finetuned to solve language-conditioned open-ended tasks with our learned\nreward model.    4)Data-driven reward functions:  Concept2Robot [105] and DVD [18] learn a\nbinary classifier to score behaviors from in-the-wild videos [42]. LOReL [81] crowd-sources humans\nlabels to train language-conditioned reward function for offline RL. AVID [113] and XIRL [142]\nextract reward signals via cycle consistency.MINEDOJO’s task benchmark and internet knowledge\nbase are generally useful for developing new algorithms in all the above categories. In Sec. 4, we\nalso propose an open-vocabulary, multi-task reward model using MINEDOJOYouTube videos.\n11",
    "7    Conclusion\nIn this work, we introduce theMINEDOJOframework for developing generally capable embodied\nagents.MINEDOJOfeatures a benchmarking suite of thousands of Programmatic and Creative tasks,\nand an internet-scale multimodal knowledge base of videos, wiki, and forum discussions.  As an\nexample of the novel research possibilities enabled byMINEDOJO, we proposeMINECLIPas an\neffective language-conditioned reward function trained with in-the-wild YouTube videos.MINECLIP\nachieves strong performance empirically and agrees well with human evaluation results, making it a\ngood automatic metric for Creative tasks. We look forward to seeing howMINEDOJOempowers the\ncommunity to make progress on the important challenge of open-ended agent learning.\n8    Acknowledgement\nWe are extremely grateful to Anssi Kanervisto, Shikun Liu, Zhiding Yu, Chaowei Xiao, Weili Nie,\nJean Kossaifi, Jonathan Raiman, Neel Kant, Saad Godil, Jaakko Haapasalo, Bryan Catanzaro, John\nSpitzer, Zhiyuan “Jerry” Lin, Yingqi Zheng, Chen Tessler, Dieter Fox, Oli Wright, Jeff Clune, Jack\nParker-Holder,  and many other colleagues and friends for their helpful feedback and insightful\ndiscussions.  We also thank the anonymous reviewers for offering us highly constructive advice\nand kind encouragement during the review and rebuttal period.  NVIDIA provides the necessary\ncomputing resource and infrastructure for this project. Guanzhi Wang is supported by the Kortschak\nfellowship in Computing and Mathematical Sciences at Caltech.\nReferences\n[1]\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin,\nRachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia\nGuy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap,\nKory Mathewson, So\nˇ\nna Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant\nVarma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating\ninteractive intelligence.arXiv preprint arXiv: Arxiv-2012.05672, 2020.\n[2]\nSami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakr-\nishnan Varadarajan, and Sudheendra Vijayanarasimhan.  Youtube-8m:  A large-scale video\nclassification benchmark.arXiv preprint arXiv: Arxiv-1609.08675, 2016.\n[3]Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey,\nSally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei\nLee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao,\nKanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton\nTan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and\nMengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances.arXiv\npreprint arXiv: Arxiv-2204.01691, 2022.\n[4]Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and\nBoqing Gong.  Vatt: Transformers for multimodal self-supervised learning from raw video,\naudio and text.arXiv preprint arXiv: Arxiv-2104.11178, 2021.\n[5]Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun hsuan Sung, Brian Strope, and Ray Kurzweil.\nConversational contextual cues: The case of personalization and history for response ranking.\narXiv preprint arXiv: Arxiv-1606.00372, 2016.\n[6]\nJean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Rama-\npuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman.   Self-\nsupervised multimodal versatile networks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors,Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URLhttps://proceedings.neurips.\ncc/paper/2020/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html.\n12",
    "[7]Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex M. Bronstein. Noise estimation using\ndensity estimation for self-supervised multimodal learning. InThirty-Fifth AAAI Conference\non Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications\nof Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 6644–6652. AAAI\nPress, 2021. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/16822.\n[8]\nJacob Andreas, Dan Klein, and Sergey Levine.  Modular multitask reinforcement learning\nwith policy sketches.  In Doina Precup and Yee Whye Teh, editors,Proceedings of the 34th\nInternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017,  volume 70 ofProceedings of Machine Learning Research,  pages 166–175.\nPMLR, 2017. URLhttp://proceedings.mlr.press/v70/andreas17a.html.\n[9]\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha.\nDocformer: End-to-end transformer for document understanding.arXiv preprint arXiv: Arxiv-\n2106.11539, 2021.\n[10]Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton,  Raul Sampedro,  and Jeff Clune.   Video pretraining (vpt):  Learning to act by\nwatching unlabeled online videos.arXiv preprint arXiv: Arxiv-2206.11795, 2022.\n[11]Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun,\nSergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, and Hao Su.\nRearrangement: A challenge for embodied ai.arXiv preprint arXiv: Arxiv-2011.01975, 2020.\n[12]Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mot-\ntaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. Objectnav revisited: On evaluation\nof embodied agents navigating to objects.arXiv preprint arXiv: Arxiv-2006.13171, 2020.\n[13]Rishi Bommasani,  Drew A. Hudson,  Ehsan Adeli,  Russ Altman,  Simran Arora,  Sydney\nvon Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik\nBrynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen,\nKathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,\nTrevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha,\nTatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,\nGeoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,\nRohith Kuditipudi,  Ananya Kumar,  Faisal Ladhak,  Mina Lee,  Tony Lee,  Jure Leskovec,\nIsabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning,\nSuvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak\nNarayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko,\nGiray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance,\nChristopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam,\nAndy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian\nTramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie,\nMichihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang,\nYuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of\nfoundation models.arXiv preprint arXiv: Arxiv-2108.07258, 2021.\n[14]\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym.arXiv preprint arXiv: Arxiv-1606.01540, 2016.\n[15]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.  Language\nmodels are few-shot learners.Advances in neural information processing systems, 33:1877–\n1901, 2020.\n[16]\nTianshi Cao, Jingkang Wang, Yining Zhang, and Sivabalan Manivasagam. Babyai++: Towards\ngrounded-language learning beyond memorization.arXiv preprint arXiv: Arxiv-2004.07200,\n2020.\n13",
    "[17]João Carreira and Andrew Zisserman.  Quo vadis, action recognition?  A new model and\nthe kinetics dataset. In2017 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 4724–4733. IEEE Computer Society,\n2017. doi: 10.1109/CVPR.2017.502. URLhttps://doi.org/10.1109/CVPR.2017.502.\n[18]\nAnnie S. Chen, Suraj Nair, and Chelsea Finn. Learning generalizable robotic reward functions\nfrom in-the-wild human videos.   In Dylan A. Shell,  Marc Toussaint,  and M. Ani Hsieh,\neditors,Robotics:  Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021.  doi:\n10.15607/RSS.2021.XVII.012. URLhttps://doi.org/10.15607/RSS.2021.XVII.012.\n[19]Lili  Chen,  Kevin  Lu,  Aravind  Rajeswaran,  Kimin  Lee,  Aditya  Grover,  Michael  Laskin,\nPieter  Abbeel,   Aravind  Srinivas,   and  Igor  Mordatch.Decision  transformer:    Rein-\nforcement  learning  via  sequence  modeling.    In  Marc’Aurelio  Ranzato,  Alina  Beygelz-\nimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors,Advances\nin  Neural  Information  Processing  Systems  34:   Annual  Conference  on  Neural  Informa-\ntion  Processing  Systems  2021,  NeurIPS  2021,  December  6-14,  2021,  virtual,  pages\n15084–15097,  2021.URLhttps://proceedings.neurips.cc/paper/2021/hash/\n7f489f642a0ddb10272b5c31057f0663-Abstract.html.\n[20]\nTing  Chen,  Simon  Kornblith,  Mohammad  Norouzi,  and  Geoffrey  E.  Hinton.   A  simple\nframework for contrastive learning of visual representations.   InProceedings of the 37th\nInternational Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\nvolume 119 ofProceedings of Machine Learning Research, pages 1597–1607. PMLR, 2020.\nURLhttp://proceedings.mlr.press/v119/chen20j.html.\n[21]Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan\nSaharia, Thien Huu Nguyen, and Yoshua Bengio.  Babyai: A platform to study the sample\nefficiency of grounded language learning.   In7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\nURLhttps://openreview.net/forum?id=rJeXCo0cYX.\n[22]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju\nDuke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie\nPellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.  Palm:\nScaling language modeling with pathways.arXiv preprint arXiv: Arxiv-2204.02311, 2022.\n[23]Jack Collins, Shelvin Chand, Anthony Vanderkop, and David Howard. A review of physics\nsimulators for robotic applications.IEEE Access, 9:51416–51431, 2021.\n[24]\nCommon Crawl. Common crawl.https://commoncrawl.org/, 2012. Accessed: 2022-06-\n06.\n[25]\nSudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeck-\npeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot\nlearning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors,3rd Annual\nConference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019,\nProceedings, volume 100 ofProceedings of Machine Learning Research, pages 885–897.\nPMLR, 2019. URLhttp://proceedings.mlr.press/v100/dasari20a.html.\n[26]Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre M. Bayen, Stuart Russell, An-\ndrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised\nenvironment design. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin, editors,Advances in Neural Information Processing Systems 33:\n14",
    "Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, 2020. URLhttps://proceedings.neurips.cc/paper/2020/hash/\n985e9a46e10005356bbaf194249f6856-Abstract.html.\n[27]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.  Bert:  Pre-training\nof deep bidirectional transformers for language understanding.arXiv preprint arXiv: Arxiv-\n1810.04805, 2018.\n[28]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale.arXiv preprint arXiv: Arxiv-2010.11929, 2020.\n[29]Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied\nAI: from simulators to research tasks.IEEE Trans. Emerg. Top. Comput. Intell., 6(2):230–244,\n2022. doi: 10.1109/TETCI.2022.3141105. URLhttps://doi.org/10.1109/TETCI.2022.\n3141105.\n[30]Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore:\na new approach for hard-exploration problems.arXiv preprint arXiv: Arxiv-1901.10995, 2019.\n[31]Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, and Charles L. Isbell. Imitating\nlatent policies from observation.arXiv preprint arXiv: Arxiv-1805.07914, 2018.\n[32]William  Falcon  and  The  PyTorch  Lightning  team.PyTorch  Lightning.Github,  3\n2019.  doi:  10.5281/zenodo.3828935.  URLhttps://github.com/PyTorchLightning/\npytorch-lightning.\n[33]Linxi Fan*, Shyamal Buch*, Guanzhi Wang, Ryan Cao, Yuke Zhu, Juan Carlos Niebles, and\nLi Fei-Fei. Rubiksnet: Learnable 3d-shift for efficient video action recognition. InProceedings\nof the European Conference on Computer Vision (ECCV), 2020.\n[34]Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke Zhu, and Anima\nAnandkumar. Secant: Self-expert cloning for zero-shot generalization of visual policies.arXiv\npreprint arXiv: Arxiv-2106.09678, 2021.\n[35]\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for\ndeep data-driven reinforcement learning.arXiv preprint arXiv: Arxiv-2004.07219, 2020.\n[36]Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, and Peter Dürr.  Super-\nhuman performance in gran turismo sport using deep reinforcement learning.IEEE Robotics\nAutom. Lett., 6(3):4257–4264, 2021. doi: 10.1109/LRA.2021.3064284. URLhttps://doi.\norg/10.1109/LRA.2021.3064284.\n[37]\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse\ntext for language modeling.arXiv preprint arXiv:2101.00027, 2020.\n[38]\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng\nLi, and Yu Qiao.  Clip-adapter: Better vision-language models with feature adapters.arXiv\npreprint arXiv: Arxiv-2110.04544, 2021.\n[39]Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna M.\nWallach, Hal Daumé III, and Kate Crawford. Datasheets for datasets.Commun. ACM, 64(12):\n86–92, 2021. doi: 10.1145/3458723. URLhttps://doi.org/10.1145/3458723.\n[40]Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxi-\ncityprompts: Evaluating neural toxic degeneration in language models.arXiv preprint arXiv:\nArxiv-2009.11462, 2020.\n[41]Jordan    Gerblick.Minecraft,the    most-watched    game    on    youtube,passes\n1trillionviews,Dec2021.URLhttps://www.gamesradar.com/\nminecraft-the-most-watched-game-on-youtube-passes-1-trillion-views/.\n15",
    "[42]Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne\nWestphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag,\net al. The\" something something\" video database for learning and evaluating visual common\nsense. InProceedings of the IEEE international conference on computer vision, pages 5842–\n5850, 2017.\n[43]Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit\nGirdhar,  Jackson Hamburger,  Hao Jiang,  Miao Liu,  Xingyu Liu,  Miguel Martin,  Tushar\nNagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma,\nMichael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv\nBatra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph\nFeichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez,\nJames Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik\nKottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya\nMangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will\nPrice, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey\nSoutherland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi,\nZiwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria\nFarinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul\nJoo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg,\nYoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei\nYan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video.arXiv\npreprint arXiv: Arxiv-2110.07058, 2021.\n[44]Jonathan Gray, Kavya Srinet, Yacine Jernite, Haonan Yu, Zhuoyuan Chen, Demi Guo, Sid-\ndharth Goyal, C. Lawrence Zitnick, and Arthur Szlam. Craftassist: A framework for dialogue-\nenabled interactive agents.arXiv preprint arXiv: Arxiv-1907.08584, 2019.\n[45]\nDjordje Grbic, Rasmus Berg Palm, Elias Najarro, Claire Glanois, and Sebastian Risi.EvoCraft:\nA New Challenge for Open-Endedness, pages 325–340.  Springer International Publishing,\n2021. doi: 10.1007/978-3-030-72699-7_21. URLhttp://link.springer.com/content/\npdf/10.1007/978-3-030-72699-7_21.\n[46]\nAgrim Gupta, Linxi Fan, Surya Ganguli, and Li Fei-Fei.  Metamorph:  Learning universal\ncontrollers with transformers. InInternational Conference on Learning Representations, 2022.\nURLhttps://openreview.net/forum?id=Opmqtk_GvYL.\n[47]William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie\nMilani,  Sharada Mohanty,  Diego Perez Liebana,  Ruslan Salakhutdinov,  Nicholay Topin,\nManuela Veloso, and Phillip Wang.  The minerl 2019 competition on sample efficient rein-\nforcement learning using human priors.arXiv preprint arXiv: Arxiv-1904.10079, 2019.\n[48]William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv: Arxiv-1907.13440, 2019.\n[49]William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno,\nCrissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov,\nJohn Schulman, Shinya Shiroshita, Nicholay Topin, Avinash Ummadisingu, and Oriol Vinyals.\nThe minerl 2020 competition on sample efficient reinforcement learning using human priors.\narXiv preprint arXiv: Arxiv-2101.11071, 2021.\n[50]Danijar Hafner.   Benchmarking the spectrum of agent capabilities.arXiv preprint arXiv:\nArxiv-2109.06780, 2021.\n[51]Laura  Hanu  and  Unitary  team.   Detoxify.   Github.https://github.com/unitaryai/\ndetoxify, 2020.\n[52]  Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast\nfor unsupervised visual representation learning. In2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9726–\n9735. Computer Vision Foundation / IEEE, 2020.  doi:  10.1109/CVPR42600.2020.00975.\nURLhttps://doi.org/10.1109/CVPR42600.2020.00975.\n16",
    "[53]Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\nautoencoders are scalable vision learners.arXiv preprint arXiv: Arxiv-2111.06377, 2021.\n[54]\nMatthew Henderson, Paweł Budzianowski, Iñigo Casanueva, Sam Coope, Daniela Gerz, Girish\nKumar, Nikola Mrkši\n ́\nc, Georgios Spithourakis, Pei-Hao Su, Ivan Vuli\n ́\nc, and Tsung-Hsien Wen.\nA repository of conversational datasets.arXiv preprint arXiv: Arxiv-1904.06472, 2019.\n[55]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\nwanathan, and Roman Garnett, editors,Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pages 6626–6637, 2017.  URLhttps://proceedings.neurips.\ncc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html.\n[56]\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas\nJackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter.  Inner monologue:\nEmbodied reasoning through planning with language models.arXiv preprint arXiv: Arxiv-\n2207.05608, 2022.\n[57]Joost Huizinga and Jeff Clune. Evolving multimodal robot behavior via many stepping stones\nwith the combinatorial multiobjective evolutionary algorithm.Evolutionary computation, 30\n(2):131–164, 2022.\n[58]Michael  Janner,   Qiyang  Li,   and  Sergey  Levine.Offline  reinforcement  learning  as\none   big   sequence   modeling   problem.In   Marc’Aurelio   Ranzato,   Alina   Beygelz-\nimer,  Yann  N.  Dauphin,  Percy  Liang,  and  Jennifer  Wortman  Vaughan,  editors,Ad-\nvances  in  Neural  Information  Processing  Systems  34:   Annual  Conference  on  Neural\nInformation  Processing  Systems  2021,  NeurIPS  2021,  December  6-14,  2021,  virtual,\npages 1273–1286, 2021. URLhttps://proceedings.neurips.cc/paper/2021/hash/\n099fe6b0b444c23836c4a5d07346082b-Abstract.html.\n[59]\nYunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen,\nLi Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation\nwith multimodal prompts.arXiv preprint arXiv: Arxiv-2210.03094, 2022.\n[60]Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for\nartificial intelligence experimentation.IJCAI, 2016. URLhttps://dl.acm.org/doi/10.\n5555/3061053.3061259.\n[61]Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter\nHenry, Adam Crespi, Julian Togelius, and Danny Lange.  Obstacle tower: A generalization\nchallenge in vision, control, and planning. In Sarit Kraus, editor,Proceedings of the Twenty-\nEighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China,\nAugust 10-16, 2019, pages 2684–2691. ijcai.org, 2019. doi: 10.24963/ijcai.2019/373. URL\nhttps://doi.org/10.24963/ijcai.2019/373.\n[62]Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin,\nJunyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue Huang,\nHaicheng Chen, Guangjun Zeng, Yue Lin, Vincent Micheli, Eloi Alonso, François Fleuret,\nAlexander Nikulin, Yury Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond\n2021 competition:  Overview,  results,  and lessons learned.arXiv preprint arXiv:  Arxiv-\n2202.10583, 2022.\n[63]\nIngmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton,\nRaul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, Oleg Klimov, and\nJeff Clune.  Multi-task curriculum learning in a complex, visual, hard-exploration domain:\nMinecraft.arXiv preprint arXiv: Arxiv-2106.14876, 2021.\n[64]\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\nnarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and\nAndrew Zisserman.  The kinetics human action video dataset.arXiv preprint arXiv: Arxiv-\n1705.06950, 2017.\n17",
    "[65]Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim.  Abstractive summarization of red-\ndit posts with multi-level memory networks.  In Jill Burstein, Christy Doran, and Thamar\nSolorio, editors,Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics:  Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages\n2519–2531. Association for Computational Linguistics, 2019.  doi: 10.18653/v1/n19-1260.\nURLhttps://doi.org/10.18653/v1/n19-1260.\n[66]Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha Mohanty, Maartje ter Hoeve,\nMikhail Burtsev, Alexey Skrynnik, Artem Zholus, Aleksandr Panov, Kavya Srinet, Arthur\nSzlam, Yuxuan Sun, Katja Hofmann, Michel Galley, and Ahmed Awadallah.  Neurips 2021\ncompetition iglu: Interactive grounded language understanding in a collaborative environment.\narXiv preprint arXiv: Arxiv-2110.06536, 2021.\n[67]Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners.arXiv preprint arXiv:  Arxiv-2205.11916,\n2022.\n[68]Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti,\nDaniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi.  Ai2-thor:  An interactive 3d\nenvironment for visual ai.arXiv preprint arXiv: Arxiv-1712.05474, 2017.\n[69]Heinrich Küttler,  Nantas Nardelli,  Alexander Miller,  Roberta Raileanu,  Marco Selvatici,\nEdward  Grefenstette,  and  Tim  Rocktäschel.The  nethack  learning  environment.In\nH.  Larochelle,  M.  Ranzato,  R.  Hadsell,  M.F.  Balcan,  and  H.  Lin,  editors,Advances\nin  Neural  Information  Processing  Systems,  volume  33,  pages  7671–7684.  Curran  As-\nsociates,  Inc.,  2020.URLhttps://proceedings.neurips.cc/paper/2020/file/\n569ff987c643b4bedf504efda8f786c2-Paper.pdf.\n[70]  Label Studio. Label studio.https://labelstud.io/, 2020. Accessed: 2022-06-06.\n[71]WB Langdon.  Pfeiffer–a distributed open-ended evolutionary system.  InAISB, volume 5,\npages 7–13. Citeseer, 2005.\n[72]\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\nHuang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\nand Yuke Zhu. Pre-trained language models for interactive decision-making.arXiv preprint\narXiv: Arxiv-2202.01771, 2022.\n[73]Ilya Loshchilov and Frank Hutter.  SGDR: stochastic gradient descent with warm restarts.\nIn5th International Conference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26,  2017,  Conference Track Proceedings. OpenReview.net,  2017.   URLhttps:\n//openreview.net/forum?id=Skq89Scxx.\n[74]\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUnified-io: A unified model for vision, language, and multi-modal tasks.arXiv preprint arXiv:\nArxiv-2206.08916, 2022.\n[75]Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip:\nAn empirical study of clip for end to end video clip retrieval.arXiv preprint arXiv: Arxiv-\n2104.08860, 2021.\n[76]Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David\nGarcía, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao\nWu. Mixed precision training. In6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net, 2018. URLhttps://openreview.net/forum?id=r1gs9JgRZ.\n[77]Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic.  Howto100m:  Learning a text-video embedding by watching hundred million\nnarrated video clips.arXiv preprint arXiv: Arxiv-1906.03327, 2019.\n18",
    "[78]Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zis-\nserman. End-to-end learning of visual representations from uncurated instructional videos. In\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle,\nWA, USA, June 13-19, 2020, pages 9876–9886. Computer Vision Foundation / IEEE, 2020. doi:\n10.1109/CVPR42600.2020.00990.   URLhttps://openaccess.thecvf.com/content_\nCVPR_2020/html/Miech_End-to-End_Learning_of_Visual_Representations_\nFrom_Uncurated_Instructional_Videos_CVPR_2020_paper.html.\n[79]Minecraft Wiki. Minecraft wiki.hhttps://minecraft.fandom.com/wiki/Minecraft_\nWiki, 2016. Accessed: 2022-06-06.\n[80]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning.arXiv preprint\narXiv: Arxiv-1312.5602, 2013.\n[81]Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, and Chelsea Finn. Learn-\ning language-conditioned robot behavior from offline data and crowd-sourced annotation. In\nAleksandra Faust, David Hsu, and Gerhard Neumann, editors,Conference on Robot Learning,\n8-11 November 2021, London, UK, volume 164 ofProceedings of Machine Learning Re-\nsearch, pages 1303–1315. PMLR, 2021. URLhttps://proceedings.mlr.press/v164/\nnair22a.html.\n[82]\nSuraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A\nuniversal visual representation for robot manipulation.arXiv preprint arXiv: Arxiv-2203.12601,\n2022.\n[83]Evgenii Nikishin, Max Schwarzer, Pierluca D’Oro, Pierre-Luc Bacon, and Aaron Courville.\nThe primacy bias in deep reinforcement learning.arXiv preprint arXiv: Arxiv-2205.07802,\n2022.\n[84]Junhyuk  Oh,  Yijie  Guo,  Satinder  Singh,  and  Honglak  Lee.   Self-imitation  learning.   In\nInternational Conference on Machine Learning, pages 3878–3887. PMLR, 2018.\n[85]OpenAI, :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław\nD ̨ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal\nJózefowicz,  Scott  Gray,  Catherine  Olsson,  Jakub  Pachocki,  Michael  Petrov,  Henrique  P.\nd. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon\nSidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep\nreinforcement learning.arXiv preprint arXiv: Arxiv-1912.06680, 2019.\n[86]Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKöpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.  Pytorch:  An imperative style,\nhigh-performance deep learning library.arXiv preprint arXiv: Arxiv-1912.01703, 2019.\n[87]\nDiego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu Kuno, Andre\nKramer, Sam Devlin, Raluca D. Gaina, and Daniel Ionita.  The multi-agent reinforcement\nlearning in malmÖ (marlÖ) competition.arXiv preprint arXiv: Arxiv-1901.08129, 2019.\n[88]PRAW: The Python Reddit API Wrapper.  Praw:  The python reddit api wrapper.https:\n//github.com/praw-dev/praw, 2010. Accessed: 2022-06-06.\n[89]Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Tor-\nralba. Virtualhome: Simulating household activities via programs. In2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, pages 8494–8502. Computer Vision Foundation / IEEE Computer Society, 2018. doi:\n10.1109/CVPR.2018.00886.URLhttp://openaccess.thecvf.com/content_cvpr_\n2018/html/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html.\n[90]\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training.OpenAI, 2018.\n19",
    "[91]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\n[92]\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. InInternational Conference on Machine Learning,\npages 8748–8763. PMLR, 2021.\n[93]Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents.arXiv preprint arXiv: Arxiv-2204.06125,\n2022.\n[94]Harish Ravichandar, Athanasios S Polydoros, Sonia Chernova, and Aude Billard.  Recent\nadvances in robot learning from demonstration.Annual review of control,  robotics,  and\nautonomous systems, 3:297–330, 2020.\n[95]\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas.  A generalist agent.arXiv preprint\narXiv: Arxiv-2205.06175, 2022.\n[96]Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help offline reinforce-\nment learning?arXiv preprint arXiv: Arxiv-2201.12122, 2022.\n[97]  Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,\nTim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.  Photorealistic text-\nto-image diffusion models with deep language understanding.arXiv preprint arXiv: Arxiv-\n2205.11487, 2022.\n[98]Tim  Salimans,  Ian  J.  Goodfellow,  Wojciech  Zaremba,  Vicki  Cheung,  Alec  Radford,\nand  Xi  Chen.Improved  techniques  for  training  gans.In  Daniel  D.  Lee,  Masashi\nSugiyama,   Ulrike  von  Luxburg,   Isabelle  Guyon,   and  Roman  Garnett,   editors,Ad-\nvances  in  Neural  Information  Processing  Systems  29:    Annual  Conference  on  Neu-\nral  Information  Processing  Systems  2016,   December  5-10,   2016,   Barcelona,   Spain,\npages 2226–2234, 2016. URLhttps://proceedings.neurips.cc/paper/2016/hash/\n8a3363abe792db2d8761d6403605aeb7-Abstract.html.\n[99]\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana\nJain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra.\nHabitat: A platform for embodied ai research. InProceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), October 2019.\n[100]Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs.arXiv preprint arXiv:2111.02114, 2021.\n[101]John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation. In Yoshua Bengio\nand Yann LeCun, editors,4th International Conference on Learning Representations, ICLR\n2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.  URL\nhttp://arxiv.org/abs/1506.02438.\n[102]John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms.arXiv preprint arXiv: Arxiv-1707.06347, 2017.\n[103]Selenium WebDriver. Selenium webdriver.https://www.selenium.dev/, 2011. Accessed:\n2022-06-06.\n[104]Rohin Shah, Cody Wild, Steven H. Wang, Neel Alex, Brandon Houghton, William Guss,\nSharada Mohanty, Anssi Kanervisto, Stephanie Milani, Nicholay Topin, Pieter Abbeel, Stuart\nRussell, and Anca Dragan. The minerl basalt competition on learning from human feedback.\narXiv preprint arXiv: Arxiv-2107.01969, 2021.\n20",
    "[105]Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot:\nLearning manipulation concepts from instructions and human demonstrations.The Interna-\ntional Journal of Robotics Research, 40(12-14):1419–1434, 2021.\n[106]\nNoam Shazeer. Glu variants improve transformer.arXiv preprint arXiv: Arxiv-2002.05202,\n2020.\n[107]Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia\nPérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi,\nKent Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: a simulation environ-\nment for interactive tasks in large realistic scenes.arXiv preprint arXiv: Arxiv-2012.02924,\n2020.\n[108]Tianlin Tim Shi, Andrej Karpathy, Linxi Jim Fan, Jonathan Hernandez, and Percy Liang.\nWorld of bits: an open-domain platform for web-based agents.ICML, 2017.  URLhttps:\n//dl.acm.org/doi/10.5555/3305890.3306005.\n[109]Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro.  Megatron-lm:  Training multi-billion parameter language models using model\nparallelism.arXiv preprint arXiv:1909.08053, 2019.\n[110]  Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-\ntaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded\ninstructions for everyday tasks.  In2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10737–10746.\nComputer Vision Foundation / IEEE, 2020.  doi:  10.1109/CVPR42600.2020.01075.  URL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Shridhar_ALFRED_\nA_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_\nCVPR_2020_paper.html.\n[111]\nMohit Shridhar, Lucas Manuelli, and Dieter Fox.  Cliport:  What and where pathways for\nrobotic manipulation.arXiv preprint arXiv: Arxiv-2109.12098, 2021.\n[112]David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKaren Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general\nreinforcement learning algorithm.arXiv preprint arXiv: Arxiv-1712.01815, 2017.\n[113]Laura  Smith,  Nikita  Dhawan,  Marvin  Zhang,  Pieter  Abbeel,  and  Sergey  Levine.   Avid:\nLearning multi-stage tasks via pixel-level translation of human videos.arXiv preprint arXiv:\nArxiv-1912.04443, 2019.\n[114]Sanjana  Srivastava,  Chengshu  Li,  Michael  Lingelbach,  Roberto  Martín-Martín,  Fei  Xia,\nKent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, C. Karen Liu, Silvio Savarese,\nHyowon Gweon, Jiajun Wu, and Li Fei-Fei. BEHAVIOR: benchmark for everyday household\nactivities in virtual, interactive, and ecological environments. In Aleksandra Faust, David Hsu,\nand Gerhard Neumann, editors,Conference on Robot Learning, 8-11 November 2021, London,\nUK, volume 164 ofProceedings of Machine Learning Research, pages 477–490. PMLR, 2021.\nURLhttps://proceedings.mlr.press/v164/srivastava22a.html.\n[115]Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning.arXiv\npreprint arXiv: Arxiv-1703.01703, 2017.\n[116]Russell K Standish. Open-ended artificial evolution.International Journal of Computational\nIntelligence and Applications, 3(02):167–175, 2003.\n[117]\nKenneth O Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge\nyou’ve never heard of.O’Reilly Online,, 2017.\n[118]Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid.  Learning video represen-\ntations using contrastive bidirectional transformer.arXiv preprint arXiv: Arxiv-1906.05743,\n2019.\n21",
    "[119]Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David\nBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin\nRiedmiller. Deepmind control suite.arXiv preprint arXiv: Arxiv-1801.00690, 2018.\n[120]Tim Taylor, Mark Bedau, Alastair Channon, David Ackley, Wolfgang Banzhaf, Guillaume\nBeslon, Emily Dolson, Tom Froese, Simon Hickinbotham, Takashi Ikegami, et al. Open-ended\nevolution: Perspectives from the oee workshop in york.Artificial life, 22(3):408–423, 2016.\n[121]Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck,\nJakob  Bauer,  Jakub  Sygnowski,  Maja  Trebacz,  Max  Jaderberg,  Michael  Mathieu,  Nat\nMcAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu,\nSteph Hughes-Fitt, Valentin Dalibard, and Wojciech Marian Czarnecki. Open-ended learning\nleads to generally capable agents.arXiv preprint arXiv: Arxiv-2107.12808, 2021.\n[122]Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation.arXiv\npreprint arXiv: Arxiv-1805.01954, 2018.\n[123]\nFaraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from\nobservation.arXiv preprint arXiv: Arxiv-1905.13566, 2019.\n[124]\nDaniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali\nAhmed, Tyler Jackson, Shibl Mourad, and Doina Precup.   Androidenv:  A reinforcement\nlearning platform for android.arXiv preprint arXiv: Arxiv-2105.13231, 2021.\n[125]Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding.arXiv preprint arXiv: Arxiv-1807.03748, 2018.\n[126]Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wo-\njciech M Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al.\nAlphastar: Mastering the real-time strategy game starcraft ii.DeepMind blog, 2, 2019.\n[127]Michael Vølske, Martin Potthast, Shahbaz Syed, and Benno Stein.  TL;DR: Mining Reddit\nto learn automatic summarization.   InProceedings of the Workshop on New Frontiers in\nSummarization, pages 59–63, Copenhagen, Denmark, sep 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/W17-4508. URLhttps://aclanthology.org/W17-4508.\n[128]Phil Wang.   x-transformers.Github,  2022.   URLhttps://github.com/lucidrains/\nx-transformers.\n[129]\nRui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired open-ended trailblazer\n(poet): Endlessly generating increasingly complex and diverse learning environments and their\nsolutions.arXiv preprint arXiv: Arxiv-1901.01753, 2019.\n[130]\nRui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, and Kenneth O. Stanley.\nEnhanced poet: Open-ended reinforcement learning through unbounded invention of learning\nchallenges and their solutions.arXiv preprint arXiv: Arxiv-2003.08536, 2020.\n[131]\nWikipedia contributors. Minecraft — Wikipedia, the free encyclopedia, 2022. URLhttps:\n//en.wikipedia.org/w/index.php?title=Minecraft&oldid=1092238294.  [Online;\naccessed 9-June-2022].\n[132]\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transform-\ners: State-of-the-art natural language processing.arXiv preprint arXiv: Arxiv-1910.03771,\n2019.\n[133]Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information\ntransfer in multi-task learning. In8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URLhttps:\n//openreview.net/forum?id=SylzhkBtDB.\n22",
    "[134]Fei Xia, William B. Shen, Chengshu Li, Priya Kasimbeg, Micael Tchapmi, Alexander Toshev,\nLi Fei-Fei, Roberto Martín-Martín, and Silvio Savarese. Interactive gibson benchmark (igibson\n0.5): A benchmark for interactive navigation in cluttered environments.arXiv preprint arXiv:\nArxiv-1910.14442, 2019.\n[135]\nSaining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spa-\ntiotemporal feature learning: Speed-accuracy trade-offs in video classification. InProceedings\nof the European conference on computer vision (ECCV), pages 305–321, 2018.\n[136]Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze,\nLuke Zettlemoyer,  and Christoph Feichtenhofer.   Videoclip:  Contrastive pre-training for\nzero-shot video-text understanding.arXiv preprint arXiv: Arxiv-2109.14084, 2021.\n[137]Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei\nFlorencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou.  Layoutlmv2: Multi-\nmodal pre-training for visually-rich document understanding.arXiv preprint arXiv: Arxiv-\n2012.14740, 2020.\n[138]Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.  Layoutlm:\nPre-training of text and layout for document image understanding.arXiv preprint arXiv:\nArxiv-1912.13318, 2019.\n[139]Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Learning semantic textual similarity from\nconversations. InProceedings of The Third Workshop on Representation Learning for NLP,\npages 164–174, Melbourne, Australia, jul 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/W18-3022. URLhttps://aclanthology.org/W18-3022.\n[140]YouTube Data API. Youtube data api.https://developers.google.com/youtube/v3/,\n2012. Accessed: 2022-06-06.\n[141]Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea\nFinn. Gradient surgery for multi-task learning.arXiv preprint arXiv: Arxiv-2001.06782, 2020.\n[142]Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and Debidatta\nDwibedi.  Xirl:  Cross-embodiment inverse reinforcement learning.arXiv preprint arXiv:\nArxiv-2106.03911, 2021.\n[143]Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek\nPurohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.\nSocratic models: Composing zero-shot multimodal reasoning with language.arXiv preprint\narXiv: Arxiv-2204.00598, 2022.\n[144]Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer.arXiv preprint\narXiv: Arxiv-2202.05607, 2022.\n[145]\nLinchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations.arXiv\npreprint arXiv: Arxiv-2011.07231, 2020.\n[146]\nYuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martín-Martín.  robosuite:  A mod-\nular simulation framework and benchmark for robot learning.arXiv preprint arXiv: Arxiv-\n2009.12293, 2020.\n23",
    "A    Minecraft Framework Comparison\nTable A.1: Comparison table of different Minecraft platforms for AI research.\nEnvironment\nSimulatorTask SuiteKnowledge Base\nFeatures\nReal\nMinecraft\nNumber\nof Tasks\nLanguage-\ngroundedFeaturesData Scale\nMINEDOJOUnified observation and action\nspace;\nunlocks   all   three   types   of\nworld   (the   Overworld,   the\nNether, and the End)\nX3,000+XAutomatically  scraped  from\nthe Internet;\nmultimodal data (videos, im-\nages,   texts,   tables  and  dia-\ngrams)\n740K YouTube videos;\n7K Wiki pages;\n350K Reddit posts\nMineRL\nv0.4 [48]\nBuilt on top of Malmo;\nactively maintained\nX11Annotated state-action pairs of\nhuman demonstrations\n60M  frames  of  recorded  hu-\nman player data\nMineRL\nv1.0\n(VPT)[10]\nMouse and keyboard controlX5Labeled contractor data;\nunlabeled videos scraped from\nthe Internet\n2K hours of contractor data;\n270K   hours   of   unlabeled\nvideos\nMarLÖ[87]Cooperative and competitive\nmultiagent tasks;\nparameterizable environments\nX14\nMalmo[60]First comprehensive release of\na  Gym-style  agent  API  for\nMinecraft\nXN/A\nCraftAssist[44] Bot assistant;\ndialogue interactions\nXN/AXInteractive dialogues;\ncrowd-sourced house building\ndataset\n800K  dialogue-action  dictio-\nnary pairs;\n2.6K   houses   with   atomic\nbuilding actions\nIGLU[66]Interactive dialogues with hu-\nmans;\naimed  at  building  structures\ndescribed by natural language\n157X\nEvoCraft[45]Aimed at generating creative\nartifacts;\nallows for direction manipula-\ntion of blocks\nN/A\nCrafter[50]2D clone of Minecraft;\nfast experimentation\n22Human experts dataset100 episodes\nBMINEDOJOSimulator\nWe design unified observation and action spaces across all tasks to facilitate the development of\nmulti-tasking and continually learning agents that can adapt to novel tasks and scenarios.   The\ncodebase is open sourced at github.com/MineDojo/MineDojo.\nB.1    Observation Space\nOur observation space contains multiple modalities. The agent perceives the world mainly through\nthe RGB screen.  To provide the same information as human players receive, we also supplement\nthe agent with observations about its inventory, location, health, surrounding blocks, etc. The full\nobservation space is shown below.  We refer readers to see our code documentation for technical\ndetails such as data type for each observable item.\nWe also support a LIDAR sensor that returns the groundtruth type of the blocks that the agent\nsees, however this is considered privileged information and does not go into the benchmarking\nspecification. However, it is still useful for hand engineering the dense reward function, which we\nuse in our experiments (Sec. 5). Amounts and directions of LIDAR rays can be arbitrarily configured\nat the cost of a lower simulation throughput.\n24",
    "ModalityShapeDescription\nRGB(3, H, W)Ego-centric RGB frames.\nEquipment(6,)Names, quantities, variants, and durabilities of equipped items.\nInventory(36,)Names, quantities, variants, and durabilities of inventory items.\nVoxel(3, 3, 3)Names, variants, and properties of3×3×3surrounding blocks.\nLife statistics\n(1,)Agent’s health, oxygen, food saturation, etc.\nGPS(3,)GPS location of the agent.\nCompass\n(2,)Yaw and pitch of the agent.\nNearby tools(2,)Indicate if crafting table and furnace are nearby the agent.\nDamage source(1,)Information about the damage on the agent.\nLidar(Num rays,)Ground-truth lidar observation.\nB.2    Action Space\nWe design a compound action space. At each step the agent chooses one movement action (forward,\nbackward, camera actions, etc.) and one optional functional action as listed in the table below. Some\nfunctional actions such ascrafttake one argument, while others likeattackdoes not take any\nargument. This compound action space can be modelled in an autoregressive manner [126]. We refer\nreaders to our code documentation for example usages of our action space.\nNameDescriptionArgument\nno_opDo nothing.∅\nuseUse the item held in the main hand.∅\ndropDrop the item held in the main hand.∅\nattackAttack with barehand or tool held in the main hand.∅\ncraft\nExecute a crafting recipe to obtain new items.Index of recipe\nequipEquip an inventory item.Slot index of the item\nplacePlace an inventory item on the ground.Slot index of the item\ndestroyDestroy an inventory item.Slot index of the item\nB.3    Customizing the Environment\nEnvironments inMINECLIPsimulator can be easily and flexibly customized. Through our simulator\nAPI, users can control terrain, weather, day-night condition (different lighting), the spawn rate and\nrange of specified entities and materials, etc.  We support a wide range of terrains, such as desert,\njungle, taiga, and iced plain, and special in-game structures, such as ocean monument, desert temple,\nand End city. Please visit our website for video demonstrations.\nCMINEDOJOTask Suite\nIn this section, we explain how we collect the Programmatic (Sec. 2.1) and Creative tasks (Sec. 2.2).\nC.1    Programmatic Tasks\nProgrammatic tasks are constructed by filling manually written templates for four categories of tasks,\nnamely “Survival”, “Harvest”, “Tech Tree”, and “Combat”. The task specifications are included in\nour codebase. Please refer to Fig. A.1 for a few samples. We briefly explain each task category:\nSurvival.\nThis task group tests the ability to stay alive in the game. It is nontrivial to survive in\nMinecraft, because the agent grows hungry as time passes and the health bar drops gradually. Hostile\nmobs like zombie and skeleton spawn at night, which are very dangerous if the agent does not have\nthe appropriate armor to protect itself or weapons to fight back. We provide two tasks with different\nlevels of difficulty for Survival.  One is to start from scratch without any assistance.  The other is\nto start with initial weapons and food.\n25",
    "1survival_sword_food:\n2category: survival\n3prompt: survive  as long as  possible  given a sword  and  some  food\n4\n5harvest_wool_with_shears_and_sheep:\n6category: harvest\n7prompt: harvest  wool  from a sheep  with  shears  and a sheep  nearby\n8\n9techtree_from_barehand_to_wooden_sword:\n10category: tech -tree\n11prompt: find  material  and  craft a wooden  sword\n12\n13combat_zombie_pigman_nether_diamond_armors_diamond_sword_shield:\n14category: combat\n15prompt: combat a zombie  pigman  in  nether  with a diamond  sword ,\n16shield , and a full  suite  of  diamond  armors\nFigure A.1: Example specifications.\nHarvest.\nThis task group tests the agent’s ability to collect useful resources such as minerals (iron,\ndiamond, obsidian), food (beef, pumpkin, carrots, milk), and other useful items (wool, oak wood,\ncoal). We construct these tasks by enumerating the Cartesian product between target items to collect,\ninitial inventory, and world conditions (terrain, weather, lighting, etc.) so that they cover a spectrum\nof difficulty.  For instance, if the task is to harvest wool, then it is relatively easy if the agent has\na shear in its initial inventory with a sheep nearby, but more difficult if the agent has to craft the\nshear from raw material and explore extensively to find a sheep.  We filter out combinations that\nare impossible (such as farming certain plants in the desert) from the Cartesian product.\nTech Tree.\nMinecraft includes several levels of tools and armors with different properties and\ndifficulties to unlock. To progress to a higher level of tools and armors, the agent needs to develop\nsystematic and compositional skills to navigate the technology tree (e.g. wood→stone→iron→\ndiamond). In this task group, the agent is asked to craft and use a hierarchy of tools starting from\na less advanced level.  For example, some task asks the agent to craft a wooden sword from bare\nhand. Another task may ask the agent to craft a gold helmet. An agent that can successfully complete\nthese tasks should have the ability to transfer similar exploration strategies to different tech levels.\nCombat.\nWe test agent’s reflex and martial skills to fight against various monsters and creatures.\nSimilar to how we develop the Harvest task group, we generate these tasks by enumerating the\nCartesian product between the target entity to combat with, initial inventory, and world conditions\nto cover a spectrum of difficulty.\nC.2    Creative Tasks\nWe construct Creative tasks using three approaches:  1) manual brainstorming,  2) mining from\nYouTube tutorial videos, and 3) generate by querying GPT-3 API. We elaborate the second and third\napproaches below.\nTask Mining from YouTube Tutorial Videos.Our YouTube dataset serves the dual purpose of\na rich task source, as many human players demonstrate and narrate creative missions in the tutorial\nplaylists. To collect high-quality tasks and accompanying videos, we design a 3-stage pipeline that\nmakes it easy to find and annotate interesting tasks.\nStage 1:\nWe search for YouTube playlists with the key phrases, “Minecraft Tutorial” and “Minecraft\nGuide”. Then we apply heuristic rules (see Sec. D.1) to filter out low-quality videos;\nStage 2:We only show the title of the video to a human annotator through a command-line interface,\nwho makes a binary decision to accept or reject it as a potential task. This step is typically\nvery fast, taking a few seconds on average;\n26",
    "Figure A.2: Labeling UI to mine tasks from YouTube. A human annotator can choose to reject the\nvideo (Invalid), adjust the timestamps, select the title, or edit and expand the original description to\nbe the new task goal.\nStage 3:For the accepted tasks in stage 2, we design a labeling UI using Label Studio [70] that\ndisplays the full video and YouTube description. A human annotator can choose to reject\nthe video, adjust the timestamps, select the title, or refine the description to be the task goal\n(Fig. A.2). Through this pipeline, we extract 1,042 task ideas from the common wisdom\nof a huge number of veteran Minecraft gamers. Some examples are “make an automated\nmining machine” and “grow cactus up to the sky”.\nC.3    GPT-3 Guidance\nWe leverage OpenAI’sGPT-3-davinciAPI to automatically generate detailed guidance for a subset\nof the tasks. Inspired by [67], we adopt the following template to prompt GPT-3:How to {task\ngoal} in Minecraft?  Let’s think step by step.Here are some examples:\nThe guidance for the task “find material and craft a gold pickaxe” is1) Find a place with\na lot of trees; 2) Cut down the trees and gather the wood; 3) Find a place\nwith a lot of stone; 4) Mine the stone and gather the cobblestone; 5) Find\na place with a lot of iron; 6) Mine the iron and gather the iron ingots;\n7) Find a place with a lot of gold; 8) Mine the gold and gather the gold\ningots; 9) Craft a gold pickaxe.\nThe  guidance  for  the  task  “sail  on  boat  with  a  sheep”  is1) Find a boat; 2) Place\nthe sheep in the boat; 3) Right-click on the boat with an empty hand to get\nin; 4) Use the WASD keys to move the boat.  The sheep should stay in the\nboat.\nC.4    Playthrough: Defeat the Ender Dragon\nOur benchmarking suite includes a special task called “Playthrough”. The agent is initialized bare-\nhanded in a freshly created world and aims to defeat theEnder dragon, which is considered the final\nboss of Minecraft. This task holds a unique position in our benchmark because killing the dragon\nmeans “beating the game” in the traditional sense of the phrase, and is considered the most significant\nachievement for a new player. This boss is optional and plenty of people choose to skip it without\naffecting their open-ended game experience.\n27",
    "051015202530\nvideo duration (minutes)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nratio of videos\nVideo Duration Histogram\nFigure A.3: Distribution of YouTube video duration. The histogram is trimmed by the 85th percentile\nto hide much longer videos that can run for many hours.\n“Playthrough” is technically a programmatic task, because we can check the simulator state for the\ndefeat of the Ender dragon. However, we decide to create its own category due to the uniqueness as\nwell as the sheer difficulty of the task. The mission requires lots of preparation, exploration, agility,\nand trial-and-error, which may take a regular human player many days to complete.  It would be\nextremely long horizon (hundreds of thousands of steps) and difficult for an agent to tackle.  We\nconsider this one of the moonshot goals in MINEDOJO.\nD    Internet-Scale Database\nWe upload our databases tozenodo.org, which is an open repository platform operated by CERN.\nThe data DOIs, URLs, and licenses are listed below.  In this section, we describe our database\nproperties and data collection process in details.\nDatabaseDOILicense\nYouTube10.5281/zenodo.6641142Creative Commons Attribution 4.0 International (CC BY 4.0)\nWiki\n10.5281/zenodo.6640448Creative Commons Attribution Non Commercial Share Alike 3.0 Unported\nReddit10.5281/zenodo.6641114Creative Commons Attribution 4.0 International (CC BY 4.0)\nD.1    YouTube Videos and Transcripts\nMinecraft is among the most streamed games on YouTube [41]. Human players have demonstrated a\nstunning range of creative activities and sophisticated missions that take hours to complete. We collect\n33 years worth of video and 2.2B words in the accompanying English transcripts. The distribution of\nvideo duration is shown in Fig. A.3. The time-aligned transcripts enable the agent to ground free-form\nnatural language in video pixels and learn the semantics of diverse activities without laborious human\nlabeling.\nWe use the official YouTube Data API [140] to collect our database, following the procedure below:\na)Search forchannelsthat contain Minecraft videos using a list of keywords, e.g., “Minecraft”,\n“Minecraft Guide”, “Minecraft Walkthrough”, “Minecraft Beginner”. We do not directly\nsearch for videos at this step because there is a limit of total results returned by the API;\nb)Search for all the video IDs uploaded by each channel that we obtain at the previous step.\nThere are many false positives at this step because some channels (like gaming news channel)\nmay cover a range of topics other than Minecraft;\nc)\nTo remove the false positives, we rely on the video category chosen by the user when the\nvideo was uploaded and filter out all the videos that do not belong to the Minecraft category;\n28",
    "Figure  A.4:  Wiki  dataset  examples.   Closewise  order:  Villager  trade  table,  mineral  ingredient\ndescriptions, monster gallery, and terrain explanation.\nd)To curate a language-grounded dataset, we favor videos that have English transcripts, which\ncan be manually uploaded by the user, automatically transcribed from audio, or automatically\ntranslated from another language by the YouTube engine. For each video, we filter it out if\n1) the view count is less than 100; or 2) the aspect ratio is less 1; or 3) the duration is less\nthan 1 minute long; or 4) marked as age-restricted.\ne)To further clean the dataset and remove potentially harmful contents, we employ the Detox-\nify [51] tool to process each video title and description. Detoxify is trained on Wikipedia\ncomments to predict multiple types of toxicity like threats, obscenity, insults, and identity-\nbased hate speech.  We delete a video if the toxicity probability in any category is above\n0.5.\nWe release all the video IDs along with metadata such as video titles, view counts, like counts,\nduration,  and FPS. In line with prior practice [64],  we do not release the actual MP4 files and\ntranscripts due to legal concerns.\nD.2    Minecraft Wiki\nThe Wiki pages cover almost every aspect of the game mechanics, and supply a rich source of un-\nstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step tutorials (example\nscreenshots in Fig. A.4 and Fig.A.5). We use Selenium [103] to scrape 6,735 pages that interleave\ntext, images, tables, and diagrams. We elaborate the details of each web element scraped by Selenium:\na)Screenshot.Using Selenium’s built-in function, we take a full screenshot of the rendered\nWiki page in order to preserve the human-readable visual formatting. We also record the\nbounding boxes of each salient web element on the page.\nb)Text.We hand-select several HTML tags that likely contain meaningful text data, such asp,\nh1, h2, ul, dl.\nc)Images and Animations.\nWe download the raw source file of each image element (JPG,\nPNG, GIF, etc.), as well as the corresponding caption if available. There are also animation\neffects enabled by JavaScript on the Wiki. We save all image frames in the animation.\nd)Sprites.Sprite elements are micro-sized image icons that are typically embedded in text\nto create multimodal tutorials and explanations.  We save all the sprites and locate their\nbounding boxes within the text too.\n29",
    "Figure A.5: More Wiki database examples with bounding boxes (annotated in red). Left: wood block\nintroduction; right: first day tutorial.\nimage\n65.7%\nvideo\n15.8%\ntext\n14.7%\nlink\n3.81%\nimage\nvideo\ntext\nlink\nFigure A.6: Distribution of Reddit post types.\ne)Tables.\nWe save the text content and bounding box of each cell that a table element contains.\nWe store the header cells separately as they carry the semantic meaning of each column. A\ntable can be easily reconstructed with the stored text strings and bounding boxes.\nD.3    Reddit\nThere are more than 1M subreddits (i.e., Reddit topics) where people can discuss a wide range of\nthemes and subjects. Prior works use Reddit data for conversational response selection [5,139,54]\nand abstractive summarization [127,65]. The r/Minecraft subreddit contains free-form discussions\nof game strategies and images/videos showcases of Minecraft builds and creations (examples in\nFig. A.7). The distribution of post types is shown in Fig. A.6.\nTo scrape the Reddit contents, we use PRAW [88], a Python wrapper on top of the official Reddit\nAPI. Our procedure is as follows:\na)Obtain the ID and metadata (e.g. post title, number of comments, content, score) of every\npost in the “r/Minecraft” subreddit since it was created. For quality control, we only consider\nposts with scores (upvotes)≥5and not marked as NSFW.\nb)\nDetermine each post’s type.  There are 4 native post types - text, image/video, link, and\npoll.  We group text and poll posts together as text posts, and store their body text.  For\nimage/video and link posts, we store the source file URLs on external media hosting sites\nlike Imgur and Gfycat. Based on the URL of each link post, we classify it as an image post,\na video post or a general link post.\n30",
    "Figure A.7: Examples of posts and comment threads from the Reddit database.\nc)\nScrape the comments and store the parent ID of each comment so that we can reconstruct\nthe threaded discussion.\nd)  Similarly to our YouTube database (Sec. D.1), we run Detoxify [51] on the scraped Reddit\ncontents to filter out potentially toxic and harmful posts.\nWe release all post IDs and their corresponding metadata. We also provide a Python function based\non PRAW for researchers to download the post contents after obtaining a license key for the official\nReddit API.\nEMINECLIP Algorithm Details\nWe  implement  all  our  neural  networks  in  PyTorch  v1.11  [86].   TrainingMINECLIPuses  the\nPyTorch-Lightning framework [32],  pre-trained models hosted on HuggingFace [132],  and the\nx-transformerslibrary for Transformer variants [128].\n31",
    "Table A.2: Training hyperparameters for MINECLIP.\nHyperparameterValue\nLR scheduleCosine with warmup [73]\nWarmup steps500\nPeak LR1.5e-4\nFinal LR1e-5\nWeight decay0.2\nLayerwise LR decay0.65\nPre-trained layers LR multiplier0.5×\nBatch size per GPU64\nParallel GPUs8\nVideo resolution\n160×256\nNumber of frames16\nImage encoderViT-B/16[28]\nE.1    Video-Text Pair Extraction\nSimilar to VideoCLIP [136], we sample 640K pairs of 16-second video snippets and time-aligned\nEnglish transcripts by the following procedure:\n1)\nCollect a list of keywords corresponding to the supported entities, blocks, and items in\nMinecraft;\n2)Perform string matching over our YouTube video transcripts to obtain 640K text segments;\n3)For each matched transcript segment, randomly grow it to 16∼77 tokens (limited by CLIP’s\ncontext length);\n4)Randomly sample a timestamp within the start and end time of the matched transcript as the\ncenter for the video clip;\n5)  Randomly grow the video clip from the center timestamp to 8∼16 seconds.\nE.2    Architecture\nMINECLIP architecture is composed of three parts:\nFrame-wise  image  encoderφ\nI\nWe  use  the  ViT-B/16  architecture  [28]  to  compute  a  512-D\nembedding for each RGB frame. We initialize the weights from OpenAI CLIP’s public checkpoint\n[92] and only finetune the last two layers during training. The input resolution is160×256, which is\ndifferent from CLIP’s default224×224resolution. We adapt the positional embeddings via bicubic\ninterpolation, which does not introduce any new learnable parameters.\nTemporal aggregatorφ\na\nGiven a sequence of frame-wise RGB features, a temporal aggregator\nnetwork summarizes the sequence into one video embedding. After the aggregator, we insert two\nextra layers of residual CLIP Adapter [38].  The residual weight is initialized such that it is very\nclose to an identity function at the beginning of training. We consider two variants ofφ\na\n:\n1.Average pooling (MINECLIP[avg]): a simple, parameter-free operator. It is fast to execute\nbut loses the temporal information, because average pooling is permutation-invariant.\n2.Self-Attention (MINECLIP[attn]): a 2-layer transformer encoder with 512 embedding size,\n8 attention heads, and Gated Linear Unit variant with Swish activation [106,22].  The\ntransformer sequence encoder is relatively slower, but captures more temporal information\nand achieves better performance in our experiments (Table 1).\nText encoderφ\nG\nWe use a 12-layer 512-wide GPT model with 8 attention heads [90,91].  The\ninput string is converted to lower-case byte pair encoding with a 49,152 vocabulary size, and capped\nat 77 tokens.  We exactly follow the text encoder settings in CLIP and initialize the weights from\ntheir public checkpoint. Only the last two layers ofφ\nG\nis finetuned during training.\n32",
    "Algorithm 1:PPO-SI Interleaved Training\nInput:policyπ\nθ\n, value functionV F(·), SI buffer threshold∆, SI frequencyω\n1Initialize empty SI buffers for all tasksD\nSI\n←{∅,∀T∈training tasks};\n2Initialize a counter for simulator stepscounter←0;\n3whilenot donedo\n4Collect set of trajectories for all tasks{τ\nT\n,∀T∈training tasks}by running policyπ\nθ\nin\n(parallel) environments;\n5forallD\nSI,T\ndo\n6ifτ\nT\nis successfulthen\n7D\nSI,T\n←D\nSI,T\n∪τ\nT\n8else ifτ\nT\n’s episode return≥μ\nreturn\n(D\nSI,T\n) + ∆×σ\nreturn\n(D\nSI,T\n)then\n9D\nSI,T\n←D\nSI,T\n∪τ\nT\n10end\n11Increasecounteraccordingly;\n12Updateπ\nθ\nfollowing Equation 2;\n13FitV F(·)by regression on mean-squared error;\n14if1(countermodω= 0)then\n15Determine the number of trajectories to sample from each buffer\n#\nsample\n= min({|D\nSI,T\n|,∀T∈training tasks});\n16Sample#\nsample\ntrajectories from each buffer in a prioritized manner to constructD\nSI\n;\n17Updateπ\nθ\nonD\nSI\nwith supervised objective;\n18end\nE.3    Training\nWe trainMINECLIPon the 640K video-text pairs for 2 epochs. We sample 16 RGB frames from each\nvideo uniformly, and apply temporally-consistent random resized crop [17,33] as data augmentation.\nWe use Cosine learning rate annealing with 500 gradient steps of warming up [73].  We apply a\nlower learning rate (×0.5) on the pre-trained weights and layer-wise learning rate decay for better\nfinetuning [53]. Training is performed on 1 node of8×V100 GPUs with FP16 mixed precision [76]\nvia the PyTorch nativeampmodule. All hyperparameters are listed in Table A.2.\nF    Policy Learning Details\nIn this section, we elaborate how a trainedMINECLIPcan be adapted as a reward function with two\ndifferent formulations. We then discuss the algorithm for policy learning. Finally, we demonstrate\nhow we combine self imitation learning and on-policy learning to further improve sample efficiency.\nF.1    Adapt MINECLIP as Reward Function\nWe investigate two ways to convertMINECLIPoutput to scalar reward, dubbedDIRECTandDELTA.\nThe ablation results forAnimal-Zootask group are presented in Table A.3.\nDirect.For a taskTwith the goal descriptionG, MINECLIPoutputs the probabilityP\nG\nthat the\nobservation video semantically corresponds toG, against a set of negative goal descriptionsG\n−\n.\nNote that we omit timestep subscript for simplicity. As an example, for the task “shear sheep”,G\nis “shear a sheep” andG\n−\nmay include negative prompts like “milk a cow”, “hunt a sheep”, “hunt a\ncow”, etc. To compute theDIRECTreward, we further process the raw probability using the formula\nr=  max(P\nG\n−\n1\nN\nT\n,0)\nwhereN\nT\nis the number of prompts passed toMINECLIP.\n1\nN\nT\nis the\nbaseline probability of randomly guessing which text string corresponds to the video. We threshold\nrat zero to avoid highly uncertain probability estimates below the random baseline.  We call the\nvariant without the post-processingDIRECT-Naive:r=P\nG\nas the reward signal for every time step.\nDelta.TheDIRECTformulation yields strong performance when the task is concerned with moving\ncreatures, e.g.  farm animals and monsters that run around constantly.  However, we discover that\nDIRECTis suboptimal if the task deals with static objects, e.g., “find a nether portal”. Simply using the\n33",
    "Table A.3: Ablation on different MINECLIP reward formulations.\nGroupTasksDIRECTDIRECT-NaiveDELTA\nMilk Cow64.5±37.18.6±1.27.6±5.2\nHunt Cow83.5±7.10.0±0.00.0±0.0\nShear Sheep12.1±9.10.8±0.61.8±1.5\nHunt Sheep8.1±4.10.1±0.20.0±0.0\nraw probability fromMINECLIPas reward can cause the learned agent to stare at the object of interest\nbut fail to move closer and interact. Therefore, we propose to use an alternative formulation,DELTA, to\nremedy this issue. Concretely, the reward value at timesteptbecomesr\nt\n=P\nG,t\n−P\nG,t−1\n. We empiri-\ncally validate that this formulation provides better shaped reward for the task group with static entities.\nF.2    Policy Network Architecture\nOur policy architecture consists of three parts: an input feature encoder, a policy head, and a value\nfunction.   To handle multimodal observations (Sec. B.1),  the feature extractor contains several\nmodality-specific components:\n•\nRGB frame: we use the frozen frame-wise image encoderφ\nI\ninMINECLIPto optimize\nfor compute efficiency and provide the agent with good visual representations from the\nbeginning (Sec. 4.2).\n•  Task goal:φ\nG\ncomputes the text embedding of the natural language task goal.\n•  Yaw and Pitch: computesin(·)andcos(·)features respectively, then pass through an MLP.\n•  GPS: normalize and featurize via MLP.\n•Voxel: to process the3×3×3surrounding voxels, we embed discrete block names to dense\nvectors, flatten them, and pass through an MLP.\n•\nPast action: our agent is conditioned on its immediate past action, which is embedded and\nfeaturized by MLP.\nFeatures from all modalities are concatenated, passed through another fusion MLP, and finally fed\ninto the policy head and value function head. We use an MLP to model the policy head that maps\nfrom the input feature vectors to the action probability distribution. We use another MLP to estimate\nthe value function, conditioned on the same input features.\nF.3    RL Training\nPPO.\nWe use the popular PPO algorithm  [102] (Proximal Policy Optimization) as our RL training\nbackbone. PPO is an on-policy method that optimizes for a surrogate objective while ensuring that\nthe deviation from the previous policy is relatively small. PPO updates the policy network by\nmaximize\nθ\nE\ns,a∼π\nθ\nold\nL(s,a,θ\nold\n,θ),(1)\nwhere\nL(s,a,θ\nold\n,θ) = min\n(\nπ\nθ\n(a|s)\nπ\nθ\nold\n(a|s)\nA\nπ\nθ\nold\n(s,a),clip\n(\nπ\nθ\n(a|s)\nπ\nθ\nold\n(a|s)\n,1−\u000f,1 +\u000f\n)\nA\nπ\nθ\nold\n(s,a)\n)\n.(2)\nAis an estimator of the advantage function (GAE [101] in our case) and\u000fis a hyperparameter that\ncontrols the deviation between the new policy and the old one.\nSelf Imitation Learning.\nWe apply self-imitation learning [84] (SI) to further improve sample effi-\nciency because computing the reward withMINECLIPin the loop makes the training more expensive.\nSelf-imitation learning is essentially supervised learning on a bufferD\nSI\nof good trajectories gener-\nated by the agent’s past self. In our case, the trajectories are generated by the behavior policy during\nPPO rollouts, and only added toD\nSI\nif it is asuccessfultrial or if the episodic return exceeds a certain\nthreshold. Self imitation optimizesπ\nθ\nfor the objectiveJ\nSI\n=E\ns,a∼D\nSI\nlogπ\nθ\n(a|s)with respect toθ.\n34",
    "(a) “Milk Cow”(b) “Shear Sheep”\nFigure A.8: Adding the self imitation technique [84] significantly improves the performance of RL\ntraining in MINEDOJO.\nWe alternate between the PPO phase and the SI phase.  A pseudocode of our interleaved training\nprocedure is given in Algorithm 1. We use aprioritizedstrategy to sample trajectories from the buffer\nD\nSI\n. Specifically, we assign equal probability to all successful trajectories. Unsuccessful trajectories\ncan still be sampled but with lower probabilities proportional to their episodic returns.\nIn Fig. A.8, we demonstrate that adding self-imitation dramatically improves the stability, perfor-\nmance, and sample efficiency of RL training in MINEDOJO.\nG    Experiment Details\nG.1    Task Details\nWe experiment with three task groups with four tasks per group. We train one multi-task agent for\neach group. In this section, we describe each task goals, initial setup, and the manual dense-shaping\nreward function.\nAnimal Zoo:4 Programmatic tasks on hunting or harvesting resource from animals. We spawn\nvarious animal types (pig, sheep, and cow) in the same environment to serve as distractors.  It is\nconsidered a failure if the agent does not take action on the correct animal specified by the prompt.\n•Milk Cow: find and approach a cow, then obtain milk from it with an empty bucket. The\nprompt ismilk a cow. We initialize the agent with an empty bucket to collect milk. We\nalso spawn sheep, cow, and pig nearby the agent.  The manual dense reward shaping is\na navigation reward based on geodesic distance obtained from privileged LIDAR. The\ncombined reward passed to PPO can be formulated asr\nt\n=λ\nnav\nmax(d\nmin,t−1\n−d\nmin,t\n,0) +\nλ\nsuccess\n1(milk collected),  whereλ\nnav\n=  10andλ\nsuccess\n=  200.d\nmin,t\n=  min(d\nmin\n,d\nt\n)\nwhered\nmin\ndenotes the minimal distance to the cow that the agent has achieved so far in the\nepisode history.\n•Hunt Cow: find and approach a cow, then hunt with a sword. The cow will run away so\nthe agent needs to chase after it. The prompt ishunt a cow. We initialize the agent with\na diamond sword. The manual dense reward shaping consists of two parts, a valid attack\nreward and a navigation reward based on geodesic distance obtained from privileged LIDAR.\nMathematically, the reward isr\nt\n=λ\nattack\n1(valid attack) +λ\nnav\nmax(d\nmin,t−1\n−d\nmin,t\n,0) +\nλ\nsuccess\n1(cow hunted)\n, whereλ\nattack\n= 5,λ\nnav\n= 1, andλ\nsuccess\n= 200.  We additionally\nresetd\nmin\nevery time the agent hits the cow to encourage the chasing behavior.\n•Shear Sheep: find and approach a sheep, then collect wool from the sheep with a shear. The\nprompt isshear a sheep. We initialize the agent with a shear. The manual dense reward\nshaping is a navigation reward based on geodesic distance obtained from the privileged\nLIDAR sensor, similar to “Milk Cow”.\n•Hunt Sheep: find and approach a sheep, then hunt with a sword. The sheep will run away\nso the agent needs to chase after it. An episode will terminate once any entity is hunted. The\nprompt ishunt a sheep. We initialize the agent with a diamond sword. The manual dense\n35",
    "Table  A.4:  Hyperparameters  in  RL  experiments.   “{state}MLP”  refers  to  MLPs  to  process\nobservations of compass, GPS, and voxel blocks. “Embed Dim” denotes the same dimension size\nused to embed all discrete observations into dense vectors.\nNN Architecture\nTraining\nHyperparameterAnimal-ZooMob-CombatCreative\nRGB Feature Size512Learning Rate10\n−4\n10\n−4\n10\n−4\nTask Prompt Feature Size512Cosine Decay Minimal LR5×10\n−6\n5×10\n−6\n5×10\n−6\n{state}MLP Hidden Size128γ0.990.990.99\n{state}MLP Output Size128Entropy Weight (Stage 1)5×10\n−3\n5×10\n−3\n5×10\n−3\n{state}MLP Hidden Depth2\nEntropy Weight (Stage 2)10\n−2\nN/A10\n−2\nEmbed Dim8PPO OptimizerAdamAdamAdam\nNum Feature Fusion Layers1SI Learning Rate10\n−4\n10\n−4\n10\n−4\nFeature Fusion Output Size512\nSI Cosine Decay Minimal LR10\n−6\n10\n−6\n10\n−6\nPrev Action ConditioningTrueSI Epoch101010\nPolicy Head Hidden Size256SI Frequency (Env Steps)100K100K100K\nPolicy Head Hidden Depth3SI OptimizerAdamAdamAdam\nVF Hidden Size256SI Buffer Threshold2σ2σ0.5σ\nVF Hidden Depth3PPO Buffer Size100K100K100K\nFrame Stack111\nVF Loss Weight0.50.50.5\nGAEλ0.950.950.95\nGradient Clip101010\nPPO\u000f0.20.20.2\nAction Smooth Weight10\n−7\n10\n−7\n10\n−7\nAction Smooth Window Size333\nMINECLIP Reward FormulationDIRECTDIRECTDELTA\nreward shaping consists of two parts, a valid attack reward and a navigation reward based on\ngeodesic distance obtained from the privileged LIDAR sensor, similar to “Hunt Cow”.\nMob Combat:fight 4 different types of hostile monsters:  Spider, Zombie, Zombie Pigman (a\ncreature in theNetherworld), and Enderman (a creature in theEndworld). The prompt template\nis\"Combat {monster}\".  For all tasks within this group, we initialize the agent with a diamond\nsword, a shield, and a full suite of diamond armors. The agent is spawned in theNetherfor Zombie\nPigman task, and in theEndfor Enderman.  The manual dense-shaping reward can be expressed\nasr\nt\n=λ\nattack\n1(valid attack) +λ\nsuccess\n1({monster}hunted)whereλ\nattack\n= 5andλ\nsuccess\n= 200.\nCreative\n:4 tasks that do not have manual dense reward shaping or code-defined success criterion.\n•Find Nether Portal: find and move close to a Nether Portal, then enter theNetherworld\nthrough the portal. The prompt isfind a nether portal.\n•Find Ocean: find and move close to an ocean. The prompt isfind an ocean.\n•Dig Hole\n: dig holes in the ground. The prompt isdig a hole. We initialize the agent with\nan iron shovel.\n•Lay Carpet: lay down carpets to cover the wooden floor inside a house. The prompt isput\ncarpets on the floor\n. We initialize the agent with a number of carpets in its inventory.\nNote that we categorize “Find Nether Portal” and “Find Ocean” as Creative tasks even though they\nseem similar to object navigation[12]. While finding terrains and other structures is semantically\nwell defined, it is not easy to define a function to evaluate success automatically because the simulator\ndoes not have the exact location information of these structures given a randomly generated world.\nIn principle, we can make a sweep by querying each chunk of voxels in the world to recognize the\nterrains, but that would be prohibitively expensive. Therefore, we opt to use MineCLIP as the reward\nsignal and treat these tasks as Creative.\nG.2    Observation and Action Space\nWe use a subset of the full observation and action space listed in Sec. B.1 and B.2, because the\ntasks in our current experiments do not involve actions like crafting or inventory management. Our\nobservation space consists of RGB frame, compass, GPS, and Voxels.\n36",
    "Our action space is a trimmed version of the full action space.  It consists of movement control,\ncamera control, “use” action, and “attack” action, which add up to 89 discrete choices. Concretely, it\nincludes 81 actions for discrete camera control (9×9resulted from the Cartesian product between\nyaw and pitch, each ranges from−60degree to60degree with a discrete interval of15degree). It\nalso includes 6 movement actions (forward, forward + jump, jump, back, move left, and move right)\nand 2 functional actions of “use” and “attack”. Note that the “no-op” action is merged into the 81\ncamera actions.\nG.3    RL Training\nAll hyperparameters used in our RL experiment are listed in Table A.4.  We visualize the learned\nbehaviors  of  4  tasks  in  Figure  2.   Demos  of  more  tasks  can  be  found  on  our  websitehttps:\n//minedojo.org.\nAction smoothing.Due to the stochastic nature of PPO, we observe a lot of action jittering in\nthe agent’s behavior during training.  This leads to two negative effects that degrade the learning\nperformance: 1) exploration difficulty due to inconsistent action sequence. For example, the agent\nmay be required to take multiple consecutiveattackactions in order to complete certain tasks; and 2)\nrapidly switching different movement and camera motions result in videos that are highly non-smooth\nand disorienting. This causes a domain gap from the training data ofMINECLIP, which are typically\nsmooth human gameplay videos. Therefore, the reward signal quality deteriorates significantly.\nTo remedy the issue, we impose an action smoothing loss to be jointly optimized with the PPO\nobjective (Eq. 2) during training. Concretely, consider a sliding windowWwith window size|W|\nthat contains|W|consecutive action distributionsW={π\nt−|W|+1\n,π\nt−|W|+2\n,...,π\nt\n}, the action\nsmoothing loss is defined as\nL\nsmooth\n=\n1\n|W|\n|W|−1\n∑\ni=1\nKL(π\nt\n‖π\nt−|W|+i\n),(3)\nwhereKL(·)denotes Kullback–Leibler divergence.\nMulti-stage training for multi-task RL.\nDue to hardware limitations, we are not able to run a\nlarge number of parallel simulators for all tasks in a task group. Therefore, we adopt a multi-stage\nstrategy to split the tasks and train them sequentially with a single policy network.  For the task\ngroupsAnimal-ZooandCreative, we split the four tasks into two stages of two parallel training\ntasks each. We carry over the self-imitation buffers when switching to the next stage. We also follow\nthe recommended practice in [83] and reset the policy head at the beginning of stage 2 to encourage\nexploration and reduce overfitting.  We adopt a similar replay buffer balancing strategy as [46] to\nprevent any task from dominating the training.\nG.4    Evaluation\nIn this section, we elaborate on our human and automatic evaluation procedure for Creative tasks.\nWe first ask the human annotators to manually label 100 successful and 100 failure trajectories. This\nproduces a combined dataset of 200 trajectories with groundtruth binary labels to evaluate the learned\nreward functions. On this dataset, we runMINECLIPto produce step-wise rewards and compute a\nscore that averages over each trajectory. We then apply K-means clustering withK= 2to all scores\nand determine a decision boundaryδfrom the mean of the two centroids. A trajectory with a score\ngreater thanδis classified as successful, and vice versa for failure. In this way, we essentially convert\nMINECLIPto a binary classifier. The quality ofMINECLIPcan be measured by the F1 score of\nits binary classification output against the human labels. We demonstrate thatMINECLIPhas high\nagreements with humans (Table 2), and thus qualifies as an effective automatic evaluation metric for\nCreative tasks in the absence of human judges.\nTo further investigateMINECLIP’s evaluation on more complex Creative tasks, we annotate 50\nYouTube video segments each for 5 more tasks that are much more semantically complex: “build\na farm”, “ build a fence”, “build a house”, “ride a minecart”, and “build a swimming pool”.  We\nthen runMINECLIPevaluation on these videos against a negative set.  As shown in Table A.5,\nthough not perfect,MINECLIPgenerally has a positive agreement with human judgment. We note\n37",
    "Table A.5:MINECLIP’sevaluation on more complex Creative tasks. Numbers represent F1 scores\nbetweenMINECLIP’s evaluation on tasks success and human labels. Scaled to percentage for better\nreadability.\nTasksBuild a FarmBuild a FenceBuild a HouseRide a MinecartBuild a Swimming Pool\nOurs (Attn)78.791.463.795.985.0\nOurs (Avg)73.483.137.496.994.7\nCLIP\nOpenAI\n62.524.552.970.071.7\nthat the currentMINECLIPis a proof-of-concept step in leveraging internet data for automated\nevaluation, and further scaling on more training data and parameters may lead to more improvements.\nMeanwhile, human judgment remains a useful and important alternative [93, 97].\nH    Limitations and Potential Societal Impact\nUnlike human demonstrations [126] or offline RL datasets [35], our YouTube dataset contains only\nthe video screen observations but not the actual control actions. This allows us to scale up the dataset\ntremendously, but at the same time poses a challenge to imitation learning algorithms that require\nobservation-action pairs to learn. Our proposed algorithm,MINECLIP, side-steps this problem by\nlearning a reward model, but we believe that directly inferring the human expert policy from YouTube\nis another important direction complementary to our approach. There are promising techniques that\ncan potentially overcome this limitation, such as the Learning-from-Observation (LfO) family of\nalgorithms [123, 122, 115, 31].\nOur database is scraped from the internet, which inevitably contains offensive YouTube videos or\ntoxic Reddit posts. While we have made our best effort to filter out these harmful contents (Sec. D.1),\nthere can still be undesirable biases and toxicity that elude our automatic filters. Furthermore, we\nadvocate the use of large pre-trained language models in our main paper, andMINECLIPis finetuned\nfrom the pre-trained weights of OpenAI CLIP [92]. These foundation models are known to contain\nharmful stereotypes and generate hateful commentary [15,13,40]. We ask the researchers who will\nuse our code and database to exercise their best judgment during new model development to avoid\nany negative social impact.\nI    Datasheet\nWe present a Datasheet [39] for documentation and responsible usage of our internet knowledge\ndatabases.\nI.1    Motivation\nFor what purpose was the dataset created?We create this internet-scale multimodal knowledge\nbase to facilitate research towards open-ended, generally capable embodied agents.\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)?This knowledge base was created by Linxi Fan (Nvidia),\nGuanzhi  Wang  (Caltech),  Yunfan  Jiang  (Stanford),  Ajay  Mandlekar  (Nvidia),  Yuncong  Yang\n(Columbia),  Haoyi Zhu (SJTU), Andrew Tang (Columbia),  De-An Huang (Nvidia),  Yuke Zhu\n(Nvidia and UT Austin), and Anima Anandkumar (Nvidia and Caltech).\nI.2    Distribution\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created?\nYes, the dataset is publicly available\non the internet.\n38",
    "How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?All datasets\ncan be downloaded fromhttps://zenodo.org/.  Please refer to this table of URL, DOI, and\nlicensing:\nDatabaseDOILicense\nYouTube10.5281/zenodo.6641142Creative Commons Attribution 4.0 International (CC BY 4.0)\nWiki10.5281/zenodo.6640448Creative Commons Attribution Non Commercial Share Alike 3.0 Unported\nReddit10.5281/zenodo.6641114Creative Commons Attribution 4.0 International (CC BY 4.0)\nHave any third parties imposed IP-based or other restrictions on the data associated with the\ninstances?No.\nDo any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances?No.\nI.3    Maintenance\nWho  will  be  supporting/hosting/maintaining  the  dataset?The  authors  will  be  supporting,\nhosting, and maintaining the dataset.\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\nPlease\ncontact Linxi Fan (linxif@nvidia.com), Guanzhi Wang (guanzhi@caltech.edu), and Yunfan\nJiang (yunfanj@cs.stanford.edu).\nIs there an erratum?No. We will make announcements if there is any.\nWill  the  dataset  be  updated  (e.g.,  to  correct  labeling  errors,  add  new  instances,  delete\ninstances)?Yes. New updates will be posted onhttps://minedojo.org.\nIf  the  dataset  relates  to  people,  are  there  applicable  limits  on  the  retention  of  the  data\nassociated with the instances (e.g., were the individuals in question told that their data would\nbe retained for a fixed period of time and then deleted)?N/A.\nWill  older  versions  of  the  dataset  continue  to  be  supported/hosted/maintained?\nYes,  old\nversions will be permanently accessible on zenodo.org.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so?Yes, please refer tohttps://minedojo.org.\nI.4    Composition\nWhat do the instances that comprise the dataset represent?\nFor YouTube videos, our data is in\nJSON format with video URLs and metadata. We do not provide the raw MP4 files for legal concerns.\nFor Wiki, we provide the text, images, tables, and diagrams embedded on the web pages. For Reddit,\nour data is in JSON format with post IDs and metadata, similar to YouTube. Users can reconstruct\nthe Reddit dataset by running our script after obtaining an official Reddit API license key.\nHow many instances are there in total (of each type,  if appropriate)?There are more than\n730K YouTube videos with 2.2B words of transcripts, 6,735 Wiki pages with 2.2M bounding boxes\nof visual elements, and more than 340K Reddit posts with 6.6M comments.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set?We provide all instances in our Zenodo data repositories.\nIs there a label or target associated with each instance?No.\nIs any information missing from individual instances?No.\n39",
    "Are relationships between individual instances made explicit (e.g., users’ movie ratings, social\nnetwork links)?We provide metadata for each YouTube video link and Reddit post ID.\nAre there recommended data splits (e.g., training, development/validation, testing)?\nNo. The\nentire database is intended for pre-training.\nAre there any errors, sources of noise, or redundancies in the dataset?Please refer to Sec. D\nIs the dataset self-contained,  or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)?\nWe follow prior works [64] and only release the video URLs\nof YouTube videos due to legal concerns. Researchers need to acquire the MP4 and transcript files\nseparately. Similarly, we only release the post IDs for the Minecraft Reddit database, but we also\nprovide a script that can reconstruct the full Reddit dataset given a free official license key.\nDoes the dataset contain data that might be considered confidential?No.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety?\nWe have made our best efforts to detoxify the contents via\nan automated procedure. Please refer to Sec. D.\nI.5    Collection Process\nThe collection procedure, preprocessing, and cleaning are explained in details in Sec. D.\nWho was involved in the data collection process (e.g.,  students,  crowdworkers,  contractors)\nand  how  were  they  compensated  (e.g.,  how  much  were  crowdworkers  paid)?\nAll  data\ncollection, curation, and filtering are done by MINEDOJOcoauthors.\nOver what timeframe was the data collected?The data was collected between Dec. 2021 and\nMay 2022.\nI.6    Uses\nHas the dataset been used for any tasks already?\nYes, we have used theMINEDOJOYouTube\ndatabase for agent pre-training. Please refer to Sec. 5 and Sec. G for algorithmic and training details.\nWhat (other) tasks could the dataset be used for?Our knowledge base is primarily intended\nto facilitate research in open-ended, generally capable embodied agents.  However, it can also be\nbroadly applicable to research in video understanding, document understanding, language modeling,\nmultimodal learning, and so on.\nIs  there  anything  about  the  composition  of  the  dataset  or  the  way  it  was  collected  and\npreprocessed/cleaned/labeled that might impact future uses?No.\nAre there tasks for which the dataset should not be used?We strongly oppose any research\nthat intentionally generates harmful or toxic contents using our YouTube, Wiki, and Reddit data.\n40"
  ]
}