{
  "key": "GA9SD6TN",
  "url": "http://arxiv.org/pdf/2309.11495",
  "metadata": {
    "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
    "abstract": "  Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.\n",
    "published": "2023-09-20T17:50:55Z"
  },
  "text": [
    "CHAIN-OF-VERIFICATIONREDUCESHALLUCINATION\nINLARGELANGUAGEMODELS\nShehzaad Dhuliawala\nMeta AI & ETH Z\n ̈\nurich\nMojtaba Komeili\nMeta AI\nJing Xu\nMeta AI\nRoberta Raileanu\nMeta AI\nXian Li\nMeta AI\nAsli Celikyilmaz\nMeta AI\nJason Weston\nMeta AI\nABSTRACT\nGeneration of plausible yet incorrect factual information, termed hallucination,\nis an unsolved issue in large language models. We study the ability of language\nmodels to deliberate on the responses they give in order to correct their mistakes.\nWe develop the Chain-of-Verification (COVE) method whereby the model first (i)\ndrafts an initial response; then (ii) plans verification questions to fact-check its\ndraft; (iii) answers those questions independently so the answers are not biased\nby other responses; and (iv) generates its final verified response. In experiments,\nwe showCOVEdecreases hallucinations across a variety of tasks, from list-based\nquestions from Wikidata, closed book MultiSpanQA and longform text generation.\n1INTRODUCTION\nLarge Language Models (LLMs) are trained on huge corpora of text documents with billions of\ntokens of text. It has been shown that as the number of model parameters is increased, performance\nat tasks such as closed book QA improve in accuracy, and larger models can generate more correct\nfactual statements (Radford et al., 2019; Petroni et al., 2019). However, even the largest models can\nstill fail, particularly on lesser known torso and tail distribution facts (Sun et al., 2023a), i.e. those\nthat occur relatively rarely in the training corpora. In those cases where the model is incorrect, they\ninstead generate an alternative response which is typically plausible looking (e.g., a similar entity, but\nan incorrect one). These factually incorrect generations are referred to as hallucinations (Maynez\net al., 2020). Further, in longform tasks consisting of generating multiple sentences or paragraphs, the\nhallucination problem can be exacerbated due to the issue of exposure bias (Wang & Sennrich, 2020).\nThe current wave of language modeling research goes beyond next word prediction, and has focused\non their ability to reason. Improved performance in reasoning tasks can be gained by encouraging\nlanguage models to first generate internal thoughts or reasoning chains before responding (Wei et al.,\n2022; Adolphs et al., 2021; Wang et al., 2022; Lanchantin et al., 2023), as well as updating their\ninitial response through self-critique (Press et al., 2022; Madaan et al., 2023).  In this work we\nfollow this line of research to study how and when language-model-based reasoning can be used to\nreduce hallucinations. We develop an approach, called Chain-of-Verification (CoVe) which, given\nan initial draft response, first plans verification questions to check its work, and then systematically\nanswers those questions in order to finally produce an improved revised response.  We find that\nindependent verification questions tend to provide more accurate facts than those in the original\nlongform answer, and hence improve the correctness of the overall response. We study variations on\nthis recipe across a range of tasks: from list-based questions, closed booked QA and longform text\ngeneration. We first propose a joint approach for generating the entire verification chain left-to-right,\nwhich improves performance and decreases hallucinations compared to the baseline language model.\nHowever, models that attend to existing hallucinations in the context from their own generations tend\nto repeat the hallucinations. Hence we also introduce further improvements with factored variants\nwhich separate out the verification chain steps, in terms of which context is attended to. We show\nhow these factored variants give further performance gains across all three tasks considered.\n1\narXiv:2309.11495v2  [cs.CL]  25 Sep 2023",
    "Query\n       1. Baseline Response\n       4. Final Verified Response\n        2. Plan Verifications\nHere are some politicians who were born in NY, New York:\n1. Hillary Clinton - former secretary of state and former Democratic presidential nominee\n2. Donald Trump - former president of the United States\n3. Michael Bloomberg - former Mayor of New York City and former Democratic presidential candidate \n  ... <list continues..>\nName some politicians who were born in NY, New York. \n  - Where was Hillary Clinton born?  \n  - Where was Donald Trump born?   \n  - Where was Michael Bloomberg born?\n  ... <questions continue..>\nHere are some politicians who were born in NY, New York:\n1. Donald Trump - former president of the United States\n2. Alexandria Ocasio-Cortez - Democratic member of the U.S. House of Representatives\n  ... <list continues..>\nHillary Clinton was born in Chicago, Illinois, United   \nStates on October 26, 1947.\nDonald Trump was born on June 14, 1946, in Queens, \nNew York City, New York, United States\n...\nMichael Bloomberg was born on February 14, 1942, in \nBoston, Massachusetts, United States.\n        3. Execute Verifications\nFigure 1: Chain-of-Verification (CoVe) method. Given a user query, a large language model generates\na baseline response that may contain inaccuracies, e.g. factual hallucinations. We show a query here\nwhich failed for ChatGPT (see section 9 for more details). To improve this, CoVe first generates a\nplan of a set of verification questions to ask, and then executes that plan by answering them and hence\nchecking for agreement. We find that individual verification questions are typically answered with\nhigher accuracy than the original accuracy of the facts in the original longform generation. Finally,\nthe revised response takes into account the verifications.  The factored version of CoVe answers\nverification questions such that they cannot condition on the original response, avoiding repetition\nand improving performance.\n2RELATEDWORK\nHallucination is a general problem in language model generations that appears across many tasks,\nfrom summarization (Maynez et al., 2020) to open-domain dialogue (Roller et al., 2020), and has not\nbeen resolved by simply scaling up training data or model size (Zhang et al., 2023). For a survey of\nthe hallucination issue, see Ji et al. (2023). A majority of the methods for reducing hallucination can\nbe divided into roughly three categories: training-time correction, generation-time correction and via\naugmentation (tool-use).\nIn training-time correction methods, an attempt is made to improve the raw left-to-right generations\nof an encoder-decoder or decoder-only language model by either training or otherwise adjusting\nthe model weights to decrease the probability of hallucinated generations.  This includes using\nreinforcement learning (Roit et al., 2023; Wu et al., 2023), constrastive learning (Chern et al., 2023b;\nSun et al., 2023b) and other methods (Li et al., 2023).\nIn generation-time correction, a common theme is to make reasoning decisions “on top of” the base\nLLM in order to make them more reliable.  For example, by considering the probabilities of the\ngenerated tokens (Mielke et al., 2022; Kadavath et al., 2022).  In Manakul et al. (2023) multiple\nsamples are drawn from the model to detect hallucinations. In Varshney et al. (2023) hallucinations\nare identified using low confidence scores, and their correctness is checked through a validation\n2",
    "procedure, mitigated, and then the generation is continued. An alternative to using the confidence\nscores is to leverage inconsistencies in the LLMs output to detect hallucination. Agrawal et al. (2023)\nuse both multiple samples and consistency detection by asking direct and indirect queries to check for\nhallucinated references. Cohen et al. (2023) introduce a method called LM vs LM which simulates\nan interactive setup between two LLMs where one LLM acts as an examiner and tests if the output\nis consistent via repeated cross-examination. Cohen et al. (2023) shows that using inconsistencies\nfor QA tasks can outperform using confidence scores for hallucination detection. COVEalso uses a\nrelated self-consistency approach, but without the multi-agent (multi-LLM) debate concept.\nA third approach is to use external tools to help mitigate hallucinations, rather than relying solely on\nthe abilities of the language model itself. For example, retrieval-augmented generation can decrease\nhallucinations by using factual documents for grounding (Shuster et al., 2021; Jiang et al., 2023b;\nYu et al., 2023) or chain-of-thought verification (Zhao et al., 2023). Other approaches include using\ntools for fact-checking (Chern et al., 2023a; Galitsky, 2023; Peng et al., 2023), or linking to external\ndocuments with attribution (Menick et al., 2022; Rashkin et al., 2023; Gao et al., 2023).\nThere are also a number of related works in improving reasoning for logical and mathematical tasks,\neven if they do not address reducing hallucination explicitly. Several approaches have been shown to\nimprove results with extended reasoning steps by the system, such as chain-of-thought (Wei et al.,\n2022), deductive verification (Ling et al., 2023), and self-verification (Miao et al., 2023; Jiang et al.,\n2023a; Weng et al., 2022). The latter tries to predict the (masked) question given the answer for math\nproblems, and use that as evidence that this is the correct solution.\n3CHAIN-OF-VERIFICATION\nOur approach assumes access to a base LLM that – despite potentially being prone to hallucination –\nis capable of being prompted with general instructions in either a few-shot or zero-shot fashion. Akey\nassumptionof our method is that this language model, when suitably prompted, can both generate\nand execute a plan of how to verify itself in order to check its own work, and finally incorporate this\nanalysis into an improved response.\nOur overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps:\n1.Generate Baseline Response: Given a query, generate the response using the LLM.\n2.Plan Verifications: Given both query and baseline response, generate a list of verification\nquestions that could help to self-analyze if there are any mistakes in the original response.\n3.Execute Verifications: Answer each verification question in turn, and hence check the answer\nagainst the original response to check for inconsistencies or mistakes.\n4.Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a\nrevised response incorporating the verification results.\nEach of these steps is performed by prompting the same LLM in different ways to obtain the desired\nresponse.  While steps (1),  (2) and (4) all can be invoked with a single prompt,  we investigate\nvariations of step (3) including joint, 2-step and factored versions. These variants either involve a\nsingle prompt, two prompts or else independent prompts per question, where more sophisticated\ndecomposition can yield improved results.\nWe describe these steps in more detail below. An overview of the approach is illustrated in Figure 1,\nand in the Appendix in Figure 3.\n3.1BASELINERESPONSE\nGiven a query, we generate left-to-right as usual using the LLM, with no special tricks. While this is\nthe first step in the CoVe pipeline, it also serves as the baseline we wish to improve in our experiments\n(i.e., we will directly compare this baseline response with the final verified response from our overall\nmethod).\nGiven such baseline generations are typically prone to hallucination, CoVe attempts to identify these\nhallucinations, and correct them, in the following steps.\n3",
    "3.2PLANVERIFICATIONS\nConditioned on the original query and the baseline response, the model is prompted to generate\na series of verification questions that test the factual claims in the original baseline response.  For\nexample if part of a longform model response contains the statement“The Mexican–American War\nwas an armed conflict between the United States and Mexico from 1846 to 1848”, then one possible\nverification question to check those dates could be“When did the Mexican American war start and\nend?”. We note that verification questions are not templated and the language model is free to phrase\nthese in any form it wants, and they also do not have to closely match the phrasing of the original text.\nIn  our  experiments,  we  perform  such  verification  planning by  providing  a  few-shot prompt  of\n(response, verification) demonstrations to our LLM. See section 8 for the few-shot prompts we will\nuse in our experiments. We note it is also possible with a sufficiently performant instruction-following\nLLM that this could be performed zero-shot.\n3.3EXECUTEVERIFICATIONS\nGiven the planned verification questions, the next step is to answer them in order to assess if any\nhallucinations exist. While techniques such as retrieval-augmentation could be used in this process,\nsuch as verification via search engine, in this work we do not explore tool-use. Instead, we consider\nonly using the LLM itself in all steps of CoVe, hence the model is used to check its own work. We\ninvestigate several variants of verification execution, called joint, 2-Step, factored and factor+revise.\nJointIn thejointmethod, the planning and execution (steps 2 and 3) are accomplished by using a\nsingle LLM prompt, whereby the few-shot demonstrations include both verification questions and\ntheir answers immediately after the questions. In this approach separate prompts are not needed.\n2-StepA potential disadvantage of thejointmethod is that because the verification questions must\ncondition on the baseline response in the LLM context, and the method is joint, the verification\nanswers have to condition on the initial response as well.   This may increase the likelihood of\nrepetition, another known issue of modern LLMs (Holtzman et al., 2019). This means the verification\nquestions might hallucinate similarly to the original baseline response, which defeats the purpose.\nWe hence instead separate the planning and execution into separate steps, both with their own LLM\nprompt. The planning prompt conditions on the baseline response in the first step. The verification\nquestions generated from planning are answered in the second step, where crucially the context given\nto the LLM prompt only contains the questions, and not the original baseline response and hence\ncannot repeat those answers directly.\nFactoredAnother, more sophisticated approach, is to answer all questions independently as separate\nprompts. Again, crucially, those prompts do not contain the original baseline response and are hence\nnot prone to simply copying or repeating it.  The factored approach has the further advantage of\nremoving any potential interference not only from the baseline response, but also between answer\ncontexts, and is somewhat related to the recent (concurrent) work of Radhakrishnan et al. (2023)\nfor subquestion answering by factored decomposition, hence we adopt their naming.  It can also\npotentially handle more verification questions by virtue of them not all having to fit with the same\nsingle context.  While this is potentially more computationally expensive, requiring the execution\nof many more LLM prompts, they can be run in parallel, and hence be batched.  In order to do\nthis, we first have to take the set of generated questions from subsection 3.2 and parse them into\nseparate questions, which is a relatively easy task as the few-shot demonstrations we provide indicate\nthey should be generated as a comma-separated list. We can then split them out into separate LLM\nprompts.\nFactor+ReviseAfter answering the verification questions, the overall CoVe pipeline then has to\neither implicitly or explicitly cross-check whether those answers indicate an inconsistency with the\noriginal responses. In the factor+revise approach, we execute this as a deliberate step via an extra\nLLM prompt, which may make it easier for the final system to reason about this step explicitly.\nDifferently to answering the verification questions, the cross-checking phase needs to condition\non both the baseline response and the verification question and answer.  We thus execute this as\nseparate LLM prompts, one “cross-check” prompt for each question, with again a set of few-shot\n4",
    "demonstrations showing the desired output. For example if the original baseline response contained\nthe phrase“It followed in the wake of the 1845 U.S. annexation of Texas. . . ”and CoVe generated a\nverification questionWhen did Texas secede from Mexico?which was answered with1836then an\ninconsistency should be detected by this step.\n3.4FINALVERIFIEDRESPONSE\nFinally, the improved response that takes verification into account is generated. This is executed by a\nfinal few-shot prompt where the context takes into account all of the previous reasoning steps, the\nbaseline response and verification question answer pairs, so that the corrections can take place. If the\nFactor+Revise approach is used from subsection 3.3 then the output of the cross-check inconsistency\ndetection is provided as well.\n4EXPERIMENTS\nWe use various experimental benchmarks to measure the efficacy of CoVe in reducing hallucination,\ncomparing against a number of baselines.\n4.1TASKS\nThe benchmarks we use range from list-based questions where the required answer is a set of entities,\nto where the answer is a longform generation of multiple freeform sentences.\n4.1.1WIKIDATA\nWe start by testing CoVe on a set of automatically generated questions using the Wikidata API\n1\n. We\ncreate list questions of the form: “Who are some [Profession]s who were born in [City]?”.\nFor example, “Who are some politicians who were born in Boston?”. The answer to these\nquestions is a set of entities, where the gold list is obtained from the Wikidata knowledge base. This\nresults in a dataset of 56 test questions, each typically containing∼600 known gold entities, but\ntypically an LLM will produce a much shorter list. We then use the precision metric (micro-averaged)\nto measure performance, in addition to reporting the averaged number of positive and negative entities\nproduced.\n4.1.2WIKI-CATEGORYLIST\nWe then proceed to a harder set-generation task. We use theQUEST(Malaviya et al., 2023) dataset\nthat was created using Wikipedia Category lists. We convert these category names to questions by\nsimply prepending a “Name some”.  Owing to the varied questions such asName some Mexican\nanimated horror filmsorName some Endemic orchids of Vietnamwe believe this task can pose a\ngreater challenge.  We collate all examples in the dataset thatdo not requirelogical operations to\ncreate a set of 55 test questions each having\n ̃\n8 answers. Similar to the Wikidata task, we measure\nprecision (micro-averaged) to measure performance, in addition to reporting the averaged number of\npositive and negative entities produced.\n4.1.3MULTISPANQA\nWe next test our approach on an reading comprehension benchmark, MultiSpanQA (Li et al., 2022).\nMultiSpanQA comprises of questions that have multiple independent answers (derived from a series\nof multiple discontiguous spans in the text, with questions originally from the Natural Questions\ndataset). We consider a closed-book setting, where we do not provide supporting documents, and\nhence consider a subset of questions which are factoid-based, so that our base LLM is more likely to\nbe able to answer them. We thus use a test set of 418 questions with shorter answers per span (up to\n3 tokens per item). For example,Q: Who invented the first printing press and in what\nyear?, A:Johannes Gutenberg, 1450.\n1\nhttps://query.wikidata.org/\n5",
    "Wikidata\n(Easier)\nWiki-Category list\n(Harder)\nLLMMethodPrec. (↑)Pos.Neg.Prec. (↑)Pos.Neg.\nLlama 2 70B ChatZero-shot0.120.553.930.050.356.85\nLlama 2 70B ChatCoT0.080.758.920.030.3011.1\nLlama 65BFew-shot0.170.592.950.120.554.05\nLlama 65BCoVe (joint)0.290.410.980.150.301.69\nLlama 65BCoVe (two-step)0.360.380.680.210.500.52\nLlama 65BCoVe (factored)0.320.380.790.220.521.52\nTable 1:  Test Precision and average number of positive and negative (hallucination) entities for\nlist-based questions on the Wikidata and Wiki-Category list tasks.\n4.1.4LONGFORM  GENERATION OFBIOGRAPHIES\nWe next validate the performance of CoVe on longform text generation. In this setting, we evaluate\nour method on generating biographies, adopting the benchmark proposed in by Min et al. (2023).\nHere the model is simply prompted to generate a biography of a selected entity using the prompt:\n“Tell me a bio of <entity>”. We evaluate the efficacy of our approach using theFACTSCORE\nmetric (Min et al., 2023) developed in that work, which uses a retrieval-augmented language model\nto fact-check the response (Instruct-Llama, “Llama + Retrieval + NP”), which they showed correlates\nwell with human judgments.\n4.2BASELINES\nWe use Llama 65B, a strong open model as our base LLM (Touvron et al., 2023a), and use greedy\ndecoding for all models. As Llama 65B is not instruction fine-tuned, we employ few-shot examples\nparticular to each task for measuring performance on each of our benchmarks. This serves as our\nmain baseline which CoVe tries to improve upon. CoVe uses the same Llama 65B base, but includes,\nfor the same few-shot examples, demonstrations of verification questions and final verified responses,\nfollowing Figure 1 and section 3. Thus, we measure the ability to improve over the original baseline\nresponse for the same LLM. For CoVe, we compare different variants, particularly the joint and\nfactored versions on all tasks.\nWe also compare to Llama instruction fine-tuned models, for which we use Llama 2 (Touvron et al.,\n2023b). We measure both zero-shot performance on the task, or zero-shot with chain-of-thought by\nadding “Let’s think step by step” to the zero-shot prompt.  We find that the instruction fine-tuned\nmodels tend to generate extraneous content when queried. This can especially be a problem for the\nlist-based tasks. To deal with this we add an extra line to our prompt: “List only the answers\nseparated by a comma\n”. We also add another layer of post-processing to extract the answers by\nusing an off-the-shelf NER model to further avoid this issue as this helped. However, we still expect\nfew-shot to improve over this, especially for tasks like Multi-Span-QA where the answers are not all\nnamed entities, and the few-shot examples effectively show the domain of the task.\nFor the longform generation of biographies we also compare to several existing model results reported\nin Min et al. (2023), in particular InstructGPT (Ouyang et al., 2022), ChatGPT\n2\nand PerplexityAI\n3\n.\n4.3RESULTS\nWe are interested in empirically answering the following research questions:\nRQ1Can COVEeffectively reduce the rate of hallucinatory content produced by the LLM?\nRQ2\nCanCOVEbe used to fix or remove incorrect generations without decreasing the amount of\ncorrect content?\n2\nhttps://openai.com/blog/chatgpt\n3\nwww.perplexity.ai\n6",
    "LLMMethodF1 (↑)Prec.Rec.\nLlama 2 70B ChatZero-shot0.200.130.40\nLlama 2 70B ChatCoT0.170.110.37\nLlama 65BFew-shot0.390.400.38\nLlama 65BCoVe (joint)0.460.500.42\nLlama 65BCoVe (factored)0.480.500.46\nTable 2: Closed book MultiSpanQA test performance, comparing CoVe with various baselines.\nLLMMethodFACTSCORE. (↑)Avg. # facts\nInstructGPT\n∗\nZero-shot41.126.3\nChatGPT\n∗\nZero-shot58.734.7\nPerplexityAI\n∗\nRetrieval-based61.640.8\nLlama 2 70B ChatZero-shot41.364.9\nLlama 2 70B ChatCoT41.149.0\nLlama 65BFew-shot55.916.6\nLlama 65BCoVe (joint)60.812.8\nLlama 65BCoVe (factored)63.711.7\nLlama 65BCoVe (factor+revise)71.412.3\nTable 3: Longform generation of biographies with metrics defined from Min et al. (2023). Models\nmarked with∗are reported from previous work.FACTSCOREautomatically computed using “Instruct-\nLlama” ( Retrieve→LM + NP), the best open-access model.\nvery rareraremediumfreqvery freq\nRarity\n0.0\n0.2\n0.4\n0.6\n0.8\nFactScore\nLlama Few-shot\nLlama CoVe (joint)\nLlama CoVe (factored)\nLlama CoVe (factor+revise)\nInstructGPT\nChatGPT\nPerplexity.ai\nLlama 2 Chat Zero-shot\nLlama 2 Chat CoT\nvery rareraremediumfreqvery freq\nRarity\n0.0\n0.2\n0.4\n0.6\n0.8\nFactScore\nFigure 2:FACTSCOREperformance distribution across head, torso and tail facts for CoVe variants\nand various baselines on longform generation of biographies.\nOur main results across the four benchmark tasks are given in Table 1, Table 2 and Table 3, and our\nmain findings are as follows.\nCoVe improves precision on list-based answer tasksWe find that CoVe provides large gains in\nprecision on the list-based tasks, e.g. more than doubles the precision from the Llama 65B few-shot\nbaseline for the Wikidata task (from 0.17 to 0.36). We find from the positive and negative breakdown\nthat there is a large reduction in the number of hallucinated answers (negatives: 2.95→0.68) while\nonly a relatively small reduction in the number of non-hallucinations (positives: 0.59→0.38).\nCoVe improves performance on closed book QAWe also find that CoVe brings improvements in\ngeneral QA problems, as measured on MultiSpanQA. We observe a 23% improvement in F1 over the\nfew-shot baseline (0.39→0.48), where the improvements come from gains in both precision and\nrecall.\n7",
    "Verification Execution\nCoVe (joint)CoVe (factored)\nVerification PlanPrec.Prec.\nRule-based questions0.130.16\nGenerated by model:\nyes/no questions0.150.19\ngeneral questions0.150.22\nTable 4: Comparison of various CoVe verification plan strategies (rows) and verification execution\ntechniques (columns) on the Wiki-Category task.\nCoVe improves precision on longform generationThese results also extend to longform genera-\ntion, where we actually see larger gains than in the QA setting.FACTSCOREincreases 28% (55.9→\n71.4) from the few-shot baseline, with again only a relatively small reduction in average number of\nfacts provided (16.6→12.3). We also show the breakdown of improvements across facts in Figure 2,\nwhere one can see CoVe improves results for both rare and more frequent facts.\nInstruction-tuning and CoT do not reduce hallucinationsWe find that the few-shot baseline that\nemploys a pre-trained Llama model outperforms Llama 2 Chat, an instruction tuned model, across all\nthe tasks. The few-shot examples lead the model to give outputs in line with those expected for the\ntask, whereas general instruction tuning produces more hallucinations or incorrect outputs. Standard\nchain-of-thought (CoT) prompting also fails to improve the results for these tasks. While CoT has\nproven to help for reasoning tasks, it seems less appropriate for the issue of hallucination we measure\nin this work.\nFactored and 2-step CoVe improve performanceWe observe a consistent performance improve-\nment across all tasks from applying the factored CoVe approach compared to joint CoVe.   For\nexample improvement from 60.8→63.7 inFACTSCOREin longform generation.  Similarly, the\n2-step approach also outperforms the joint approach, as tested on the Wikidata and Wiki-Category list\ntasks, with 2-step giving the best results for Wikidata, and factored the best for Wiki-Category. All\nthese results support our hypothesis that verifying questions should not attend to the original baseline\nresponse as they may be prone to repeating it (as the joint method can do).\nFurther explicit reasoning helps remove hallucinationsIn the longform generation task we also\nexplore more sophisticated reasoning steps in the CoVe “factor+revise” method, which explicitly\ncross-checks whether verification answers indicate an inconsistency.  We see large gains in the\nFACTSCOREmetric from this further explicit reasoning from 63.7 (factored)→71.4 (factor+revise).\nThis gives further indication that appropriate and explicit reasoning in LLMs can bring improvements\nin mitigating hallucinations.\nCoVe-based Llama outperforms InstructGPT, ChatGPT and PerplexityAIOn the longform\ngeneration task, our baseline few-shot Llama 65B is outperformed by the ChatGPT and PerplexityAI\nmodels in terms of theFACTSCOREmetric.  However, applying CoVe to the baseline Llama 65B\nlifts its performance above both ChatGPT and PerplexityAI, as well as outperforming InstructGPT.\nThis is particularly impressive compared to PerplexityAI considering that is a model that can support\nits facts with retrieval-augmentation, whereas CoVe uses only the base language model itself with\nimproved reasoning via deliberation (verification). However, we can see in Figure 2 PerplexityAI still\noutperforms CoVe for very rare facts where retrieval is essential, but CoVe outperforms PerplexityAI\nfor more frequent facts. We note that some models produce less overall facts than others, however\ntheFACTSCOREmetric  is  normalized  and  hence  comparable  across  models.   We  verified  this\nexperimentally by clipping Llama 2 70B chat’s output to present less facts (as it contains the largest\nnumber in its output out of all models), but this did not change itsFACTSCOREsubstantially, e.g.\nclipping to 10 sentences increased its score from 41.3→42.7. We note the length of the generations\nof the few-shot-based models are essentially governed by the few-shot examples, which in-turn are\nconstrained by the context length.\n8",
    "Shortform verification questions are more accurately answered than longform queriesIn a\nlongform response, LLMs are prone to generate a number of hallucinations. However, it can often\nbe the case that the LLM itself would know these hallucinations are wrong if queried specifically\nfor that individual fact, independent of the rest of the longform generation, see Figure 1, Figure 3,\nand section 9. This can be seen quantitatively on the Wikidata task, where only∼17% of the Llama\nfew-shot baseline answer entities are correct in list-based questions. However, when querying each\nindividual entity via a verification question, we find∼70% are correctly answered.\nLLM-based verification questions outperforms heuristicsIn our method, CoVe, the verification\nquestions are generated by the LLM dependent on the task. We compare the quality of these questions\nto heuristically constructed ones in order to measure their quality, by replacing the LLM questions\nwith templated yes/no questions of the form “DoesXanswer the question” for list-based questions\nwith elementsXin the answer. Results on the Wiki-Category task, given in Table 4, show a reduced\nprecision with rule-based verification questions.  We believe this difference would be larger for\nlongform generation where the types of required verification questions can be more diverse, and\nLLM-based verification becomes even more necesary.\nOpen verification questions outperform yes/no-based questionsIn our main experiments we use\nverification questions where the expected answers are true facts. An alternative setup is to include\nthe fact as part of the verification question and ask it in a yes/no answer format. We evaluate this\ndifference in Table 4, and find that yes/no type questions perform worse for the factored version of\nCoVe. Some anecdotal examples are included in Appendix section 9 for ChatGPT where we find the\nmodel tends to agree with facts in a yes/no question format whether they are right or wrong.\n5CONCLUSION\nWe introduced Chain-of-Verification (CoVe), an approach to reduce hallucinations in a large language\nmodel by deliberating on its own responses and self-correcting them. In particular, we showed that\nmodels are able to answer verification questions with higher accuracy than when answering the\noriginal query by breaking down the verification into a set of simpler questions.  Secondly, when\nanswering the set of verification questions, we showed that controlling the attention of the model\nso that it cannot attend to its previous answers (factored CoVe) helps alleviate copying the same\nhallucinations. Overall, our method provides substantial performance gains over the original language\nmodel response just by asking the same model to deliberate on (verify) its answer.  An obvious\nextension to our work is to equip CoVe with tool-use, e.g., to use retrieval augmentation in the\nverification execution step which would likely bring further gains.\n6LIMITATIONS\nWhile our Chain-of-Verification (CoVe) method seeks to reduce hallucinations, it does not remove\nthem completely from generations. This means that CoVe can still generate incorrect or misleading\ninformation for a given query,  even if it improves over the baseline.   We also note that in our\nexperiments we have only addressed hallucinations in the form of directly stated factual inaccuracies.\nHowever, hallucinations could come in other forms, such as during incorrect reasoning steps, as part\nof opinions, etc. We also note that the generations CoVe produces come with verifications which,\nif viewed by the user, add more interpretability to its decisions, but come at the cost of increased\ncomputational expense due to generating more tokens in the output,  similar to other reasoning\nmethods such as Chain-of-Thought.\nOur method seeks to make a large language model produce improved responses by spending more\ntime deliberating to identify its own mistakes. While we have shown this gives clear improvements,\nthe upper bound to the improvement is clearly limited by the overall capabilities of the model, e.g. in\nidentifying and knowing what it knows. In this regard, an orthogonal line of research, as discussed in\nsection 2 is the use of external tools by language models, to gain further information beyond what is\nstored in its weights. While we do not explore that avenue in this work those techniques would likely\nbe fruitful to combine with the findings here.\n9",
    "REFERENCES\nLeonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. Reason first, then\nrespond: Modular generation for knowledge-infused dialogue.arXiv preprint arXiv:2111.05204,\n2021.\nAyush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they’re\nhallucinating references?arXiv preprint arXiv:2305.18248, 2023.\nI Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham\nNeubig,  Pengfei Liu,  et al.   Factool:  Factuality detection in generative ai–a tool augmented\nframework for multi-task and multi-domain scenarios.arXiv preprint arXiv:2307.13528, 2023a.\nI-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig, et al.\nImproving factuality of abstractive summarization via contrastive reward learning.arXiv preprint\narXiv:2307.04507, 2023b.\nRoi Cohen, May Hamri, Mor Geva, and Amir Globerson.  Lm vs lm: Detecting factual errors via\ncross examination.arXiv preprint arXiv:2305.13281, 2023.\nBoris A Galitsky. Truth-o-meter: Collaborating with llm in fighting its hallucinations. 2023.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan,\nVincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what\nlanguage models say, using language models. InProceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 16477–16508, 2023.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration.arXiv preprint arXiv:1904.09751, 2019.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM\nComputing Surveys, 55(12):1–38, 2023.\nWeisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T Kwok.\nBackward reasoning in large language models for verification.arXiv preprint arXiv:2308.07758,\n2023a.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie  Callan,  and  Graham  Neubig.   Active  retrieval  augmented  generation.arXiv  preprint\narXiv:2305.06983, 2023b.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly)\nknow what they know.arXiv preprint arXiv:2207.05221, 2022.\nJack Lanchantin, Shubham Toshniwal, Jason Weston, Arthur Szlam, and Sainbayar Sukhbaatar.\nLearning to reason and memorize with self-notes.arXiv preprint arXiv:2305.00833, 2023.\nHaonan Li, Martin Tomko, Maria Vasardani, and Timothy Baldwin.  Multispanqa:  A dataset for\nmulti-span question answering.  InProceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n1250–1260, 2022.\nKenneth Li, Oam Patel, Fernanda Vi\n ́\negas, Hanspeter Pfister, and Martin Wattenberg. Inference-time\nintervention: Eliciting truthful answers from a language model.arXiv preprint arXiv:2306.03341,\n2023.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su.\nDeductive verification of chain-of-thought reasoning.arXiv preprint arXiv:2306.03872, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.  Self-refine: Iterative refinement\nwith self-feedback.arXiv preprint arXiv:2303.17651, 2023.\n10",
    "Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.  Quest:\nA  retrieval  dataset  of  entity-seeking  queries  with  implicit  set  operations.arXiv  preprint\narXiv:2305.11694, 2023.\nPotsawee Manakul,  Adian Liusie,  and Mark JF Gales.   Selfcheckgpt:  Zero-resource black-box\nhallucination detection for generative large language models.arXiv preprint arXiv:2303.08896,\n2023.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\nin abstractive summarization.arXiv preprint arXiv:2005.00661, 2020.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\nMia Glaese,  Susannah Young,  Lucy Campbell-Gillingham,  Geoffrey Irving,  et al.   Teaching\nlanguage models to support answers with verified quotes.arXiv preprint arXiv:2203.11147, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own\nstep-by-step reasoning.arXiv preprint arXiv:2308.00436, 2023.\nSabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents’\noverconfidence through linguistic calibration.Transactions of the Association for Computational\nLinguistics, 10:857–872, 2022.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.arXiv preprint arXiv:2305.14251, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.Advances in Neural Information Processing Systems,  35:\n27730–27744, 2022.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars\nLiden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language\nmodels with external knowledge and automated feedback.arXiv preprint arXiv:2302.12813, 2023.\nFabio Petroni, Tim Rockt\n ̈\naschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases?arXiv preprint arXiv:1909.01066,\n2019.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models.arXiv preprint arXiv:2210.03350,\n2022.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAnsh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez,\nEsin Durmus, Evan Hubinger, Jackson Kernion, Kamil\n ̇\ne Luko\nˇ\nsi\n ̄\nut\n ̇\ne, et al. Question decomposition\nimproves the faithfulness of model-generated reasoning.arXiv preprint arXiv:2307.11768, 2023.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das,\nSlav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural\nlanguage generation models.Computational Linguistics, pp. 1–66, 2023.\nPaul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu\nGeist, Sertan Girgin, L\n ́\neonard Hussenot, Orgad Keller, et al. Factually consistent summarization\nvia reinforcement learning with textual entailment feedback.arXiv preprint arXiv:2306.00186,\n2023.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle\nOtt, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot.arXiv preprint\narXiv:2004.13637, 2020.\n11",
    "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation\nreduces hallucination in conversation.arXiv preprint arXiv:2104.07567, 2021.\nKai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledge-\nable are large language models (llm)? aka will llms replace knowledge graphs?arXiv preprint\narXiv:2308.10168, 2023a.\nWeiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun Ren. Con-\ntrastive learning reduces hallucination in conversations. InProceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, pp. 13618–13626, 2023b.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\n ́\nee\nLacroix, Baptiste Rozi\n`\nere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.  Llama:  Open and\nefficient foundation language models.arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom.  Llama 2:  Open foundation and fine-tuned chat models,\n2023b.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves\nnine:  Detecting and mitigating hallucinations of llms by validating low-confidence generation.\narXiv preprint arXiv:2307.03987, 2023.\nChaojun Wang and Rico Sennrich.   On exposure bias, hallucination and domain shift in neural\nmachine translation.arXiv preprint arXiv:2005.03642, 2020.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in\nNeural Information Processing Systems, 35:24824–24837, 2022.\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners\nwith self-verification.arXiv preprint arXiv:2212.09561, 2022.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith,\nMari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for\nlanguage model training.arXiv preprint arXiv:2306.01693, 2023.\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language\nmodels via plug-and-play retrieval feedback.arXiv preprint arXiv:2305.14002, 2023.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith.  How language model\nhallucinations can snowball.arXiv preprint arXiv:2305.13534, 2023.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing.  Verify-and-edit:  A\nknowledge-enhanced chain-of-thought framework.arXiv preprint arXiv:2305.03268, 2023.\n12",
    "7COVE- FURTHER DETAILS\nQuery\n       1. Baseline Response\n       4. Final Verified Response\n        2. Plan Verifications\nThe Mexican–American War was an armed conflict between the United States and Mexico from 1846 \nto 1848. It followed in the wake of the 1845 U.S. annexation of Texas, which Mexico considered part of \nits territory in spite of its de facto secession in the 1835 Texas Revolution.\nWhat was the primary cause of the Mexican-American war? \n  - When did the Mexican American war start and end?  \n  - When did the US annex Texas?\n  - When did Texas secede from Mexico?  \n  ... <questions continue..>\nThe Mexican–American War was an armed conflict between the United States and Mexico from 1846 \nto 1848. It followed in the wake of the 1845 U.S. annexation of Texas.\n1846-1848\n1845\n...\n1836\n        3. Execute Verifications\n✅\n❌\n✅\nFigure 3: For longform generation, the Chain-of-Verification (CoVe) Factor + Revise method is the\nmost effective in our longform generation experiments. CoVe Factor + Revise has the model indepen-\ndently identify (cross-check) which facts are consistent with its executed verifications (indicated by\ntickmark and crosses in the figure). With this extra step we aim to disregard the inconsistent facts and\nuse the consistent facts to regenerate the response.\n13",
    "8PROMPTTEMPLATES\nWe provide prompt templates for the longform generation of biographies task below for the different\nsteps and variants of CoVe (see section 3).  Templates for the other tasks are similar, but using\nfew-shot examples from those tasks instead.\n8.1GENERATEBASELINERESPONSE\nQ: Tell me a bio of <person>\nA: <bio of person>\nQ: Tell me a bio of <person>\nA: <bio of person>\nQ: Tell me a bio of <person>\nA: <bio of person>\nQ: Tell me a bio of <person>\nA:\nTable 5: Few-shot prompting with 3 few-shot examples for the longform generation of biographies\ntask. Other tasks use the same standard few-shot setup as well (with 3 examples from that particular\ntask).\n8.2PLANVERIFICATIONS\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\n<fact in passage>, Verification Question\n<fact in passage>, Verification Question\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\n<fact in passage>, Verification Question\n<fact in passage>, Verification Question\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\n<fact in passage>, Verification Question\n<fact in passage>, Verification Question\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\nTable 6: Step (2) of CoVe involves planning the verification questions. In the biography task case we\nsplit the longform generation into its individual passages (e.g. sentences in the biography case, this\nwas done due to excessive context length, which we don’t need to do for the other tasks). The model\nthen generates a verification question for each fact it observes in each passage (a passage may have\nmultiple facts).\n14",
    "8.3EXECUTEVERIFICATIONS\nQ: Verification Question\nA: Answer\nQ: Verification Question\nA: Answer\nQ: Verification Question\nA: Answer\nQ: Verification Question\nA:\nTable 7: In step (3) of CoVe, the model then generates an answer for each of the verification questions.\nAgain we use 3 few-shot examples.\n8.4GENERATEFINALVERIFIEDRESPONSE\nContext: <Original Passage>.\nFrom another source,\n<output of execute verification step: Q + A>\n<output of execute verification step: Q + A>\nResponse: <revised and consistent Passage>\nContext: <Original Passage>.\nFrom another source,\n<output of execute verification step: Q + A>\n<output of execute verification step: Q + A>\nResponse: <revised and consistent Passage>\nContext: <Original Passage>.\nFrom another source,\n<output of execute verification step: Q + A>\n<output of execute verification step: Q + A>\nResponse: <revised and consistent Passage>\nContext: <Original passage>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse:\nTable 8: In step (4) of CoVe (factored) the model is then presented with its original generation (split\ninto passages, e.g. sentences, in the biography case, due to excessive context length which we do not\nneed to do for the other tasks) along with its own verification step results. The model is told that this\ninformation comes from “another source”. The model is required to synthesize a new final answer\nbased on facts that are consistent between the two sources.\n15",
    "8.5FACTOR+REVISE: IDENTIFY WHICH FACTS ARE CONSISTENT\nContext: <Original Fact>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse: CONSISTENT. <Consistent fact>\nContext: <Original Fact>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse: INCONSISTENT.\nContext: <Original Fact>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse: PARTIALLY CONSISTENT. <Consistent part>\nTable 9: In the CoVe (Factor + Revise) variant, as part of step (3) after subsection 8.3, the model is\nmade to explicitly identify which facts are consistent between the two sources. The consistent facts\ncan then be spliced together.\n16",
    "9CHATGPTEXAMPLE SCREENSHOTS\nFigure 4:    ChatGPT generates several hallucinations for this question, e.g.  Hillary Clinton and\nMichael Bloomberg.\nFigure 5:Even when the longform answer is provided for a rewritten query (see query from\nFigure 4), while giving a slightly different answer, ChatGPT still generates several hallucinations for\nthis question, e.g. Hillary Clinton and Michael Bloomberg.\n17",
    "Figure 6:  Shortform questions (which could be verification questions) appear to be answered more\nfactually than the longform answers in Figure 4 and Figure 5.\nFigure 7:   Another example of hallucinations for a different query, e.g., John F. Kennedy Jr was born\nin Washington D.C.\n18",
    "Figure 8:   Examples where questions asking for a fact are answered correctly, but verifying via a\nyes/no question is incorrect (the model tends to agree with the way the question is stated, even if it\nwas stated incorrectly).\n19"
  ]
}