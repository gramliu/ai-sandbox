{
  "key": "NG8MTS58",
  "url": "http://arxiv.org/pdf/2404.19756",
  "metadata": {
    "title": "KAN: Kolmogorov-Arnold Networks",
    "abstract": "  Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.\n",
    "published": "2024-04-30T17:58:29Z"
  },
  "text": [
    "KAN: Kolmogorov–Arnold Networks\nZiming Liu\n1,4∗\nYixuan Wang\n2\nSachin Vaidya\n1\nFabian Ruehle\n3,4\nJames Halverson\n3,4\nMarin Solja\nˇ\nci\n ́\nc\n1,4\nThomas Y. Hou\n2\nMax Tegmark\n1,4\n1\nMassachusetts Institute of Technology\n2\nCalifornia Institute of Technology\n3\nNortheastern University\n4\nThe NSF Institute for Artificial Intelligence and Fundamental Interactions\nAbstract\nInspired  by  the  Kolmogorov-Arnold  representation  theorem,  we  propose  Kolmogorov-\nArnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).\nWhile MLPs havefixedactivation functions onnodes(“neurons”), KANs havelearnable\nactivation functions onedges(“weights”).   KANs have no linear weights at all – every\nweight parameter is replaced by a univariate function parametrized as a spline.  We show\nthat this seemingly simple change makes KANs outperform MLPs in terms of accuracy\nand interpretability.  For accuracy, much smaller KANs can achieve comparable or better\naccuracy than much larger MLPs in data fitting and PDE solving.  Theoretically and em-\npirically, KANs possess faster neural scaling laws than MLPs.  For interpretability, KANs\ncan be intuitively visualized and can easily interact with human users.  Through two ex-\namples in mathematics and physics, KANs are shown to be useful “collaborators” helping\nscientists (re)discover mathematical and physical laws.  In summary, KANs are promising\nalternatives for MLPs, opening opportunities for further improving today’s deep learning\nmodels which rely heavily on MLPs.\nTheorem\nFormula \n(Shallow)\nModel \n(Shallow)\nModel \n(Deep)\nMulti-Layer Perceptron (MLP)\nKolmogorov-Arnold Network (KAN)\nUniversal Approximation Theorem\nKolmogorov-Arnold Representation Theorem\nf(x)≈\nN(ε)\n∑\ni=1\na\ni\nσ(w\ni\n⋅x+b\ni\n)\nf(x)=\n2n+1\n∑\nq=1\nΦ\nq\nn\n∑\np=1\nφ\nq,p\n(x\np\n)\nModel\nfixed activation functions \non nodes \nFormula \n(Deep)\nlearnable weights \non edges \nlearnable activation functions \non edges \nsum operation on nodes \nMLP(x)=(W\n3\n∘σ\n2\n∘W\n2\n∘σ\n1\n∘W\n1\n)(x)KAN(x)=(Φ\n3\n∘Φ\n2\n∘Φ\n1\n)(x)\nW\n1\nσ\n1\nW\n2\nσ\n2\nW\n3\nΦ\n3\nΦ\n2\nΦ\n1\nx\nx\nMLP(x)\nKAN(x)\nlinear,  \nlearnable\nnonlinear, \nfixed\nnonlinear, \nlearnable\n(a)\n(b)\n(c)\n(d)\nFigure 0.1: Multi-Layer Perceptrons (MLPs) vs. Kolmogorov-Arnold Networks (KANs)\n∗\nzmliu@mit.edu\nPreprint. Under review.\narXiv:2404.19756v2  [cs.LG]  2 May 2024",
    "1    Introduction\nMulti-layer perceptrons (MLPs) [1, 2, 3], also known as fully-connected feedforward neural net-\nworks, are foundational building blocks of today’s deep learning models. The importance of MLPs\ncan never be overstated, since they are the default models in machine learning for approximating\nnonlinear functions, due to their expressive power guaranteed by the universal approximation theo-\nrem [3]. However, are MLPs the best nonlinear regressors we can build? Despite the prevalent use of\nMLPs, they have significant drawbacks. In transformers [4] for example, MLPs consume almost all\nnon-embedding parameters and are typically less interpretable (relative to attention layers) without\npost-analysis tools [5].\nWe  propose  a  promising  alternative  to  MLPs,  called  Kolmogorov-Arnold  Networks  (KANs).\nWhereas  MLPs  are  inspired  by  the  universal  approximation  theorem,  KANs  are  inspired  by  the\nKolmogorov-Arnold representation theorem [6, 7].  Like MLPs, KANs have fully-connected struc-\ntures.  However, while MLPs place fixed activation functions onnodes(“neurons”), KANs place\nlearnable activation functions onedges(“weights”), as illustrated in Figure 0.1.  As a result, KANs\nhave no linear weight matrices at all: instead, each weight parameter is replaced by a learnable 1D\nfunction parametrized as a spline. KANs’ nodes simply sum incoming signals without applying any\nnon-linearities.  One might worry that KANs are hopelessly expensive, since each MLP’s weight\nparameter becomes KAN’s spline function. Fortunately, KANs usually allow much smaller compu-\ntation graphs than MLPs.  For example, we show that for PDE solving, a 2-Layer width-10 KAN\nis100 times more accuratethan a 4-Layer width-100 MLP (10\n−7\nvs10\n−5\nMSE) and100 times\nmore parameter efficient(10\n2\nvs10\n4\nparameters).\nUnsurprisingly, the possibility of using Kolmogorov-Arnold representation theorem to build neural\nnetworks has been studied [8, 9, 10, 11, 12, 13].   However,  most work has stuck with the origi-\nnal depth-2 width-(2n+ 1) representation, and did not have the chance to leverage more modern\ntechniques (e.g., back propagation) to train the networks.  Our contribution lies in generalizing the\noriginal  Kolmogorov-Arnold  representation  to  arbitrary  widths  and  depths,  revitalizing  and  con-\ntextualizing it in today’s deep learning world, as well as using extensive empirical experiments to\nhighlight its potential role as a foundation model for AI + Science due to its accuracy and inter-\npretability.\nDespite their elegant mathematical interpretation,  KANs are nothing more than combinations of\nsplines and MLPs, leveraging their respective strengths and avoiding their respective weaknesses.\nSplines are accurate for low-dimensional functions, easy to adjust locally, and able to switch between\ndifferent  resolutions.   However,  splines  have  a  serious  curse  of  dimensionality  (COD)  problem,\nbecause of their inability to exploit compositional structures.  MLPs, On the other hand, suffer less\nfrom COD thanks to their feature learning,  but are less accurate than splines in low dimensions,\nbecause of their inability to optimize univariate functions.  To learn a function accurately, a model\nshould not only learn the compositional structure (externaldegrees of freedom),  but should also\napproximate well the univariate functions (internaldegrees of freedom).  KANs are such models\nsince they have MLPs on the outside and splines on the inside. As a result, KANs can not only learn\nfeatures (thanks to their external similarity to MLPs), but can also optimize these learned features to\ngreat accuracy (thanks to their internal similarity to splines). For example, given a high dimensional\nfunction\nf(x\n1\n,···,x\nN\n) = exp\n \n1\nN\nN\nX\ni=1\nsin\n2\n(x\ni\n)\n!\n,(1.1)\n2",
    "Figure 2.1: Our proposed Kolmogorov-Arnold networks are in honor of two great late mathematicians, Andrey\nKolmogorov and Vladimir Arnold. KANs are mathematically sound, accurate and interpretable.\nsplines would fail for largeNdue to COD; MLPs can potentially learn the the generalized additive\nstructure, but they are very inefficient for approximating the exponential and sine functions with say,\nReLU activations.  In contrast, KANs can learn both the compositional structure and the univariate\nfunctions quite well, hence outperforming MLPs by a large margin (see Figure 3.1).\nThroughout this paper, we will use extensive numerical experiments to show that KANs can lead\nto remarkable accuracy and interpretability improvement over MLPs. The organization of the paper\nis illustrated in Figure 2.1.  In Section 2, we introduce the KAN architecture and its mathematical\nfoundation, introduce network simplification techniques to make KANs interpretable, and introduce\na grid extension technique to make KANs increasingly more accurate.  In Section 3, we show that\nKANs are more accurate than MLPs for data fitting and PDE solving: KANs can beat the curse of\ndimensionality when there is a compositional structure in data, achieving much better scaling laws\nthan MLPs. In Section 4, we show that KANs are interpretable and can be used for scientific discov-\neries. We use two examples from mathematics (knot theory) and physics (Anderson localization) to\ndemonstrate that KANs can be helpful “collaborators” for scientists to (re)discover math and phys-\nical laws.  Section 5 summarizes related works.  In Section 6, we conclude by discussing broad im-\npacts and future directions. Codes are available athttps://github.com/KindXiaoming/pykan\nand can also be installed viapip install pykan.\n2    Kolmogorov–Arnold Networks (KAN)\nMulti-Layer Perceptrons (MLPs) are inspired by the universal approximation theorem.  We instead\nfocus on the Kolmogorov-Arnold representation theorem, which can be realized by a new type of\nneural network called Kolmogorov-Arnold networks (KAN). We review the Kolmogorov-Arnold\ntheorem in Section 2.1, to inspire the design of Kolmogorov-Arnold Networks in Section 2.2.  In\nSection 2.3, we provide theoretical guarantees for the expressive power of KANs and their neural\nscaling laws.  In Section 2.4, we propose a grid extension technique to make KANs increasingly\nmore accurate. In Section 2.5, we propose simplification techniques to make KANs interpretable.\n2.1    Kolmogorov-Arnold Representation theorem\nVladimir Arnold and Andrey Kolmogorov established that iffis a multivariate continuous function\non a bounded domain, thenfcan be written as a finite composition of continuous functions of a\n3",
    "Figure  2.2:  Left:  Notations  of  activations  that  flow  through  the  network.   Right:  an  activation  function  is\nparameterized as a B-spline, which allows switching between coarse-grained and fine-grained grids.\nsingle variable and the binary operation of addition. More specifically, for a smoothf: [0,1]\nn\n→R,\nf(x) =f(x\n1\n,···,x\nn\n) =\n2n+1\nX\nq=1\nΦ\nq\n \nn\nX\np=1\nφ\nq,p\n(x\np\n)\n!\n,(2.1)\nwhereφ\nq,p\n: [0,1]→RandΦ\nq\n:R→R.  In a sense, they showed that the only true multivariate\nfunction is addition, since every other function can be written using univariate functions and sum.\nOne might naively consider this great news for machine learning: learning a high-dimensional func-\ntion boils down to learning a polynomial number of 1D functions.  However, these 1D functions\ncan be non-smooth and even fractal, so they may not be learnable in practice [14].  Because of this\npathological behavior, the Kolmogorov-Arnold representation theorem was basically sentenced to\ndeath in machine learning, regarded as theoretically sound but practically useless [14].\nHowever, we are more optimistic about the usefulness of the Kolmogorov-Arnold theorem for ma-\nchine learning. First of all, we need not stick to the original Eq. (2.1) which has only two-layer non-\nlinearities and a small number of terms (2n+ 1) in the hidden layer: we will generalize the network\nto arbitrary widths and depths.  Secondly, most functions in science and daily life are often smooth\nand have sparse compositional structures, potentially facilitating smooth Kolmogorov-Arnold rep-\nresentations.  The philosophy here is close to the mindset of physicists, who often care more about\ntypical cases rather than worst cases. After all, our physical world and machine learning tasks must\nhave structures to make physics and machine learning useful or generalizable at all [15].\n2.2    KAN architecture\nSuppose we have a supervised learning task consisting of input-output pairs{x\ni\n,y\ni\n}, where we want\nto findfsuch thaty\ni\n≈f(x\ni\n)for all data points.  Eq. (2.1) implies that we are done if we can find\nappropriate univariate functionsφ\nq,p\nandΦ\nq\n.  This inspires us to design a neural network which\nexplicitly parametrizes Eq. (2.1).  Since all functions to be learned are univariate functions, we can\nparametrize each 1D function as a B-spline curve, with learnable coefficients of local B-spline basis\nfunctions (see Figure 2.2 right).  Now we have a prototype of KAN, whose computation graph is\nexactly specified by Eq. (2.1) and illustrated in Figure 0.1 (b) (with the input dimensionn=  2),\nappearing as a two-layer neural network with activation functions placed on edges instead of nodes\n(simple summation is performed on nodes), and with width2n+ 1in the middle layer.\n4",
    "As mentioned, such a network is known to be too simple to approximate any function arbitrarily\nwell in practice with smooth splines!  We therefore generalize our KAN to be wider and deeper.\nIt is not immediately clear how to make KANs deeper, since Kolmogorov-Arnold representations\ncorrespond  to  two-layer  KANs.   To  the  best  of  our  knowledge,  there  is  not  yet  a  “generalized”\nversion of the theorem that corresponds to deeper KANs.\nThe breakthrough occurs when we notice the analogy between MLPs and KANs. In MLPs, once we\ndefine a layer (which is composed of a linear transformation and nonlinearties), we can stack more\nlayers to make the network deeper.  To build deep KANs, we should first answer:  “what is a KAN\nlayer?” It turns out that a KAN layer withn\nin\n-dimensional inputs andn\nout\n-dimensional outputs can\nbe defined as a matrix of 1D functions\nΦ={φ\nq,p\n},   p= 1,2,···,n\nin\n,   q= 1,2···,n\nout\n,(2.2)\nwhere the functionsφ\nq,p\nhave trainable parameters, as detaild below. In the Kolmogov-Arnold theo-\nrem, the inner functions form a KAN layer withn\nin\n=nandn\nout\n= 2n+ 1, and the outer functions\nform a KAN layer withn\nin\n= 2n+ 1andn\nout\n= 1. So the Kolmogorov-Arnold representations in\nEq. (2.1) are simply compositions of two KAN layers. Now it becomes clear what it means to have\ndeeper Kolmogorov-Arnold representations: simply stack more KAN layers!\nLet us introduce some notation. This paragraph will be a bit technical, but readers can refer to Fig-\nure 2.2 (left) for a concrete example and intuitive understanding. The shape of a KAN is represented\nby an integer array\n[n\n0\n,n\n1\n,···,n\nL\n],(2.3)\nwheren\ni\nis the number of nodes in thei\nth\nlayer of the computational graph.  We denote thei\nth\nneuron in thel\nth\nlayer by(l,i), and the activation value of the(l,i)-neuron byx\nl,i\n. Between layerl\nand layerl+ 1, there aren\nl\nn\nl+1\nactivation functions: the activation function that connects(l,i)and\n(l+ 1,j)is denoted by\nφ\nl,j,i\n,  l= 0,···,L−1,  i= 1,···,n\nl\n,  j= 1,···,n\nl+1\n.(2.4)\nThe  pre-activation  ofφ\nl,j,i\nis  simplyx\nl,i\n;  the  post-activation  ofφ\nl,j,i\nis  denoted  by ̃x\nl,j,i\n≡\nφ\nl,j,i\n(x\nl,i\n).  The activation value of the(l+ 1,j)neuron is simply the sum of all incoming post-\nactivations:\nx\nl+1,j\n=\nn\nl\nX\ni=1\n ̃x\nl,j,i\n=\nn\nl\nX\ni=1\nφ\nl,j,i\n(x\nl,i\n),   j= 1,···,n\nl+1\n.(2.5)\nIn matrix form, this reads\nx\nl+1\n=\n\n\n\n\n\n\nφ\nl,1,1\n(·)φ\nl,1,2\n(·)···φ\nl,1,n\nl\n(·)\nφ\nl,2,1\n(·)φ\nl,2,2\n(·)···φ\nl,2,n\nl\n(·)\n.\n.\n.\n.\n.\n.\n.\n.\n.\nφ\nl,n\nl+1\n,1\n(·)φ\nl,n\nl+1\n,2\n(·)···φ\nl,n\nl+1\n,n\nl\n(·)\n\n\n\n\n\n\n|\n{z}\nΦ\nl\nx\nl\n,(2.6)\nwhereΦ\nl\nis the function matrix corresponding to thel\nth\nKAN layer.  A general KAN network is a\ncomposition ofLlayers: given an input vectorx\n0\n∈R\nn\n0\n, the output of KAN is\nKAN(x) = (Φ\nL−1\n◦Φ\nL−2\n◦···◦Φ\n1\n◦Φ\n0\n)x.(2.7)\n5",
    "We can also rewrite the above equation to make it more analogous to Eq. (2.1), assuming output\ndimensionn\nL\n= 1, and definef(x)≡KAN(x):\nf(x) =\nn\nL−1\nX\ni\nL−1\n=1\nφ\nL−1,i\nL\n,i\nL−1\n\n\nn\nL−2\nX\ni\nL−2\n=1\n···\n \nn\n2\nX\ni\n2\n=1\nφ\n2,i\n3\n,i\n2\n \nn\n1\nX\ni\n1\n=1\nφ\n1,i\n2\n,i\n1\n \nn\n0\nX\ni\n0\n=1\nφ\n0,i\n1\n,i\n0\n(x\ni\n0\n)\n!!!\n···\n\n\n,\n(2.8)\nwhich is quite cumbersome.  In contrast, our abstraction of KAN layers and their visualizations are\ncleaner and intuitive.  The original Kolmogorov-Arnold representation Eq. (2.1) corresponds to a\n2-Layer KAN with shape[n,2n+ 1,1]. Notice that all the operations are differentiable, so we can\ntrain KANs with back propagation. For comparison, an MLP can be written as interleaving of affine\ntransformationsWand non-linearitiesσ:\nMLP(x) = (W\nL−1\n◦σ◦W\nL−2\n◦σ◦···◦W\n1\n◦σ◦W\n0\n)x.(2.9)\nIt is clear that MLPs treat linear transformations and nonlinearities separately asWandσ, while\nKANs treat them all together inΦ. In Figure 0.1 (c) and (d), we visualize a three-layer MLP and a\nthree-layer KAN, to clarify their differences.\nImplementation details.Although a KAN layer Eq. (2.5) looks extremely simple, it is non-trivial\nto make it well optimizable. The key tricks are:\n(1)  Residual activation functions.   We include a basis functionb(x)(similar to residual connec-\ntions) such that the activation functionφ(x)is the sum of the basis functionb(x)and the spline\nfunction:\nφ(x) =w(b(x) + spline(x)).(2.10)\nWe set\nb(x) = silu(x) =x/(1 +e\n−x\n)(2.11)\nin most cases.spline(x)is parametrized as a linear combination of B-splines such that\nspline(x) =\nX\ni\nc\ni\nB\ni\n(x)(2.12)\nwherec\ni\ns  are  trainable.   In  principlewis  redundant  since  it  can  be  absorbed  intob(x)and\nspline(x). However, we still include thiswfactor to better control the overall magnitude of the\nactivation function.\n(2)  Initialization  scales.   Each  activation  function  is  initialized  to  havespline(x)≈0\n2\n.wis\ninitialized according to the Xavier initialization, which has been used to initialize linear layers\nin MLPs.\n(3)  Update of spline grids.   We update each grid on the fly according to its input activations,  to\naddress the issue that splines are defined on bounded regions but activation values can evolve\nout of the fixed region during training\n3\n.\nParameter count.For simplicity, let us assume a network\n(1)  of depthL,\n2\nThis is done by drawing B-spline coefficientsc\ni\n∼N(0,σ\n2\n)with a smallσ, typically we setσ= 0.1.\n3\nOther possibilities are: (a) the grid is learnable with gradient descent, e.g., [16]; (b) use normalization such\nthat the input range is fixed. We tried (b) at first but its performance is inferior to our current approach.\n6",
    "PaperIdeaScaling exponentα\nSharma & Kaplan [17]Intrinsic dimensionality(k+ 1)/d\nMichaud et al. [18]maximum arity(k+ 1)/2\nPoggio et al. [14]compositional sparsitym/2\nOursK-A representationk+ 1\nTable  1:  Scaling  exponents  from  different  theoriesℓ∝N\n−α\n.ℓ:  test  RMSE  loss,N:  number  of  model\nparameters,d: input intrinsic dimension,k: order of piecewise polynomial,m: derivative order as in function\nclassW\nm\n.\n(2)  with layers of equal widthn\n0\n=n\n1\n=···=n\nL\n=N,\n(3)  with each spline of orderk(usuallyk= 3) onGintervals (forG+ 1grid points).\nThen there are in totalO(N\n2\nL(G+k))∼O(N\n2\nLG)parameters. In contrast, an MLP with depth\nLand widthNonly needsO(N\n2\nL)parameters, which appears to be more efficient than KAN. For-\ntunately, KANs usually require much smallerNthan MLPs, which not only saves parameters, but\nalso achieves better generalization (see e.g., Figure 3.1 and 3.3) and facilitates interpretability.  We\nremark that for 1D problems, we can takeN=L= 1and the KAN network in our implementation\nis nothing but a spline approximation.  For higher dimensions, we characterize the generalization\nbehavior of KANs with a theorem below.\n2.3    KAN’s Approximation Abilities and Scaling Laws\nRecall  that  in  Eq.  (2.1),  the  2-Layer  width-(2n+ 1)representation  may  be  non-smooth.   How-\never, deeper representations may bring the advantages of smoother activations.  For example, the\n4-variable function\nf(x\n1\n,x\n2\n,x\n3\n,x\n4\n) = exp\n\u0000\nsin(x\n2\n1\n+x\n2\n2\n) + sin(x\n2\n3\n+x\n2\n4\n)\n\u0001\n(2.13)\ncan be smoothly represented by a[4,2,1,1]KAN which is 3-Layer, but may not admit a 2-Layer\nKAN with smooth activations. To facilitate an approximation analysis, we still assume smoothness\nof activations,  but allow the representations to be arbitrarily wide and deep,  as in Eq. (2.7).   To\nemphasize the dependence of our KAN on the finite set of grid points, we useΦ\nG\nl\nandΦ\nG\nl,i,j\nbelow\nto replace the notationΦ\nl\nandΦ\nl,i,j\nused in Eq. (2.5) and (2.6).\nTheorem 2.1(Approximation theory, KAT).Letx=  (x\n1\n,x\n2\n,···,x\nn\n).  Suppose that a function\nf(x)admits a representation\nf= (Φ\nL−1\n◦Φ\nL−2\n◦···◦Φ\n1\n◦Φ\n0\n)x,(2.14)\nas in Eq.(2.7), where each one of theΦ\nl,i,j\nare(k+ 1)-times continuously differentiable.  Then\nthere exists a constantCdepending onfand its representation, such that we have the following\napproximation bound in terms of the grid sizeG:  there existk-th order B-spline functionsΦ\nG\nl,i,j\nsuch that for any0≤m≤k, we have the bound\n∥f−(Φ\nG\nL−1\n◦Φ\nG\nL−2\n◦···◦Φ\nG\n1\n◦Φ\nG\n0\n)x∥\nC\nm\n≤CG\n−k−1+m\n.(2.15)\nHere we adopt the notation ofC\nm\n-norm measuring the magnitude of derivatives up to orderm:\n∥g∥\nC\nm\n=  max\n|β|≤m\nsup\nx∈[0,1]\nn\n\f\n\f\nD\nβ\ng(x)\n\f\n\f\n.\nProof.By the classical 1D B-spline theory [19] and the fact thatΦ\nl,i,j\nas continuous functions can\nbe uniformly bounded on a bounded domain, we know that there exist finite-grid B-spline functions\n7",
    "Φ\nG\nl,i,j\nsuch that for any0≤m≤k,\n∥(Φ\nl,i,j\n◦Φ\nl−1\n◦Φ\nl−2\n◦···◦Φ\n1\n◦Φ\n0\n)x−(Φ\nG\nl,i,j\n◦Φ\nl−1\n◦Φ\nl−2\n◦···◦Φ\n1\n◦Φ\n0\n)x∥\nC\nm\n≤CG\n−k−1+m\n,\nwith a constantCindependent ofG. We fix those B-spline approximations. Therefore we have that\nthe residueR\nl\ndefined via\nR\nl\n:\n= (Φ\nG\nL−1\n◦···◦Φ\nG\nl+1\n◦Φ\nl\n◦Φ\nl−1\n◦···◦Φ\n0\n)x−(Φ\nG\nL−1\n◦···◦Φ\nG\nl+1\n◦Φ\nG\nl\n◦Φ\nl−1\n◦···◦Φ\n0\n)x\nsatisfies\n∥R\nl\n∥\nC\nm\n≤CG\n−k−1+m\n,\nwith a constant independent ofG. Finally notice that\nf−(Φ\nG\nL−1\n◦Φ\nG\nL−2\n◦···◦Φ\nG\n1\n◦Φ\nG\n0\n)x=R\nL−1\n+R\nL−2\n+···+R\n1\n+R\n0\n,\nwe know that (2.15) holds.\nWe know that asymptotically, provided that the assumption in Theorem 2.1 holds, KANs with fi-\nnite grid size can approximate the function well with a residue rateindependent of the dimension,\nhence beating curse of dimensionality!This comes naturally since we only use splines to approx-\nimate 1D functions.  In particular, form= 0, we recover the accuracy inL\n∞\nnorm, which in turn\nprovides a bound of RMSE on the finite domain, which gives a scaling exponentk+ 1. Of course,\nthe constantCis dependent on the representation; hence it will depend on the dimension.  We will\nleave the discussion of the dependence of the constant on the dimension as a future work.\nWe remark that although the Kolmogorov-Arnold theorem Eq. (2.1) corresponds to a KAN repre-\nsentation with shape[d,2d+ 1,1], its functions are not necessarily smooth. On the other hand, if we\nare able to identify a smooth representation (maybe at the cost of extra layers or making the KAN\nwider than the theory prescribes), then Theorem 2.1 indicates that we can beat the curse of dimen-\nsionality (COD). This should not come as a surprise since we can inherently learn the structure of\nthe function and make our finite-sample KAN approximation interpretable.\nNeural scaling laws:  comparison to other theories.Neural scaling laws are the phenomenon\nwhere test loss decreases with more model parameters, i.e.,ℓ∝N\n−α\nwhereℓis test RMSE,Nis\nthe number of parameters, andαis the scaling exponent.  A largerαpromises more improvement\nby simply scaling up the model.  Different theories have been proposed to predictα.  Sharma &\nKaplan [17] suggest thatαcomes from data fitting on an input manifold of intrinsic dimensionality\nd.   If the model function class is piecewise polynomials of orderk(k=  1for ReLU), then the\nstandard approximation theory impliesα= (k+ 1)/dfrom the approximation theory.  This bound\nsuffers from the curse of dimensionality, so people have sought other bounds independent ofdby\nleveraging compositional structures.   In particular,  Michaud et al. [18] considered computational\ngraphs that only involve unary (e.g., squared, sine, exp) and binary (+and×) operations, finding\nα= (k+ 1)/d\n∗\n= (k+ 1)/2, whered\n∗\n= 2is the maximum arity. Poggio et al. [14] leveraged the\nidea of compositional sparsity and proved that given function classW\nm\n(function whose derivatives\nare continuous up tom-th order), one needsN=O(ε\n−\n2\nm\n)number of parameters to achieve errorε,\nwhich is equivalent toα=\nm\n2\n. Our approach, which assumes the existence of smooth Kolmogorov-\nArnold representations, decomposes the high-dimensional function into several 1D functions, giving\nα=k+1(wherekis the piecewise polynomial order of the splines). We choosek= 3cubic splines\nsoα= 4which is the largest and best scaling exponent compared to other works. We will show in\nSection 3.1 that this boundα= 4can in fact be achieved empirically with KANs, while previous\n8",
    "020040060080010001200140016001800\nstep\n10\n9\n10\n7\n10\n5\n10\n3\n10\n1\n10\n1\nRMSE\ngrid=3grid=5grid=10grid=20grid=50grid=100grid=200grid=500grid=1000\ninterpolation threshold\nKAN [2,5,1]\ntrain\ntest\n020040060080010001200140016001800\nstep\n10\n9\n10\n7\n10\n5\n10\n3\n10\n1\n10\n1\nRMSE\ngrid=3grid=5grid=10grid=20grid=50grid=100grid=200grid=500grid=1000\ninterpolation threshold\nKAN [2,1,1]\ntrain\ntest\n10\n1\n10\n2\ngrid size G\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\ntest loss\nG\n2\nG\n3\nG\n4\nKAN [2,5,1] sqrt(mean of squared)\nKAN [2,1,1] sqrt(mean of squared)\nKAN [2,1,1] sqrt(median of squared)\n10\n1\n10\n2\n10\n3\ngrid size G\n10\n1\n10\n0\ntraining time (seconds/step)\nKAN [2,5,1]\nKAN [2,1,1]\nFitting f(x, y) = exp(sin(  x) + y\n2\n)\nFigure 2.3: We can make KANs more accurate by grid extension (fine-graining spline grids).  Top left (right):\ntraining dynamics of a[2,5,1]([2,1,1]) KAN. Both models display staircases in their loss curves, i.e., loss\nsuddently drops then plateaus after grid extension.  Bottom left:  test RMSE follows scaling laws against grid\nsizeG. Bottom right: training time scales favorably with grid sizeG.\nwork  [18]  reported  that  MLPs  have  problems  even  saturating  slower  bounds  (e.g.,α=  1)  and\nplateau quickly. Of course, we can increasekto match the smoothness of functions, but too highk\nmight be too oscillatory, leading to optimization issues.\nComparison between KAT and UAT.The power of fully-connected neural networks is justified by\nthe universal approximation theorem (UAT), which states that given a function and error tolerance\nε >0, a two-layer network withk > N(ε)neurons can approximate the function within errorε.\nHowever, the UAT guarantees no bound for howN(ε)scales withε. Indeed, it suffers from the COD,\nandNhas been shown to grow exponentially withdin some cases [15].  The difference between\nKAT  and  UAT  is  a  consequence  that  KANs  take  advantage  of  the  intrinsically  low-dimensional\nrepresentation  of  the  function  while  MLPs  do  not.   Indeed,  we  will  show  that  KANs  are  nicely\naligned with symbolic functions while MLPs are not.\n2.4    For accuracy: Grid Extension\nIn principle, a spline can be made arbitrarily accurate to a target function as the grid can be made\narbitrarily fine-grained.  This good feature is inherited by KANs.  By contrast, MLPs do not have\nthe  notion  of  “fine-graining”.   Admittedly,  increasing  the  width  and  depth  of  MLPs  can  lead  to\nimprovement in performance (“neural scaling laws”).  However, these neural scaling laws are slow\n(discussed in the last section).  They are also expensive to obtain, because models of varying sizes\nare trained independently. By contrast, for KANs, one can first train a KAN with fewer parameters\nand then extend it to a KAN with more parameters by simply making its spline grids finer, without\nthe need to retraining the larger model from scratch.\nWe  next  describe  how  to  perform  grid  extension  (illustrated  in  Figure  2.2  right),  which  is  basi-\ncally  fitting  a  new  fine-grained  spline  to  an  old  coarse-grained  spline.   Suppose  we  want  to  ap-\nproximate a 1D functionfin a bounded region[a,b]with B-splines of orderk.  A coarse-grained\ngrid withG\n1\nintervals has grid points at{t\n0\n=a,t\n1\n,t\n2\n,···,t\nG\n1\n=b},  which is augmented to\n9",
    "{t\n−k\n,···,t\n−1\n,t\n0\n,···,t\nG\n1\n,t\nG\n1\n+1\n,···,t\nG\n1\n+k\n}.  There areG\n1\n+kB-spline basis functions, with\nthei\nth\nB-splineB\ni\n(x)being non-zero only on[t\n−k+i\n,t\ni+1\n] (i=  0,···,G\n1\n+k−1).   Thenf\non the coarse grid is expressed in terms of linear combination of these B-splines basis functions\nf\ncoarse\n(x)  =\nP\nG\n1\n+k−1\ni=0\nc\ni\nB\ni\n(x).  Given a finer grid withG\n2\nintervals,fon the fine grid is cor-\nrespondinglyf\nfine\n(x)  =\nP\nG\n2\n+k−1\nj=0\nc\n′\nj\nB\n′\nj\n(x).  The parametersc\n′\nj\ns can be initialized from the pa-\nrametersc\ni\nby minimizing the distance betweenf\nfine\n(x)tof\ncoarse\n(x)(over some distribution of\nx):\n{c\n′\nj\n}= argmin\n{c\n′\nj\n}\nE\nx∼p(x)\n\n\nG\n2\n+k−1\nX\nj=0\nc\n′\nj\nB\n′\nj\n(x)−\nG\n1\n+k−1\nX\ni=0\nc\ni\nB\ni\n(x)\n\n\n2\n,(2.16)\nwhich can be implemented by the least squares algorithm. We perform grid extension for all splines\nin a KAN independently.\nToy example:  staricase-like loss curves.We use a toy examplef(x,y) = exp(sin(πx) +y\n2\n)to\ndemonstrate the effect of grid extension. In Figure 2.3 (top left), we show the train and test RMSE for\na[2,5,1]KAN. The number of grid points starts as 3, increases to a higher value every 200 LBFGS\nsteps, ending up with 1000 grid points. It is clear that every time fine graining happens, the training\nloss drops faster than before (except for the finest grid with 1000 points, where optimization ceases\nto work probably due to bad loss landscapes).  However, the test losses first go down then go up,\ndisplaying a U-shape, due to the bias-variance tradeoff (underfitting vs. overfitting). We conjecture\nthat the optimal test loss is achieved at the interpolation threshold when the number of parameters\nmatch the number of data points. Since our training samples are 1000 and the total parameters of a\n[2,5,1]KAN is15G(Gis the number of grid intervals), we expect the interpolation threshold to be\nG= 1000/15≈67, which roughly agrees with our experimentally observed valueG∼50.\nSmall KANs generalize better.Is this the best test performance we can achieve?  Notice that the\nsynthetic task can be represented exactly by a[2,1,1]KAN, so we train a[2,1,1]KAN and present\nthe training dynamics in Figure 2.3 top right.  Interestingly, it can achieve even lower test losses\nthan the[2,5,1]KAN, with clearer staircase structures and the interpolation threshold is delayed\nto a larger grid size as a result of fewer parameters.  This highlights a subtlety of choosing KAN\narchitectures.  If we do not know the problem structure, how can we determine the minimal KAN\nshape?  In Section 2.5, we will propose a method to auto-discover such minimal KAN architecture\nvia regularization and pruning.\nScaling laws: comparison with theory.We are also interested in how the test loss decreases as the\nnumber of grid parameters increases.  In Figure 2.3 (bottom left), a [2,1,1] KAN scales roughly as\ntest RMSE∝G\n−3\n. However, according to the Theorem 2.1, we would expecttest RMSE∝G\n−4\n.\nWe found that the errors across samples are not uniform.  This is probably attributed to boundary\neffects [18]. In fact, there are a few samples that have significantly larger errors than others, making\nthe overall scaling slow down.  If we plot the square root of themedian(notmean) of the squared\nlosses, we get a scaling closer toG\n−4\n.  Despite this suboptimality (probably due to optimization),\nKANs still have much better scaling laws than MLPs, for data fitting (Figure 3.1) and PDE solving\n(Figure 3.3). In addition, the training time scales favorably with the number of grid pointsG, shown\nin Figure 2.3 bottom right\n4\n.\nExternal vs Internal degrees of freedom.A new concept that KANs highlights is a distinction\nbetween external versus internal degrees of freedom (parameters). The computational graph of how\n4\nWhenG= 1000, training becomes significantly slower, which is specific to the use of the LBFGS opti-\nmizer with line search. We conjecture that the loss landscape becomes bad forG= 1000, so line search with\ntrying to find an optimal step size within maximal iterations without early stopping.\n10",
    "nodes are connected represents external degrees of freedom (“dofs”), while the grid points inside\nan activation function are internal degrees of freedom.  KANs benefit from the fact that they have\nboth external dofs and internal dofs.  External dofs (that MLPs also have but splines do not) are\nresponsible for learning compositional structures of multiple variables.  Internal dofs (that splines\nalso have but MLPs do not) are responsible for learning univariate functions.\n2.5    For Interpretability: Simplifying KANs and Making them interactive\nOne loose end from the last subsection is that we do not know how to choose the KAN shape that\nbest matches the structure of a dataset. For example, if we know that the dataset is generated via the\nsymbolic formulaf(x,y) = exp(sin(πx) +y\n2\n), then we know that a[2,1,1]KAN is able to express\nthis function.  However, in practice we do not know the information a priori, so it would be nice to\nhave approaches to determine this shape automatically. The idea is to start from a large enough KAN\nand train it with sparsity regularization followed by pruning. We will show that these pruned KANs\nare much more interpretable than non-pruned ones.  To make KANs maximally interpretable, we\npropose a few simplification techniques in Section 2.5.1, and an example of how users can interact\nwith KANs to make them more interpretable in Section 2.5.2.\n2.5.1    Simplification techniques\n1.  Sparsification.For MLPs, L1 regularization of linear weights is used to favor sparsity.  KANs\ncan adapt this high-level idea, but need two modifications:\n(1)  There is no linear “weight” in KANs. Linear weights are replaced by learnable activation func-\ntions, so we should define the L1 norm of these activation functions.\n(2)  We find L1 to be insufficient for sparsification of KANs; instead an additional entropy regular-\nization is necessary (see Appendix C for more details).\nWe define the L1 norm of an activation functionφto be its average magnitude over itsN\np\ninputs,\ni.e.,\n|φ|\n1\n≡\n1\nN\np\nN\np\nX\ns=1\n\f\n\f\n\f\nφ(x\n(s)\n)\n\f\n\f\n\f\n.(2.17)\nThen for a KAN layerΦwithn\nin\ninputs andn\nout\noutputs, we define the L1 norm ofΦto be the\nsum of L1 norms of all activation functions, i.e.,\n|Φ|\n1\n≡\nn\nin\nX\ni=1\nn\nout\nX\nj=1\n|φ\ni,j\n|\n1\n.(2.18)\nIn addition, we define the entropy ofΦto be\nS(Φ)≡−\nn\nin\nX\ni=1\nn\nout\nX\nj=1\n|φ\ni,j\n|\n1\n|Φ|\n1\nlog\n\u0012\n|φ\ni,j\n|\n1\n|Φ|\n1\n\u0013\n.(2.19)\nThe total training objectiveℓ\ntotal\nis the prediction lossℓ\npred\nplus L1 and entropy regularization of\nall KAN layers:\nℓ\ntotal\n=ℓ\npred\n+λ\n \nμ\n1\nL−1\nX\nl=0\n|Φ\nl\n|\n1\n+μ\n2\nL−1\nX\nl=0\nS(Φ\nl\n)\n!\n,(2.20)\nwhereμ\n1\n,μ\n2\nare relative magnitudes usually set toμ\n1\n=μ\n2\n= 1, andλcontrols overall regulariza-\ntion magnitude.\n11",
    "Figure 2.4: An example of how to do symbolic regression with KAN.\n2. Visualization.When we visualize a KAN, to get a sense of magnitudes, we set the transparency\nof an activation functionφ\nl,i,j\nproportional totanh(βA\nl,i,j\n)whereβ= 3.  Hence, functions with\nsmall magnitude appear faded out to allow us to focus on important ones.\n3.  Pruning.After training with sparsification penalty, we may also want to prune the network to a\nsmaller subnetwork.  We sparsify KANs on the node level (rather than on the edge level).  For each\nnode (say thei\nth\nneuron in thel\nth\nlayer), we define its incoming and outgoing score as\nI\nl,i\n= max\nk\n(|φ\nl−1,k,i\n|\n1\n),   O\nl,i\n= max\nj\n(|φ\nl+1,j,i\n|\n1\n),(2.21)\nand consider a node to be important if both incoming and outgoing scores are greater than a threshold\nhyperparameterθ= 10\n−2\nby default. All unimportant neurons are pruned.\n4.   Symbolification.In  cases  where  we  suspect  that  some  activation  functions  are  in  fact  sym-\nbolic  (e.g.,cosorlog),  we  provide  an  interface  to  set  them  to  be  a  specified  symbolic  form,\nfix_symbolic(l,i,j,f)can set the(l,i,j)activation to bef.  However, we cannot simply set\nthe activation function to be the exact symbolic formula, since its inputs and outputs may have shifts\nand scalings.  So, we obtain preactivationsxand postactivationsyfrom samples, and fit affine pa-\nrameters(a,b,c,d)such thaty≈cf(ax+b) +d. The fitting is done by iterative grid search ofa,b\nand linear regression.\nBesides these techniques, we provide additional tools that allow users to apply more fine-grained\ncontrol to KANs, listed in Appendix A.\n2.5.2    A toy example: how humans can interact with KANs\nAbove  we  have  proposed  a  number  of  simplification  techniques  for  KANs.   We  can  view  these\nsimplification choices as buttons one can click on. A user interacting with these buttons can decide\nwhich button is most promising to click next to make KANs more interpretable. We use an example\nbelow to showcase how a user could interact with a KAN to obtain maximally interpretable results.\nLet us again consider the regression task\nf(x,y) = exp\n\u0000\nsin(πx) +y\n2\n\u0001\n.(2.22)\n12",
    "Given data points(x\ni\n,y\ni\n,f\ni\n),i= 1,2,···,N\np\n, a hypothetical user Alice is interested in figuring\nout  the  symbolic  formula.   The  steps  of  Alice’s  interaction  with  the  KANs  are  described  below\n(illustrated in Figure 2.4):\nStep 1: Training with sparsification.Starting from a fully-connected[2,5,1]KAN, training with\nsparsification regularization can make it quite sparse. 4 out of 5 neurons in the hidden layer appear\nuseless, hence we want to prune them away.\nStep 2:  Pruning.Automatic pruning is seen to discard all hidden neurons except the last one,\nleaving a[2,1,1]KAN. The activation functions appear to be known symbolic functions.\nStep 3:  Setting symbolic functions.Assuming that the user can correctly guess these symbolic\nformulas from staring at the KAN plot, they can set\nfix_symbolic(0,0,0,‘sin’)\nfix_symbolic(0,1,0,‘xˆ2’)\nfix_symbolic(1,0,0,‘exp’).\n(2.23)\nIn case the user has no domain knowledge or no idea which symbolic functions these activation\nfunctions might be, we provide a functionsuggest_symbolicto suggest symbolic candidates.\nStep 4: Further training.After symbolifying all the activation functions in the network, the only\nremaining parameters are the affine parameters.  We continue training these affine parameters, and\nwhen we see the loss dropping to machine precision, we know that we have found the correct sym-\nbolic expression.\nStep 5:  Output the symbolic formula.Sympyis used to compute the symbolic formula of the\noutput node.  The user obtains1.0e\n1.0y\n2\n+1.0sin(3.14x)\n, which is the true answer (we only displayed\ntwo decimals forπ).\nRemark: Why not symbolic regression (SR)?It is reasonable to use symbolic regression for this\nexample.  However,  symbolic regression methods are in general brittle and hard to debug.  They\neither return a success or a failure in the end without outputting interpretable intermediate results.\nIn contrast, KANs do continuous search (with gradient descent) in function space, so their results\nare more continuous and hence more robust.   Moreover,  users have more control over KANs as\ncompared to SR due to KANs’ transparency. The way we visualize KANs is like displaying KANs’\n“brain” to users, and users can perform “surgery” (debugging) on KANs.  This level of control is\ntypically unavailable for SR. We will show examples of this in Section 4.4. More generally, when the\ntarget function is not symbolic, symbolic regression will fail but KANs can still provide something\nmeaningful.  For example, a special function (e.g., a Bessel function) is impossible to SR to learn\nunless it is provided in advance, but KANs can use splines to approximate it numerically anyway\n(see Figure 4.1 (d)).\n3    KANs are accurate\nIn this section, we demonstrate that KANs are more effective at representing functions than MLPs\nin various tasks (regression and PDE solving). When comparing two families of models, it is fair to\ncompare both their accuracy (loss) and their complexity (number of parameters). We will show that\nKANs display more favorable Pareto Frontiers than MLPs. Moreover, in Section 3.5, we show that\nKANs can naturally work in continual learning without catastrophic forgetting.\n13",
    "10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nNumber of parameters\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\ntest RMSE\nN\n4\nN\n4\nf(x) = J\n0\n(20x)\nKAN (depth 2)\nMLP (depth 2)\nMLP (depth 3)\nMLP (depth 4)\nMLP (depth 5)\nTheory (KAN)\nTheory (ID)\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nNumber of parameters\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nN\n4\nN\n2\nf(x, y) = exp(sin(  x) + y\n2\n)\nKAN (depth 2)\nMLP (depth 2)\nMLP (depth 3)\nMLP (depth 4)\nMLP (depth 5)\nTheory (KAN)\nTheory (ID)\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nNumber of parameters\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\nN\n4\nN\n2\nf(x, y) = xy\nKAN (depth 2)\nMLP (depth 2)\nMLP (depth 3)\nMLP (depth 4)\nMLP (depth 5)\nTheory (KAN)\nTheory (ID)\n10\n3\n10\n4\n10\n5\nNumber of parameters\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nN\n4\nN\n0.04\nf(x\n1\n,, x\n100\n) = exp(\n1\n100\n(\n100\ni = 1\nsin\n2\n(\nx\ni\n2\n)))\nKAN (depth 2)\nMLP (depth 2)\nMLP (depth 3)\nMLP (depth 4)\nMLP (depth 5)\nTheory (KAN)\nTheory (ID)\n10\n2\n10\n3\n10\n4\nNumber of parameters\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nN\n4\nN\n1\nf(x\n1\n, x\n2\n, x\n3\n, x\n4\n) = exp(sin(x\n2\n1\n+ x\n2\n2\n) + sin(x\n2\n3\n+ x\n2\n4\n))\nKAN (depth 3)\nKAN (depth 2)\nMLP (depth 2)\nMLP (depth 3)\nMLP (depth 4)\nMLP (depth 5)\nTheory (KAN)\nTheory (ID)\nFigure 3.1:  Compare KANs to MLPs on five toy examples.  KANs can almost saturate the fastest scaling law\npredicted by our theory(α= 4), while MLPs scales slowly and plateau quickly.\n3.1    Toy datasets\nIn Section 2.3, our theory suggested that test RMSE lossℓscales asℓ∝N\n−4\nwith model parameters\nN. However, this relies on the existence of a Kolmogorov-Arnold representation. As a sanity check,\nwe construct five examples we know have smooth KA representations:\n(1)f(x)  =J\n0\n(20x),  which  is  the  Bessel  function.   Since  it  is  a  univariate  function,  it  can  be\nrepresented by a spline, which is a[1,1]KAN.\n(2)f(x,y) = exp(sin(πx) +y\n2\n). We know that it can be exactly represented by a[2,1,1]KAN.\n(3)f(x,y) =xy. We know from Figure 4.1 that it can be exactly represented by a[2,2,1]KAN.\n(4)  A high-dimensional examplef(x\n1\n,···,x\n100\n) = exp(\n1\n100\nP\n100\ni=1\nsin\n2\n(\nπx\ni\n2\n))which can be rep-\nresented by a[100,1,1]KAN.\n(5)  A four-dimensional examplef(x\n1\n,x\n2\n,x\n3\n,x\n4\n) = exp(\n1\n2\n(sin(π(x\n2\n1\n+x\n2\n2\n)) + sin(π(x\n2\n3\n+x\n2\n4\n))))\nwhich can be represented by a[4,4,2,1]KAN.\nWe  train  these  KANs  by  increasing  grid  points  every  200  steps,   in  total  coveringG=\n{3,5,10,20,50,100,200,500,1000}.   We train MLPs with different depths and widths as base-\nlines. Both MLPs and KANs are trained with LBFGS for 1800 steps in total. We plot test RMSE as\na function of the number of parameters for KANs and MLPs in Figure 3.1, showing that KANs have\nbetter scaling curves than MLPs, especially for the high-dimensional example. For comparison, we\nplot the lines predicted from our KAN theory as red dashed (α=k+ 1 = 4), and the lines predicted\nfrom Sharma & Kaplan [17] as black-dashed (α= (k+ 1)/d= 4/d). KANs can almost saturate the\nsteeper red lines, while MLPs struggle to converge even as fast as the slower black lines and plateau\nquickly. We also note that for the last example, the 2-Layer KAN[4,9,1]behaves much worse than\nthe 3-Layer KAN (shape[4,2,2,1]). This highlights the greater expressive power of deeper KANs,\nwhich is the same for MLPs: deeper MLPs have more expressive power than shallower ones.\n3.2    Special functions\nOne caveat for the above results is that we assume knowledge of the “true” KAN shape. In practice,\nwe do not know the existence of KA representations.  Even when we are promised that such a KA\nrepresentation  exists,  we  do  not  know  the  KAN  shape  a  priori.   Special  functions  in  more  than\none variables are such cases, because it would be (mathematically) surprising if multivariate special\nfunctions (e.g., a Bessel functionf(ν,x) =J\nν\n(x)) could be written in KA represenations, involving\nonly univariate functions and sums). We show below that:\n(1)  Finding (approximate) compact KA representations of special functions is possible, revealing\nnovel mathematical properties of special functions from the perspective of Kolmogorov-Arnold\nrepresentations.\n14",
    "10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nellipj\nKAN train\nKAN test\nMLP train\nMLP test\nellipkincellipeincjvyv\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nkvivlpmv_m_0lpmv_m_1lpmv_m_2\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nsph_harm_m_0_n_1\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nsph_harm_m_1_n_1\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nsph_harm_m_0_n_2\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nsph_harm_m_1_n_2\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nsph_harm_m_2_n_2\nFigure 3.2: Fitting special functions. We show the Pareto Frontier of KANs and MLPs in the plane spanned by\nthe number of model parameters and RMSE loss. Consistently accross all special functions, KANs have better\nPareto Frontiers than MLPs. The definitions of these special functions are in Table 2.\n(2)  KANs are more efficient and accurate in representing special functions than MLPs.\nWe collect 15 special functions common in math and physics, summarized in Table 2.  We choose\nMLPs with fixed width 5 or 100 and depths swept in{2,3,4,5,6}.  We run KANs both with and\nwithout pruning.KANs without pruning:  We fix the shape of KAN, whose width are set to 5 and\ndepths are swept in {2,3,4,5,6}.KAN with pruning. We use the sparsification(λ= 10\n−2\nor 10\n−3\n)\nand pruning technique in Section 2.5.1 to obtain a smaller KAN pruned from a fixed-shape KAN.\nEach KAN is initialized to haveG= 3, trained with LBFGS, with increasing number of grid points\nevery 200 steps to coverG={3,5,10,20,50,100,200}.  For each hyperparameter combination,\nwe run 3 random seeds.\nFor  each  dataset  and  each  model  family  (KANs  or  MLPs),  we  plot  the  Pareto  frontier\n5\n,  in  the\n(number of parameters, RMSE) plane, shown in Figure 3.2.  KANs’ performance is shown to be\nconsistently better than MLPs, i.e., KANs can achieve lower training/test losses than MLPs, given\nthe same number of parameters. Moreover, we report the (surprisingly compact) shapes of our auto-\ndiscovered KANs for special functions in Table 2.  On one hand, it is interesting to interpret what\nthese compact representations mean mathematically (we include the KAN illustrations in Figure F.1\nand F.2 in Appendix F). On the other hand, these compact representations imply the possibility of\nbreaking down a high-dimensional lookup table into several 1D lookup tables, which can potentially\nsave a lot of memory, with the (almost negligible) overhead to perform a few additions at inference\ntime.\n3.3    Feynman datasets\nThe setup in Section 3.1 is when we clearly know “true” KAN shapes.  The setup in Section 3.2 is\nwhen we clearly donotknow “true” KAN shapes. This part investigates a setup lying in the middle:\nGiven the structure of the dataset, we may construct KANs by hand, but we are not sure if they are\noptimal.  In this regime, it is interesting to compare human-constructed KANs and auto-discovered\nKANs via pruning (techniques in Section 2.5.1).\n5\nPareto frontier is defined as fits that are optimal in the sense of no other fit being both simpler and more\naccurate.\n15",
    "Namescipy.special API\nMinimal KAN shape\ntest RMSE<10\n−2\nMinimal KAN test RMSEBest KAN shapeBest KAN test RMSEMLP test RMSE\nJacobian elliptic functionsellipj(x,y)[2,2,1]7.29×10\n−3\n[2,3,2,1,1,1]1.33×10\n−4\n6.48×10\n−4\nIncomplete elliptic integral of the first kindellipkinc(x,y)[2,2,1,1]1.00×10\n−3\n[2,2,1,1,1]1.24×10\n−4\n5.52×10\n−4\nIncomplete elliptic integral of the second kindellipeinc(x,y)[2,2,1,1]8.36×10\n−5\n[2,2,1,1]8.26×10\n−5\n3.04×10\n−4\nBessel function of the first kindjv(x,y)[2,2,1]4.93×10\n−3\n[2,3,1,1,1]1.64×10\n−3\n5.52×10\n−3\nBessel function of the second kindyv(x,y)[2,3,1]1.89×10\n−3\n[2,2,2,1]1.49×10\n−5\n3.45×10\n−4\nModified Bessel function of the second kindkv(x,y)[2,1,1]4.89×10\n−3\n[2,2,1]2.52×10\n−5\n1.67×10\n−4\nModified Bessel function of the first kindiv(x,y)[2,4,3,2,1,1]9.28×10\n−3\n[2,4,3,2,1,1]9.28×10\n−3\n1.07×10\n−2\nAssociated Legendre function(m= 0)lpmv(0,x,y)[2,2,1]5.25×10\n−5\n[2,2,1]5.25×10\n−5\n1.74×10\n−2\nAssociated Legendre function(m= 1)lpmv(1,x,y)[2,4,1]6.90×10\n−4\n[2,4,1]6.90×10\n−4\n1.50×10\n−3\nAssociated Legendre function(m= 2)lpmv(2,x,y)[2,2,1]4.88×10\n−3\n[2,3,2,1]2.26×10\n−4\n9.43×10\n−4\nspherical harmonics(m= 0,n= 1)sph_harm(0,1,x,y)[2,1,1]2.21×10\n−7\n[2,1,1]2.21×10\n−7\n1.25×10\n−6\nspherical harmonics(m= 1,n= 1)sph_harm(1,1,x,y)[2,2,1]7.86×10\n−4\n[2,3,2,1]1.22×10\n−4\n6.70×10\n−4\nspherical harmonics(m= 0,n= 2)sph_harm(0,2,x,y)[2,1,1]1.95×10\n−7\n[2,1,1]1.95×10\n−7\n2.85×10\n−6\nspherical harmonics(m= 1,n= 2)sph_harm(1,2,x,y)[2,2,1]4.70×10\n−4\n[2,2,1,1]1.50×10\n−5\n1.84×10\n−3\nspherical harmonics(m= 2,n= 2)sph_harm(2,2,x,y)[2,2,1]1.12×10\n−3\n[2,2,3,2,1]9.45×10\n−5\n6.21×10\n−4\nTable 2: Special functions\nFeynman Eq.Original FormulaDimensionless formulaVariables\nHuman-constructed\nKAN shape\nPruned\nKAN shape\n(smallest shape\nthat achieves\nRMSE <10\n−2\n)\nPruned\nKAN shape\n(lowest loss)\nHuman-constructed\nKAN loss\n(lowest test RMSE)\nPruned\nKAN loss\n(lowest test RMSE)\nUnpruned\nKAN loss\n(lowest test RMSE)\nMLP\nloss\n(lowest test RMSE)\nI.6.2exp(−\nθ\n2\n2σ\n2\n)/\n√\n2πσ\n2\nexp(−\nθ\n2\n2σ\n2\n)/\n√\n2πσ\n2\nθ,σ[2,2,1,1][2,2,1][2,2,1,1]7.66×10\n−5\n2.86×10\n−5\n4.60×10\n−5\n1.45×10\n−4\nI.6.2bexp(−\n(θ−θ\n1\n)\n2\n2σ\n2\n)/\n√\n2πσ\n2\nexp(−\n(θ−θ\n1\n)\n2\n2σ\n2\n)/\n√\n2πσ\n2\nθ,θ\n1\n,σ[3,2,2,1,1][3,4,1][3,2,2,1,1]1.22×10\n−3\n4.45×10\n−4\n1.25×10\n−3\n7.40×10\n−4\nI.9.18\nGm\n1\nm\n2\n(x\n2\n−x\n1\n)\n2\n+(y\n2\n−y\n1\n)\n2\n+(z\n2\n−z\n1\n)\n2\na\n(b−1)\n2\n+(c−d)\n2\n+(e−f)\n2\na,b,c,d,e,f[6,4,2,1,1][6,4,1,1][6,4,1,1]1.48×10\n−3\n8.62×10\n−3\n6.56×10\n−3\n1.59×10\n−3\nI.12.11q(E\nf\n+Bvsinθ)1 +asinθa,θ[2,2,2,1][2,2,1][2,2,1]2.07×10\n−3\n1.39×10\n−3\n9.13×10\n−4\n6.71×10\n−4\nI.13.12Gm\n1\nm\n2\n(\n1\nr\n2\n−\n1\nr\n1\n)a(\n1\nb\n−1)a,b[2,2,1][2,2,1][2,2,1]7.22×10\n−3\n4.81×10\n−3\n2.72×10\n−3\n1.42×10\n−3\nI.15.3x\nx−ut\n√\n1−(\nu\nc\n)\n2\n1−a\n√\n1−b\n2\na,b[2,2,1,1][2,1,1][2,2,1,1,1]7.35×10\n−3\n1.58×10\n−3\n1.14×10\n−3\n8.54×10\n−4\nI.16.6\nu+v\n1+\nuv\nc\n2\na+b\n1+ab\na,b[2,2,2,2,2,1][2,2,1][2,2,1]1.06×10\n−3\n1.19×10\n−3\n1.53×10\n−3\n6.20×10\n−4\nI.18.4\nm\n1\nr\n1\n+m\n2\nr\n2\nm\n1\n+m\n2\n1+ab\n1+a\na,b[2,2,2,1,1][2,2,1][2,2,1]3.92×10\n−4\n1.50×10\n−4\n1.32×10\n−3\n3.68×10\n−4\nI.26.2arcsin(nsinθ\n2\n)arcsin(nsinθ\n2\n)n,θ\n2\n[2,2,2,1,1][2,2,1][2,2,2,1,1]1.22×10\n−1\n7.90×10\n−4\n8.63×10\n−4\n1.24×10\n−3\nI.27.6\n1\n1\nd\n1\n+\nn\nd\n2\n1\n1+ab\na,b[2,2,1,1][2,1,1][2,1,1]2.22×10\n−4\n1.94×10\n−4\n2.14×10\n−4\n2.46×10\n−4\nI.29.16\np\nx\n2\n1\n+x\n2\n2\n−2x\n1\nx\n2\ncos(θ\n1\n−θ\n2\n)\np\n1 +a\n2\n−2acos(θ\n1\n−θ\n2\n)a,θ\n1\n,θ\n2\n[3,2,2,3,2,1,1][3,2,2,1][3,2,3,1]2.36×10\n−1\n3.99×10\n−3\n3.20×10\n−3\n4.64×10\n−3\nI.30.3I\n∗,0\nsin\n2\n(\nnθ\n2\n)\nsin\n2\n(\nθ\n2\n)\nsin\n2\n(\nnθ\n2\n)\nsin\n2\n(\nθ\n2\n)\nn,θ[2,3,2,2,1,1][2,4,3,1][2,3,2,3,1,1]3.85×10\n−1\n1.03×10\n−3\n1.11×10\n−2\n1.50×10\n−2\nI.30.5arcsin(\nλ\nnd\n)arcsin(\na\nn\n)a,n[2,1,1][2,1,1][2,1,1,1,1,1]2.23×10\n−4\n3.49×10\n−5\n6.92×10\n−5\n9.45×10\n−5\nI.37.4I\n∗\n=I\n1\n+I\n2\n+ 2\n√\nI\n1\nI\n2\ncosδ1 +a+ 2\n√\nacosδa,δ[2,3,2,1][2,2,1][2,2,1]7.57×10\n−5\n4.91×10\n−6\n3.41×10\n−4\n5.67×10\n−4\nI.40.1n\n0\nexp(−\nmgx\nk\nb\nT\n)n\n0\ne\n−a\nn\n0\n,a[2,1,1][2,2,1][2,2,1,1,1,2,1]3.45×10\n−3\n5.01×10\n−4\n3.12×10\n−4\n3.99×10\n−4\nI.44.4nk\nb\nTln(\nV\n2\nV\n1\n)nlnan,a[2,2,1][2,2,1][2,2,1]2.30×10\n−5\n2.43×10\n−5\n1.10×10\n−4\n3.99×10\n−4\nI.50.26x\n1\n(cos(ωt) +αcos\n2\n(wt))cosa+αcos\n2\naa,α[2,2,3,1][2,3,1][2,3,2,1]1.52×10\n−4\n5.82×10\n−4\n4.90×10\n−4\n1.53×10\n−3\nII.2.42\nk(T\n2\n−T\n1\n)A\nd\n(a−1)ba,b[2,2,1][2,2,1][2,2,2,1]8.54×10\n−4\n7.22×10\n−4\n1.22×10\n−3\n1.81×10\n−4\nII.6.15a\n3\n4πε\np\nd\nz\nr\n5\np\nx\n2\n+y\n2\n1\n4π\nc\n√\na\n2\n+b\n2\na,b,c[3,2,2,2,1][3,2,1,1][3,2,1,1]2.61×10\n−3\n3.28×10\n−3\n1.35×10\n−3\n5.92×10\n−4\nII.11.7n\n0\n(1 +\np\nd\nE\nf\ncosθ\nk\nb\nT\n)n\n0\n(1 +acosθ)n\n0\n,a,θ[3,3,3,2,2,1][3,3,1,1][3,3,1,1]7.10×10\n−3\n8.52×10\n−3\n5.03×10\n−3\n5.92×10\n−4\nII.11.27\nnα\n1−\nnα\n3\nεE\nf\nnα\n1−\nnα\n3\nn,α[2,2,1,2,1][2,1,1][2,2,1]2.67×10\n−5\n4.40×10\n−5\n1.43×10\n−5\n7.18×10\n−5\nII.35.18\nn\n0\nexp(\nμ\nm\nB\nk\nb\nT\n)+exp(−\nμ\nm\nB\nk\nb\nT\n)\nn\n0\nexp(a)+exp(−a)\nn\n0\n,a[2,1,1][2,1,1][2,1,1,1]4.13×10\n−4\n1.58×10\n−4\n7.71×10\n−5\n7.92×10\n−5\nII.36.38\nμ\nm\nB\nk\nb\nT\n+\nμ\nm\nαM\nεc\n2\nk\nb\nT\na+αba,α,b[3,3,1][3,2,1][3,2,1]2.85×10\n−3\n1.15×10\n−3\n3.03×10\n−3\n2.15×10\n−3\nII.38.3\nY Ax\nd\na\nb\na,b[2,1,1][2,1,1][2,2,1,1,1]1.47×10\n−4\n8.78×10\n−5\n6.43×10\n−4\n5.26×10\n−4\nIII.9.52\np\nd\nE\nf\nh\nsin\n2\n((ω−ω\n0\n)t/2)\n((ω−ω\n0\n)t/2)\n2\na\nsin\n2\n(\nb−c\n2\n)\n(\nb−c\n2\n)\n2\na,b,c[3,2,3,1,1][3,3,2,1][3,3,2,1,1,1]4.43×10\n−2\n3.90×10\n−3\n2.11×10\n−2\n9.07×10\n−4\nIII.10.19μ\nm\nq\nB\n2\nx\n+B\n2\ny\n+B\n2\nz\n√\n1 +a\n2\n+b\n2\na,b[2,1,1][2,1,1][2,1,2,1]2.54×10\n−3\n1.18×10\n−3\n8.16×10\n−4\n1.67×10\n−4\nIII.17.37β(1 +αcosθ)β(1 +αcosθ)α,β,θ[3,3,3,2,2,1][3,3,1][3,3,1]1.10×10\n−3\n5.03×10\n−4\n4.12×10\n−4\n6.80×10\n−4\nTable 3: Feynman dataset\nFeynman dataset.The Feynman dataset collects many physics equations from Feynman’s text-\nbooks [20, 21].  For our purpose, we are interested in problems in theFeynman_no_unitsdataset\nthat have at least 2 variables, since univariate problems are trivial for KANs (they simplify to 1D\nsplines). A sample equation from the Feynman dataset is the relativisic velocity addition formula\nf(u,v) = (u+v)/(1 +uv).(3.1)\nThe dataset can be constructed by randomly drawingu\ni\n∈(−1,1),v\ni\n∈(−1,1), and computing\nf\ni\n=f(u\ni\n,v\ni\n).  Given many tuples(u\ni\n,v\ni\n,f\ni\n), a neural network is trained and aims to predictf\nfromuandv.  We are interested in (1) how well a neural network can perform on test samples; (2)\nhow much we can learn about the structure of the problem from neural networks.\nWe compare four kinds of neural networks:\n(1)  Human-constructued  KAN.  Given  a  symbolic  formula,  we  rewrite  it  in  Kolmogorov-Arnold\nrepresentations.  For example, to multiply two numbersxandy, we can use the identityxy=\n16",
    "(x+y)\n2\n4\n−\n(x−y)\n2\n4\n, which corresponds to a[2,2,1]KAN. The constructued shapes are listed in\nthe “Human-constructed KAN shape” in Table 3.\n(2)  KANs without pruning. We fix the KAN shape to width 5 and depths are swept over {2,3,4,5,6}.\n(3)  KAN with pruning.  We use the sparsification(λ= 10\n−2\nor 10\n−3\n)and the pruning technique\nfrom Section 2.5.1 to obtain a smaller KAN from a fixed-shape KAN from (2).\n(4)  MLPs  with  fixed  width  20,  depths  swept  in{2,3,4,5,6},  and  activations  chosen  from\n{Tanh,ReLU,SiLU}.\nEach KAN is initialized to haveG= 3, trained with LBFGS, with increasing number of grid points\nevery 200 steps to coverG={3,5,10,20,50,100,200}.  For each hyperparameter combination,\nwe try 3 random seeds.  For each dataset (equation) and each method, we report the results of the\nbest model (minimal KAN shape, or lowest test loss) over random seeds and depths in Table 3. We\nfind that MLPs and KANs behave comparably on average. For each dataset and each model family\n(KANs or MLPs), we plot the Pareto frontier in the plane spanned by the number of parameters\nand RMSE losses, shown in Figure D.1 in Appendix D. We conjecture that the Feynman datasets\nare too simple to let KANs make further improvements, in the sense that variable dependence is\nusually smooth or monotonic,  which is in contrast to the complexity of special functions which\noften demonstrate oscillatory behavior.\nAuto-discovered KANs are smaller than human-constructed ones.We report the pruned KAN\nshape in two columns of Table 3; one column is for the minimal pruned KAN shape that can achieve\nreasonable loss (i.e., test RMSE smaller than10\n−2\n); the other column is for the pruned KAN that\nachieves lowest test loss. For completeness, we visualize all 54 pruned KANs in Appendix D (Fig-\nure D.2 and D.3).  It is interesting to observe that auto-discovered KAN shapes (for both minimal\nand best) are usually smaller than our human constructions. This means that KA representations can\nbe more efficient than we imagine. At the same time, this may make interpretability subtle because\ninformation is being squashed into a smaller space than what we are comfortable with.\nConsider the relativistic velocity compositionf(u,v)  =\nu+v\n1+uv\n, for example.  Our construction is\nquite deep because we were assuming that multiplication ofu,vwould use two layers (see Figure 4.1\n(a)), inversion of1 +uvwould use one layer, and multiplication ofu+vand1/(1 +uv)would\nuse another two layers\n6\n, resulting a total of 5 layers. However, the auto-discovered KANs are only 2\nlayers deep! In hindsight, this is actually expected if we recall the rapidity trick in relativity: define\nthe two “rapidities”a≡arctanhuandb≡arctanhv.  The relativistic composition of velocities\nare simple additions in rapidity space, i.e.,\nu+v\n1+uv\n= tanh(arctanhu+ arctanhv), which can be\nrealized by a two-layer KAN. Pretending we do not know the notion of rapidity in physics, we could\npotentially discover this concept right from KANs without trial-and-error symbolic manipulations.\nThe interpretability of KANs which can facilitate scientific discovery is the main topic in Section 4.\n3.4    Solving partial differential equations\nWe consider a Poisson equation with zero Dirichlet boundary data.  ForΩ = [−1,1]\n2\n, consider the\nPDE\nu\nxx\n+u\nyy\n=finΩ,\nu= 0on∂Ω.\n(3.2)\nWe consider the dataf=−π\n2\n(1 + 4y\n2\n) sin(πx) sin(πy\n2\n) + 2πsin(πx) cos(πy\n2\n)for whichu=\nsin(πx) sin(πy\n2\n)is the true solution.  We use the framework of physics-informed neural networks\n6\nNote that we cannot use the logarithmic construction for division, becauseuandvhere might be negative\nnumbers.\n17",
    "050100150200250\nstep\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nL2 error squared\nKAN [2,10,1]\nMLP [2,10,1]\nMLP [2,100,100,100,1]\n050100150200250\nstep\n10\n4\n10\n3\n10\n2\n10\n1\n10\n0\nH1 error squared\nKAN [2,10,1]\nMLP [2,10,1]\nMLP [2,100,100,100,1]\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nNumber of parameters\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nL2 error squared\nKAN [2,5,1]\nKAN [2,7,1]\nKAN [2,10,1]\nMLP (depth 2)\nMLP (depth 3)\nMLP (depth 4)\nMLP (depth 5)\nN\n4\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nNumber of parameters\n10\n4\n10\n3\n10\n2\n10\n1\n10\n0\nH1 error squared\nKAN [2,5,1]\nKAN [2,7,1]\nKAN [2,10,1]\nMLP (depth 2)\nMLP (depth 3)\nMLP (depth 4)\nMLP (depth 5)\nN\n4\nFigure 3.3: The PDE example. We plot L2 squared and H1 squared losses between the predicted solution and\nground truth solution.  First and second:  training dynamics of losses.  Third and fourth:  scaling laws of losses\nagainst the number of parameters.  KANs converge faster, achieve lower losses, and have steeper scaling laws\nthan MLPs.\n0\n1\nData\nPhase 1Phase 2Phase 3Phase 4Phase 5\n0\n1\nKAN\n-101\n0\n1\nMLP\n-101-101-101-101\nFigure 3.4: A toy continual learning problem.  The dataset is a 1D regression task with 5 Gaussian peaks (top\nrow).  Data around each peak is presented sequentially (instead of all at once) to KANs and MLPs.  KANs\n(middle row) can perfectly avoid catastrophic forgetting, while MLPs (bottom row) display severe catastrophic\nforgetting.\n(PINNs) [22, 23] to solve this PDE, with the loss function given by\nloss\npde\n=αloss\ni\n+loss\nb\n:\n=α\n1\nn\ni\nn\ni\nX\ni=1\n|u\nxx\n(z\ni\n) +u\nyy\n(z\ni\n)−f(z\ni\n)|\n2\n+\n1\nn\nb\nn\nb\nX\ni=1\nu\n2\n,\nwhere we use loss\ni\nto denote the interior loss, discretized and evaluated by a uniform sampling ofn\ni\npointsz\ni\n= (x\ni\n,y\ni\n)inside the domain, and similarly we use loss\nb\nto denote the boundary loss, dis-\ncretized and evaluated by a uniform sampling ofn\nb\npoints on the boundary.αis the hyperparameter\nbalancing the effect of the two terms.\nWe compare the KAN architecture with that of MLPs using the same hyperparametersn\ni\n= 10000,\nn\nb\n= 800, andα= 0.01. We measure both the error in theL\n2\nnorm and energy (H\n1\n) norm and see\nthat KAN achieves a much better scaling law with a smaller error, using smaller networks and fewer\nparameters; see Figure 3.3.  Therefore we speculate that KANs might have the potential of serving\nas a good neural network representation for model reduction of PDEs.\n3.5    Continual Learning\nCatastrophic forgetting is a serious problem in current machine learning [24]. When a human mas-\nters a task and switches to another task, they do not forget how to perform the first task.  Unfortu-\nnately, this is not the case for neural networks. When a neural network is trained on task 1 and then\nshifted to being trained on task 2, the network will soon forget about how to perform task 1. A key\n18",
    "difference between artificial neural networks and human brains is that human brains have function-\nally distinct modules placed locally in space. When a new task is learned, structure re-organization\nonly occurs in local regions responsible for relevant skills [25, 26], leaving other regions intact. Most\nartificial neural networks, including MLPs, do not have this notion of locality, which is probably the\nreason for catastrophic forgetting.\nWe show that KANs have local plasticity and can avoid catastrophic forgetting by leveraging the\nlocality of splines.   The idea is simple:  since spline bases are local,  a sample will only affect a\nfew  nearby  spline  coefficients,  leaving  far-away  coefficients  intact  (which  is  desirable  since  far-\naway regions may have already stored information that we want to preserve).  By contrast, since\nMLPs usually use global activations, e.g., ReLU/Tanh/SiLU etc., any local change may propagate\nuncontrollably to regions far away, destroying the information being stored there.\nWe use a toy example to validate this intuition.  The 1D regression task is composed of 5 Gaussian\npeaks. Data around each peak is presented sequentially (instead of all at once) to KANs and MLPs,\nas shown in Figure 3.4 top row.  KAN and MLP predictions after each training phase are shown in\nthe middle and bottom rows.  As expected, KAN only remodels regions where data is present on\nin the current phase, leaving previous regions unchanged.  By contrast, MLPs remodels the whole\nregion after seeing new data samples, leading to catastrophic forgetting.\nHere we simply present our preliminary results on an extremely simple example, to demonstrate\nhow one could possibly leverage locality in KANs (thanks to spline parametrizations) to reduce\ncatastrophic forgetting.  However, it remains unclear whether our method can generalize to more\nrealistic setups, which we leave for future work.  We would also like to study how our method can\nbe connected to and combined with SOTA methods in continual learning [27, 28].\n4    KANs are interpretable\nIn this section, we show that KANs are interpretable and interactive thanks to the techniques we\ndeveloped in Section 2.5. We want to test the use of KANs not only on synthetic tasks (Section 4.1\nand 4.2), but also in real-life scientific research.  We demonstrate that KANs can (re)discover both\nhighly non-trivial relations in knot theory (Section 4.3) and phase transition boundaries in condensed\nmatter physics (Section 4.4). KANs could potentially be the foundation model for AI + Science due\nto their accuracy (last section) and interpretability (this section).\n4.1    Supervised toy datasets\nWe first examine KANs’ ability to reveal the compositional structures in symbolic formulas.  Six\nexamples are listed below and their KANs are visualized in Figure 4.1. KANs are able to reveal the\ncompositional structures present in these formulas, as well as learn the correct univariate functions.\n(a)  Multiplicationf(x,y) =xy.  A[2,5,1]KAN is pruned to a[2,2,1]KAN. The learned acti-\nvation functions are linear and quadratic.  From the computation graph, we see that the way it\ncomputesxyis leveraging2xy= (x+y)\n2\n−(x\n2\n+y\n2\n).\n(b)  Division of positive numbersf(x,y)  =x/y.   A[2,5,1]KAN is pruned to a[2,1,1]KAN.\nThe  learned  activation  functions  are  logarithmic  and  exponential  functions,  and  the  KAN  is\ncomputingx/yby leveraging the identityx/y= exp(logx−logy).\n(c)  Numerical to categorical. The task is to convert a real number in[0,1]to its first decimal digit (as\none hots), e.g.,0.0618→[1,0,0,0,0,···],0.314→[0,0,0,1,0,···].  Notice that activation\nfunctions are learned to be spikes located around the corresponding decimal digits.\n19",
    "Figure 4.1: KANs are interepretable for simple symbolic tasks\n(d)  Special functionf(x,y) = exp(J\n0\n(20x) +y\n2\n). One limitation of symbolic regression is that it\nwill never find the correct formula of a special function if the special function is not provided as\nprior knowledge. KANs can learn special functions – the highly wiggly Bessel functionJ\n0\n(20x)\nis learned (numerically) by KAN.\n(e)  Phase transitionf(x\n1\n,x\n2\n,x\n3\n)  =  tanh(5(x\n4\n1\n+x\n4\n2\n+x\n4\n3\n−1)).  Phase transitions are of great\ninterest in physics, so we want KANs to be able to detect phase transitions and to identify the\ncorrect order parameters.  We use the tanh function to simulate the phase transition behavior,\nand the order parameter is the combination of the quartic terms ofx\n1\n,x\n2\n,x\n3\n.  Both the quartic\ndependence and tanh dependence emerge after KAN training.   This is a simplified case of a\nlocalization phase transition discussed in Section 4.4.\n(f)  Deeper compositionsf(x\n1\n,x\n2\n,x\n3\n,x\n4\n)  =\np\n(x\n1\n−x\n2\n)\n2\n+ (x\n3\n−x\n4\n)\n2\n.   To compute this,  we\nwould  need  the  identity  function,  squared  function,  and  square  root,  which  requires  at  least\na three-layer KAN. Indeed, we find that a[4,3,3,1]KAN can be auto-pruned to a[4,2,1,1]\nKAN, which exactly corresponds to the computation graph we would expect.\nMore examples from the Feynman dataset and the special function dataset are visualized in Fig-\nure D.2, D.3, F.1, F.2 in Appendices D and F.\n4.2    Unsupervised toy dataset\nOften, scientific discoveries are formulated as supervised learning problems, i.e., given input vari-\nablesx\n1\n,x\n2\n,···,x\nd\nand output variable(s)y, we want to find an interpretable functionfsuch that\ny≈f(x\n1\n,x\n2\n,···,x\nd\n).  However, another type of scientific discovery can be formulated as unsu-\npervised learning, i.e., given a set of variables(x\n1\n,x\n2\n,···,x\nd\n), we want to discover a structural\nrelationship between the variables. Specifically, we want to find a non-zerofsuch that\nf(x\n1\n,x\n2\n,···,x\nd\n)≈0.(4.1)\n20",
    "Figure  4.2:   Unsupervised  learning  of  a  toy  task.   KANs  can  identify  groups  of  dependent  variables,  i.e.,\n(x\n1\n,x\n2\n,x\n3\n)and(x\n4\n,x\n5\n)in this case.\nFor example, consider a set of features(x\n1\n,x\n2\n,x\n3\n)that satisfiesx\n3\n= exp(sin(πx\n1\n) +x\n2\n2\n). Then a\nvalidfisf(x\n1\n,x\n2\n,x\n3\n) = sin(πx\n1\n) +x\n2\n2\n−log(x\n3\n) = 0, implying that points of(x\n1\n,x\n2\n,x\n3\n)form\na 2D submanifold specified byf= 0instead of filling the whole 3D space.\nIf an algorithm for solving the unsupervised problem can be devised, it has a considerable advantage\nover the supervised problem, since it requires only the sets of featuresS= (x\n1\n,x\n2\n,···,x\nd\n).  The\nsupervised problem, on the other hand, tries to predict subsets of features in terms of the others, i.e.\nit splitsS=S\nin\n∪S\nout\ninto input and output features of the function to be learned. Without domain\nexpertise to advise the splitting, there are2\nd\n−2possibilities such that|S\nin\n|>0and|S\nout\n|>0.\nThis exponentially large space of supervised problems can be avoided by using the unsupervised\napproach.  This unsupervised learning approach will be valuable to the knot dataset in Section 4.3.\nA Google Deepmind team [29] manually chose signature to be the target variable, otherwise they\nwould face this combinatorial problem described above.  This raises the question whether we can\ninstead tackle the unsupervised learning directly. We present our method and a toy example below.\nWe tackle the unsupervised learning problem by turning it into a supervised learning problem on\nall of thedfeatures,  without requiring the choice of a splitting.   The essential idea is to learn a\nfunctionf(x\n1\n,...,x\nd\n)  =  0such thatfis not the0-function.   To do this,  similar to contrastive\nlearning, we define positive samples and negative samples:  positive samples are feature vectors of\nreal data. Negative samples are constructed by feature corruption. To ensure that the overall feature\ndistribution for each topological invariant stays the same, we perform feature corruption by random\npermutation of each feature across the entire training set.  Now we want to train a networkgsuch\nthatg(x\nreal\n) = 1andg(x\nfake\n) = 0which turns the problem into a supervised problem.  However,\nremember that we originally wantf(x\nreal\n) = 0andf(x\nfake\n)̸= 0.  We can achieve this by having\ng=σ◦fwhereσ(x) = exp(−\nx\n2\n2w\n2\n)is a Gaussian function with a small widthw, which can be\nconveniently realized by a KAN with shape[...,1,1]whose last activation is set to be the Gaussian\nfunctionσand all previous layers formf. Except for the modifications mentioned above, everything\nelse is the same for supervised training.\nNow we demonstrate that the unsupervised paradigm works for a synthetic example. Let us consider\na 6D dataset, where(x\n1\n,x\n2\n,x\n3\n)are dependent variables such thatx\n3\n= exp(sin(x\n1\n) +x\n2\n2\n);(x\n4\n,x\n5\n)\nare dependent variables withx\n5\n=x\n3\n4\n;x\n6\nis independent of the other variables.  In Figure 4.2, we\nshow that for seed = 0, KAN reveals the functional dependence amongx\n1\n,x\n2\n, andx\n3\n; for another\nseed = 2024, KAN reveals the functional dependence betweenx\n4\nandx\n5\n.  Our preliminary results\nrely on randomness (different seeds) to discover different relations; in the future we would like to\ninvestigate a more systematic and more controlled way to discover a complete set of relations. Even\n21",
    "so, our tool in its current status can provide insights for scientific tasks. We present our results with\nthe knot dataset in Section 4.3.\n4.3    Application to Mathematics: Knot Theory\nKnot theory is a subject in low-dimensional topology that sheds light on topological aspects of three-\nmanifolds and four-manifolds and has a variety of applications, including in biology and topological\nquantum computing.  Mathematically, a knotKis an embedding ofS\n1\nintoS\n3\n.  Two knotsKand\nK\n′\nare topologically equivalent if one can be deformed into the other via deformation of the ambient\nspaceS\n3\n, in which case we write[K] = [K\n′\n].  Some knots are topologically trivial, meaning that\nthey can be smoothly deformed to a standard circle.  Knots have a variety of deformation-invariant\nfeaturesfcalled topological invariants, which may be used to show that two knots are topologically\ninequivalent,[K]̸= [K\n′\n]iff(K)̸=f(K\n′\n). In some cases the topological invariants are geometric\nin nature. For instance, a hyperbolic knotKhas a knot complementS\n3\n\\Kthat admits a canonical\nhyperbolic metricgsuch that vol\ng\n(K)is a topological invariant known as the hyperbolic volume.\nOther topological invariants are algebraic in nature, such as the Jones polynomial.\nGiven the fundamental nature of knots in mathematics and the importance of its applications, it is\ninteresting to study whether ML can lead to new results. For instance, in [30] reinforcement learning\nwas utilized to establish ribbonness of certain knots, which ruled out many potential counterexam-\nples to the smooth 4d Poincaré conjecture.\nSupervised learningIn [29], supervised learning and human domain experts were utilized to arrive\nat a new theorem relating algebraic and geometric knot invariants.  In this case, gradient saliency\nidentified key invariants for the supervised problem, which led the domain experts to make a con-\njecture  that  was  subsequently  refined  and  proven.   We  study  whether  a  KAN  can  achieve  good\ninterpretable results on the same problem, which predicts the signature of a knot. Their main results\nfrom studying the knot theory dataset are:\n(1)  They use network attribution methods to find that the signatureσis mostly dependent on merid-\ninal distanceμ(realμ\nr\n, imagμ\ni\n) and longitudinal distanceλ.\n(2)  Human scientists later identified thatσhas high correlation with theslope≡Re(\nλ\nμ\n) =\nλμ\nr\nμ\n2\nr\n+μ\n2\ni\nand derived a bound for|2σ−slope|.\nWe show below that KANs not only rediscover these results with much smaller networks and much\nmore automation, but also present some interesting new results and insights.\nTo investigate (1),  we treat 17 knot invariants as inputs and signature as outputs.   Similar to the\nsetup in [29], signatures (which are even numbers) are encoded as one-hot vectors and networks are\ntrained with cross-entropy loss.  We find that an extremely small[17,1,14]KAN is able to achieve\n81.6%test accuracy (while Deepmind’s 4-layer width-300 MLP achieves 78% test accuracy).  The\n[17,1,14]KAN (G= 3,k= 3) has≈200parameters, while the MLP has≈3×10\n5\nparameters,\nshown in Table 4. It is remarkable that KANs can be both more accurate and much more parameter\nefficient than MLPs at the same time. In terms of interpretability, we scale the transparency of each\nactivation according to its magnitude, so it becomes immediately clear which input variables are\nimportant without the need for feature attribution (see Figure 4.3 left): signature is mostly dependent\nonμ\nr\n, and slightly dependent onμ\ni\nandλ, while dependence on other variables is small.  We then\ntrain a[3,1,14]KAN on the three important variables, obtaining test accuracy78.2%.  Our results\nhave one subtle difference from results in [29]:  they find that signature is mostly dependent onμ\ni\n,\nwhile we find that signature is mostly dependent onμ\nr\n.   This difference could be due to subtle\nalgorithmic choices, but has led us to carry out the following experiments: (a) ablation studies. We\n22",
    "Figure 4.3:  Knot dataset, supervised mode.  With KANs, we rediscover Deepmind’s results that signature is\nmainly dependent on meridinal translation (real and imaginary parts).\nMethodArchitectureParameter CountAccuracy\nDeepmind’s MLP4 layer, width-3003×10\n5\n78.0%\nKANs2 layer,[17,1,14](G= 3,k= 3)2×10\n2\n81.6%\nTable 4: KANs can achieve better accuracy than MLPs with much fewer parameters in the signature classifica-\ntion problem.\nshow thatμ\nr\ncontributes more to accuracy thanμ\ni\n(see Figure 4.3):  for example,μ\nr\nalone can\nachieve65.0%accuracy, whileμ\ni\nalone can only achieve43.8%accuracy.  (b) We find a symbolic\nformula (in Table 5) which only involvesμ\nr\nandλ, but can achieve77.8%test accuracy.\nTo investigate (2), i.e., obtain the symbolic form ofσ, we formulate the problem as a regression\ntask.  Using auto-symbolic regression introduced in Section 2.5.1, we can convert a trained KAN\ninto symbolic formulas.  We train KANs with shapes[3,1],[3,1,1],[3,2,1], whose corresponding\nsymbolic  formulas  are  displayed  in  Table  5  B-D.  It  is  clear  that  by  having  a  larger  KAN,  both\naccuracy and complexity increase. So KANs provide not just a single symbolic formula, but a whole\nPareto frontier of formulas, trading off simplicity and accuracy.  However, KANs need additional\ninductive biases to further simplify these equations to rediscover the formula from [29] (Table 5 A).\nWe have tested two scenarios:  (1) in the first scenario, we assume the ground truth formula has a\nmulti-variate Pade representation (division of two multi-variate Taylor series). We first train[3,2,1]\nand then fit it to a Pade representation. We can obtain Formula E in Table 5, which bears similarity\nwith Deepmind’s formula. (2) We hypothesize that the division is not very interpretable for KANs,\nso we train two KANs (one for the numerator and the other for the denominator) and divide them\nmanually.  Surprisingly, we end up with the formula F (in Table 5) which only involvesμ\nr\nandλ,\nalthoughμ\ni\nis also provided but ignored by KANs.\nSo far, we have rediscovered the main results from [29].  It is remarkable to see that KANs made\nthis discovery very intuitive and convenient.  Instead of using feature attribution methods (which\nare great methods), one can instead simply stare at visualizations of KANs.  Moreover, automatic\nsymbolic regression also makes the discovery of symbolic formulas much easier.\nIn the next part, we propose a new paradigm of “AI for Math” not included in the Deepmind pa-\nper, where we aim to use KANs’ unsupervised learning mode to discover more relations (besides\nsignature) in knot invariants.\n23",
    "IdFormulaDiscovered by\ntest\nacc\nr\n2\nwith\nSignature\nr\n2\nwith DM\nformula\nA\nλμ\nr\n(μ\n2\nr\n+μ\n2\ni\n)\nHuman (DM)\n83.1%0.9461\nB−0.02sin(4.98μ\ni\n+ 0.85) + 0.08|4.02μ\nr\n+ 6.28|−0.52−\n0.04e\n−0.88(1−0.45λ)\n2\n[3,1]KAN62.6%0.8370.897\nC0.17tan(−1.51+0.1e\n−1.43(1−0.4μ\ni\n)\n2\n+0.09e\n−0.06(1−0.21λ)\n2\n+\n1.32e\n−3.18(1−0.43μ\nr\n)\n2\n)\n[3,1,1]KAN71.9%0.8710.934\nD−0.09 + 1.04exp(−9.59(−0.62sin(0.61μ\nr\n+ 7.26))−\n0.32tan(0.03λ−6.59) + 1−0.11e\n−1.77(0.31−μ\ni\n)\n2\n)\n2\n−\n1.09e\n−7.6(0.65(1−0.01λ)\n3\n+  0.27atan(0.53μ\ni\n−0.6)  +\n0.09 + exp(−2.58(1−0.36μ\nr\n)\n2\n))\n[3,2,1]KAN84.0%0.9470.997\nE\n4.76λμ\nr\n3.09μ\ni\n+6.05μ\n2\nr\n+3.54μ\n2\ni\n[3,2,1] KAN\n+ Pade approx\n82.8%0.9460.997\nF\n2.94−2.92(1−0.10μ\nr\n)\n2\n0.32(0.18−μ\nr\n)\n2\n+5.36(1−0.04λ)\n2\n+0.50\n[3,1]KAN/[3,1]KAN77.8%0.9250.977\nTable 5:  Symbolic formulas of signature as a function of meridinal translationμ(realμ\nr\n, imagμ\ni\n) and lon-\ngitudinal translationλ.   In [29],  formula A was discovered by human scientists inspired by neural network\nattribution results.  Formulas B-F are auto-discovered by KANs.  KANs can trade-off between simplicity and\naccuracy (B, C, D). By adding more inductive biases,  KAN is able to discover formula E which is not too\ndissimilar from formula A. KANs also discovered a formula F which only involves two variables (μ\nr\nandλ)\ninstead of all three variables, with little sacrifice in accuracy.\nFigure 4.4:  Knot dataset, unsupervised mode.  With KANs, we rediscover three mathematical relations in the\nknot dataset.\nUnsupervised learningAs we mentioned in Section 4.2, unsupervised learning is the setup that is\nmore promising since it avoids manual partition of input and output variables which have combina-\ntorially many possibilities.  In the unsupervised learning mode, we treat all 18 variables (including\nsignature) as inputs such that they are on the same footing.  Knot data are positive samples, and\nwe randomly shuffle features to obtain negative samples.  An[18,1,1]KAN is trained to classify\nwhether a given feature vector belongs to a positive sample (1) or a negative sample (0).  We man-\nually set the second layer activation to be the Gaussian function with a peak one centered at zero,\nso positive samples will have activations at (around) zero, implicitly giving a relation among knot\ninvariants\nP\n18\ni=1\ng\ni\n(x\ni\n) = 0wherex\ni\nstands for a feature (invariant), andg\ni\nis the corresponding\n24",
    "activation function which can be readily read off from KAN diagrams.  We train the KANs with\nλ={10\n−2\n,10\n−3\n}to favor sparse combination of inputs, andseed ={0,1,···,99}.  All 200 net-\nworks can be grouped into three clusters, with representative KANs displayed in Figure 4.4. These\nthree groups of dependent variables are:\n(1)  The first group of dependent variables is signature, real part of meridinal distance, and longi-\ntudinal distance (plus two other variables which can be removed because of (3)).  This is the\nsignature dependence studied above, so it is very interesting to see that this dependence relation\nis rediscovered again in the unsupervised mode.\n(2)  The second group of variables involve cusp volumeV, real part of meridinal translationμ\nr\nand\nlongitudinal translationλ.  Their activations all look like logarithmic functions (which can be\nverified by the implied symbolic functionality in Section 2.5.1).  So the relation is−logV+\nlogμ\nr\n+ logλ= 0which is equivalent toV=μ\nr\nλ, which is true by definition. It is, however,\nreassuring that we discover this relation without any prior knowledge.\n(3)  The third group of variables includes the real part of short geodesicg\nr\nand injectivity radius.\nTheir activations look qualitatively the same but differ by a minus sign, so it is conjectured that\nthese two variables have a linear correlation. We plot 2D scatters, finding that2rupper bounds\ng\nr\n, which is also a well-known relation [31].\nIt is interesting that KANs’ unsupervised mode can rediscover several known mathematical rela-\ntions.  The good news is that the results discovered by KANs are probably reliable; the bad news\nis that we have not discovered anything new yet.  It is worth noting that we have chosen a shallow\nKAN for simple visualization, but deeper KANs can probably find more relations if they exist. We\nwould like to investigate how to discover more complicated relations with deeper KANs in future\nwork.\n4.4    Application to Physics: Anderson localization\nAnderson localization is the fundamental phenomenon in which disorder in a quantum system leads\nto the localization of electronic wave functions, causing all transport to be ceased [32].  In one and\ntwo dimensions, scaling arguments show that all electronic eigenstates are exponentially localized\nfor an infinitesimal amount of random disorder [33, 34].  In contrast, in three dimensions, a critical\nenergy forms a phase boundary that separates the extended states from the localized states, known\nas a mobility edge.   The understanding of these mobility edges is crucial for explaining various\nfundamental phenomena such as the metal-insulator transition in solids [35], as well as localization\neffects of light in photonic devices [36, 37, 38, 39, 40].  It is therefore necessary to develop micro-\nscopic models that exhibit mobility edges to enable detailed investigations. Developing such models\nis often more practical in lower dimensions, where introducing quasiperiodicity instead of random\ndisorder can also result in mobility edges that separate localized and extended phases. Furthermore,\nexperimental realizations of analytical mobility edges can help resolve the debate on localization in\ninteracting systems [41, 42]. Indeed, several recent studies have focused on identifying such models\nand deriving exact analytic expressions for their mobility edges [43, 44, 45, 46, 47, 48, 49].\nHere, we apply KANs to numerical data generated from quasiperiodic tight-binding models to ex-\ntract their mobility edges.  In particular,  we examine three classes of models:  the Mosaic model\n(MM)  [47],  the  generalized  Aubry-André  model  (GAAM)  [46]  and  the  modified  Aubry-André\nmodel (MAAM) [44].  For the MM, we testify KAN’s ability to accurately extract mobility edge\nas a 1D function of energy. For the GAAM, we find that the formula obtained from a KAN closely\nmatches the ground truth.  For the more complicated MAAM, we demonstrate yet another exam-\nple of the symbolic interpretability of this framework.  A user can simplify the complex expression\n25",
    "obtained from KANs (and corresponding symbolic formulas) by means of a “collaboration” where\nthe human generates hypotheses to obtain a better match (e.g., making an assumption of the form of\ncertain activation function), after which KANs can carry out quick hypotheses testing.\nTo quantify the localization of states in these models, the inverse participation ratio (IPR) is com-\nmonly used. The IPR for thek\nth\neigenstate,ψ\n(k)\n, is given by\nIPR\nk\n=\nP\nn\n|ψ\n(k)\nn\n|\n4\n\u0010\nP\nn\n|ψ\n(k)\nn\n|\n2\n\u0011\n2\n(4.2)\nwhere the sum runs over the site index. Here, we use the related measure of localization – the fractal\ndimension of the states, given by\nD\nk\n=−\nlog(IPR\nk\n)\nlog(N)\n(4.3)\nwhereNis the system size.D\nk\n= 0(1)indicates localized (extended) states.\nMosaic Model (MM)We first consider a class of tight-binding models defined by the Hamilto-\nnian [47]\nH=t\nX\nn\n\u0010\nc\n†\nn+1\nc\nn\n+H.c.\n\u0011\n+\nX\nn\nV\nn\n(λ,φ)c\n†\nn\nc\nn\n,(4.4)\nwheretis the nearest-neighbor coupling,c\nn\n(c\n†\nn\n)is the annihilation (creation) operator at sitenand\nthe potential energyV\nn\nis given by\nV\nn\n(λ,φ) =\n(\nλcos(2πnb+φ)j=mκ\n0,otherwise,\n(4.5)\nTo introduce quasiperiodicity, we setbto be irrational (in particular, we choosebto be the golden\nratio\n1+\n√\n5\n2\n).κis an integer and the quasiperiodic potential occurs with intervalκ.   The energy\n(E) spectrum for this model generically contains extended and localized regimes separated by a\nmobility edge. Interestingly, a unique feature found here is that the mobility edges are present for an\narbitrarily strong quasiperiodic potential (i.e. there are always extended states present in the system\nthat co-exist with localized ones).\nThe mobility edge can be described byg(λ,E)≡λ−|f\nκ\n(E)|= 0.g(λ,E)> 0 andg(λ,E)<\n0 correspond to localized and extended phases, respectively.  Learning the mobility edge therefore\nhinges on learning the “order parameter”g(λ,E). Admittedly, this problem can be tackled by many\nother theoretical methods for this class of models [47], but we will demonstrate below that our KAN\nframework is ready and convenient to take in assumptions and inductive biases from human users.\nLet us assume a hypothetical user Alice, who is a new PhD student in condensed matter physics,\nand she is provided with a[2,1]KAN as an assistant for the task.   Firstly,  she understands that\nthis  is  a  classification  task,  so  it  is  wise  to  set  the  activation  function  in  the  second  layer  to  be\nsigmoid by using thefix_symbolicfunctionality.  Secondly, she realizes that learning the whole\n2D functiong(λ,E)is unnecessary because in the end she only cares aboutλ=λ(E)determined\nbyg(λ,E) = 0. In so doing, it is reasonable to assumeg(λ,E) =λ−h(E) = 0. Alice simply sets\nthe activation function ofλto be linear by again using thefix_symbolicfunctionality. Now Alice\ntrains the KAN network and conveniently obtains the mobility edge, as shown in Figure 4.5.  Alice\ncan get both intuitive qualitative understanding (bottom) and quantitative results (middle), which\nwell match the ground truth (top).\n26",
    "Figure 4.5:  Results for the Mosaic Model.  Top:  phase diagram.  Middle and Bottom:  KANs can obtain both\nqualitative intuition (bottom) and extract quantitative results (middle).φ=\n1+\n√\n5\n2\nis the golden ratio.\nGeneralized Andre-Aubry Model (GAAM)We next consider a class of tight-binding models de-\nfined by the Hamiltonian [46]\nH=t\nX\nn\n\u0010\nc\n†\nn+1\nc\nn\n+H.c.\n\u0011\n+\nX\nn\nV\nn\n(α,λ,φ)c\n†\nn\nc\nn\n,(4.6)\nwheretis the nearest-neighbor coupling,c\nn\n(c\n†\nn\n)is the annihilation (creation) operator at sitenand\nthe potential energyV\nn\nis given by\nV\nn\n(α,λ,φ) = 2λ\ncos(2πnb+φ)\n1−αcos(2πnb+φ)\n,(4.7)\nwhich is smooth forα∈(−1,1).  To introduce quasiperiodicity, we again setbto be irrational (in\nparticular, we choosebto be the golden ratio). As before, we would like to obtain an expression for\nthe mobility edge.  For these models, the mobility edge is given by the closed form expression [46,\n48],\nαE= 2(t−λ).(4.8)\nWe randomly sample the model parameters:φ,αandλ(setting the energy scalet= 1) and calculate\nthe  energy  eigenvalues  as  well  as  the  fractal  dimension  of  the  corresponding  eigenstates,  which\nforms our training dataset.\nHere the “order parameter” to be learned isg(α,E,λ,φ)  =αE+ 2(λ−1)and mobility edge\ncorresponds tog=  0.  Let us again assume that Alice wants to figure out the mobility edge but\nonly has access to IPR or fractal dimension data, so she decides to use KAN to help her with the\ntask.  Alice wants the model to be as small as possible, so she could either start from a large model\nand use auto-pruning to get a small model, or she could guess a reasonable small model based on\nher understanding of the complexity of the given problem.  Either way, let us assume she arrives\nat a[4,2,1,1]KAN. First, she sets the last activation to be sigmoid because this is a classification\n27",
    "SystemOriginMobility Edge FormulaAccuracy\nGAAM\nTheoryαE+ 2λ−2 = 099.2%\nKAN auto\n\u0018\n\u0018\n\u0018\n\u0018\n1.52E\n2\n+ 21.06αE+\n\u0018\n\u0018\n\u0018\n0.66E+\n\u0018\n\u0018\n\u0018\n\u0018\n3.55α\n2\n+\n\u0018\n\u0018\n\u0018\n0.91α+ 45.13λ−54.45 = 0\n99.0%\nMAAM\nTheory\nE+ exp(p)−λcoshp= 098.6%\nKAN auto\n13.99sin(0.28sin(0.87λ+ 2.22)−0.84arctan(0.58E−0.26) + 0.85arctan(0.94p+\n0.13)−8.14)−16.74+43.08exp(−0.93(0.06(0.13−p)\n2\n−0.27tanh(0.65E+0.25)+\n0.63arctan(0.54λ−0.62) + 1)\n2\n) = 0\n97.1%\nKAN man (step 2) + auto\n4.19(0.28sin(0.97λ+ 2.17)−0.77arctan(0.83E−0.19) + arctan(0.97p+ 0.15)−\n0.35)\n2\n−28.93 + 39.27exp(−0.6(0.28cosh\n2\n(0.49p−0.16)−0.34arctan(0.65E+\n0.51) + 0.83arctan(0.54λ−0.62) + 1)\n2\n) = 0\n97.7%\nKAN man (step 3) + auto\n−4.63E−10.25(−0.94sin(0.97λ−6.81)  +  tanh(0.8p−0.45)  +  0.09)\n2\n+\n11.78sin(0.76p−1.41) + 22.49arctan(1.08λ−1.32) + 31.72 = 0\n97.7%\nKAN man (step 4A)\n6.92E−6.23(−0.92λ−1)\n2\n+ 2572.45(−0.05λ+ 0.95cosh(0.11p+ 0.4)−1)\n2\n−\n12.96cosh\n2\n(0.53p+ 0.16) + 19.89 = 0\n96.6%\nKAN man (step 4B)\n7.25E−8.81(−0.83λ−1)\n2\n−4.08(−p−0.04)\n2\n+ 12.71(−0.71λ+ (0.3p+ 1)\n2\n−\n0.86)\n2\n+ 10.29 = 0\n95.4%\nTable 6:  Symbolic formulas for two systems GAAM and MAAM, ground truth ones and KAN-discovered\nones.\nproblem. She trains her KAN with some sparsity regularization to accuracy 98.7% and visualizes the\ntrained KAN in Figure 4.6 (a) step 1. She observes thatφis not picked up on at all, which makes her\nrealize that the mobility edge is independent ofφ(agreeing with Eq. (4.8)). In addition, she observes\nthat almost all other activation functions are linear or quadratic, so she turns on automatic symbolic\nsnapping, constraining the library to be only linear or quadratic.  After that, she immediately gets a\nnetwork which is already symbolic (shown in Figure 4.6 (a) step 2), with comparable (even slightly\nbetter) accuracy 98.9%.  By usingsymbolic_formulafunctionality, Alice conveniently gets the\nsymbolic form ofg, shown in Table 6 GAAM-KAN auto (row three). Perhaps she wants to cross out\nsome small terms and snap coefficient to small integers, which takes her close to the true answer.\nThis hypothetical story for Alice would be completely different if she is using a symbolic regres-\nsion method.  If she is lucky, SR can return the exact correct formula.  However, the vast majority\nof the  time SR does not  return  useful results and  it is impossible for  Alice to “debug” or  inter-\nact with the underlying process of symbolic regression.  Furthermore, Alice may feel uncomfort-\nable/inexperienced to provide a library of symbolic terms as prior knowledge to SR before SR is\nrun. By constrast in KANs, Alice does not need to put any prior information to KANs. She can first\nget some clues by staring at a trained KAN and only then it is her job to decide which hypothesis\nshe wants to make (e.g., “all activations are linear or quadratic”) and implement her hypothesis in\nKANs.  Although it is not likely for KANs to return the correct answer immediately, KANs will\nalways return something useful, and Alice can collaborate with it to refine the results.\nModified Andre-Aubry Model (MAAM)The last class of models we consider is defined by the\nHamiltonian [44]\nH=\nX\nn̸=n\n′\nte\n−p|n−n\n′\n|\n\u0000\nc\n†\nn\nc\nn\n′\n+H.c.\n\u0001\n+\nX\nn\nV\nn\n(λ,φ)c\n†\nn\nc\nn\n,(4.9)\nwheretis the strength of the exponentially decaying coupling in space,c\nn\n(c\n†\nn\n)is the annihilation\n(creation) operator at sitenand the potential energyV\nn\nis given by\nV\nn\n(λ,φ) =λcos(2πnb+φ),(4.10)\n28",
    "Figure 4.6: Human-KAN collaboration to discover mobility edges of GAAM and MAAM. The human user can\nchoose to be lazy (using the auto mode) or more involved (using the manual mode). More details in text.\nAs before, to introduce quasiperiodicity, we setbto be irrational (the golden ratio). For these models,\nthe mobility edge is given by the closed form expression [44],\nλcosh(p) =E+t=E+t\n1\nexp(p)(4.11)\nwhere we definet\n1\n≡texp(−p)as the nearest neighbor hopping strength, and we sett\n1\n= 1below.\nLet us assume Alice wants to figure out the mobility edge for MAAM. This task is more complicated\nand requires more human wisdom.  As in the last example, Alice starts from a[4,2,1,1]KAN and\ntrains it but gets an accuracy around 75% which is less than acceptable.  She then chooses a larger\n[4,3,1,1]KAN  and  successfully  gets  98.4%  which  is  acceptable  (Figure  4.6  (b)  step  1).   Alice\nnotices thatφis not picked up on by KANs, which means that the mobility edge is independent of\nthe phase factorφ(agreeing with Eq. (4.11)).  If Alice turns on the automatic symbolic regression\n(using a large library consisting of exp, tanh etc.), she would get a complicated formula in Tabel 6-\nMAAM-KAN auto, which has 97.1% accuracy. However, if Alice wants to find a simpler symbolic\nformula, she will want to use the manual mode where she does the symbolic snapping by herself.\nBefore that she finds that the[4,3,1,1]KAN after training can then be pruned to be[4,2,1,1], while\nmaintaining97.7%accuracy (Figure 4.6 (b)).  Alice may think that all activation functions except\nthose dependent onpare linear or quadratic and snap them to be either linear or quadratic manually\nby usingfix_symbolic. After snapping and retraining, the updated KAN is shown in Figure 4.6 (c)\nstep 3, maintaining97.7%accuracy. From now on, Alice may make two different choices based on\nher prior knowledge. In one case, Alice may have guessed that the dependence onpiscosh, so she\nsets the activations ofpto becoshfunction. She retrains KAN and gets 96.9% accuracy (Figure 4.6\n(c) Step 4A). In another case, Alice does not know thecoshpdependence, so she pursues simplicity\n29",
    "and again assumes the functions ofpto be quadratic.  She retrains KAN and gets 95.4% accuracy\n(Figure 4.6 (c) Step 4B). If she tried both, she would realize thatcoshis better in terms of accuracy,\nwhile quadratic is better in terms of simplicity. The formulas corresponding to these steps are listed\nin Table 6.  It is clear that the more manual operations are done by Alice, the simpler the symbolic\nformula is (which slight sacrifice in accuracy). KANs have a “knob\" that a user can tune to trade-off\nbetween simplicity and accuracy (sometimes simplicity can even lead to better accuracy, as in the\nGAAM case).\n5    Related works\nKolmogorov-Arnold theorem and neural networks.The connection between the Kolmogorov-\nArnold theorem (KAT) and neural networks is not new in the literature  [50, 51, 8, 9, 10, 11, 12, 13],\nbut the pathological behavior of inner functions makes KAT appear unpromising in practice [50].\nMost of these prior works stick to the original 2-layer width-(2n+ 1) networks, which were lim-\nited in expressive power and many of them are even predating back-propagation.  Therefore, most\nstudies were built on theories with rather limited or artificial toy experiments. Our contribution lies\nin generalizing the network to arbitrary widths and depths, revitalizing and contexualizing them in\ntoday’s deep learning stream, as well as highlighting its potential role as a foundation model for AI\n+ Science.\nNeural Scaling Laws (NSLs).NSLs are the phenomena where test losses behave as power laws\nagainst model size,  data,  compute etc [52, 53, 54, 55, 17, 56, 57, 58].   The origin of NSLs still\nremains mysterious, but competitive theories include intrinsic dimensionality [52], quantization of\ntasks [57],  resource theory [58],  random features [56],  compositional sparsity [50],  and maximu\narity [18].  This paper contributes to this space by showing that a high-dimensional function can\nsurprisingly scale as a 1D function (which is the best possible bound one can hope for) if it has\na smooth Kolmogorov-Arnold representation.  Our paper brings fresh optimism to neural scaling\nlaws, since it promises the fastest scaling exponent ever.  We have shown in our experiments that\nthis fast neural scaling law can be achieved on synthetic datasets, but future research is required\nto  address  the  question  whether  this  fast  scaling  is  achievable  for  more  complicated  tasks  (e.g.,\nlanguage modeling):  Do KA representations exist for general tasks?  If so, does our training find\nthese representations in practice?\nMechanistic Interpretability (MI).MI is an emerging field that aims to mechanistically understand\nthe inner workings of neural networks [59, 60, 61, 62, 63, 64, 65, 66, 5]. MI research can be roughly\ndivided into passive and active MI research. Most MI research is passive in focusing on understand-\ning existing neural networks trained with standard methods. Active MI research attempts to achieve\ninterpretability by designing intrinsically interpretable architectures or developing training methods\nto explicitly encourage interpretability [65, 66].  Our work lies in the second category, where the\nmodel and training method are by design interpretable.\nLearnable  activations.The  idea  of  learnable  activations  in  neural  networks  is  not  new  in  ma-\nchine learning.  Trainable activations functions are learned in a differentiable way [67, 13, 68, 69]\nor  searched  in  a  discrete  way  [70].   Activation  function  are  parametrized  as  polynomials  [67],\nsplines  [13,  71,  72],  sigmoid  linear  unit  [68],  or  neural  networks  [69].   KANs  use  B-splines  to\nparametrize their activation functions.  We also present our preliminary results on learnable activa-\ntion networks (LANs), whose properties lie between KANs and MLPs and their results are deferred\nto Appendix B to focus on KANs in the main paper.\n30",
    "Symbolic Regression.There are many off-the-shelf symbolic regression methods based on genetic\nalgorithms  (Eureka  [73],  GPLearn  [74],  PySR  [75]),  neural-network  based  methods  (EQL  [76],\nOccamNet [77]), physics-inspired method (AI Feynman [20, 21]), and reinforcement learning-based\nmethods [78].  KANs are most similar to neural network-based methods, but differ from previous\nworks in that our activation functions are continuously learned before symbolic snapping rather than\nmanually fixed [73, 77].\nPhysics-Informed   Neural   Networks   (PINNs)   and   Physics-Informed   Neural   Operators\n(PINOs).In Subsection 3.4, we demonstrate that KANs can replace the paradigm of using MLPs\nfor imposing PDE loss when solving PDEs.  We refer to Deep Ritz Method [79], PINNs [22, 23]\nfor PDE solving, and Fourier Neural operator [80], PINOs [81, 82, 83], DeepONet [84] for operator\nlearning methods learning the solution map.  There is potential to replace MLPs with KANs in all\nthe aforementioned networks.\nAI for Mathematics.As we saw in Subsection 4.3, AI has recently been applied to several problems\nin Knot theory, including detecting whether a knot is the unknot [85, 86] or a ribbon knot [30], and\npredicting knot invariants and uncovering relations among them [87, 88, 89, 29]. For a summary of\ndata science applications to datasets in mathematics and theoretical physics see e.g. [90, 91], and for\nideas how to obtain rigorous results from ML techniques in these fields, see [92].\n6    Discussion\nIn this section, we discuss KANs’ limitations and future directions from the perspective of mathe-\nmatical foundation, algorithms and applications.\nMathematical aspects:Although we have presented preliminary mathematical analysis of KANs\n(Theorem 2.1),  our mathematical understanding of them is still very limited.   The Kolmogorov-\nArnold representation theorem has been studied thoroughly in mathematics, but the theorem corre-\nsponds to KANs with shape[n,2n+ 1,1], which is a very restricted subclass of KANs.  Does our\nempirical success with deeper KANs imply something fundamental in mathematics?  An appeal-\ning generalized Kolmogorov-Arnold theorem could define “deeper” Kolmogorov-Arnold represen-\ntations beyond depth-2 compositions, and potentially relate smoothness of activation functions to\ndepth.  Hypothetically, there exist functions which cannot be represented smoothly in the original\n(depth-2) Kolmogorov-Arnold representations, but might be smoothly represented with depth-3 or\nbeyond. Can we use this notion of “Kolmogorov-Arnold depth” to characterize function classes?\nAlgorithmic aspects:We discuss the following:\n(1)  Accuracy.   Multiple  choices  in  architecture  design  and  training  are  not  fully  investigated  so\nalternatives can potentially further improve accuracy.  For example, spline activation functions\nmight be replaced by radial basis functions or other local kernels.  Adaptive grid strategies can\nbe used.\n(2)  Efficiency.  One major reason why KANs run slowly is because different activation functions\ncannot leverage batch computation (large data through the same function).  Actually, one can\ninterpolate between activation functions being all the same (MLPs) and all different (KANs),\nby grouping activation functions into multiple groups (“multi-head”), where members within a\ngroup share the same activation function.\n(3)  Hybrid of KANs and MLPs. KANs have two major differences compared to MLPs:\n(i)  activation functions are on edges instead of on nodes,\n31",
    "(ii)  activation functions are learnable instead of fixed.\nWhich change is more essential to explain KAN’s advantage? We present our preliminary results\nin Appendix B where we study a model which has (ii), i.e., activation functions are learnable\n(like KANs), but not (i), i.e., activation functions are on nodes (like MLPs). Moreover, one can\nalso construct another model with fixed activations (like MLPs) but on edges (like KANs).\n(4)  Adaptivity. Thanks to the intrinsic locality of spline basis functions, we can introduce adaptivity\nin the design and training of KANs to enhance both accuracy and efficiency:  see the idea of\nmulti-level training like multigrid methods as in [93, 94], or domain-dependent basis functions\nlike multiscale methods as in [95].\nApplication aspects:We have presented some preliminary evidences that KANs are more effective\nthan MLPs in science-related tasks, e.g., fitting physical equations and PDE solving. We expect that\nKANs may also be promising for solving Navier-Stokes equations, density functional theory, or any\nother tasks that can be formulated as regression or PDE solving. We would also like to apply KANs\nto machine-learning-related tasks, which would require integrating KANs into current architectures,\ne.g., transformers – one may propose “kansformers” which replace MLPs by KANs in transformers.\nKAN  as  a  “language  model”  for  AI  +  ScienceThe  reason  why  large  language  models  are  so\ntransformative is because they are useful to anyone who can speak natural language.  The language\nof science is functions. KANs are composed of interpretable functions, so when a human user stares\nat a KAN, it is like communicating with it using the language of functions.  This paragraph aims\nto promote the AI-Scientist-Collaboration paradigm rather than our specific tool KANs.  Just like\npeople use different languages to communicate, we expect that in the future KANs will be just one\nof the languages for AI + Science, although KANs will be one of the very first languages that would\nenable AI and human to communicate. However, enabled by KANs, the AI-Scientist-Collaboration\nparadigm has never been this easy and convenient, which leads us to rethink the paradigm of how\nwe want to approach AI + Science: Do we want AI scientists, or do we want AI that helps scientists?\nThe intrinsic difficulty of (fully automated) AI scientists is that it is hard to make human preferences\nquantitative, which would codify human preferences into AI objectives. In fact, scientists in different\nfields may feel differently about which functions are simple or interpretable. As a result, it is more\ndesirable  for  scientists  to  have  an  AI  that  can  speak  the  scientific  language  (functions)  and  can\nconveniently interact with inductive biases of individual scientist(s) to adapt to a specific scientific\ndomain.\nFinal takeaway: Should I use KANs or MLPs?\nCurrently, the biggest bottleneck of KANs lies in its slow training.  KANs are usually 10x slower\nthan MLPs, given the same number of parameters.  We should be honest that we did not try hard\nto optimize KANs’ efficiency though,  so we deem KANs’ slow training more as an engineering\nproblem to be improved in the future rather than a fundamental limitation.  If one wants to train a\nmodel fast, one should use MLPs.  In other cases, however, KANs should be comparable or better\nthan MLPs, which makes them worth trying.  The decision tree in Figure 6.1 can help decide when\nto use a KAN. In short, if you care about interpretability and/or accuracy, and slow training is not a\nmajor concern, we suggest trying KANs.\nAcknowledgement\nWe would like to thank Mikail Khona, Tomaso Poggio, Pingchuan Ma, Rui Wang, Di Luo, Sara\nBeery,  Catherine Liang and Matthieu Darcy for fruitful discussion and constructive suggestions.\nZ.L.,  F.R.,  J.H.,  M.S.  and  M.T.  are  supported  by  IAIFI  through  NSF  grant  PHY-2019786.   The\n32",
    "Figure 6.1: Should I use KANs or MLPs?\nwork of FR is in addition supported by the NSF grant PHY-2210333 and by startup funding from\nNortheastern University. Y.W and T.H are supported by the NSF Grant DMS-2205590 and the Choi\nFamily Gift Fund.  S. V. and M. S. acknowledge support from the U.S. Office of Naval Research\n(ONR) Multidisciplinary University Research Initiative (MURI) under Grant No.   N00014-20-1-\n2325 on Robust Photonic Materials with Higher-Order Topological Protection.\nReferences\n[1]  Simon Haykin.Neural networks: a comprehensive foundation. Prentice Hall PTR, 1994.\n[2]  George Cybenko.  Approximation by superpositions of a sigmoidal function.Mathematics of\ncontrol, signals and systems, 2(4):303–314, 1989.\n[3]  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are\nuniversal approximators.Neural networks, 2(5):359–366, 1989.\n[4]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information\nprocessing systems, 30, 2017.\n[5]  Hoagy Cunningham,  Aidan Ewart,  Logan Riggs,  Robert Huben,  and Lee Sharkey.   Sparse\nautoencoders   find   highly   interpretable   features   in   language   models.arXiv   preprint\narXiv:2309.08600, 2023.\n[6]  A.N. Kolmogorov. On the representation of continuous functions of several variables as super-\npositions of continuous functions of a smaller number of variables.Dokl. Akad. Nauk, 108(2),\n1956.\n[7]  Jürgen Braun and Michael Griebel.   On a constructive proof of kolmogorov’s superposition\ntheorem.Constructive approximation, 30:653–675, 2009.\n[8]  David A Sprecher and Sorin Draghici.   Space-filling curves and kolmogorov superposition-\nbased neural networks.Neural Networks, 15(1):57–67, 2002.\n[9]  Mario  Köppen.On  the  training  of  a  kolmogorov  network.InArtificial  Neural  Net-\nworks—ICANN 2002: International Conference Madrid, Spain, August 28–30, 2002 Proceed-\nings 12, pages 474–479. Springer, 2002.\n33",
    "[10]  Ji-Nan Lin and Rolf Unbehauen.  On the realization of a kolmogorov network.Neural Com-\nputation, 5(1):18–20, 1993.\n[11]  Ming-Jun  Lai  and  Zhaiming  Shen.   The  kolmogorov  superposition  theorem  can  break  the\ncurse  of  dimensionality  when  approximating  high  dimensional  functions.arXiv  preprint\narXiv:2112.09963, 2021.\n[12]  Pierre-Emmanuel Leni, Yohan D Fougerolle, and Frédéric Truchetet.  The kolmogorov spline\nnetwork for image processing.   InImage Processing:  Concepts,  Methodologies,  Tools,  and\nApplications, pages 54–78. IGI Global, 2013.\n[13]  Daniele Fakhoury, Emanuele Fakhoury, and Hendrik Speleers. Exsplinet: An interpretable and\nexpressive spline-based neural network.Neural Networks, 152:332–346, 2022.\n[14]  Tomaso Poggio,  Andrzej Banburski,  and Qianli Liao.   Theoretical issues in deep networks.\nProceedings of the National Academy of Sciences, 117(48):30039–30045, 2020.\n[15]  Henry W Lin, Max Tegmark, and David Rolnick.  Why does deep and cheap learning work so\nwell?Journal of Statistical Physics, 168:1223–1247, 2017.\n[16]  Hongyi Xu, Funshing Sin, Yufeng Zhu, and Jernej Barbi\nˇ\nc.  Nonlinear material design using\nprincipal stretches.ACM Transactions on Graphics (TOG), 34(4):1–11, 2015.\n[17]  Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data mani-\nfold.arXiv preprint arXiv:2004.10802, 2020.\n[18]  Eric  J  Michaud,  Ziming  Liu,  and  Max  Tegmark.    Precision  machine  learning.Entropy,\n25(1):175, 2023.\n[19]  Carl De Boor and Carl De Boor.A practical guide to splines, volume 27. springer-verlag New\nYork, 1978.\n[20]  Silviu-Marian Udrescu and Max Tegmark.  Ai feynman:  A physics-inspired method for sym-\nbolic regression.Science Advances, 6(16):eaay2631, 2020.\n[21]  Silviu-Marian  Udrescu,  Andrew  Tan,  Jiahai  Feng,  Orisvaldo  Neto,  Tailin  Wu,  and  Max\nTegmark.  Ai feynman 2.0:  Pareto-optimal symbolic regression exploiting graph modularity.\nAdvances in Neural Information Processing Systems, 33:4860–4871, 2020.\n[22]  Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:\nA deep learning framework for solving forward and inverse problems involving nonlinear par-\ntial differential equations.Journal of Computational physics, 378:686–707, 2019.\n[23]  George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu\nYang. Physics-informed machine learning.Nature Reviews Physics, 3(6):422–440, 2021.\n[24]  Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Mea-\nsuring catastrophic forgetting in neural networks.  InProceedings of the AAAI conference on\nartificial intelligence, volume 32, 2018.\n[25]  Bryan Kolb and Ian Q Whishaw.  Brain plasticity and behavior.Annual review of psychology,\n49(1):43–64, 1998.\n[26]  David Meunier, Renaud Lambiotte, and Edward T Bullmore. Modular and hierarchically mod-\nular organization of brain networks.Frontiers in neuroscience, 4:7572, 2010.\n34",
    "[27]  James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, An-\ndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\nOvercoming catastrophic forgetting in neural networks.Proceedings of the national academy\nof sciences, 114(13):3521–3526, 2017.\n[28]  Aojun Lu, Tao Feng, Hangjie Yuan, Xiaotian Song, and Yanan Sun. Revisiting neural networks\nfor continual learning: An architectural perspective, 2024.\n[29]  Alex Davies, Petar Veli\nˇ\nckovi\n ́\nc, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev,\nRichard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al.  Advancing mathe-\nmatics by guiding human intuition with ai.Nature, 600(7887):70–74, 2021.\n[30]  Sergei Gukov, James Halverson, Ciprian Manolescu, and Fabian Ruehle. Searching for ribbons\nwith machine learning, 2023.\n[31]  P. Petersen.Riemannian Geometry. Graduate Texts in Mathematics. Springer New York, 2006.\n[32]  Philip  W  Anderson.Absence  of  diffusion  in  certain  random  lattices.Physical  review,\n109(5):1492, 1958.\n[33]  David J Thouless.  A relation between the density of states and range of localization for one\ndimensional random systems.Journal of Physics C: Solid State Physics, 5(1):77, 1972.\n[34]  Elihu  Abrahams,  PW  Anderson,  DC  Licciardello,  and  TV  Ramakrishnan.   Scaling  theory\nof localization:  Absence of quantum diffusion in two dimensions.Physical Review Letters,\n42(10):673, 1979.\n[35]  Ad Lagendijk, Bart van Tiggelen, and Diederik S Wiersma.  Fifty years of anderson localiza-\ntion.Physics today, 62(8):24–29, 2009.\n[36]  Mordechai Segev, Yaron Silberberg, and Demetrios N Christodoulides. Anderson localization\nof light.Nature Photonics, 7(3):197–204, 2013.\n[37]  Z Valy Vardeny, Ajay Nahata, and Amit Agrawal.  Optics of photonic quasicrystals.Nature\nphotonics, 7(3):177–187, 2013.\n[38]  Sajeev John. Strong localization of photons in certain disordered dielectric superlattices.Phys-\nical review letters, 58(23):2486, 1987.\n[39]  Yoav Lahini,  Rami Pugatch,  Francesca Pozzi,  Marc Sorel,  Roberto Morandotti,  Nir David-\nson, and Yaron Silberberg.  Observation of a localization transition in quasiperiodic photonic\nlattices.Physical review letters, 103(1):013901, 2009.\n[40]  Sachin Vaidya, Christina Jörg, Kyle Linn, Megan Goh, and Mikael C Rechtsman.  Reentrant\ndelocalization transition in one-dimensional photonic quasicrystals.Physical Review Research,\n5(3):033170, 2023.\n[41]  Wojciech De Roeck, Francois Huveneers, Markus Müller, and Mauro Schiulaz.  Absence of\nmany-body mobility edges.Physical Review B, 93(1):014203, 2016.\n[42]  Xiaopeng Li,  Sriram Ganeshan,  JH Pixley,  and S Das Sarma.   Many-body localization and\nquantum nonergodicity in a model with a single-particle mobility edge.Physical review letters,\n115(18):186601, 2015.\n[43]  Fangzhao Alex An, Karmela Padavi\n ́\nc, Eric J Meier, Suraj Hegde, Sriram Ganeshan, JH Pixley,\nSmitha Vishveshwara,  and Bryce Gadway.   Interactions and mobility edges:  Observing the\ngeneralized aubry-andré model.Physical review letters, 126(4):040603, 2021.\n35",
    "[44]  J  Biddle  and  S  Das  Sarma.   Predicted  mobility  edges  in  one-dimensional  incommensurate\noptical lattices:  An exactly solvable model of anderson localization.Physical review letters,\n104(7):070601, 2010.\n[45]  Alexander Duthie, Sthitadhi Roy, and David E Logan. Self-consistent theory of mobility edges\nin quasiperiodic chains.Physical Review B, 103(6):L060201, 2021.\n[46]  Sriram Ganeshan, JH Pixley, and S Das Sarma. Nearest neighbor tight binding models with an\nexact mobility edge in one dimension.Physical review letters, 114(14):146601, 2015.\n[47]  Yucheng Wang, Xu Xia, Long Zhang, Hepeng Yao, Shu Chen, Jiangong You, Qi Zhou, and\nXiong-Jun Liu. One-dimensional quasiperiodic mosaic lattice with exact mobility edges.Phys-\nical Review Letters, 125(19):196604, 2020.\n[48]  Yucheng Wang, Xu Xia, Yongjian Wang, Zuohuan Zheng, and Xiong-Jun Liu.   Duality be-\ntween two generalized aubry-andré models with exact mobility edges.Physical Review B,\n103(17):174205, 2021.\n[49]  Xin-Chi Zhou, Yongjian Wang, Ting-Fung Jeffrey Poon, Qi Zhou, and Xiong-Jun Liu.   Ex-\nact  new  mobility  edges  between  critical  and  localized  states.Physical  Review  Letters,\n131(17):176401, 2023.\n[50]  Tomaso Poggio.   How deep sparse networks avoid the curse of dimensionality:  Efficiently\ncomputable functions are compositionally sparse.CBMM Memo, 10:2022, 2022.\n[51]  Johannes Schmidt-Hieber.  The kolmogorov–arnold representation theorem revisited.Neural\nnetworks, 137:119–126, 2021.\n[52]  Jared  Kaplan,  Sam  McCandlish,  Tom  Henighan,  Tom  B  Brown,  Benjamin  Chess,  Rewon\nChild,  Scott  Gray,  Alec  Radford,  Jeffrey  Wu,  and  Dario  Amodei.   Scaling  laws  for  neural\nlanguage models.arXiv preprint arXiv:2001.08361, 2020.\n[53]  Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Hee-\nwoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al.  Scaling laws for autoregressive\ngenerative modeling.arXiv preprint arXiv:2010.14701, 2020.\n[54]  Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural\nmachine translation. InACL Rolling Review - May 2021, 2021.\n[55]  Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\nninejad,  Md  Mostofa  Ali  Patwary,  Yang  Yang,  and  Yanqi  Zhou.   Deep  learning  scaling  is\npredictable, empirically.arXiv preprint arXiv:1712.00409, 2017.\n[56]  Yasaman Bahri,  Ethan Dyer,  Jared Kaplan,  Jaehoon Lee,  and Utkarsh Sharma.   Explaining\nneural scaling laws.arXiv preprint arXiv:2102.06701, 2021.\n[57]  Eric J Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural\nscaling. InThirty-seventh Conference on Neural Information Processing Systems, 2023.\n[58]  Jinyeop Song, Ziming Liu, Max Tegmark, and Jeff Gore.  A resource model for neural scaling\nlaw.arXiv preprint arXiv:2402.05164, 2024.\n[59]  Catherine  Olsson,  Nelson  Elhage,  Neel  Nanda,  Nicholas  Joseph,  Nova  DasSarma,  Tom\nHenighan,  Ben Mann,  Amanda Askell,  Yuntao Bai,  Anna Chen,  et al.   In-context learning\nand induction heads.arXiv preprint arXiv:2209.11895, 2022.\n36",
    "[60]  Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\nassociations in gpt.Advances in Neural Information Processing Systems, 35:17359–17372,\n2022.\n[61]  Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\nInterpretability in the wild:  a circuit for indirect object identification in GPT-2 small.  InThe\nEleventh International Conference on Learning Representations, 2023.\n[62]  Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna\nKravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al.  Toy models of\nsuperposition.arXiv preprint arXiv:2209.10652, 2022.\n[63]  Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress mea-\nsures for grokking via mechanistic interpretability.  InThe Eleventh International Conference\non Learning Representations, 2023.\n[64]  Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas.  The clock and the pizza: Two\nstories in mechanistic explanation of neural networks. InThirty-seventh Conference on Neural\nInformation Processing Systems, 2023.\n[65]  Ziming Liu, Eric Gan, and Max Tegmark. Seeing is believing: Brain-inspired modular training\nfor mechanistic interpretability.Entropy, 26(1):41, 2023.\n[66]  Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston,\nSheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda\nAskell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli,\nLiane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav\nFort,  Saurav  Kadavath,  Josh  Jacobson,  Eli  Tran-Johnson,  Jared  Kaplan,  Jack  Clark,  Tom\nBrown, Sam McCandlish, Dario Amodei, and Christopher Olah.  Softmax linear units.Trans-\nformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/solu/index.html.\n[67]  Mohit Goyal, Rajan Goyal, and Brejesh Lall.  Learning activation functions: A new paradigm\nfor understanding neural networks.arXiv preprint arXiv:1906.09529, 2019.\n[68]  Prajit Ramachandran, Barret Zoph, and Quoc V Le.  Searching for activation functions.arXiv\npreprint arXiv:1710.05941, 2017.\n[69]  Shijun Zhang, Zuowei Shen, and Haizhao Yang.  Neural network architecture beyond width\nand depth.Advances in Neural Information Processing Systems, 35:5669–5681, 2022.\n[70]  Garrett Bingham and Risto Miikkulainen. Discovering parametric activation functions.Neural\nNetworks, 148:48–65, 2022.\n[71]  Pakshal  Bohra,  Joaquim  Campos,  Harshit  Gupta,  Shayan  Aziznejad,  and  Michael  Unser.\nLearning activation functions in deep (spline) neural networks.IEEE Open Journal of Sig-\nnal Processing, 1:295–309, 2020.\n[72]  Shayan Aziznejad and Michael Unser.  Deep spline networks with control of lipschitz regu-\nlarity.  InICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 3242–3246. IEEE, 2019.\n[73]  Renáta Dubcáková. Eureqa: software review.Genetic Programming and Evolvable Machines,\n12:173–178, 2011.\n[74]  Gplearn.https://github.com/trevorstephens/gplearn. Accessed: 2024-04-19.\n37",
    "[75]  Miles Cranmer.  Interpretable machine learning for science with pysr and symbolicregression.\njl.arXiv preprint arXiv:2305.01582, 2023.\n[76]  Georg Martius and Christoph H Lampert. Extrapolation and learning equations.arXiv preprint\narXiv:1610.02995, 2016.\n[77]  Owen Dugan, Rumen Dangovski, Allan Costa, Samuel Kim, Pawan Goyal, Joseph Jacobson,\nand Marin Solja\nˇ\nci\n ́\nc.  Occamnet:  A fast neural model for symbolic regression at scale.arXiv\npreprint arXiv:2007.10784, 2020.\n[78]  Terrell N. Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P. Santiago, Daniel faissol, and\nBrenden K. Petersen.  Symbolic regression via deep reinforcement learning enhanced genetic\nprogramming seeding.   In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan,\neditors,Advances in Neural Information Processing Systems, 2021.\n[79]  Bing Yu et al.  The deep ritz method:  a deep learning-based numerical algorithm for solving\nvariational problems.Communications in Mathematics and Statistics, 6(1):1–12, 2018.\n[80]  Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,\nAndrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differ-\nential equations.arXiv preprint arXiv:2010.08895, 2020.\n[81]  Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kam-\nyar Azizzadenesheli, and Anima Anandkumar.  Physics-informed neural operator for learning\npartial differential equations.ACM/JMS Journal of Data Science, 2021.\n[82]  Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya,\nAndrew Stuart, and Anima Anandkumar.  Neural operator:  Learning maps between function\nspaces with applications to pdes.Journal of Machine Learning Research, 24(89):1–97, 2023.\n[83]  Haydn Maust, Zongyi Li, Yixuan Wang, Daniel Leibovici, Oscar Bruno, Thomas Hou, and An-\nima Anandkumar.  Fourier continuation for exact derivative computation in physics-informed\nneural operators.arXiv preprint arXiv:2211.15960, 2022.\n[84]  Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning\nnonlinear operators via deeponet based on the universal approximation theorem of operators.\nNature machine intelligence, 3(3):218–229, 2021.\n[85]  Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski.   Learning to Unknot.\nMach. Learn. Sci. Tech., 2(2):025035, 2021.\n[86]  L. H. Kauffman, N. E. Russkikh, and I. A. Taimanov. Rectangular knot diagrams classification\nwith deep learning, 2020.\n[87]  Mark C Hughes.   A neural network approach to predicting and computing knot invariants.\nJournal of Knot Theory and Its Ramifications, 29(03):2050005, 2020.\n[88]  Jessica Craven, Vishnu Jejjala, and Arjun Kar.  Disentangling a deep learned volume formula.\nJHEP, 06:040, 2021.\n[89]  Jessica Craven, Mark Hughes, Vishnu Jejjala, and Arjun Kar.   Illuminating new and known\nrelations between knot invariants. 11 2022.\n[90]  Fabian Ruehle. Data science applications to string theory.Phys. Rept., 839:1–117, 2020.\n[91]  Y.H.  He.Machine  Learning  in  Pure  Mathematics  and  Theoretical  Physics.    G  -  Refer-\nence,Information and Interdisciplinary Subjects Series. World Scientific, 2023.\n38",
    "[92]  Sergei Gukov, James Halverson, and Fabian Ruehle.  Rigor with machine learning from field\ntheory to the poincaréconjecture.Nature Reviews Physics, 2024.\n[93]  Shumao  Zhang,  Pengchuan  Zhang,  and  Thomas  Y  Hou.    Multiscale  invertible  generative\nnetworks for high-dimensional bayesian inference.  InInternational Conference on Machine\nLearning, pages 12632–12641. PMLR, 2021.\n[94]  Jinchao Xu and Ludmil Zikatanov. Algebraic multigrid methods.Acta Numerica, 26:591–721,\n2017.\n[95]  Yifan Chen, Thomas Y Hou, and Yixuan Wang.   Exponentially convergent multiscale finite\nelement method.Communications on Applied Mathematics and Computation,  pages 1–17,\n2023.\n[96]  Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein.\nImplicit neural representations with periodic activation functions.Advances in neural infor-\nmation processing systems, 33:7462–7473, 2020.\n39",
    "FunctionalityDescriptions\nmodel.train(dataset)training model on dataset\nmodel.plot()plotting\nmodel.prune()pruning\nmodel.fix_symbolic(l,i,j,fun)\nfix the activation functionφ\nl,i,j\nto be the symbolic functionfun\nmodel.suggest_symbolic(l,i,j)\nsuggest symbolic functions that match\nthe numerical value ofφ\nl,i,j\nmodel.auto_symbolic()\nuse top 1 symbolic suggestions fromsuggest_symbolic\nto replace all activation functions\nmodel.symbolic_formula()return the symbolic formula\nTable 7: KAN functionalities\nAppendix\nA    KAN Functionalities\nTable 7 includes common functionalities that users may find useful.\nB    Learnable activation networks (LANs)\nB.1    Architecture\nBesides KAN, we also proposed another type of learnable activation networks (LAN), which are\nalmost MLPs but with learnable activation functions parametrized as splines. KANs have two main\nchanges to standard MLPs:  (1) the activation functions become learnable rather than being fixed;\n(2) the activation functions are placed on edges rather than nodes. To disentangle these two factors,\nwe also propose learnable activation networks (LAN) which only has learnable activations but still\non nodes, illustrated in Figure B.1.\nFor a LAN with widthN, depthL, and grid point numberG, the number of parameters isN\n2\nL+\nNLGwhereN\n2\nLis the number of parameters for weight matrices andNLGis the number of\nparameters  for  spline  activations,  which  causes  little  overhead  in  addition  to  MLP  since  usually\nG≪NsoNLG≪N\n2\nL.  LANs are similar to MLPs so they can be initialized from pretrained\nMLPs  and  fine-tuned  by  allowing  learnable  activation  functions.   An  example  is  to  use  LAN  to\nimprove SIREN, presented in Section  B.3.\nComparison of LAN and KAN.Pros of LANs:\n(1)  LANs are conceptually simpler than KANs. They are closer to standard MLPs (the only change\nis that activation functions become learnable).\n(2)  LANs scale better than KANs. LANs/KANs have learnable activation functions on nodes/edges,\nrespectively. So activation parameters in LANs/KANs scale asN/N\n2\n, whereNis model width.\nCons of LANs:\n(1)  LANs seem to be less interpretable (weight matrices are hard to interpret, just like in MLPs);\n(2)  LANs also seem to be less accurate than KANs, but still more accurate than MLPs. Like KANs,\nLANs also admit grid extension if theLANs’ activation functions are parametrized by splines.\n40",
    "Figure B.1: Training of a learnable activation network (LAN) on the toy examplef(x,y) = exp(sin(πx)+y\n2\n).\nFigure B.2: LANs on synthetic examples. LANs do not appear to be very interpretable. We conjecture that the\nweight matrices leave too many degree of freedoms.\nB.2    LAN interpretability results\nWe present preliminary interpretabilty results of LANs in Figure B.2.  With the same examples in\nFigure 4.1 for which KANs are perfectly interpretable, LANs seem much less interpretable due to\nthe existence of weight matrices.  First, weight matrices are less readily interpretable than learn-\nable activation functions.  Second, weight matrices bring in too many degrees of freedom, making\nlearnable activation functions too unconstrained. Our preliminary results with LANs seem to imply\nthat getting rid of linear weight matrices (by having learnable activations on edges, like KANs) is\nnecessary for interpretability.\nB.3    Fitting Images (LAN)\nImplicit neural representations view images as 2D functionsf(x,y), where the pixel valuefis a\nfunction of two coordinates of the pixelxandy.   To compress an image,  such an implicit neu-\nral representation (fis a neural network) can achieve impressive compression of parameters while\nmaintaining almost original image quality.  SIREN [96] proposed to use MLPs with periodic acti-\nvation functions to fit the functionf.  It is natural to consider other activation functions, which are\n41",
    "Figure B.3:  A SIREN network (fixed sine activations) can be adapted to LANs (learnable activations) to im-\nprove image representations.\nallowed in LANs.  However, since we initialize LAN activations to be smooth but SIREN requires\nhigh-frequency features,  LAN does not work immediately.   Note that each activation function in\nLANs is a sum of the base function and the spline function, i.e.,φ(x) =b(x) + spline(x), we set\nb(x)to sine functions, the same setup as in SIREN but letspline(x)be trainable.  For both MLP\nand LAN, the shape is [2,128,128,128,128,128,1].  We train them with the Adam optimizer, batch\nsize 4096, for 5000 steps with learning rate10\n−3\nand 5000 steps with learning rate10\n−4\n. As shown\nin Figure B.3, the LAN (orange) can achieve higher PSNR than the MLP (blue) due to the LAN’s\nflexibility to fine tune activation functions. We show that it is also possible to initialize a LAN from\nan MLP and further fine tune the LAN (green) for better PSNR. We have chosenG=  5in our\nexperiments, so the additional parameter increase is roughlyG/N= 5/128≈4%over the original\nparameters.\nC    Dependence on hyperparameters\nWe show the effects of hyperparamters on thef(x,y) = exp(sin(πx) +y\n2\n)case in Figure C.1. To\nget an interpretable graph, we want the number of active activation functions to be as small (ideally\n3) as possible.\n(1)  We need entropy penalty to reduce the number of active activation functions.  Without entropy\npenalty, there are many duplicate functions.\n(2)  Results can depend on random seeds.  With some unlucky seed, the pruned network could be\nlarger than needed.\n(3)  The overall penalty strengthλeffectively controls the sparsity.\n(4)  The grid numberGalso has a subtle effect on interpretability.  WhenGis too small, because\neach one of activation function is not very expressive, the network tends to use the ensembling\nstrategy, making interpretation harder.\n(5)  The piecewise polynomial orderkonly has a subtle effect on interpretability.  However, it be-\nhaves a bit like the random seeds which do not display any visible pattern in this toy example.\nD    Feynman KANs\nWe include more results on the Feynman dataset (Section 3.3).  Figure D.1 shows the pareto fron-\ntiers of KANs and MLPs for each Feynman dataset.  Figure D.3 and D.2 visualize minimal KANs\n42",
    "Figure C.1: Effects of hyperparameters on interpretability results.\n(under the constraint test RMSE<10\n−2\n) and best KANs (with the lowest test RMSE loss) for each\nFeynman equation fitting task.\nE    Remark on grid size\nFor both PDE and regression tasks, when we choose the training data on uniform grids, we witness\na sudden increase in training loss (i.e., sudden drop in performance) when the grid size is updated to\na large level, comparable to the different training points in one spatial direction.  This could be due\nto implementation of B-spline in higher dimensions and needs further investigation.\nF    KANs for special functions\nWe include more results on the special function dataset (Section 3.2).  Figure F.2 and F.1 visualize\nminimal  KANs  (under  the  constraint  test  RMSE<10\n−2\n)  and  best  KANs  (with  the  lowest  test\nRMSE loss) for each special function fitting task.\n43",
    "10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nI.6.2\nKAN train\nKAN test\nReLU MLP train\nReLU MLP test\nTanh MLP train\nTanh MLP test\nSiLU MLP train\nSiLU MLP test\nI.6.2bI.9.18I.12.11\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nI.13.12I.15.3xI.16.6I.18.4\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nI.26.2I.27.6I.29.16I.30.3\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nI.30.5I.37.4I.40.1I.44.4\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nI.50.26II.2.42II.6.15aII.11.7\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nII.11.27II.35.18II.36.38\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nII.38.3\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\n10\n7\n10\n5\n10\n3\n10\n1\nRMSE\nIII.9.52\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nIII.10.19\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nIII.17.37\n10\n1\n10\n2\n10\n3\n10\n4\nnumber of parameters\nIII.17.37\nFigure D.1: The Pareto Frontiers of KANs and MLPs for Feynman datasets.\n44",
    "Figure D.2: Best Feynman KANs\n45",
    "Figure D.3: Minimal Feynman KANs\n46",
    "Figure F.1: Best special KANs\n47",
    "Figure F.2: Minimal special KANs\n48"
  ]
}