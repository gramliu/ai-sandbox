{
  "key": "K32KHB3N",
  "url": "http://arxiv.org/pdf/2403.09040",
  "metadata": {
    "title": "RAGGED: Towards Informed Design of Retrieval Augmented Generation\n  Systems",
    "abstract": "  Retrieval-augmented generation (RAG) greatly benefits language models (LMs)\nby providing additional context for tasks such as document-based question\nanswering (DBQA). Despite its potential, the power of RAG is highly dependent\non its configuration, raising the question: What is the optimal RAG\nconfiguration? To answer this, we introduce the RAGGED framework to analyze and\noptimize RAG systems. On a set of representative DBQA tasks, we study two\nclassic sparse and dense retrievers, and four top-performing LMs in\nencoder-decoder and decoder-only architectures. Through RAGGED, we uncover that\ndifferent models suit substantially varied RAG setups. While encoder-decoder\nmodels monotonically improve with more documents, we find decoder-only models\ncan only effectively use &lt; 5 documents, despite often having a longer context\nwindow. RAGGED offers further insights into LMs' context utilization habits,\nwhere we find that encoder-decoder models rely more on contexts and are thus\nmore sensitive to retrieval quality, while decoder-only models tend to rely on\nknowledge memorized during training.\n",
    "published": "2024-03-14T02:26:31Z"
  },
  "text": [
    "RAGGED: Towards Informed Design of\nRetrieval Augmented Generation Systems\nJennifer Hsia\n*\nAfreen Shaikh\n∗\nZhiruo WangGraham Neubig\nCarnegie Mellon University\n{jhsia2,afreens,zhiruow,gneubig}@cs.cmu.edu\nAbstract\nRetrieval-augmented generation (RAG) greatly\nbenefits language models (LMs) by providing\nadditional context for tasks such as document-\nbased  question  answering  (DBQA).  Despite\nits  potential,  the  power  of  RAG  is  highly\ndependent  on  its  configuration,  raising  the\nquestion:What  is  the  optimal  RAG  config-\nuration?To  answer  this,  we  introduce  the\nRAGGED framework to analyze and optimize\nRAG  systems.On  a  set  of  representative\nDBQA tasks, we study two classic sparse and\ndense retrievers, and four top-performing LMs\nin encoder-decoder and decoder-only architec-\ntures. Through RAGGED, we uncover thatdif-\nferent models suit substantially varied RAG se-\ntups. While encoder-decoder models monoton-\nically improve with more documents, we find\ndecoder-only models can only effectively use<\n5\ndocuments, despite often having a longer con-\ntext window. RAGGED offers further insights\ninto LMs’ context utilization habits, where we\nfind that encoder-decoder models rely more on\ncontexts and are thus more sensitive to retrieval\nquality, while decoder-only models tend to rely\non knowledge memorized during training.\n1\n1    Introduction\nRetrieval-augmented  generation  (RAG)  (Chen\net al., 2017a; Lewis et al., 2020) is a technique\nwidely applied to enhance the performance of top-\nperforming LMs on knowledge-intensive genera-\ntion tasks like document-based question answering\n(Karpukhin et al., 2020).   Given a question,  the\ntechnique includes using aretrievermodel to ob-\ntain multiple relevant passages (i.e.  paragraphs)\nacross  potentially  different  documents,  then  in-\nputting these passages to areadermodel as ad-\nditional contexts for generating an answer.\n*\nEqual contribution.\n1\nCode/data for the RAGGED framework are available at\nhttps://github.com/neulab/ragged\n...\nReaderRetriever\n... \nwhich \none?\nwhich \none?\nhow \nmany?\ndocuments\n§7\n§5\n§6\nFigure 1: Illustration of our RAGGED framework.\n4k token limit2k token limit\nDecoder\n-only\nEncoder\n-decoder\nreluctant to \nmore docs\nsensitive to \nnoisy docs\nrobust to more, \nnoisy docs\nmemorize more \nknowledge\nQA acc\n\nFigure  2:   Example  insight  from  using  RAGGED:\nDecoder-only models (e.g.,LLAMA) memorize more\nknowledge from training, yet are reluctant to use pro-\nvided  contexts  and  are  noise-sensitive.   In  contrast,\nencoder-decoder models (e.g.,FLAN) monotonically\nimprove with more provided contexts.\nHowever, using RAG effectively is not straight-\nforward,  and  existing  literature  provides  mixed,\neven  contradictory,  suggestions  for  configuring\nRAG. While different LMs have different context\nlength limits (Chung et al., 2022; Tay et al., 2023;\nTouvron et al., 2023b; Liu et al., 2023), these limits\nalone do not simply determine the optimal num-\nber of passages to provide in context. Early works\nsuggest that providing more retrieved passages re-\nsults in strictly better outputs (Izacard and Grave,\n2021).  In contrast, other works find that provid-\ning a selected set of passages (Asai et al., 2022),\nsentences (Wang et al., 2023), or tokens (Berchan-\nsky et al., 2023) outperforms providing the full set,\npresumably due to increased context relevance and\nquality.  In addition to context quality and quan-\ntity, how robust a reader model is to noisy context\nalso determines downstream performance. For ex-\nample,  Hoffmann  et  al.  (2022)  train  models  on\narXiv:2403.09040v1  [cs.CL]  14 Mar 2024",
    "high-quality data, while Yu et al. (2023) intention-\nally inject noisy content into input context during\nmodel training, and observe increased robustness\nof these models to low-quality contexts.\nTo  provide  more  concrete  suggestions  of  the\nbest practicesunder various cases, we introduce\nan analysis framework, RAGGED,\n2\nto test RAG\ncombinations on a suite of representative document-\nbased question answering (DBQA) tasks, includ-\ning open-domain datasets like Natural Questions\n(Kwiatkowski et al., 2019) and HotpotQA (Yang\net al., 2018), which respectively focus on single-\nhop and multi-hop questions, as well as BioASQ,\nwhich targets the specialized, biomedical domain.\nTo ensure a comprehensive evaluation, we incorpo-\nrate both classic sparse and dense retrievers, BM25\n(Robertson et al., 2009) and ColBERT (Khattab\nand Zaharia, 2020), and four top-performing reader\nmodels in encoder-decoder and decoder-only ar-\nchitectures, fromFLAN(Chung et al., 2022; Tay\net al., 2023) andLLAMA(Touvron et al., 2023b)\nfamilies, respectively.\nWe begin by exploring”How many contexts can\nreaders benefit from?”(§5).  Our analysis iden-\ntifies the optimal context quantity for individual\nLMs.  We find that encoder-decoder models can\neffectively utilize up to 30 passages within their 2k-\ntoken limit, whereas decoder-only models’ perfor-\nmance declines beyond 5 passages, despite having\ntwice the size of the context limit (4k).\nGiven this intriguing difference, we investigate\nthe models’ context utilization behaviors (§6) and\nask”How  reliant  are  models  on  provided  con-\ntexts?”. We find that decoder-only models, which\nmemorize more during training, exhibit compara-\ntively less reliance on additional, test-time contexts.\nIn contrast, encoder-decoder models, which mem-\norize less during training, are more faithful to the\nprovided contexts. This suggests that providing pas-\nsages for context-reliant encoder-decoder models is\nbeneficial, whereas it is less so for memory-reliant\ndecoder-only models.\nGiven that some models are reliant on context,\nwe also examine the importance of context quality\nby asking“How does the retriever quality affect\nreaders’  contextualization  behavior?”(§7)  Our\nanalysis considers two aspects:  a retriever’s abil-\nity to identify high-quality passages and a reader’s\nresponse to varying passage quality. While dense,\n2\nFor “retrieval augmented generation generalized evalua-\ntion device”.\nneural retrievers perform better on open-domain\nquestions, sparse, lexical retrievers readily achieve\ncomparable  accuracy  on  special  domains,  with\nmuch less computation. Neural retrievers’ advan-\ntage readily benefits encoder-decoder models, espe-\ncially for single-hop questions. However, the ben-\nefits are much less pronounced for decoder-only\nmodels and multi-hop questions.\nIn summary, we demonstrate how RAGGED en-\nables us to derive actionable insights about the con-\nditions under which state-of-the-art RAG compo-\nnents combine to excel.  We introduce a reusable\nframework that can easily be adapted to analyze\nnew RAG components, such as retriever and reader\nmodels, as they evolve. We release our full dataset\nand code, aiming to provide the community with\na deeper understanding of the nuanced interplay\nbetween context quantity, quality, and model archi-\ntecture in RAG systems.\n2    The RAGGED Framework\n2.1    Framework Overview\nWe first explain the three aspects we vary in our\nanalysis, then explain the three research questions\nwe can answer with our analysis. The three aspects\nwe vary in our framework are:\n•RAG system components.  For example, we\nvary the choice of the retriever (e.g., BM25,\nColBERT), the reader family, (e.g.,LLAMA2,\nFLANT5), and the max input token length.\n•\nThe number of retrieved passages,  denoted\nask. We vary fromkfrom 1 to 100, though\nfind the most insightful variations in behavior\noccur beforek= 30.\n•Data  slices  of  data  to  examine.   For  exam-\nple,  we  examine  slices where  the  top-kre-\ntrieved passages include gold passages and\nwhere they do not include gold passages.\nBy varying these, we analyze the three following\naspects of RAG system behavior:\nEffective Number of Context Passages (§5)We\nvary  the  number  of  passages  and  the  choice  of\nreader  to  see  how  different  model  architectures\nand context limits affect the effective number of\ncontext passages a reader can process.  Here, we\nevaluate all instances instead of specific subsets.\nWhile some models monotonically improve with\nmore provided passages, others may have an early",
    "peak and have limited ability to utilize more pas-\nsages.\nContext Utilization Behaviors (§6)We focus\non varying the choice of reader and the slice of\ninstances to examine how different readers use con-\ntext when the context quality varies. We examine\nthe impact of context quality by analyzing slices\nof data where the top-k retrieved passages include\ngold passages and when they do not. As a result of\nthis analysis, one can better understand how sensi-\ntive a reader is to positive context and how robust\nit is against incorrectly retrieved passages. We also\ninvestigate when models benefit from context and\nwhen models may be harmed by context.\nInfluence of Retriever Quality (§7)We focus\non varying the retriever and data domain, and ob-\nserving how well retrievers can perform based on\nthe nature of the questions and how sensitive read-\ners are to the quality of the retrieved passages. As\nreader models may have varied sensitivity to re-\ntrieval quality, one could select more appropriate\nmodels given the question characteristics and re-\ntrieval performance.\n2.2    Implementation Details\nFor all experiments, we use the following prompt:\nInstruction:Give simple short one phrase answers\nfor the questions based on the context\nContext:[passage\n1\n, passage\n2\n,···, passage\nk\n]\nQuestion:[the question of the current example]\nAnswer:\n.\nWe truncate theContextto make sure the latter\nquestionis still included in the prompt. While per-\nforming generation for with top-kpassages with\n∀k∈ {1,2,···,30}requires demanding compu-\ntation,  we  samplek∈ {1,2,3,5,10,20,30}to\nrepresent the general trend. More model implemen-\ntation details can be found in §A.\n3    Experimenting with RAG Systems\nBelow, we explain the representative retrievers and\nreaders we analyze in our experiments.\n3.1    Retriever\nFor  retrieval,   we  employ  two  contrasting  ap-\nproaches: (1) sparse retrieval, based on lexical in-\nformation, and (2) dense retrieval, based on neural\nmodels.\nBM25BM25, a probabilistic retrieval model for-\nmulated by Robertson et al. (2009), leverages TF-\nIDF  (Chen  et  al.,  2017b)  principles  to  estimate\npassage relevance via term weighting and passage\nlength normalization. BM25 uses term-matching\nand hence experiences fewer out-of-domain fail-\nures when facing special-domain vocabulary, such\nas legal or medical texts.\nColBERTOne  of  the  best-performing  neural-\nbased retrievers is ColBERT (Khattab and Zaharia,\n2020),  i.e.,  contextualized  late  interaction  over\nBERT. The transformer-based, contextualized em-\nbedding of ColBERT makes it more proficient than\nBM25 at identifying semantic similarities between\nqueries and passages beyond lexical matching.\n3.2    Reader\nWe  compare  four  top-performing,  open-source\nreader  models  of  varied  architectures:  encoder-\ndecoder  models  from  theFLANfamily,   and\ndecoder-only models from the LLAMA2 family.\nFLANModelsTheFLANmodels are encoder-\ndecoder  models.We  use  theFLANT5-XXL\n(Chung  et  al.,  2022)  with  11B  parameters  and\nFLAN-UL2  (Tay  et  al.,  2023)  with  20B  param-\neters,  both  with  a  context  length  of  2ktokens.\nFLANT5-XXL is an instruction-tuned variant of\nthe T5 model (Raffel et al., 2023).FLAN-UL2\n(Tay et al., 2023) is an upgraded T5-based model\nthat is trained with Unifying Language Learning\nParadigm, a pertaining process that uses a mixture-\nof-denoisers and mode switching to improve the\nmodel’s adaptability to different scenarios.\nLLAMAModelsThe  Llama2  models  are  de-\ncoder models. ForLLAMAmodels (Touvron et al.,\n2023a), we adopt the latestLLAMAmodels (Tou-\nvron et al., 2023b) in 7B and 70B parameters, both\nhaving  a  context  length  of  4k  tokens.   The  key\nfeature is thatLLAMAmodels are trained with\nreinforcement learning human feedback (RLHF).\n4    Datasets and Evaluation Metrics\n4.1    Dataset\nWe adopt three DBQA datasets from various do-\nmains (Wikipedia, biomedical) and of various types\n(single-hop, multi-hop, list, yes/no).\nNatural QuestionsWe choose the Natural Ques-\ntions (NQ) dataset (Kwiatkowski et al., 2019) to\nexamine model performance on the most generic",
    "open-domain,  single-hop  questions.    NQ  ques-\ntions are real user-search queries on Google.  We\nadopt the KILT version (Petroni et al., 2021) of the\ndataset, which for each example, provides at least\none gold relevant passage and one short answer.\nHotpotQAWe choose HotpotQA (Yang et al.,\n2018) which provides challenging multi-hop ques-\ntions.   Each question requires reasoning over at\nleast two passages to answer.  While maintaining\nthe same Wikipedia domain with the NQ dataset,\nHotpotQA enables comparisons of model reason-\ning skills over multiple evidence pieces.\nBioASQWe   choose   BioASQ’s   Task   11B\n(Krithara et al., 2023) with biomedical questions\nas  a  representative  of  special-domain  questions.\nOur  evaluation  dataset  is  a  compilation  of  the\nBioASQ Task 11B training and golden enriched\nset.  BioASQ also presents challenging question\ntypes, such as lists and yes/no questions.\nFor NQ and HotpotQA datasets in the open do-\nmain, we retrieve passages from the Wikipedia cor-\npus provided by the KILT benchmark (Petroni et al.,\n2021), with a total of 111M passages. For BioASQ,\nwe use the PubMed Annual Baseline Repository\nfor 2023 (of Medicine, 2023), with a total of 58M\npassages,  which  are  either  titles  or  passages  of\nPubMed papers.\n4.2    Metrics\nBelow we describe the metric definition for each\ninstance. Overall performance on a dataset is com-\nputed by averaging over all instances.\n4.2.1    Retrieval\nFollowing Petroni et al. (2021),  we evaluate re-\ntrieval performance using the recall@k metric.\nRecall@kFor a given example with a query and\nground-truth  passage(s),  recall@k  measures  the\nratio of ground-truth passages among the top-kre-\ntrieved passages. This measure can readily support\nretrieval for multi-hop questions (i.e., in HotpotQA)\nas well, where instead of retrieving any one of the\ngold passages,  all passages along the reasoning\npath are necessary.\n4.2.2    Reader Metrics\nWe  evaluate  the  reader  predictions  using  exact\nmatch and unigram F\n1\nmetrics.\nExact MatchExact match (EM) measures if the\nmodel-generated output is the same as at least one\nof  the  ground-truth  answers  (Richardson  et  al.,\n2013). This metric requires models to be precise,\nyet may be overly strict when models produce ver-\nbose answers.  Given the extractive nature of the\nNQ dataset (i.e., answers are spans in supporting\npassages), we use EM to evaluate NQ.\nUnigram F\n1\nUnigram F\n1\nis a well-adopted met-\nric for QA, which quantifies the overlap of uni-\ngrams in the generated and reference texts, com-\nputed through the harmonic mean of precision and\nrecall at the unigram level (Petroni et al., 2021).\nFor each query, we compute the F\n1\nscore of the\nreader output against all possible gold answers and\nreport the highest score. This is suitable if we do\nnot need the generated answer to match the exact\nformatting and wording of the ground truth answer.\nFor example, “five seasons” and “5 seasons” would\nfail EM but can get a partial credit of 0.5 by F\n1\n. We\nevaluate HotpotQA and BioASQ with F\n1\nmetric.\n5    Are More Contexts Always Better?\nIn this section, we study how models perform with\nvarious numbers of passages (§5.2), and if these\nresults relate to context limits (§5.3).\n5.1    RAGGED Analysis Guidelines\nWhat to Vary:Experiment with different num-\nbers  of  context  passages  provided  to  the  reader\nmodel. Start from a small number first since many\nreader models’ performance peaks before their con-\ntext limits are reached.\nBehaviors to Expect:Expect a non-linear rela-\ntionship between the number of contexts and model\nperformance. Initially, reader performance may im-\nprove with more contexts due to the increased avail-\nability of “signal”, or supporting information. How-\never, as the number of context passages increases,\nthe amount of “noise”, or irrelevant, information\nalso increases. As a result, reader performance may\nplateau or even degrade.\nModel Behavior Implications:Fitting as many\ncontexts as the reader’s context limit can hold does\nnot always guarantee optimal downstream perfor-\nmance. Using these experiments, one can better un-\nderstand a reader’s ability to sift for “signal” among\n“noise”, and thereby find the optimal range of con-\ntext passages for your model. Staying within this\nrange maximizes performance without incurring\nunnecessary computational costs.",
    "Reader Performance\nTop-k documents\nNQHotpotQABioASQ\nFlanT5FlanUL2LLaMa 7BLLaMa 70BLLaMa 7B (trun)LLaMa 70B (trun)\n1322\n+17\n+33\nFigure 3: Reader output scores when using varying numbers of passages on three datasets. Colored circles mark the\nbest performance on the line. Dashed lines indicate results when truncating LLaMa inputs by 2ktokens. For NQ,\nwe use an exact match. For HotpotQA and BioASQ, we use F1.\n5.2    Number of Provided Contexts\nWe analyze the effect of the number of retrieved\npassages on reader model performance by provid-\ning the top-kpassages retrieved by ColBERT and\nevaluating the EM scores of answers generated by\nfour reader models.\nResults in Figure 3 reveal substantially differ-\nent patterns betweenFLANandLLAMAmodels.\nOn the NQ dataset, EM scores of bothLLAMA\nmodels peak early atk≤3before continuing to\ndecline steadily.  In contrast,FLANmodels peak\nlater aroundk= 20,  and only level off instead\nof declining.  When conditioning on a total of 30\npassages,LLAMA7B andLLAMA70B underper-\nformFLANmodels by about 17 to 33 point increase\nin exact match.\nSimilarly on multi-hop questions,FLANmod-\nels can still benefit from more than 10 passages,\nwhereasLLAMAcan only benefit from 1 or 2 pas-\nsages.  In contrast, on the BioASQ dataset, both\nmodels have a small peak around 2 passages and\nroughly maintain the results afterwards.\nThese findings imply thatencoder-decoder mod-\nels can more effectively process tens of passages.\nIn contrast,decoder-only models can use at most\n2–3 passages in all cases. This observation under-\nscores the necessity to carefully select the number\nof provided passages for different models to opti-\nmize their performance.\n5.3    Context Limit vs. Early Truncation\nDespiteLLAMAmodels having twice the context\nlength ofFLANmodels (4kversus 2k), they do not\nachieve superior performance when there is a large\nnumber of contexts.  Nonetheless,  this evidence\nalone can not allow us to claim thatLLAMAmod-\nels are worse thanFLANmodels at processing long\ncontexts.LLAMA’s longer context window could\nbe a confounding factor that exposes the model\nto more (noisy) passages.   To make a fair com-\nparison, we truncateLLAMAmodel inputs by 2k\ntokens as we do to theFLANmodels, then evaluate\nLLAMA’s truncated-context performance.\nAs shown by the dashed lines in Figure 3, early\ntruncation can only preventLLAMAfrom further\ndegrading after 15 passages on Wikipedia-domain\nquestions  but  it  doesnotimprove  results  with\nsmallerk.   The  best-performingLLAMA(70B,\ntruncated) still scores lower than the smallestFLAN\nmodel, despite an almost 7 times difference in their\nnumber of parameters (70B vs. 11B).\nThis  outcome  challenges  the  assumption  that\nmodels trained with extended context limits are\ninherently better at processing more contexts. Con-\ntrary  to  expectations,  our  results  show  thatthe\nshort-window encoder-decoder models are more\nefficient in leveraging long contexts.\nBased on this intriguing observation, in the fol-\nlowing sections, we investigate the cause of this\ndifference in context utilization, from models’ will-\ningness to integrate external contexts (§6) and the\nquality of retrieved passages (§7).\n6    Context Utilization Habits\nWe study how readers utilize contexts, by examin-\ning how capable models are at answering questions\nwithout context, but instead simply from their pre-\ntrained knowledge and generalization capabilities\n(§6.2). We then study how the readers incorporate\ntest-time contexts with varied qualities (§6.3, §6.4).\n6.1    RAGGED Analysis Guidelines\nWhat to Vary:Analyze the reader model’s per-\nformance using different slices of instances that\nrepresent different qualities of retrieved contexts.\nThe first slice is where the retrieved context in-",
    "cludes at least one gold, human-annotated passage.\nIn this scenario, there is sufficient information in\nthe top-k passages to answer the question.   The\nalternative reader context settings to compare with\nare when there are a) only the gold passages in the\ntop-k retrieved passages and b) no context. Com-\nparing with a) shows how good the reader is at\nignoring noise and how close it is to using only\nsignal and no noise. Comparing with b) shows if\nthe reader is prone to performing worse\nThe second slice of instances is where none of\nthe top-k retrieved contexts include gold passages.\nIn this scenario, all the passages are noise.  The\nalternative reader context setting to compare with\nis when there is no context.\nBehaviors to Expect:Models may show vary-\ning degrees of sensitivity to the quality of context.\nReader models provided with only gold passages\noften act as an upper bound for the reader models’\nperformance with top-k passages.  Although one\nmight expect reader models provided with no con-\ntext passages to act as a lower bound for the reader\nmodels’ performance with top-k passages, that is\nnot always the case. It depends on the reader’s abil-\nity to discern between potentially sufficient internal\nknowledge and irrelevant context knowledge.\nImplications  of  Behaviors:For  practitioners,\nthese comparisons would tell us how effective the\nreader model sifts for signal among noise.  If the\nmodel performs very closely to how it would with\nonly gold passages, then it is highly effective at\nusing relevant information and ignoring irrelevant\nones. Conversely, a significant drop in performance\nwith lower-quality contexts suggests a reliance on\nhigh-quality retrieval and potential vulnerability to\nnoise.  This analysis can guide the development\nor selection of more robust models that can main-\ntain performance despite the variability in context\nquality.\n6.2    No-context Generalization from\nPre-trained Knowledge\nWe evaluate how capable models are at answer-\ning questions directly from their parameters and\nwithout any context by evaluating reader test-time\nperformance without any context. We denote this\nsetup asno-ctxand compare it with the standard\nsetting withtop-1passages from ColBERT.\nAs shown in Table 1, under the no-context set-\nting,LLAMA70B outperformsFLANmodels by a\nlarge margin, suggesting they have at leastencoded\nModel\nNQHotpotQABioASQ\nno-ctxtop-1no-ctxtop-1no-ctxtop-1\nFlan-T5 XXL16.733.827.443.518.240.0\nFlan-UL223.735.029.643.419.230.2\nLLaMa2 7B21.529.223.538.221.132.8\nLLaMa2 70B34.135.632.546.426.035.9\nTable 1: Performance of four reader models when using\nno context or top-1 retrieved passages.\nmore knowledge from pretraining or are better at\ngeneralizing from memorized knowledge.  While\nLLAMAhas potentially memorized more relevant\ninformation during training, this knowledge may\nbe more likely to cause conflicts with passages pro-\nvided in context at test time. Next, we examine two\nparticular slices of examples and study how reader\nmodels resolve conflict with irrelevant passages\nwhen gold (human-annotated) passages are present\n(§6.3) or absent (§6.4).\n6.3    With Ground-Truth Passages\nWe study how readers handle context with both\npositive (gold) passages and negative (non-gold)\npassages.Specifically,   for  each  givenk∈\n{1,···,50}\n,  we first find the slice of examples\nwhere the retriever returns at least one gold passage\nin top-kcandidates.  With this slice of examples,\nwe compare the results when providing (1) top-\nkretrieved passages, (2) only the gold passage(s)\nwithin the top-k passages, and (3) no passages at\nall.\nTop-k documents\nExact Match (%)\nno-ctxtop-k\nworse than no-ctx\nbetter than \nno-ctx\nno-ctxtop-k\nExact Match (%)\nTop-k documents\nbenefit from \nnegative ctx\ndegraded by \nnegative ctx\nFlanT5\nFlanUL2\nLLaMa 7B\nLLaMa 70B\nFlanT5\nFlanUL2\nLLaMa 7B\nLLaMa 70B\nLLaMa 7B\nLLaMa 70B\nFigure 4: NQ results when gold passages are included\nin the top-k passages. Gray lines on the top show results\nwhen providing only the gold passages within the top-k\nretrieved passages.\nFigure 4 shows thatLLAMAmodels are more\nseverely affected by noisy context, as their scores\ndecrease  more  sharply  askincreases.   Notably,\naroundk= 15andk= 20,  LLAMA7B  and",
    "70B models’ performance deteriorates to a level\nbelow that of their no-context baseline. In compar-\nison,FLANmodels consistently outperform their\nno-context counterparts by a large margin.\nSpecial Questions and DomainsWe perform\nsimilar   analyses   on   HotpotQA   and   BioASQ\ndatasets. WhileFLANmodels exhibit trends akin to\ntheir open-domain behaviors, we observe substan-\ntial behavioral shifts onLLAMAmodels (Figure 5).\nUnlike the severe degradation on NQ questions,\nLLAMAconsistently outperforms the no-context\nbaseline.Additional special-domain contexts seem\nto induce less conflict withLLAMA’s parametric\nknowledgethan additional contexts do in NQ poten-\ntially because they provide new, useful information\nthat the model cannot derive from its pertaining\nmemory alone.\nHotpotQA\nBioASQ\nTop-k documents\nF1 (%)\nno-ctxtop-k\nLLaMa 7B\nLLaMa 70B\nFigure  5:LLAMAresults  on  HotpotQA  (left)  and\nBioASQ (right) when gold passages are provided. Gray\nlines on the top indicate gold performance.\n6.4    Response to Negative passages\nWe now study the setting where no passages are\nexplicitly useful to answer the questions. At eachk,\nwe select the slice of examples where no gold pas-\nsages exist among the top-kretrieved candidates.\nWe report model results when providing (1) these\nknegative contexts or (2) no contexts at all.\nAs shown in Figure 6, compared to theno-ctx\nbaseline,LLAMAperformance degrades when con-\ntextualizing more negative passages (i.e., askin-\ncreases).   On the other hand,FLANmodels can\nbenefit from negative contexts, consistently outper-\nformingno-ctxbaselines across allk. Surprisingly,\nFLANmodels perform better with negative pas-\nsages than with contexts at all.  One explanation\nis that the negative passages may still be partially\nhelpful even though they do not contain the exact\ncorrect answer since they may still contain some\nkeywords or related topics that provide hints to\nTop-k documents\nExact Match (%)\nno-ctxtop-k\nworse than no-ctx\nbetter than \nno-ctx\nno-ctxtop-k\nExact Match (%)\nTop-k documents\nbenefit from \nnegative ctx\ndegraded by \nnegative ctx\nFlanT5\nFlanUL2\nLLaMa 7B\nLLaMa 70B\nFlanT5\nFlanUL2\nLLaMa 7B\nLLaMa 70B\nLLaMa 7B\nLLaMa 70B\nFigure 6: NQ results with negative passages.\nthe reader models. WhileLLAMAmodels fail to\nidentify such subtle information,FLANmodels are\nmore sensitive to related knowledge and capture\nthese cues effectively.\nSpecial QuestionsWe perform similar analyses\non  HotpotQA  and  BioASQ  datasets.\n3\nOn  Hot-\npotQA,FLANmodels obtain less gain using nega-\ntive passages and levels off above theno-ctxbase-\nline (Figure 7). In contrast,LLAMAcan use nega-\ntive passages more effectively on special questions,\noutperformingno-ctxfor mostk’s on HotpotQA\nandk <10on BioASQ.\nComparing models of different sizes,smaller\nmodels benefit more from negative contextswhile\nlarger models suffer.  Between the twoLLAMA\nmodels, whileLLAMA70B significantly regresses\nbeyond its no-context baseline,LLAMA7B per-\nforms comparably with its no-context baseline. Be-\ntween the twoFLANmodels, whileFLANT5bene-\nfits a lot from negative passages and outperforms\nno-ctxby  a  large  margin,FLAN-UL2becomes\nsensitive to negative information and deteriorates\nbelow itsno-ctxbaseline across nearly all k values.\nCase Study: What Negative Contexts Help?If\na  negative  context  is  related  but  insufficient  for\nanswering the question, it can still help improve the\nreader model’s performance. For instances where\nFLANT5answers correctly with thetop-5, negative\ncontexts but incorrectly with no contexts, 43% of\ninstances have top-k passages that include at least\none passage from the Wikipedia page that the gold\npassages  are  from.   Even  though  none  of  these\nnegative passages include the exact correct answer,\nthey are topically related enough thatFLANT5can\nbridge the knowledge gap.\n3\nFor multi-hop questions, we select examples retrieved\nwith all gold passages within the top-kpassages since all\npassages are necessary to answer the question.",
    "HotpotQA\nF1 (%)\nTop-k documents\nLLaMa 7B\nLLaMa 70B\nBioASQ\nFlanT5\nFlanUL2\ntop-k\nno-ctx\nFigure 7: Results on special questions when all retrieved\npassages are negative.\nNonetheless, not all negative passages from the\ngold page are helpful. Among instances wheretop-\n5underperformsno-ctx, 28% include at least one\npassage from the ground-truth Wikipedia page.\n7    Comparing Different Retrievers\nWe compare retrievers of different quality (§7.2)\nand study their influence on the reader model’s\ndownstream performance (§7.3).\n7.1    Ragged Analysis Guidelines\nWhat to Vary:Use different retrieval algorithms\n(varying in architecture, computational cost, and\nstorage cost). Here, we compare traditional, lexical\nand dense, neural retrievers.\nBehaviors  to  Expect:Retriever  performance\nhighly depends on the domain and complexity of\nthe question (number of hops). However, interest-\ningly,good retriever performance does not always\nguarantee good reader performance.  The reader\nperformance also depends on the reader’s sensitiv-\nity to retriever quality, with some models showing\nrobustness to suboptimal retrieval.\nImplications of Behaviors:This aspect of the\nanalysis informs the optimal choice of retriever\nfor your RAG system and how much optimizing\nfor that choice helps the reader’s performance. A\nmodel that performs well with various retrievers\noffers flexibility and might be more versatile in\nreal-world applications. In contrast, a model highly\nsensitive to retrieval quality might require a state-\nof-the-art retriever to achieve optimal performance.\n7.2    Evaluating Different Retrievers\nWe evaluate BM25 and ColBERT’s retrieval perfor-\nmance by computing recall@k for all k from 1 to\n50 (Table 2). In general, ColBERT performs better\nthan BM25 on questions about Wikipedia knowl-\nedge  (NQ  and  HotpotQA).  However,ColBERT\nonly offers a negligible advantage over BM25 on\nthe special-domainBioASQ, yet at a much higher\ncomputation cost.\nAveraged overkranging from 1 to 50, ColBERT\noffers a 50-point gain over BM25 for NQ, but only\noffers a<1point gain over BM25 for BioASQ\nfor paragraph recall. When applying to the out-of-\ndomain questions, ColBERT does not gain many\nadvantages from its neural, contextual embeddings.\nModel\nRecall@k\n125102050\nNQ\nBM25\n0.71.12.54.16.07.5\n10.316.327.836.847.753.2\nColBERT\n12.318.025.732.138.141.8\n27.238.854.465.072.977.2\nHotpotQA\nBM25\n0.20.41.01.62.43.0\n23.331.242.752.159.162.8\nColBERT\n31.140.149.956.261.964.9\n34.244.756.363.669.973.1\nBioASQ\nBM25\n8.812.919.625.833.337.8\n12.416.423.930.638.743.6\nColBERT\n8.813.520.727.134.338.6\n14.218.225.632.239.844.2\nTable 2:  passage/paragraph  (top)  and page  (bottom)\nrecall of BM25 and ColBERT retrievers.\n7.3    Generation with Different Retrievers\nMore importantly, we are interested in the question:\nhow much does the choice of retriever affect down-\nstream performance? We find that the nature of the\ntask (e.g., single-hop vs. multi-hop, domain speci-\nficity) can significantly affect the choice between\nneural and sparse retrievers (Figure 8). While neu-\nral  retrievers  may  require  more  resources,  their\ngreat  advantage  in  single-hop,  Wikipedia-based\nquestions and small advantage in specialized do-\nmains may still justify the investment. In contrast,\nneural retrieves are less beneficial for multi-hop",
    "Top-k documents\nNQHotpotQABioASQ\nReader Performance\nFlanT5FlanUL2LLaMa 7BLLaMa 70B\nFigure 8: Reader results on NQ, HotpotQA, and BioASQ (from left to right) using BM25 retrieved passages.\nquestions, even for the Wikipedia domain.\nOpen  Domain  (Wikipedia)For  open-domain\nsingle-hop questions, using ColBERT offers sub-\nstantial improvements over using BM25. Atk= 5,\nColBERT helpsFLANmodels achieve a signifi-\ncant 16–18 points EM improvement andLLAMA2\nmodels a more modest 4–6 point increase.  This\nsuggests there is value in using high-quality neural\nretrieval for open-domain, single-hop questions.\nSpecial Question Type: Multi-HopFor multi-\nhop questions,  despite the stark retrieval perfor-\nmance gap between BM25 and ColBERT, the im-\npact on reader performance is minimal. Atk= 5,\nFLANmodels improve by 3–4 in F\n1\nwhen paired\nwith ColBERT over BM25, whileLLAMA2 mod-\nels  improve  by  2–3  in  F\n1\n.   Both  models  show\nmarginal  gains  with  ColBERT,  suggesting  that\nmulti-hop questions challenge reader models more\nin reasoning than context utilization.\nWhile BM25 consistently gets recall scores 10\npoints lower than ColBERT, it is surprising that\nthe reader performs comparably to using ColBERT-\nretrieved passages. This could be explained by how\nBM25 still has a high Wikipediapagerecall, indi-\ncating its capability of findingrelevant but inexact\ninformation that readers may still benefit from.\nSpecial   Biomedical   DomainColBERT   and\nBM25 perform similarly in the biomedical domain,\nso we might also expect the downstream perfor-\nmances  to  be  similar.   However,  there  is  still  a\ndifference, albeit a small one, between using Col-\nBERT  and  BM25.   Fork= 5,  FLANmodels\nachieve a 2–5 point improvement when paired with\nColBERT  over  being  paired  with  BM25,  while\nLLAMA2  models  exhibit  a  much  smaller,<1\npoint improvement. Although there are only slight\ndifferences in retrieval quality, the specialized na-\nture  of  the  domain  can  amplify  the  impact  on\nreader performance. As a result, there is still a dis-\ncernible, though modest, preference for ColBERT\nover BM25 in accessing quality context.\n8    Related Work\nContext Limit and Processing CapacityLMs\nwith longer context windows are applicable across\nvarious knowledge-intensive generation tasks (Belt-\nagy  et  al.,  2020;  Bertsch  et  al.,  2023;  Su  et  al.,\n2024). However, it is unclear how performant these\nmodels are in processing long contexts. Liu et al.\n(2023) study if LMs can be sensitive to the posi-\ntion of useful content within a long context, and\nstruggle when it is in the middle.  Moreover, Xu\net al. (2024) show that an LM with a smaller con-\ntext window (4k) using RAG performs comparably\nwith finetuning with a longer-window LM (16k).\nFollowing this query, our work studies the effec-\ntiveness of LMs in utilizing long contexts, when\nthey have different input capacities.\nDomain Influence on Downstream Performance\nIt is crucial to know when LMs benefit from in-\ncluding retrieved passages in context. Mallen et al.\n(2023) find that retrieving contexts may be unnec-\nessary  and  even  detrimental  when  asking  about\ncommon knowledge, but it benefits questions about\nrare knowledge.   In contrast,  we find that using\nRAG under the right configurations still offers sig-\nnificant downstream performance boosts even for\ncommon, Wikipedia-based questions.\nRobustness to Noisy ContextsFeeding in noisy\ncontexts deteriorates LM performance. Asai et al.\n(2022) propose to select documents with high evi-\ndentiality scores. Wang et al. (2023) learn a filter\nmodel to remove the noisy sentences, and Berchan-\nsky et al. (2023) adopt a similar approach at the\ntoken level.  Further, (Yu et al., 2023; Xu et al.,\n2023) use a neural summarizer model to aid the\nLM in identifying relevant information in long re-\ntrieved passages. Instead of reducing noise at test",
    "time, Yoran et al. (2023) train LMs to be robust to\nirrelevant content. Lastly, Chen et al. (2023) build\nan evaluation benchmark to test LMs’ noise robust-\nness. Our work similarly studies LMs’ responses to\nnoisy content but is more fine-grained with varied\nnoise ratios.\n9    Conclusion\nWe propose RAGGED, a framework designed to\nassist researchers and practitioners in making in-\nformed decisions about designing RAG systems,\nfocusing on three key aspects: the number of con-\ntexts, the reader model, and the retriever model.\nWe demonstrate the framework’s utility in deriving\ninsights about RAG behaviors in response to varied\ncontext volumes, document quality, and question\ndomains. We hope that our framework will be uti-\nlized by the community to deepen the understand-\ning and customization of RAG systems.\nLimitations\nAlthough this study provides valuable insights into\nRAG  systems,  it  has  several  limitations.   First,\nthe  RAGGED  framework,  although  comprehen-\nsive, focuses mainly on document-based question-\nanswering tasks, which may not fully capture the\nnuances of other knowledge-intensive NLP tasks\nsuch as summarization, fact verification, and ma-\nchine reading comprehension. Second, our experi-\nments were conducted with a specific set of models\nand datasets,  which may limit the generalizabil-\nity of our findings to other models, languages, or\ndomains not covered in this study. However, pro-\nviding a comprehensive analysis is not the main\nmotivation or contribution of the paper. We encour-\nage readers to use our framework to evaluate other\nmodels and datasets and share the insights with the\ncommunity.\nAcknowledgements\nSpecial thanks to Alex Cabrera, Alex Bäuerle, Jun\nAraki, Md Rizwan Parvez for providing Zeno sup-\nport for analysis visualization.  Our appreciation\nextends  to  Hao  Zhu,  Jacob  Springer,  and  Vijay\nViswanathan for providing feedback for our paper.\nThis paper was supported in part by a gift from\nBosch research.\nReferences\nAkari   Asai,    Matt   Gardner,    and   Hannaneh   Ha-\njishirzi. 2022.  Evidentiality-guided generation for\nknowledge-intensive nlp tasks.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nMoshe  Berchansky,  Peter  Izsak,  Avi  Caciularu,  Ido\nDagan, and Moshe Wasserblat. 2023.  Optimizing\nretrieval-augmented reader models via token elimina-\ntion.\nAmanda  Bertsch,  Uri  Alon,  Graham  Neubig,  and\nMatthew R. Gormley. 2023.  Unlimiformer:  Long-\nrange transformers with unlimited length input.  In\nThirty-seventh  Conference  on  Neural  Information\nProcessing Systems.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017a. Reading Wikipedia to answer open-\ndomain questions. InProceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017b. Reading wikipedia to answer open-\ndomain questions. InProceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers).\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023.Benchmarking  large  language  models  in\nretrieval-augmented generation.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert  Webson,  Shixiang  Shane  Gu,  Zhuyun  Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu,  Vincent  Zhao,  Yanping  Huang,  Andrew  Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.arXiv preprint arXiv:2210.11416.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya,  Trevor Cai,  Eliza Rutherford,\nDiego de las Casas, Lisa Anne Hendricks, Johannes\nWelbl,  Aidan Clark,  Tom Hennigan,  Eric Noland,\nKatherine Millican, George van den Driessche, Bog-\ndan Damoc, Aurelia Guy, Simon Osindero, Karen\nSimonyan, Erich Elsen, Oriol Vinyals, Jack William\nRae, and Laurent Sifre. 2022. An empirical analysis\nof compute-optimal large language model training.\nInAdvances in Neural Information Processing Sys-\ntems.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering.",
    "Vladimir Karpukhin, Barlas O\n ̆\nguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over BERT.CoRR.\nAnastasia Krithara, Anastasios Nentidis, Konstantinos\nBougiatiotis, and Georgios Paliouras. 2023. Bioasq-\nqa: A manually curated corpus for biomedical ques-\ntion answering.Scientific Data, 10:170.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019.  Natu-\nral questions: A benchmark for question answering\nresearch.Transactions of the Association for Compu-\ntational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks.Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang.  2023.   Lost  in  the  middle:  How  language\nmodels use long contexts.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel  Khashabi,  and  Hannaneh  Hajishirzi.  2023.\nWhen not to trust language models:  Investigating\neffectiveness of parametric and non-parametric mem-\nories.\nNational Library of Medicine. 2023. Pubmed baseline\n2023 repository.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021.  Kilt: a benchmark for knowledge in-\ntensive language tasks.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2023. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.arXiv preprint arXiv:1910.10683.\nMatthew Richardson, Christopher JC Burges, and Erin\nRenshaw. 2013. Mctest: A challenge dataset for the\nopen-domain machine comprehension of text.   In\nProceedings  of  the  2013  conference  on  empirical\nmethods in natural language processing, pages 193–\n203.\nStephen Robertson, Hugo Zaragoza, et al. 2009.  The\nprobabilistic relevance framework:  Bm25 and be-\nyond.Foundations and Trends®in Information Re-\ntrieval, 3(4):333–389.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,\nWen Bo,  and Yunfeng Liu. 2024.   Roformer:  En-\nhanced transformer with rotary position embedding.\nNeurocomputing, 568:127063.\nYi  Tay,  Mostafa  Dehghani,  Vinh  Q.  Tran,  Xavier\nGarcia,  Jason  Wei,  Xuezhi  Wang,  Hyung  Won\nChung,  Siamak  Shakeri,  Dara  Bahri,  Tal  Schus-\nter,   Huaixiu  Steven  Zheng,   Denny  Zhou,   Neil\nHoulsby, and Donald Metzler. 2023.   Ul2:  Unify-\ning  language  learning  paradigms.arXiv  preprint\narXiv:2205.05131.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand  efficient  foundation  language  models.arXiv\npreprint arXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert,  Amjad Almahairi,  Yasmine Babaei,  Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog,  Yixin Nie,  Andrew Poulton,  Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian,  Xiaoqing  Ellen  Tan,  Binh  Tang,  Ross  Tay-\nlor,  Adina Williams,  Jian Xiang Kuan,  Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur,  Sharan Narang,  Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and fine-\ntuned chat models.arXiv preprint arXiv:2307.09288.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023. Learning to filter\ncontext for retrieval-augmented generation.arXiv\npreprint arXiv:2311.08377.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-\ncomp: Improving retrieval-augmented lms with com-\npression and selective augmentation.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee,\nChen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2024.  Retrieval meets long context large lan-\nguage models.",
    "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning.\nOri  Yoran,  Tomer  Wolfson,  Ori  Ram,  and  Jonathan\nBerant. 2023. Making retrieval-augmented language\nmodels robust to irrelevant context.\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin\nMa, Hongwei Wang, and Dong Yu. 2023. Chain-of-\nnote: Enhancing robustness in retrieval-augmented\nlanguage models.",
    "A    Implementation Details\nReader modelSpecifically, when usingFLANT5\nandFLANUL2readers,  we  use  T5Tokenizer  to\ntruncate sequences to up to 2ktokens; when using\nLLAMA2 models, we apply the LlamaTokenizer\nand truncate sequences by 4ktokens. Subsequently,\nwe incorporate a concise question-and-answer for-\nmat that segments the query using \"Question:\" and\ncues the model’s response with \"Answer:\", ensur-\ning precise and targeted answers.\nFor our reader decoding strategy, we used greedy\ndecoding with a beam size of 1 and temperature of\n1, selecting the most probabble next word at each\nstep without sampling. The output generation was\nconfigured to produce responses with 10 and 256\ntokens to examine the effect of varying response\nlengths on model performance.  The experiments\nwere conducted on NVIDIA A6000 GPUs,  sup-\nported by an environment with 60GB RAM. The\naverage response time was∼1.1s per query when\nprocessing with a batch size of 50.\nB    Dataset Details\nAll corpus and datasets use English.\nThe Medline Corpus is from of Medicine (2023)\nprovided by the National Library of Medicine.\nCorpus# of par# of docAvg # of doc\nWikipedia111M5M18.9\nMedline\n58M34M1.7\nTable 3: Retrieval corpus information\nFor NQ and HotpotQA, we use KILT’s dev set\nversions of the datasets, allowed under the MIT Li-\ncense (Petroni et al., 2021). For BioASQ (Krithara\net al., 2023), we use Task 11B, distributed under\nCC BY 2.5 license.\nDataset# of Queries\nNQ2837\nHotpotQA5600\nBioASQ3837\nTable 4: Dataset information"
  ]
}