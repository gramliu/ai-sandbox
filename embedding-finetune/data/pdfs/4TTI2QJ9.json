{
  "key": "4TTI2QJ9",
  "url": "https://arxiv.org/pdf/2111.00396",
  "metadata": {
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "abstract": "  A central goal of sequence modeling is designing a single principled model\nthat can address sequence data across a range of modalities and tasks,\nparticularly on long-range dependencies. Although conventional models including\nRNNs, CNNs, and Transformers have specialized variants for capturing long\ndependencies, they still struggle to scale to very long sequences of $10000$ or\nmore steps. A promising recent approach proposed modeling sequences by\nsimulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t),\ny(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state\nmatrix \\( A \\), this system could handle long-range dependencies mathematically\nand empirically. However, this method has prohibitive computation and memory\nrequirements, rendering it infeasible as a general sequence modeling solution.\nWe propose the Structured State Space sequence model (S4) based on a new\nparameterization for the SSM, and show that it can be computed much more\nefficiently than prior approaches while preserving their theoretical strengths.\nOur technique involves conditioning \\( A \\) with a low-rank correction,\nallowing it to be diagonalized stably and reducing the SSM to the well-studied\ncomputation of a Cauchy kernel. S4 achieves strong empirical results across a\ndiverse range of established benchmarks, including (i) 91\\% accuracy on\nsequential CIFAR-10 with no data augmentation or auxiliary losses, on par with\na larger 2-D ResNet, (ii) substantially closing the gap to Transformers on\nimage and language modeling tasks, while performing generation $60\\times$\nfaster (iii) SoTA on every task from the Long Range Arena benchmark, including\nsolving the challenging Path-X task of length 16k that all prior work fails on,\nwhile being as efficient as all competitors.\n",
    "published": "2021-10-31T03:32:18Z"
  },
  "text": [
    "Efficiently Modeling Long Sequences with Structured State Spaces\nAlbert Gu, Karan Goel, and Christopher R ́e\nDepartment of Computer Science, Stanford University\n{albertgu,krng}@stanford.edu,chrismre@cs.stanford.edu\nAbstract\nA central goal of sequence modeling is designing a single principled model that can address sequence\ndata across a range of modalities and tasks, particularly on long-range dependencies.  Although conventional\nmodels including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies,\nthey still struggle to scale to very long sequences of 10000 or more steps.  A promising recent approach\nproposed modeling sequences by simulating the fundamental state space model (SSM)x\n′\n(t) =Ax(t) +\nBu(t), y(t)  =Cx(t) +Du(t),  and  showed  that  for  appropriate  choices  of  the  state  matrixA,  this\nsystem could handle long-range dependencies mathematically and empirically.  However, this method has\nprohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling\nsolution.  We propose the Structured State Space sequence model (S4) based on a new parameterization for\nthe SSM, and show that it can be computed much more efficiently than prior approaches while preserving\ntheir theoretical strengths.  Our technique involves conditioningAwith a low-rank correction, allowing\nit to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel.\nS4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91%\naccuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger\n2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks,\nwhile performing generation 60×faster (iii) SoTA on every task from the Long Range Arena benchmark,\nincluding solving the challenging Path-X task of length 16k that all prior work fails on, while being as\nefficient as all competitors.\n1\n1    Introduction\nA central problem in sequence modeling is efficiently handling data that contains long-range dependencies\n(LRDs).  Real-world time-series data often requires reasoning over tens of thousands of time steps, while few\nsequence models address even thousands of time steps.  For instance, results from the long-range arena (LRA)\nbenchmark [40] highlight that sequence models today perform poorly on LRD tasks, including one (Path-X)\nwhere no model performs better than random guessing.\nSince LRDs are perhaps the foremost challenge for sequence models, all standard model families such as\ncontinuous-time models (CTMs), RNNs, CNNs, and Transformers include many specialized variants designed\nto address them.  Modern examples include orthogonal and Lipschitz RNNs [1,13] to combat vanishing\ngradients, dilated convolutions to increase context size [3,28], and an increasingly vast family of efficient\nTransformers that reduce the quadratic dependence on sequence length [8,22].  Despite being designed\nfor LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio\nclassification [18].\nAn alternative approach to LRDs was recently introduced based on thestate space model (SSM)(Fig. 1).\nSSMs are a foundational scientific model used in fields such as control theory, computational neuroscience,\nand many more, but have not been applicable to deep learning for concrete theoretical reasons.  In particular,\nGu et al.[18]showed that deep SSMs actually struggle even on simple tasks, but can perform exceptionally\n1\nCode is publicly available athttps://github.com/HazyResearch/state-spaces.\n1\narXiv:2111.00396v3  [cs.LG]  5 Aug 2022",
    "̇푥=퐴푥+퐵푢\n푦=퐶푥+퐷푢\n푥=\n̅\n퐴푥+\n,\n퐵푢\n푦=\n̅\n퐶푥+\n-\n퐷푢\n푦=\n-\n퐾∗푢\n퐴=\n100\n120\n133\nContinuous \nState Space\nFast Discrete Representations\nLong-Range \nDependencies\n푥\n푢\n푦\n퐴\nFigure 1:  (Left) State Space Models (SSM) parameterized by matricesA,B,C,Dmap an input signalu(t) to\noutputy(t) through a latent statex(t).  (Center) Recent theory on continuous-time memorization derives special\nAmatrices that allow SSMs to capture LRDs mathematically and empirically.  (Right) SSMs can be computed\neither as a recurrence (left) or convolution (right).  However, materializing these conceptual views requires utilizing\ndifferent representations of its parameters (red, blue, green) which are very expensive to compute.  S4 introduces a\nnovel parameterization that efficiently swaps between these representations, allowing it to handle a wide range of\ntasks, be efficient at both training and inference, and excel at long sequences.\nwell when equipped with special state matricesArecently derived to solve a problem of continuous-time\nmemorization [16,45].  Their Linear State Space Layer (LSSL) conceptually unifies the strengths of CTM,\nRNN and CNN models, and provides a proof of concept that deep SSMs can address LRDs in principle.\nUnfortunately, the LSSL is infeasible to use in practice because of prohibitive computation and memory\nrequirements induced by the state representation.  For state dimensionNand sequence lengthL, computing\nthe latent state requiresO(N\n2\nL) operations andO(NL) space – compared to a Ω(L+N) lower bound for\nboth.  Thus for reasonably sized models (e.g.N= 256 in Gu et al.[18]), the LSSL uses orders of magnitude\nmore memory than comparably-sized RNNs or CNNs.  Although theoretically efficient algorithms for the\nLSSL were proposed, we show that these are numerically unstable.  In particular, the specialAmatrix is\nhighly non-normal in the linear algebraic sense, which prevents the application of conventional algorithmic\ntechniques.  Consequently, although the LSSL showed that SSMs have strong performance, they are currently\ncomputationally impractical as a general sequence modeling solution.\nIn this work, we introduce theStructured State Space (S4)sequence model based on the SSM that solves\nthe critical computational bottleneck in previous work.  Technically, S4 reparameterizes the structured state\nmatricesAappearing in Gu et al.[16], Voelker et al.[45]by decomposing them as the sum of a low-rank\nand normal term.  Additionally, instead of expanding the standard SSM in coefficient space, we compute its\ntruncated generating function in frequency space, which can be simplified into a multipole-like evaluation.\nCombining these two ideas, we show that the low-rank term can be corrected by the Woodbury identity\nwhile the normal term can be diagonalized stably, ultimately reducing to a well-studied and theoretically\nstable Cauchy kernel [29,30].  This results in\n ̃\nO(N+L) computation andO(N+L) memory usage, which is\nessentially tight for sequence models.  Compared to the LSSL, S4 is up to 30×faster with 400×less memory\nusage, while exceeding the LSSL’s performance empirically.\nEmpirically, S4 significantly advances the state-of-the-art for LRD. On the LRA benchmark for efficient\nsequence models, S4 is as fast as all baselines while outperforming them by 20+ points on average.  S4 is the\nfirst model to solve the difficult LRA Path-X task (length-16384), achieving88% accuracy compared to\n50% random guessing\nfor all prior work.  On speech classification with length-16000 sequences, S4 halves\nthe test error (1.7%) of specialized Speech CNNs – by contrast, all RNN and Transformer baselines fail to\nlearn (≥70% error).\nTowards a general-purpose sequence model.Beyond LRD, a broad goal of machine learning is to\ndevelop  a  single  model  that  can  be  used  across  a  wide  range  of  problems.   Models  today  are  typically\n2",
    "specialized to solve problems from a particular domain (e.g.  images, audio, text, time-series), and enable a\nnarrow range of capabilities (e.g.  efficient training, fast generation, handling irregularly sampled data).  This\nspecialization is typically expressed via domain-specific preprocessing, inductive biases, and architectures.\nSequence models provide a general framework for solving many of these problems with reduced specialization\n– e.g.  Vision Transformers for image classification with less 2D information [12].  However, most models such\nas Transformers generally still require substantial specialization per task to achieve high performance.\nDeep SSMs in particular have conceptual strengths that suggest they may be promising as a general sequence\nmodeling solution.  These strengths include a principled approach to handling LRDs, as well as the ability\nto move between continuous-time, convolutional, and recurrent model representations, each with distinct\ncapabilities (Fig. 1).  Our technical contributions enable SSMs to be applied successfully to a varied set of\nbenchmarks with minimal modification:\n•Large-scale  generative  modeling.On  CIFAR-10  density  estimation,  S4  is  competitive  with  the  best\nautoregressive models (2.85 bits per dim).  On WikiText-103 language modeling, S4 substantially closes the\ngap to Transformers (within 0.8 perplexity), setting SoTA for attention-free models.\n•Fast autoregressive generation.Like RNNs, S4 can use its latent state to perform 60×faster pixel/token\ngeneration than standard autoregressive models on CIFAR-10 and WikiText-103.\n•Sampling  resolution  change.Like specialized CTMs, S4 can adapt to changes in time-series sampling\nfrequency without retraining, e.g. at 0.5×frequency on speech classification.\n•Learning with weaker inductive biases.With no architectural changes, S4 surpasses Speech CNNs on speech\nclassification, outperforms the specialized Informer model on time-series forecasting problems, and matches\na 2-D ResNet on sequential CIFAR with over 90% accuracy.\n2    Background:  State Spaces\nSections 2.1 to 2.4 describe the four properties of SSMs in Fig. 1:  the classic continuous-time representation,\naddressing LRDs with the HiPPO framework, the discrete-time recurrent representation, and the parallelizable\nconvolution representation.  In particular, Section 2.4 introduces the SSM convolution kernelK, which is the\nfocus of our theoretical contributions in Section 3.\n2.1    State Space Models:  A Continuous-time Latent State Model\nThe state space model is defined by the simple equation(1).  It maps a 1-D input signalu(t) to anN-D\nlatent statex(t) before projecting to a 1-D output signaly(t).\nx\n′\n(t) =Ax(t) +Bu(t)\ny(t) =Cx(t) +Du(t)\n(1)\nSSMs are broadly used in many scientific disciplines and related to latent state models such as Hidden Markov\nModels (HMM). Our goal is to simply use the SSM as a black-box representation in a deep sequence model,\nwhereA,B,C,Dare parameters learned by gradient descent.  For the remainder of this paper, we will omit\nthe parameterDfor exposition (or equivalently, assumeD= 0) because the termDucan be viewed as a\nskip connection and is easy to compute.\n2.2    Addressing Long-Range Dependencies with HiPPO\nPrior work found that the basic SSM(1)actually performs very poorly in practice.  Intuitively, one explanation\nis that linear first-order ODEs solve to an exponential function, and thus may suffer from gradients scaling\nexponentially in the sequence length (i.e., the vanishing/exploding gradients problem [32]).  To address this\n3",
    "problem, the LSSL leveraged the HiPPO theory of continuous-time memorization [16].  HiPPO specifies a\nclass of certain matricesA∈R\nN×N\nthat when incorporated into(1), allows the statex(t) to memorize the\nhistory of the inputu(t).  The most important matrix in this class is defined by equation (2), which we will\ncall the HiPPO matrix.  For example, the LSSL found that simply modifying an SSM from a random matrix\nAto equation (2) improved its performance on the sequential MNIST benchmark from 60% to 98%.\n(HiPPO Matrix)A\nnk\n=−\n\n\n\n\n\n(2n+ 1)\n1/2\n(2k+ 1)\n1/2\nifn > k\nn+ 1ifn=k\n0ifn < k\n.(2)\n2.3    Discrete-time SSM: The Recurrent Representation\nTo be applied on a discrete input sequence (u\n0\n,u\n1\n,...) instead of continuous functionu(t),(1)must be\ndiscretized by astep size∆ that represents the resolution of the input.  Conceptually, the inputsu\nk\ncan be\nviewed as sampling an implicit underlying continuous signalu(t), whereu\nk\n=u(k∆).\nTo discretize the continuous-time SSM, we follow prior work in using the bilinear method [43], which converts\nthe state matrixAinto an approximationA.  The discrete SSM is\nx\nk\n=Ax\nk−1\n+Bu\nk\nA= (I−∆/2·A)\n−1\n(I+ ∆/2·A)\ny\nk\n=Cx\nk\nB= (I−∆/2·A)\n−1\n∆BC=C.\n(3)\nEquation(3)is now asequence-to-sequencemapu\nk\n7→y\nk\ninstead of function-to-function.  Moreover the state\nequation is now a recurrence inx\nk\n, allowing the discrete SSM to be computed like an RNN. Concretely,\nx\nk\n∈R\nN\ncan be viewed as ahidden statewith transition matrixA.\nNotationally, throughout this paper we useA,B,...to denote discretized SSM matrices defined by(3).\nNote that these matrices are a function of bothAas well as a step size ∆; we suppress this dependence for\nnotational convenience when it is clear.\n2.4    Training SSMs:  The Convolutional Representation\nThe recurrent SSM(3)is not practical for training on modern hardware due to its sequentiality.  Instead, there\nis a well-known connection between linear time-invariant (LTI) SSMs such as(1)and continuous convolutions.\nCorrespondingly, (3) can actually be written as a discrete convolution.\nFor simplicity let the initial state bex\n−1\n= 0.  Then unrolling (3) explicitly yields\nx\n0\n=Bu\n0\nx\n1\n=ABu\n0\n+Bu\n1\nx\n2\n=A\n2\nBu\n0\n+ABu\n1\n+Bu\n2\n...\ny\n0\n=CBu\n0\ny\n1\n=CABu\n0\n+CBu\n1\ny\n2\n=CA\n2\nBu\n0\n+CABu\n1\n+CBu\n2\n...\nThis can be vectorized into a convolution (4) with an explicit formula for the convolution kernel (5).\ny\nk\n=CA\nk\nBu\n0\n+CA\nk−1\nBu\n1\n+···+CABu\nk−1\n+CBu\nk\ny=K∗u.\n(4)\nK∈R\nL\n:=K\nL\n(A,B,C) :=\n(\nCA\ni\nB\n)\ni∈[L]\n= (CB,CAB,...,CA\nL−1\nB).(5)\nIn other words, equation(4)is a single (non-circular) convolution and can be computed very efficiently with\nFFTs,providedthatKis known.  However, computingKin(5)is non-trivial and is the focus of our technical\ncontributions in Section 3.  We callKtheSSM convolution kernelor filter.\n4",
    "3    Method:  Structured State Spaces (S4)\nOur technical results focus on developing the S4 parameterization and showing how to efficiently compute\nall views of the SSM (Section 2):  the continuous representation (A,B,C)(1), the recurrent representation\n(A,B,C) (3), and the convolutional representationK(4).\nSection 3.1 motivates our approach,  which is based on the linear algebraic concepts of conjugation and\ndiagonalization, and discusses why the naive application of this approach does not work.  Section 3.2 gives\nan overview of the key technical components of our approach and formally defines the S4 parameterization.\nSection 3.3 sketches the main results,  showing that S4 is asymptotically efficient (up to log factors) for\nsequence models.  Proofs are in Appendices B and C.\n3.1    Motivation:  Diagonalization\nThe fundamental bottleneck in computing the discrete-time SSM(3)is that it involves repeated matrix\nmultiplication byA.  For example, computing(5)naively as in the LSSL involvesLsuccessive multiplications\nbyA, requiringO(N\n2\nL) operations andO(NL) space.\nTo overcome this bottleneck, we use a structural result that allows us to simplify SSMs.\nLemma 3.1.Conjugation is an equivalence relation on SSMs(A,B,C)∼(V\n−1\nAV,V\n−1\nB,CV).\nProof.Write out the two SSMs with state denoted byxand  ̃xrespectively:\nx\n′\n=Ax+Bu ̃x\n′\n=V\n−1\nAV ̃x+V\n−1\nBu\ny=Cxy=CV ̃x\nAfter multiplying the right side SSM byV, the two SSMs become identical withx=V ̃x.  Therefore these\ncompute the exact same operatoru7→y, but with a change of basis byVin the statex.\nLemma 3.1 motivates puttingAinto a canonical form by conjugation\n2\n, which is ideally more structured\nand allows faster computation.  For example, ifAwere diagonal, the resulting computations become much\nmore tractable.  In particular, the desiredK(equation(4)) would be aVandermonde productwhich\ntheoretically only needsO((N+L) log\n2\n(N+L)) arithmetic operations [29].\nUnfortunately, the naive application of diagonalization does not work due to numerical issues.  Werive the\nexplicit diagonalization for the HiPPO matrix(2)and show it has entries exponentially large in the state size\nN, rendering the diagonalization numerically infeasible (e.g.CVin Lemma 3.1 would not be computable).\nWe note that Gu et al.[18]proposed a different (unimplemented) algorithm to computeKfaster than the\nnaive algorithm.  In Appendix B, we prove that it is also numerically unstable for related reasons.\nLemma 3.2.\nThe HiPPO matrixAin equation(2)is diagonalized by the matrixV\nij\n=\n(\ni+j\ni−j\n)\n.  In particular,\nV\n3i,i\n=\n(\n4i\n2i\n)\n≈2\n4i\n.  ThereforeVhas entries of magnitude up to2\n4N/3\n.\n3.2    The S4 Parameterization:  Normal Plus Low-Rank\nThe previous discussion implies that we should only conjugate by well-conditioned matricesV.  The ideal\nscenario is when the matrixAis diagonalizable by a perfectly conditioned (i.e., unitary) matrix.  By the\nSpectral Theorem of linear algebra, this is exactly the class ofnormal matrices.  However, this class of\nmatrices is restrictive; in particular, it does not contain the HiPPO matrix (2).\nWe make the observation that although the HiPPO matrix is not normal, it can be decomposed as thesum of\na normal and low-rank matrix.  However, this is still not useful by itself:  unlike a diagonal matrix, powering\nup this sum (in(5)) is still slow and not easily optimized.  We overcome this bottleneck by simultaneously\napplying three new techniques.\n2\nNote that although we ultimately require\nA, conjugation commutes with discretization so we refer toA.\n5",
    "Algorithm 1S4 Convolution Kernel (Sketch)\nInput:S4 parametersΛ,P,Q,B,C∈C\nN\nand step size ∆\nOutput:SSM convolution kernelK=K\nL\n(A,B,C) forA=Λ−P Q\n∗\n(equation (5))\n1:\n ̃\nC←\n(\nI−A\nL\n)\n∗\nC.Truncate SSM generating function (SSMGF) to lengthL\n2:\n[\nk\n00\n(ω)k\n01\n(ω)\nk\n10\n(ω)k\n11\n(ω)\n]\n←\n[\n ̃\nC Q\n]\n∗\n(\n2\n∆\n1−ω\n1+ω\n−Λ\n)\n−1\n[B P].Black-box Cauchy kernel\n3:\nˆ\nK(ω)←\n2\n1+ω\n[\nk\n00\n(ω)−k\n01\n(ω)(1 +k\n11\n(ω))\n−1\nk\n10\n(ω)\n]\n.Woodbury Identity\n4:\nˆ\nK={\nˆ\nK(ω) :ω= exp(2πi\nk\nL\n)}.Evaluate SSMGF at all roots of unityω∈Ω\nL\n5:K←iFFT(\nˆ\nK).Inverse Fourier Transform\n•Instead of computingKdirectly,  we compute its spectrum by evaluating itstruncated  generating\nfunction\n∑\nL−1\nj=0\nK\nj\nζ\nj\nat the roots of unityζ.Kcan then be found by applying an inverse FFT.\n•\nThis generating function is closely related to the matrix resolvent, and now involves a matrixinverse\ninstead ofpower.  The low-rank term can now be corrected by applying theWoodbury identitywhich\nreduces (A+P Q\n∗\n)\n−1\nin terms ofA\n−1\n, truly reducing to the diagonal case.\n•Finally, we show that the diagonal matrix case is equivalent to the computation of aCauchy kernel\n1\nω\nj\n−ζ\nk\n, a well-studied problem with stable near-linear algorithms [30, 31].\nOur techniques apply to any matrix that can be decomposed asNormal Plus Low-Rank (NPLR).\nTheorem 1.All HiPPO matrices from [16] have a NPLR representation\nA=VΛV\n∗\n−P Q\n>\n=V(Λ−(V\n∗\nP) (V\n∗\nQ)\n∗\n)V\n∗\n(6)\nfor unitaryV∈C\nN×N\n, diagonalΛ, and low-rank factorizationP,Q∈R\nN×r\n.  These matrices HiPPO- LegS,\nLegT, LagT all satisfyr= 1orr= 2.  In particular, equation(2)is NPLR withr= 1.\n3.3    S4 Algorithms and Computational Complexity\nBy equation(6), note that NPLR matrices can be conjugated intodiagonal plus low-rank(DPLR) form (now\noverCinstead ofR).  Theorems 2 and 3 describe the complexities of SSMs whereAis in DPLR form.  S4 is\noptimal or near-optimal for both recurrent and convolutional representations.\nTheorem 2(S4 Recurrence).Given any step size∆, computing one step of the recurrence(3)can be done\ninO(N)operations whereNis the state size.\nTheorem 2 follows from the fact that the inverse of a DPLR matrix is also DPLR (e.g.  also by the Woodbury\nidentity).  This implies that the discretized matrixAis the product of two DPLR matrices and thus has\nO(N) matrix-vector multiplication.  Appendix C.2 computesAin closed DPLR form.\nTheorem 3(S4 Convolution).Given  any  step  size∆,  computing  the  SSM  convolution  filterKcan  be\nreduced to 4 Cauchy multiplies, requiring only\n ̃\nO(N+L)operations andO(N+L)space.\nAppendix  C,  Definition  3  formally  defines  Cauchy  matrices,  which  are  related  to  rational  interpolation\nproblems.  Computing with Cauchy matrices is an extremely well-studied problem in numerical analysis,\nwith both fast arithmetic and numerical algorithms based on the famous Fast Multipole Method (FMM)\n[29,30,31].  The computational complexities of these algorithms under various settings are described in\nAppendix C, Proposition 5.\nWe reiterate that Theorem 3 is our core technical contribution, and its algorithm is the very motivation of\nthe NPLR S4 parameterization.  This algorithm is formally sketched in Algorithm 1.\n6",
    "Table 1:  Complexity of various sequence models in terms of sequence length (L), batch size (B), and hidden dimension\n(H); tildes denote log factors.  Metrics are parameter count, training computation, training space requirement, training\nparallelizability, and inference computation (for 1 sample and time-step).  For simplicity, the state sizeNof S4 is tied\ntoH.  Bold denotes model is theoretically best for that metric.  Convolutions are efficient for training while recurrence\nis efficient for inference, while SSMs combine the strengths of both.\nConvolution\n3\nRecurrenceAttentionS4\nParametersLHH\n2\nH\n2\nH\n2\nTraining\n ̃\nLH(B+H)BLH\n2\nB(L\n2\nH+LH\n2\n)BH(\n ̃\nH+\n ̃\nL) +B\n ̃\nLH\nSpaceBLHBLHB(L\n2\n+HL)BLH\nParallelYesNoYesYes\nInferenceLH\n2\nH\n2\nL\n2\nH+H\n2\nLH\n2\n3.4    Architecture Details of the Deep S4 Layer\nConcretely, an S4 layer is parameterized as follows.  First initialize a SSM withAset to the HiPPO matrix\n(2).  By Lemma 3.1 and Theorem 1, this SSM is unitarily equivalent to some (Λ−P Q\n∗\n,B,C) for some\ndiagonalΛand vectorsP,Q,B,C∈C\nN×1\n.  These comprise S4’s 5Ntrainable parameters.\nThe overall deep neural network (DNN) architecture of S4 is similar to prior work.  As defined above, S4\ndefines a map fromR\nL\n→R\nL\n, i.e.  a 1-D sequence map.  Typically, DNNs operate on feature maps of sizeH\ninstead of 1.  S4 handles multiple features by simply definingHindependent copies of itself, and then mixing\ntheHfeatures with a position-wise linear layer for a total ofO(H\n2\n) +O(HN) parameters per layer.  Nonlinear\nactivation functions are also inserted between these layers.  Overall, S4 defines a sequence-to-sequence map of\nshape (batch size, sequence length, hidden dimension), exactly the same as related sequence models such as\nTransformers, RNNs, and CNNs.\nNote that the core S4 module is a linear transformation, but the addition of non-linear transformations\nthrough the depth of the network makes the overall deep SSM non-linear.  This is analogous to a vanilla CNN,\nsince convolutional layers are also linear.  The broadcasting acrossHhidden features described in this section\nis also analogous to depthwise-separable convolutions.  Thus, the overall deep S4 model is closely related to a\ndepthwise-separable CNN but withglobalconvolution kernels.\nFinally, we note that follow-up work found that this version of S4 can sometimes suffer from numerical\ninstabilities when theAmatrix has eigenvalues on the right half-plane [14].  It introduced a slight change to\nthe NPLR parameterization for S4 fromΛ−P Q\n∗\ntoΛ−P P\n∗\nthat corrects this potential problem.\nTable 1 compares the complexities of the most common deep sequence modeling mechanisms.\n4    Experiments\nSection 4.1 benchmarks S4 against the LSSL and efficient Transformer models.  Section 4.2 validates S4 on\nLRDs:  the LRA benchmark and raw speech classification.  Section 4.3 investigates whether S4 can be used as\na general sequence model to perform effectively and efficiently in a wide variety of settings including image\nclassification, image and text generation, and time series forecasting.\n4.1    S4 Efficiency Benchmarks\nWe benchmark that S4 can be trained quickly and efficiently, both compared to the LSSL, as well as efficient\nTransformer variants designed for long-range sequence modeling.  As outlined in Section 3, S4 is theoretically\nmuch more efficient than the LSSL, and Table 2 confirms that the S4 is orders of magnitude more speed- and\nmemory-efficient for practical layer sizes.  In fact, S4’s speed and memory use is competitive with the most\n3\nRefers to global (in the sequence length) and depthwise-separable convolutions, similar to the convolution version of S4.\n7",
    "Table 2:  Deep SSMs:  The S4 parameterization with Algorithm 1\nis asymptotically more efficient than the LSSL.\nTraining Step (ms)   Memory Alloc. (MB)\nDim.128256512128256512\nLSSL9.3220.6140.7222.1168513140\nS44.773.074.755.312.633.5\nRatio1.9×6.7×29.6×42.0×133×392×\nTable 3:  Benchmarks vs.  efficient Transformers\nLength 1024    Length 4096\nSpeedMem.SpeedMem.\nTransformer1×1×1×1×\nPerformer1.23×0.43×3.79×0.086×\nLinear Trans.1.58×0.37×5.35×0.067×\nS41.58×0.43×5.19×0.091×\nFigure 2:  Visualizations of a trained S4 model on LRA Path-X. SSM convolution kernelsK∈R\n16384\nare reshaped\ninto a 128×128 image.  (Left) Example from the Path-X task, which involves deducing if the markers are connected\nby a path (Top) Filters from the first layer (Bottom) Filters from the last layer.\nTable 4:  (Long Range Arena) (Top) Original Transformer variants in LRA. Full results in Appendix D.2.  (Bottom)\nOther models reported in the literature.Please read Appendix D.5 before citing this table.\nModelListOps  Text   Retrieval  Image  Pathfinder  Path-X  Avg\nTransformer36.3764.2757.4642.4471.40753.66\nReformer37.2756.1053.4038.0768.50750.56\nBigBird36.0564.0259.2940.8374.87754.17\nLinear Trans.16.1365.9053.0942.3475.30750.46\nPerformer18.0165.4053.8242.7777.05751.18\nFNet35.3365.1159.6138.6777.80754.42\nNystr ̈omformer37.1565.5279.5641.5870.94757.46\nLuna-25637.2564.5779.2947.3877.72759.37\nS459.6086.8290.9088.6594.2096.3586.09\nefficient Transformer variants benchmarked by Tay et al.[40]—Linear Transformer [22] and Performer [8]—in\na parameter-matched setting (Table 3, following the protocol of Tay et al. [40]).\n4.2    Learning Long Range Dependencies\nAs described in Sections 2.2 and 3.1, S4 uses a principled approach to address LRDs based on the HiPPO\ntheory  of  continuous-time  memorization.   Our  goal  in  this  section  is  to  validate  that  S4  achieves  high\nperformance on difficult tasks that require long-range reasoning.  We focus here on two problems:  (i) the\nLong-Range Arena, a well-known benchmark designed to test efficient sequence models on LRDs, and (ii) a\nspeech classification problem as a real-world test of LRDs.\nLong Range Arena (LRA).LRA [40] contains 6 tasks with lengths 1K-16K steps, encompassing modalities\n8",
    "and objectives that require similarity, structural, and visuospatial reasoning.  Table 4 compares S4 against\nthe 11 Transformer variants from Tay et al.[40]as well as follow-up work.  S4 substantially advances the\nSoTA, outperforming all baselines on all tasks and averaging 80.48% compared to less than 60% for every\nbaseline.  Notably, S4 solves the Path-X task, an extremely challenging task that involves reasoning about\nLRDs over sequences of length 128×128 = 16384.  All previous models have failed (i.e. random guessing) due\nto memory or computation bottlenecks, or simply being unable to learn such long dependencies.\nWe analyze S4’s performance on Path-X by visualizing its learned representations, in particular 1-D convolution\nkernelsKwhich are the focus of our technical results in Section 3.  Fig. 2 shows that S4 learns a variety of\nfilters that display spatially consistent structure and demonstrate awareness of the 2-D nature of the data.  In\nparticular, the lower layers learn simple kernels that extract features from just a few rows of local context\nwhile ignoring the rest of the image.  On the other hand, higher layers aggregate information globally across\nfull columns of the image at varying spatial frequencies.  Filters in these higher layers span the entire context\n(16384 pixels), confirming S4’s ability to learn LRDs.\nRaw Speech Classification.\nSpeech is a typical real-world time series domain, involving signals sampled\nfrom an underlying physical process at high frequency.  We perform speech classification using the SC10 subset\nof theSpeech Commandsdataset [47] (see Appendix D.5).  While most sequence models for speech rely on\nextensive preprocessing (e.g.  to MFCC features), we classify raw speech (length-16000) following Romero et al.\n[35].  S4 achieves 98.3% accuracy, higher than all baselines that use the 100×shorter MFCC features, and\nvalidates that a powerful LRD model is able to extract more information from the raw data and outperform\nhand-crafted pre-processing.  Additionally, we include a baseline CNN specifically designed for raw speech,\nthe discriminator from the WaveGAN model [11], which performs worse than S4 while having 90×more\nparameters and incorporating many more architectural heuristics (Appendix D.2).\n4.3    S4 as a General Sequence Model\nA key goal of sequence modeling research is to develop a single model that can be applied in many domains\n(e.g.  images, audio, text, time-series) with a broad range of capabilities (e.g.  efficient training, fast generation,\nhandling irregularly sampled data).  As a fundamental scientific model, SSMs are a promising candidate that\ncome with a range of capabilities, and S4’s strong results on LRD benchmarks spanning images, text, and\nspeech are evidence of S4’s potential as a general sequence model.  In this section, we focus on understanding\nthis question in more depth by highlighting key strengths of S4 in settings that usually require specialized\nTable  5:  (SC10  classification)  Transformer,  CTM,\nRNN, CNN, and SSM models.  (MFCC) Standard pre-\nprocessed MFCC features (length 161).  (Raw) Unpro-\ncessed signals (length 16000).  (0.5×) Frequency change\nat test time.7denotes not applicable or computation-\nally infeasible on single GPU.Please read Appendix D.5\nbefore citing this table.\nMFCC  Raw0.5×\nTransformer90.7577\nPerformer80.8530.7730.68\nODE-RNN65.977\nNRDE89.816.4915.12\nExpRNN82.1311.610.8\nLipschitzRNN88.3877\nCKConv95.371.6665.96\nWaveGAN-D796.257\nLSSL93.5877\nS493.9698.3296.30\nTable  6:   (Pixel-level  1-D  image  classification)\nComparison against reported test accuracies from prior\nworks  (Transformer,  RNN,  CNN,  and  SSM  models).\nExtended results and citations in Appendix D.\nsMNIST  pMNIST  sCIFAR\nTransformer98.997.962.2\nLSTM98.995.1163.01\nr-LSTM98.495.272.2\nUR-LSTM99.2896.9671.00\nUR-GRU99.2796.5174.4\nHiPPO-RNN98.998.361.1\nLMU-FFT-98.49-\nLipschitzRNN99.496.364.2\nTCN99.097.2-\nTrellisNet99.2098.1373.42\nCKConv99.3298.5463.74\nLSSL99.5398.7684.65\nS499.6398.7091.13\n9",
    "Table  7:   (CIFAR-10  density  estimation)  As  a  generic\nsequencemodel, S4 is competitive with previous autoregressive\nmodels (in bits per dim.)  while incorporating no 2D inductive\nbias, and has fast generation through its recurrence mode.\nModelbpd2D biasImages / sec\nTransformer3.47None0.32 (1×)\nLinear Transf.3.40None17.85 (56×)\nPixelCNN3.142D conv.-\nRow PixelRNN3.002D BiLSTM-\nPixelCNN++2.922D conv.19.19\n(59.97×)\nImage Transf.2.902D local attn.0.54 (1.7×)\nPixelSNAIL2.85\n2D conv.  + attn.0.13 (0.4×)\nSparse Transf.2.802D sparse attn.-\nS4(base)2.92None20.84(65.1×)\nS4(large)2.85None3.36 (10.5×)\nTable 8:  (WikiText-103 language modeling) S4 ap-\nproaches the performance of Transformers with much\nfaster generation.  (Top) Transformer baseline which our\nimplementation is based on, with attention replaced by\nS4.  (Bottom) Attention-free models (RNNs and CNNs).\nModelParamsTest ppl.Tokens / sec\nTransformer247M20.510.8K (1×)\nGLU CNN229M37.2-\nAWD-QRNN151M33.0-\nLSTM + Hebb.-29.2-\nTrellisNet180M29.19-\nDynamic Conv.255M25.0-\nTaLK Conv.240M23.3-\nS4249M20.9548K(60×)\nmodels.   The  tasks  we  focus  on  (generative  modeling,  image  classification,  time-series  forecasting)  are\nconsidered as LRD tasks in the literature, and serve as additional validation that S4 handles LRDs efficiently.\nLarge-scale generative modeling.We investigate two well-studied image and text benchmarks to validate\nthe scalability, flexibility, and efficiency of S4.  These tasks require much larger models than our previous\ntasks – up to 250M parameters.\nFirst, CIFAR density estimation is a popular benchmark for autoregressive models, where images are flattened\ninto a sequence of 3072 RGB subpixels that are predicted one by one.  Table 7 shows thatwith no 2D inductive\nbias, S4 is competitive with the best models designed for this task.\nSecond, WikiText-103 is an established benchmark for language modeling, an important task for large-scale\nsequence models where tokens are predicted sequentially based on past context.  Although RNNs were the\nmodel of choice for many years, Transformers are now the dominant model in such applications that contain\ndata that is inherently discrete.  We show that alternative models to Transformers can still be competitive in\nthese settings.  By simply taking a strong Transformer baseline [2] and replacing the self-attention layers, S4\nsubstantially closes the gap to Transformers (within 0.8 ppl), setting SoTA for attention-free models by over\n2 ppl.\nFast autoregressive inference.A prominent limitation of autoregressive models is inference speed (e.g.\ngeneration), since they require a pass over the full context for every new sample.  Several methods have been\nspecifically crafted to overcome this limitation such as the Linear Transformer, a hybrid Transformer/RNN\nthat switches to a stateful, recurrent view at inference time for speed.\nAs a stateful model, SSMs automatically have this ability (Fig. 1).  By switching to its recurrent representation\n(Section  2.3),  S4  requiresconstant  memory  and  computationper  time  step  –  in  contrast  to  standard\nautoregressive models which scale in the context length.  On both CIFAR-10 and WikiText-103, we report\nthe throughput of various models at generation time, with S4 around 60×faster than a vanilla Transformer\non both tasks (details in Appendix D.3.3).\nSampling resolution change.As a continuous-time model, S4 automatically adapts to data sampled\nat different rates, a challenging setting for time series with a dedicated line of work [10,35,37].  Without\nre-training, S4 achieves 96.3% accuracy at 0.5×the frequency on Speech Commands 10 (Table 5), simply by\nchanging its internal step size ∆ (Section 2.3).\nLearning with weaker inductive bias.\nBeyond our results on speech (Section 4.2), we further validate\nthat S4 can be applied with minimal modifications on two domains that typically require specialized domain-\nspecific preprocessing and architectures.  First,  we compare S4 to the Informer [50],  a new Transformer\narchitecture that uses a complex encoder-decoder designed for time-series forecasting problems.  A simple\napplication of S4 that treats forecasting as a masked sequence-to-sequence transformation (Fig. 5) outperforms\nthe Informer and other baselines on 40/50 settings across 5 forecasting tasks.  Notably, S4 is better on the\n10",
    "longest setting in each task, e.g. reducing MSE by 37% when forecasting 30 days of weather data (Table 9).\nFinally, we evaluate S4 on pixel-level sequential image classification tasks (Table 6), popular benchmarks\nwhich were originally LRD tests for RNNs [1].  Beyond LRDs, these benchmarks point to a recent effort of\nthe ML community to solve vision problems with reduced domain knowledge, in the spirit of models such as\nVision Transformers [12] and MLP-Mixer [41] which involve patch-based models that without 2-D inductive\nbias.  Sequential CIFAR is a particularly challenging dataset where outside of SSMs, all sequence models have\na gap of over 25% to a simple 2-D CNN. By contrast, S4 is competitive with a larger ResNet18 (7.9M vs.\n11.0M parameters), both with (93.16%vs.  95.62%) or without (91.12%vs.  89.46%) data augmentation.\nMoreover, it is much more robust to other architectural choices (e.g.90.46%vs.  79.52% when swapping\nBatchNorm for LayerNorm).\n4.4    SSM Ablations:  the Importance of HiPPO\nA critical motivation of S4 was the use of the HiPPO matrices to initialize an SSM. We consider several\nsimplifications of S4 to ablate the importance of each of these components, including:  (i) how important is\nthe HiPPO initialization?  (ii) how important is training the SSM on top of HiPPO? (iii) are the benefits of\nS4 captured by the NPLR parameterization without HiPPO?\nAs a simple testbed, all experiments in this section were performed on the sequential CIFAR-10 task, whicih\nwe found transferred well to other settings.  Models were constrained to at most 100K trainable parameters\nand trained with a simple plateau learning rate scheduler and no regularization.\nUnconstrained  SSMs.We first investigate generic SSMs with various initializations.   We consider a\nrandom Gaussian initialization (with variance scaled down until it did not NaN), and the HiPPO initialization.\nWe also consider a random diagonal Gaussian matrix as a potential structured method; parameterizingAas\na diagonal matrix would allow substantial speedups without going through the complexity of S4’s NPLR\nparameterization.  We consider both freezing theAmatrix and training it.\nFig. 3 shows both training and validation curves, from which we can make several observations.  First, training\nthe SSM improved all methods, particularly the randomly initialized ones.  For all methods, training the SSM\nled to improvements in both training and validation curves.\nSecond, a large generalization gap exists between the initializations.  In particular, note that whenAis\ntrained, all initializations are able to reach perfect training accuracy.  However, their validation accuracies are\nseparated by over 15%.\nNPLR SSMs.The previous experiment validates the importance of HiPPO in SSMs.  This was the main\nmotivation of the NPLR algorithm in S4, which utilizes structure of the HiPPO matrix(2)to make SSMs\ncomputationally feasible.  Fig. 4a shows that random NPLR matrices still do not perform well, which validates\nthat S4’s effectiveness primarily comes from the HiPPO initialization, not the NPLR parameterization.\nFinally, Fig. 4b considers the main ablations considered in this section (with trainable SSMs) and adds minor\nregularization.  With 0.1 Dropout, the same trends still hold, and the HiPPO initialization—in other words,\nthe full S4 method—achieves 84.27% test accuracy with just 100K parameters.\nTable 9:  Univariate long sequence time-series forecasting results.  Full results in Appendix D.3.5.\nS4InformerLogTransReformerLSTMaDeepARARIMAProphet\nMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAE\nETTh\n1\n0.116  0.2710.269  0.4350.273  0.4632.112  1.4360.683  0.7680.658  0.7070.659  0.7662.735  3.253\nETTh\n2\n0.187  0.3580.277  0.4310.303  0.4932.030  1.7210.640  0.6810.429  0.5802.878  1.0443.355  4.664\nETTm\n1\n0.292  0.4660.512  0.6440.598  0.7021.793  1.5281.064  0.8732.437  1.3520.639  0.6972.747  1.174\nWeather0.245  0.3750.359  0.4660.388  0.4992.087  1.5340.866  0.8090.499  0.5961.062  0.9433.859  1.144\nECL0.432  0.4970.582  0.6080.624  0.6457.019  5.1051.545  1.0060.657  0.6831.370  0.9826.901  4.264\n11",
    "Figure 3:  CIFAR-10 classification with unconstrained, real-valued SSMs with various initializations.  (Left) Train\naccuracy.  (Right) Validation accuracy.\n(a)(b)\nFigure 4:  CIFAR-10 validation accuracy of SSMs with different initializations and parameterizations.  (Left) NPLR\nparameterization with random versus HiPPO initialization.  (Right) All methods considered in this section, including\nminor Dropout regularization.  S4 achieves SotA accuracy on sequential CIFAR-10 with just 100K parameters.\n5    Conclusion\nWe introduce S4, a sequence model that uses a new parameterization for the state space model’s continuous-\ntime, recurrent, and convolutional views to efficiently model LRDs in a principled manner.  Results across\nestablished benchmarks evaluating a diverse range of data modalities and model capabilities suggest that S4\nhas the potential to be an effective general sequence modeling solution.\nAcknowledgments\nWe thank Aditya Grover and Chris Cundy for helpful discussions about earlier versions of the method.  We\nthank Simran Arora, Sabri Eyuboglu, Bibek Paudel, and Nimit Sohoni for valuable feedback on earlier drafts\nof this work.  This work was done with the support of Google Cloud credits under HAI proposals 540994170283\nand 578192719349.  We gratefully acknowledge the support of NIH under No.  U54EB020405 (Mobilize),\nNSF under Nos.  CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML);\nONR under No.  N000141712266 (Unifying Weak Supervision);  ONR N00014-20-1-2480:  Understanding\nand Applying Non-Euclidean Geometry in Machine Learning;  N000142012275 (NEPTUNE); the Moore\nFoundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF,\nAccenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google\nCloud, Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science\nInitiative (SDSI), and members of the Stanford DAWN project:  Facebook, Google, and VMWare.  The\nMobilize Center is a Biomedical Technology Resource Center,  funded by the NIH National Institute of\nBiomedical Imaging and Bioengineering through Grant P41EB027060.  The U.S. Government is authorized\nto reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation\nthereon.  Any opinions, findings, and conclusions or recommendations expressed in this material are those of\n12",
    "the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of\nNIH, ONR, or the U.S. Government.\nReferences\n[1]Martin Arjovsky, Amar Shah, and Yoshua Bengio.  Unitary evolution recurrent neural networks.  InThe\nInternational Conference on Machine Learning (ICML), pages 1120–1128, 2016.\n[2]\nAlexei Baevski and Michael Auli.  Adaptive input representations for neural language modeling.arXiv\npreprint arXiv:1809.10853, 2018.\n[3]\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.  An empirical evaluation of generic convolutional and\nrecurrent networks for sequence modeling.arXiv preprint arXiv:1803.01271, 2018.\n[4]Shaojie  Bai,  J  Zico  Kolter,  and  Vladlen  Koltun.   Trellis  networks  for  sequence  modeling.   InThe\nInternational Conference on Learning Representations (ICLR), 2019.\n[5]\nShiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock,\nMark Hasegawa-Johnson, and Thomas S Huang.  Dilated recurrent neural networks.  InAdvances in\nNeural Information Processing Systems (NeurIPS), 2017.\n[6]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.  Generating long sequences with sparse\ntransformers.arXiv preprint arXiv:1904.10509, 2019.\n[7]\nNarsimha Chilkuri and Chris Eliasmith.  Parallelizing legendre memory unit training.The International\nConference on Machine Learning (ICML), 2021.\n[8]Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al.  Rethinking attention with\nperformers.  InThe International Conference on Learning Representations (ICLR), 2020.\n[9]\nYann N Dauphin,  Angela Fan,  Michael Auli,  and David Grangier.  Language modeling with gated\nconvolutional networks.  InInternational conference on machine learning, pages 933–941. PMLR, 2017.\n[10]Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau.  Gru-ode-bayes:  Continuous modeling\nof sporadically-observed time series.  InAdvances in Neural Information Processing Systems (NeurIPS),\n2019.\n[11]  Chris Donahue, Julian McAuley, and Miller Puckette.  Adversarial audio synthesis.  InICLR, 2019.\n[12]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.  An image is\nworth 16x16 words:  Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020.\n[13]N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney.\nLipschitz recurrent neural networks.  InInternational Conference on Learning Representations, 2021.\n[14]Karan Goel, Albert Gu, Chris Donahue, and Christopher R ́e. It’s raw!  audio generation with state-space\nmodels.arXiv preprint arXiv:2202.09729, 2022.\n[15]  Gene H Golub and Charles F Van Loan.Matrix computations, volume 3.  JHU press, 2013.\n[16]Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R ́e.  Hippo:  Recurrent memory with\noptimal polynomial projections. InAdvances in Neural Information Processing Systems (NeurIPS), 2020.\n[17]Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu.  Improving the gating\nmechanism of recurrent neural networks. InThe International Conference on Machine Learning (ICML),\n2020.\n13",
    "[18]Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R ́e. Combining\nrecurrent, convolutional, and continuous-time models with the structured learnable linear state space\nlayer.  InAdvances in Neural Information Processing Systems (NeurIPS), 2021.\n[19]Albert Gu, Ankit Gupta, Karan Goel, and Christopher R ́e.  On the parameterization and initialization\nof diagonal state space models.arXiv preprint arXiv:2206.11893, 2022.\n[20]  Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R ́e.  How to train your hippo:\nState space models with generalized basis projections.arXiv preprint arXiv:2206.12037, 2022.\n[21]\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory.Neural computation, 9(8):1735–1780,\n1997.\n[22]\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran ̧cois Fleuret.  Transformers are rnns:\nFast autoregressive transformers with linear attention. InInternational Conference on Machine Learning,\npages 5156–5165. PMLR, 2020.\n[23]Patrick Kidger, James Morrill, James Foster, and Terry Lyons.  Neural controlled differential equations\nfor irregular time series.arXiv preprint arXiv:2005.08926, 2020.\n[24]Mario Lezcano-Casado and David Mart ́ınez-Rubio.  Cheap orthogonal constraints in neural networks:\nA simple parametrization of the orthogonal and unitary group.  InThe International Conference on\nMachine Learning (ICML), 2019.\n[25]Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao.  Independently recurrent neural network\n(IndRNN): Building a longer and deeper RNN.  InProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 5457–5466, 2018.\n[26]  Vasileios Lioutas and Yuhong Guo.  Time-aware large kernel convolutions.  InInternational Conference\non Machine Learning, pages 6172–6183. PMLR, 2020.\n[27]\nStephen Merity, Nitish Shirish Keskar, James Bradbury, and Richard Socher. Scalable language modeling:\nWikitext-103 on a single gpu in 12 hours.SysML, 2018.\n[28]Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\nKalchbrenner, Andrew Senior, and Koray Kavukcuoglu.  Wavenet:  A generative model for raw audio.\narXiv preprint arXiv:1609.03499, 2016.\n[29]Victor Pan.Structured  matrices  and  polynomials:  unified  superfast  algorithms.  Springer Science &\nBusiness Media, 2001.\n[30]Victor Pan.  Fast approximate computations with cauchy matrices and polynomials.Mathematics of\nComputation, 86(308):2799–2826, 2017.\n[31]\nVictor Y Pan.  Transformations of matrix structures work again.Linear Algebra and Its Applications,\n465:107–138, 2015.\n[32]Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.  On the difficulty of training recurrent neural\nnetworks.  InInternational conference on machine learning, pages 1310–1318, 2013.\n[33]\nJack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap.  Fast parametric learning with activation\nmemorization.The International Conference on Machine Learning (ICML), 2018.\n[34]Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang\nZhang,  Mark  A  Hasegawa-Johnson,  Roy  H  Campbell,  and  Thomas  S  Huang.   Fast  generation  for\nconvolutional autoregressive models.arXiv preprint arXiv:1704.06001, 2017.\n[35]David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn.  Ckconv:\nContinuous kernel convolution for sequential data.arXiv preprint arXiv:2102.02611, 2021.\n14",
    "[36]David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and\nJan C van Gemert.  Flexconv:  Continuous kernel convolutions with differentiable kernel sizes.  InThe\nInternational Conference on Learning Representations (ICLR), 2022.\n[37]Yulia Rubanova, Tian Qi Chen, and David K Duvenaud.  Latent ordinary differential equations for\nirregularly-sampled time series. InAdvances in Neural Information Processing Systems, pages 5321–5331,\n2019.\n[38]  T Konstantin Rusch and Siddhartha Mishra.  Unicornn:  A recurrent model for learning very long time\ndependencies.The International Conference on Machine Learning (ICML), 2021.\n[39]\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++:  Improving the pixelcnn\nwith discretized logistic mixture likelihood and other modifications.arXiv preprint arXiv:1701.05517,\n2017.\n[40]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler.  Long range arena :  A benchmark for efficient transformers.\nInInternational Conference on Learning Representations, 2021. URLhttps://openreview.net/forum?\nid=qVyeW-grC2k.\n[41]\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,\nJessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al.  Mlp-mixer:  An all-mlp architecture\nfor vision.arXiv preprint arXiv:2105.01601, 2021.\n[42]Trieu H Trinh, Andrew M Dai, Minh-Thang Luong, and Quoc V Le.  Learning longer-term dependencies\nin RNNs with auxiliary losses.  InThe International Conference on Machine Learning (ICML), 2018.\n[43]Arnold Tustin.  A method of analysing the behaviour of linear systems in terms of time series.Journal\nof the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms, 94(1):\n130–142, 1947.\n[44]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin.  Attention is all you need.  InAdvances in Neural Information Processing\nSystems (NeurIPS), 2017.\n[45]Aaron Voelker, Ivana Kaji ́c, and Chris Eliasmith. Legendre memory units: Continuous-time representation\nin recurrent neural networks. InAdvances in Neural Information Processing Systems, pages 15544–15553,\n2019.\n[46]Aaron Russell Voelker.Dynamical systems in spiking neuromorphic hardware.  PhD thesis, University of\nWaterloo, 2019.\n[47]Pete  Warden.    Speech  commands:   A  dataset  for  limited-vocabulary  speech  recognition.ArXiv,\nabs/1804.03209, 2018.\n[48]  Max A Woodbury.  Inverting modified matrices.Memorandum report, 42:106, 1950.\n[49]Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli.  Pay less attention with\nlightweight and dynamic convolutions.  InThe International Conference on Learning Representations\n(ICLR), 2019.\n[50]Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\nInformer:  Beyond efficient transformer for long sequence time-series forecasting.  InThe Thirty-Fifth\nAAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference, volume 35, pages 11106–\n11115. AAAI Press, 2021.\n15",
    "A    Discussion\nRelated Work.Our work is most closely related to a line of work originally motivated by a particular\nbiologically-inspired SSM, which led to mathematical models for addressing LRDs.  Voelker et al.[45], Voelker\n[46]derived a non-trainable SSM motivated from approximating a neuromorphic spiking model, and Chilkuri\nand Eliasmith[7]showed that it could be sped up at train time with a convolutional view.  Gu et al.[16]\nextended this special case to a general continuous-time function approximation framework with several more\nspecial cases ofAmatrices designed for long-range dependencies.  However, instead of using a true SSM, all\nof these works fixed a choice ofAand built RNNs around it.  Most recently, Gu et al.[18]used the full(1)\nexplicitly as a deep SSM model, exploring new conceptual views of SSMs, as well as allowingAto be trained.\nAs mentioned in Section 1, their method used a naive instantiation of SSMs that suffered from an additional\nfactor ofNin memory andN\n2\nin computation.\nBeyond this work, our technical contributions (Section 3) on the S4 parameterization and algorithms are\napplicable to a broader family of SSMs including these investigated in prior works, and our techniques for\nworking with these models may be of independent interest.\nImplementation.\nThe computational core of S4’s training algorithm is the Cauchy kernel discussed in\nSections  3.2  and  3.3  and  Appendix  C.3.   As  described  in  Appendix  C.3  Proposition  5,  there  are  many\nalgorithms for it with differing computational complexities and sophistication.  Our current implementation\nof S4 actually uses the naiveO(NL) algorithm which is easily parallelized on GPUs and has more easily\naccessible libraries allowing it to be implemented;  we leverage thepykeopslibrary for memory-efficient\nkernel operations.  However, this library is a much more general library that may not be optimized for the\nCauchy kernels used here, and we believe that a dedicated CUDA implementation can be more efficient.\nAdditionally, as discussed in this work, there are asymptotically faster and numerically stable algorithms for\nthe Cauchy kernel (Proposition 5).  However, these algorithms are currently not implemented for GPUs due\nto a lack of previous applications that require them.  We believe that more efficient implementations of these\nself-contained computational kernels are possible, and that S4 (and SSMs at large) may have significant room\nfor further improvements in efficiency.\nLimitations and Future Directions.In this work, we show that S4 can address a wide variety of data\neffectively.  However, it may not necessarily be the most suitable model for all types of data.  For example,\nTable 8 still found a gap compared to Transformers for language modeling.  An interesting future direction is\nexploring combinations of S4 with other sequence models to complement their strengths.  We are excited\nabout other directions, including continuing to explore the benefits of S4 on audio data (e.g.  pre-training\nor generation settings), and generalizing HiPPO and S4 to higher-dimensional data for image and video\napplications.\nB    Numerical Instability of LSSL\nThis section proves the claims made in Section 3.1 about prior work.  We first derive the explicit diagonalization\nof the HiPPO matrix, confirming its instability because of exponentially large entries.  We then discuss the\nproposed theoretically fast algorithm from [18] (Theorem 2) and show that it also involves exponentially large\nterms and thus cannot be implemented.\n16",
    "B.1    HiPPO Diagonalization\nProof of Lemma 3.2.The HiPPO matrix (2) is equal, up to sign and conjugation by a diagonal matrix, to\nA=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n−12\n1−33\n−13−54\n1−35−75\n−13−57−96\n1−35−79−117\n−13−57−911−13    8\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nnk\n=\n\n\n\n\n\n(−1)\nn−k\n(2k+ 1)n > k\nk+ 1n=k\n0n < k\n.\nOur goal is to show that thisAis diagonalized by the matrix\nV=\n(\ni+j\ni−j\n)\nij\n=\n\n\n\n\n\n\n\n\n\n\n\n1\n11\n131\n1651\n1    10    1571\n1    15    35    28    9    1\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\n\n,\nor in other words that columns of this matrix are eigenvectors ofA.\nConcretely, we will show that thej-th column of this matrixv\n(j)\nwith elements\nv\n(j)\ni\n=\n{\n0i < j\n(\ni+j\ni−j\n)\n=\n(\ni+j\n2j\n)\ni≥j\nis an eigenvector with eigenvaluej+ 1.  In other words we must show that for all indicesk∈[N],\n(Av\n(j)\n)\nk\n=\n∑\ni\nA\nki\nv\n(j)\ni\n= (j+ 1)v\n(j)\nk\n.(7)\nIfk < j, then for alliinside the sum, eitherk < iori < j.  In the first caseA\nki\n= 0 and in the second case\nv\n(j)\ni\n= 0, so both sides of equation (7) are equal to 0.\nIt remains to show the casek≥j, which proceeds by induction onk.  Expanding equation(7)using the\nformula forAyields\n(Av)\n(j)\nk\n=\n∑\ni\nA\nki\nv\n(j)\ni\n=\nk−1\n∑\ni=j\n(−1)\nk−i\n(2i+ 1)\n(\ni+j\n2j\n)\n+ (k+ 1)\n(\nk+j\n2j\n)\n.\nIn the base casek=j, the sum disappears and we are left with (Av\n(j)\n)\nj\n= (j+ 1)\n(\n2j\n2j\n)\n= (j+ 1)v\n(j)\nj\n, as\ndesired.\nOtherwise, the sum for (Av)\n(j)\nk\nis the same as the sum for (Av)\n(j)\nk−1\nbut with sign reversed and a few edge\n17",
    "terms.  The result follows from applying the inductive hypothesis and algebraic simplification:\n(Av)\n(j)\nk\n=−(Av)\n(j)\nk−1\n−(2k−1)\n(\nk−1 +j\n2j\n)\n+k\n(\nk−1 +j\n2j\n)\n+ (k+ 1)\n(\nk+j\n2j\n)\n=−(j+ 1)\n(\nk−1 +j\n2j\n)\n−(k−1)\n(\nk−1 +j\n2j\n)\n+ (k+ 1)\n(\nk+j\n2j\n)\n=−(j+k)\n(\nk−1 +j\n2j\n)\n+ (k+ 1)\n(\nk+j\n2j\n)\n=−(j+k)\n(k−1 +j)!\n(k−1−j)!(2j)!\n+ (k+ 1)\n(\nk+j\n2j\n)\n=−\n(k+j)!\n(k−1−j)!(2j)!\n+ (k+ 1)\n(\nk+j\n2j\n)\n=−(k−j)\n(k+j)!\n(k−j)!(2j)!\n+ (k+ 1)\n(\nk+j\n2j\n)\n= (j−k)(k+ 1)\n(\nk+j\n2j\n)\n+ (k+ 1)\n(\nk+j\n2j\n)\n= (j+ 1)v\n(j)\nk\n.\nB.2    Fast but Unstable LSSL Algorithm\nInstead of diagonalization, Gu et al. [18, Theorem 2] proposed a sophisticated fast algorithm to compute\nK\nL\n(A,B,C) = (CB,CAB,...,CA\nL−1\nB).\nThis algorithm runs inO(Nlog\n2\nN+LlogL) operations andO(N+L) space.  However, we now show that\nthis algorithm is also numerically unstable.\nThere are several reasons for the instability of this algorithm, but most directly we can pinpoint a particular\nintermediate quantity that they use.\nDefinition 1.The fast LSSL algorithm computes coefficients ofp(x), the characteristic polynomial ofA, as\nan intermediate computation.  Additionally, it computes the coefficients of its inverse,p(x)\n−1\n(modx\nL\n).\nWe now claim that this quantity is numerically unfeasible.  We narrow down to the case whenA=Iis the\nidentity matrix.  Note that this case is actually in some sense the most typical case:  when discretizing the\ncontinuous-time SSM to discrete-time by a step-size ∆, the discretized transition matrixAis brought closer\nto the identity.  For example, with the Euler discretizationA=I+ ∆A, we haveA→Ias the step size\n∆→0.\nLemma B.1.WhenA=I, the fast LSSL algorithm requires computing terms exponentially large inN.\nProof.The characteristic polynomial ofIis\np(x) =det|I−xI|= (1−x)\nN\n.\nThese coefficients have size up to\n(\nN\nN\n2\n)\n≈\n2\nN\n√\nπN/2\n.\nThe inverse ofp(x) has even larger coefficients.  It can be calculated in closed form by the generalized binomial\nformula:\n(1−x)\n−N\n=\n∞\n∑\nk=0\n(\nN+k−1\nk\n)\nx\nk\n.\n18",
    "Taking this   (modx\nL\n), the largest coefficient is\n(\nN+L−2\nL−1\n)\n=\n(\nN+L−2\nN−1\n)\n=\n(L−1)(L−2)...(L−N+ 1)\n(N−1)!\n.\nWhenL=N−1 this is\n(\n2(N−1)\nN−1\n)\n≈\n2\n2N\n√\nπN\nalready larger than the coefficients of (1−x)\nN\n, and only increases asLgrows.\nC    S4 Algorithm Details\nThis section proves the results of Section 3.3, providing complete details of our efficient algorithms for S4.\nAppendices C.1 to C.3 prove Theorems 1 to 3 respectively.\nC.1    NPLR Representations of HiPPO Matrices\nWe first prove Theorem 1, showing that all HiPPO matrices for continuous-time memory fall under the S4\nnormal plus low-rank (NPLR) representation.\nProof of Theorem 1.We consider each of the three cases HiPPO-LagT, HiPPO-LegT, and HiPPO-LegS\nseparately.  Note that the primary HiPPO matrix defined in this work (equation(2)) is the HiPPO-LegT\nmatrix.\nHiPPO-LagT.The HiPPO-LagT matrix is simply\nA\nnk\n=\n\n\n\n\n\n0n < k\n−\n1\n2\nn=k\n−1n > k\nA=−\n\n\n\n\n\n\n\n1\n2\n...\n1\n1\n2\n11\n1\n2\n111\n1\n2\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n.\nAdding the matrix of all\n1\n2\n, which is rank 1, yields\n−\n\n\n\n\n−\n1\n2\n−\n1\n2\n−\n1\n2\n1\n2\n−\n1\n2\n−\n1\n2\n1\n2\n1\n2\n−\n1\n2\n1\n2\n1\n2\n1\n2\n\n\n\n\n.\nThis matrix is now skew-symmetric.  Skew-symmetric matrices are a particular case of normal matrices with\npure-imaginary eigenvalues.\nGu et al.[16]also consider a case of HiPPO corresponding to the generalized Laguerre polynomials that\ngeneralizes the above HiPPO-LagT case.  In this case, the matrixA(up to conjugation by a diagonal matrix)\nends up being close to the above matrix, but with a different element on the diagonal.  After adding the\nrank-1 correction, it becomes the above skew-symmetric matrix plus a multiple of the identity.  Thus after\ndiagonalization by the same matrix as in the LagT case, it is still reduced to diagonal plus low-rank (DPLR)\nform, where the diagonal is now pure imaginary plus a real constant.\n19",
    "HiPPO-LegS.We restate the formula from equation (2) for convenience.\nA\nnk\n=−\n\n\n\n\n\n(2n+ 1)\n1/2\n(2k+ 1)\n1/2\nifn > k\nn+ 1ifn=k\n0ifn < k\n.\nAdding\n1\n2\n(2n+ 1)\n1/2\n(2k+ 1)\n1/2\nto the whole matrix gives\n−\n\n\n\n\n\n1\n2\n(2n+ 1)\n1/2\n(2k+ 1)\n1/2\nifn > k\n1\n2\nifn=k\n−\n1\n2\n(2n+ 1)\n1/2\n(2k+ 1)\n1/2\nifn < k\nNote that this matrix is not skew-symmetric, but is\n1\n2\nI\n+SwhereSis a skew-symmetric matrix.  This is\ndiagonalizable by the same unitary matrix that diagonalizesS.\nHiPPO-LegT.\nUp to the diagonal scaling, the LegT matrix is\nA=−\n\n\n\n\n\n\n\n1−11−1...\n11−11\n111−1\n1111\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n.\nBy adding−1 to this matrix and then the matrix\n\n\n\n\n22\n22\n\n\n\n\nthe matrix becomes\n\n\n\n\n−2−2\n2\n−2\n22\n\n\n\n\nwhich is skew-symmetric.  In fact, this matrix is the inverse of the Chebyshev Jacobi.\nAn alternative way to see this is as follows.  The LegT matrix is the inverse of the matrix\n\n\n\n\n−110\n−11\n−11\n−1−1\n\n\n\n\nThis can obviously be converted to a skew-symmetric matrix by adding a rank 2 term.  The inverses of these\nmatrices are also rank-2 differences from each other by the Woodbury identity.\nA final form is\n\n\n\n\n−11−11\n−1−11−1\n−1−1−11\n−1−1−1−1\n\n\n\n\n+\n\n\n\n\n1    0    1    0\n0    1    0    1\n1    0    1    0\n0    1    0    1\n\n\n\n\n=\n\n\n\n\n0101\n−1010\n0−101\n−10−1    0\n\n\n\n\nThis has the advantage that the rank-2 correction is symmetric (like the others),  but the normal skew-\nsymmetric matrix is now 2-quasiseparable instead of 1-quasiseparable.\n20",
    "C.2    Computing the S4 Recurrent View\nWe prove Theorem 2 showing the efficiency of the S4 parameterization for computing one step of the recurrent\nrepresentation (Section 2.3).\nRecall that without loss of generality, we can assume that the state matrixA=Λ−P Q\n∗\nis diagonal plus\nlow-rank (DPLR), potentially overC.  Our goal in this section is to explicitly write out a closed form for the\ndiscretized matrixA.\nRecall from equation (3) that\nA= (I−∆/2·A)\n−1\n(I+ ∆/2·A)\nB= (I−∆/2·A)\n−1\n∆B.\nWe first simplify both terms in the definition ofAindependently.\nForward discretization.The first term is essentially the Euler discretization motivated in Section 2.3.\nI+\n∆\n2\nA=I+\n∆\n2\n(Λ−P Q\n∗\n)\n=\n∆\n2\n[\n2\n∆\nI+ (Λ−P Q\n∗\n)\n]\n=\n∆\n2\nA\n0\nwhereA\n0\nis defined as the term in the final brackets.\nBackward discretization.The second term is known as the Backward Euler’s method.  Although this\ninverse term is normally difficult to deal with, in the DPLR case we can simplify it using Woodbury’s Identity\n(Proposition 4).\n(\nI−\n∆\n2\nA\n)\n−1\n=\n(\nI−\n∆\n2\n(Λ−P Q\n∗\n)\n)\n−1\n=\n2\n∆\n[\n2\n∆\n−Λ+P Q\n∗\n]\n−1\n=\n2\n∆\n[\nD−DP(I+Q\n∗\nDP)\n−1\nQ\n∗\nD\n]\n=\n2\n∆\nA\n1\nwhereD=\n(\n2\n∆\n−Λ\n)\n−1\nandA\n1\nis defined as the term in the final brackets.  Note that(1 +Q\n∗\nDP)is actually\na scalar in the case when the low-rank term has rank 1.\nS4 Recurrence.Finally, the full bilinear discretization can be rewritten in terms of these matrices as\nA=A\n1\nA\n0\nB=\n2\n∆\nA\n1\n∆B= 2A\n1\nB.\nThe discrete-time SSM (3) becomes\nx\nk\n=\nAx\nk−1\n+Bu\nk\n=A\n1\nA\n0\nx\nk−1\n+ 2A\n1\nBu\nk\ny\nk\n=Cx\nk\n.\nNote thatA\n0\n,A\n1\nare accessed only through matrix-vector multiplications.  Since they are both DPLR, they\nhaveO(N) matrix-vector multiplication, showing Theorem 2.\n21",
    "C.3    Computing the Convolutional View\nThe most involved part of using SSMs efficiently is computingK.  This algorithm was sketched in Section 3.2\nand is the main motivation for the S4 parameterization.  In this section, we define the necessary intermediate\nquantities and prove the main technical result.\nThe algorithm for Theorem 3 falls in roughly three stages, leading to Algorithm 1.  AssumingAhas been\nconjugated  into  diagonal  plus  low-rank  form,  we  successively  simplify  the  problem  of  computingKby\napplying the techniques outlined in Section 3.2.\nRemark C.1.We note that for the remainder of this section, we transposeCto be a column vector\nof shapeC\nN\norC\nN×1\ninstead of matrix or row vectorC\n1×N\nas in(1).  In other words the SSM is\nx\n′\n(t) =Ax(t) +Bu(t)\ny(t) =C\n∗\nx(t) +Du(t).\n(8)\nThis convention is made so thatChas the same shape asB,P,Qand simplifies the implementation of S4.\nReduction 0:  Diagonalization\nBy Lemma 3.1, we can switch the representation by conjugating with\nany unitary matrix.  For the remainder of this section, we can assume thatAis (complex) diagonal plus\nlow-rank (DPLR).\nNote  that  unlike  diagonal  matrices,  a  DPLR  matrix  does  not  lend  itself  to  efficient  computation  ofK.\nThe reason is thatKcomputes termsC\n∗\nA\ni\nBwhich involve powers of the matrixA.  These are trivially\ncomputable whenAis diagonal, but is no longer possible for even simple modifications to diagonal matrices\nsuch as DPLR.\nReduction  1:  SSM  Generating  FunctionTo  address  the  problem  of  computing  powers  ofA,  we\nintroduce another technique.  Instead of computing the SSM convolution filterKdirectly, we introduce a\ngenerating function on its coefficients and compute evaluations of it.\nDefinition 2(SSM Generating Function).We define the following quantities:\n•TheSSM convolution functionisK(A,B,C) = (C\n∗\nB,C\n∗\nAB,...)and the (truncated) SSM filter of\nlengthL\nK\nL\n(A,B,C) = (C\n∗\nB,C\n∗\nAB,...,C\n∗\nA\nL−1\nB)∈R\nL\n(9)\n•TheSSM generating functionat nodezis\nˆ\nK(z;\nA,B,C)∈C:=\n∞\n∑\ni=0\nC\n∗\nA\ni\nBz\ni\n=C\n∗\n(I−Az)\n−1\nB(10)\nand thetruncated SSM generating functionat nodezis\nˆ\nK\nL\n(z;A,B,C)\n∗\n∈C:=\nL−1\n∑\ni=0\nC\n∗\nA\ni\nBz\ni\n=C\n∗\n(I−A\nL\nz\nL\n)(I−Az)\n−1\nB(11)\n•The truncated SSM generating function at nodesΩ∈C\nM\nis\nˆ\nK\nL\n(Ω;A,B,C)∈C\nM\n:=\n(\nˆ\nK\nL\n(ω\nk\n;\nA,B,C)\n)\nk∈[M]\n(12)\nIntuitively, the generating function essentially converts the SSM convolution filter from the time domain to\nfrequency domain.  Importantly, it preserves the same information, and the desired SSM convolution filter\ncan be recovered from evaluations of its generating function.\n22",
    "Lemma C.2.The SSM functionK\nL\n(A,B,C)can be computed from the SSM generating function\nˆ\nK\nL\n(Ω;A,B,C)\nat the roots of unityΩ ={exp(−2πi\nk\nL\n:k∈[L]}stably inO(LlogL)operations.\nProof.For convenience define\nK=K\nL\n(A,B,C)\nˆ\nK=\nˆ\nK\nL\n(Ω;\nA,B,C)\nˆ\nK(z) =\nˆ\nK\nL\n(z;A,B,C).\nNote that\nˆ\nK\nj\n=\nL−1\n∑\nk=0\nK\nk\nexp\n(\n−2πi\njk\nL\n)\n.\nNote that this is exactly the same as the Discrete Fourier Transform (DFT):\nˆ\nK=F\nL\nK.\nThereforeKcan be recovered from\nˆ\nKwith a single inverse DFT, which requiresO(LlogL) operations with\nthe Fast Fourier Transform (FFT) algorithm.\nReduction 2:  Woodbury CorrectionThe primary motivation of Definition 2 is that it turnspowersof\nAinto a singleinverseofA(equation(10)).  While DPLR matrices cannot be powered efficiently due to the\nlow-rank term, they can be inverted efficiently by the well-known Woodbury identity.\nProposition 4(Binomial Inverse Theorem or Woodbury matrix identity [15,48]).Over a commutative ring\nR, letA∈R\nN×N\nandU,V∈R\nN×p\n.  SupposeAandA+UV\n∗\nare invertible.  ThenI\np\n+V\n∗\nA\n−1\nU∈R\np×p\nis invertible and\n(A+UV\n∗\n)\n−1\n=A\n−1\n−A\n−1\nU(I\np\n+V\n∗\nA\n−1\nU)\n−1\nV\n∗\nA\n−1\nWith this identity, we can convert the SSM generating function on a DPLR matrixAinto one on just its\ndiagonal component.\nLemma C.3.LetA=Λ−P Q\n∗\nbe a diagonal plus low-rank representation.  Then for any root of unity\nz∈Ω, the truncated generating function satisfies\nˆ\nK(z) =\n2\n1 +z\n[\n ̃\nC\n∗\nR(z)B−\n ̃\nC\n∗\nR(z)P(1 +Q\n∗\nR(z)P)\n−1\nQ\n∗\nR(z)B\n]\n ̃\nC= (I−\nA\nL\n)\n∗\nC\nR(z;Λ) =\n(\n2\n∆\n1−z\n1 +z\n−Λ\n)\n−1\n.\nProof.Directly expanding Definition 2 yields\nK\nL\n(z;A,B,C) =C\n∗\nB+C\n∗\nABz+···+C\n∗\nA\nL−1\nBz\nL−1\n=C\n∗\n(\nI−A\nL\n)\n(\nI−Az\n)\n−1\nB\n=\n ̃\nC\n∗\n(\nI−\nAz\n)\n−1\nB\nwhere\n ̃\nC\n∗\n=C\n∗\n(\nI−A\nL\n)\n.\n23",
    "We can now explicitly expand the discretized SSM matricesAandBback in terms of the original SSM\nparametersAandB.  Lemma C.4 provides an explicit formula, which allows further simplifying\n ̃\nC\n∗\n(\nI−Az\n)\n−1\nB=\n2\n1 +z\n ̃\nC\n∗\n(\n2\n∆\n1−z\n1 +z\n−A\n)\n−1\nB\n=\n2\n1 +z\n ̃\nC\n∗\n(\n2\n∆\n1−z\n1 +z\n−Λ+P Q\n∗\n)\n−1\nB\n=\n2\n1 +z\n[\n ̃\nC\n∗\nR(z)B−\n ̃\nC\n∗\nR(z)P(1 +Q\n∗\nR(z)P)\n−1\nQ\n∗\nR(z)B\n]\n.\nThe last line applies the Woodbury Identity (Proposition 4) whereR(z) =\n(\n2\n∆\n1−z\n1+z\n−Λ\n)\n−1\n.\nThe previous proof used the following self-contained result to back out the original SSM matrices from the\ndiscretization.\nLemma C.4.LetA,Bbe the SSM matricesA,Bdiscretized by the bilinear discretization with step size∆.\nThen\nC\n∗\n(\nI−\nAz\n)\n−1\nB=\n2∆\n1 +z\nC\n∗\n[\n2\n1−z\n1 +z\n−∆A\n]\n−1\nB\nProof.Recall that the bilinear discretization that we use (equation (3)) is\nA=\n(\nI−\n∆\n2\nA\n)\n−1\n(\nI+\n∆\n2\nA\n)\nB=\n(\nI−\n∆\n2\nA\n)\n−1\n∆B\nThe result is proved algebraic manipulations.\nC\n∗\n(\nI−Az\n)\n−1\nB=C\n∗\n[\n(\nI−\n∆\n2\nA\n)\n−1\n(\nI−\n∆\n2\nA\n)\n−\n(\nI−\n∆\n2\nA\n)\n−1\n(\nI+\n∆\n2\nA\n)\nz\n]\n−1\nB\n=C\n∗\n[(\nI−\n∆\n2\nA\n)\n−\n(\nI+\n∆\n2\nA\n)\nz\n]\n−1\n(\nI−\n∆\n2\nA\n)\nB\n=C\n∗\n[\nI(1−z)−\n∆\n2\nA(1 +z)\n]\n−1\n∆B\n=\n∆\n1−z\nC\n∗\n[\nI−\n∆A\n2\n1−z\n1+z\n]\n−1\nB\n=\n2∆\n1 +z\nC\n∗\n[\n2\n1−z\n1 +z\nI−∆A\n]\n−1\nB\nNote that in the S4 parameterization, instead of constantly computing\n ̃\nC\n=\n(\nI−\nA\nL\n)\n∗\nC, we can simply\nreparameterize  our  parameters  to  learn\n ̃\nC\ndirectly  instead  ofC,  saving  a  minor  computation  cost  and\nsimplifying the algorithm.\n24",
    "Reduction 3:  Cauchy KernelWe have reduced the original problem of computingKto the problem\nof computing the SSM generating function\nˆ\nK\nL\n(Ω;A,B,C) in the case thatAis a diagonal matrix.  We\nshow that this is exactly the same as a Cauchy kernel, which is a well-studied problem with fast and stable\nnumerical algorithms.\nDefinition 3.ACauchy matrixor kernel on nodesΩ = (ω\ni\n)∈C\nM\nandΛ = (λ\nj\n)∈C\nN\nis\nM∈C\nM×N\n=M(Ω,Λ) = (M\nij\n)\ni∈[M],j∈[N]\nM\nij\n=\n1\nω\ni\n−λ\nj\n.\nThe computation time of a Cauchy matrix-vector product of sizeM×Nis denoted byC(M,N).\nComputing with Cauchy matrices is an extremely well-studied problem in numerical analysis, with both fast\narithmetic algorithms and fast numerical algorithms based on the famous Fast Multipole Method (FMM)\n[29, 30, 31].\nProposition 5(Cauchy).A Cauchy kernel requiresO(M+N)space, and operation count\nC(M,N) =\n\n\n\n\n\nO(MN)naively\nO\n(\n(M+N) log\n2\n(M+N)\n)\nin exact arithmetic\nO\n(\n(M+N) log(M+N) log\n1\nε\n)\nnumerically to precisionε.\nCorollary C.5.\nEvaluatingQ\n∗\nR(Ω; Λ)P(defined in Lemma C.3) for any set of nodesΩ∈C\nL\n, diagonal\nmatrixΛ, and vectorsP,Qcan be computed inC(L,N)operations andO(L+N)space, whereC(L,N) =\n ̃\nO(L+N)is the cost of a Cauchy matrix-vector multiplication.\nProof.\nFor any fixedω∈Ω, we want to compute\n∑\nj\nq\n∗\nj\np\nj\nω−λ\nj\n.  Computing this over allω\ni\nis therefore exactly a\nCauchy matrix-vector multiplication.\nThis completes the proof of Theorem 3.  In Algorithm 1, note that the work is dominated by Step 2, which\nhas a constant number of calls to a black-box Cauchy kernel, with complexity given by Proposition 5.\nD    Experiment Details and Full Results\nThis section contains full experimental procedures and extended results and citations for our experimental\nevaluation in Section 4.  Appendix D.1 corresponds to benchmarking results in Section 4.1, Appendix D.2\ncorresponds to LRD experiments (LRA and Speech Commands) in Section 4.2, and Appendix D.3 corresponds\nto the general sequence modeling experiments (generation, image classification, forecasting) in Section 4.3.\nD.1    Benchmarking\nBenchmarking results from Table 2 and Table 3 were tested on a single A100 GPU.\nBenchmarks against LSSLFor a given dimensionH, a single LSSL or S4 layer was constructed withH\nhidden features.  For LSSL, the state sizeNwas set toHas done in [18].  For S4, the state sizeNwas set to\nparameter-match the LSSL, which was a state size of\nN\n4\ndue to differences in the parameterization.  Table 2\nbenchmarks a single forward+backward pass of a single layer.\n25",
    "Table 10:  Full results for the Long Range Arena (LRA) benchmark for long-range dependencies in sequence models.\n(Top):  Original Transformer variants in LRA. (Bottom):  Other models reported in the literature.\nModelListOps  Text   Retrieval  Image  Pathfinder  Path-X  Avg\nRandom10.0050.0050.0010.0050.0050.0036.67\nTransformer36.3764.2757.4642.4471.40753.66\nLocal Attention15.8252.9853.3941.4666.63746.71\nSparse Trans.17.0763.5859.5944.2471.71751.03\nLongformer35.6362.8556.8942.2269.71752.88\nLinformer35.7053.9452.2738.5676.34751.14\nReformer37.27\n56.1053.4038.0768.50750.56\nSinkhorn Trans.33.6761.2053.8341.2367.45751.23\nSynthesizer36.9961.6854.6741.6169.45752.40\nBigBird36.0564.0259.2940.8374.87754.17\nLinear Trans.16.1365.90\n53.0942.3475.30750.46\nPerformer18.0165.4053.8242.7777.05751.18\nFNet35.3365.1159.6138.6777.80754.42\nNystr ̈omformer37.1565.5279.56\n41.5870.94757.46\nLuna-25637.2564.5779.2947.3877.72759.37\nS4(original)58.3576.0287.0987.2686.0588.1080.48\nS4(updated)59.6086.8290.9088.6594.2096.3586.09\nBenchmarks  against  Efficient  TransformersFollowing  [40],  the  Transformer  models  had  4  layers,\nhidden dimension 256 with 4 heads, query/key/value projection dimension 128, and batch size 32, for a total\nof roughly 600kparameters.  The S4 model was parameter tied while keeping the depth and hidden dimension\nconstant (leading to a state size ofN= 256).\nWe note that the relative orderings of these methods can vary depending on the exact hyperparameter\nsettings.\nD.2    Long-Range Dependencies\nThis section includes information for reproducing our experiments on the Long-Range Arena and Speech\nCommands long-range dependency tasks.\nLong Range ArenaTable 10 contains extended results table with all 11 methods considered in [40].\nFor the S4 model, hyperparameters for all datasets are reported in Table 11.  For all datasets, we used the\nAdamW optimizer with a constant learning rate schedule with decay on validation plateau.  However, the\nlearning rate on HiPPO parameters (in particularΛ,P,Q,B,C,∆) were reduced to a maximum starting\nLR of 0.001, which improves stability since the HiPPO equation is crucial to performance.\nThe S4 state size was always fixed toN= 64.\nAs S4 is a sequence-to-sequence model with output shape (batch, length, dimension) and LRA tasks are\nclassification, mean pooling along the length dimension was applied after the last layer.\nWe note that most of these results were trained for far longer than what was necessary to achieve SotA results\n(e.g., theImagetask reaches SotA in 1 epoch).  Results often keep improving with longer training times.\nUpdated results.\nThe above hyperparameters describe the results reported in the original paper, shown in\nTable 10, which have since been improved.  See Appendix D.5.\nHardware.All models were run on single GPU. Some tasks used an A100 GPU (notably,  the Path-X\nexperiments), which has a larger max memory of 40Gb.  To reproduce these on smaller GPUs, the batch size\ncan be reduced or gradients can be accumulated for two batches.\n26",
    "Table 11:  The values of the best hyperparameters found for classification datasets; LRA (Top) and images/speech\n(Bottom).   LR  is  learning  rate  and  WD  is  weight  decay.   BN  and  LN  refer  to  Batch  Normalization  and  Layer\nNormalization.\nDepth    FeaturesHNorm    Pre-norm    Dropout    LRBatch Size    Epochs    WD    Patience\nListOps6128BNFalse00.01100500.015\nText464BNTrue00.001502005\nRetrieval6256BNTrue00.0026420020\nImage6512LNFalse0.20.004502000.0120\nPathfinder6256BNTrue0.10.004100200010\nPath-X6256BNTrue0.00.000532100020\nCIFAR-1061024LNFalse0.250.01502000.0120\nSpeech Commands (MFCC)4256LNFalse0.20.011005005\nSpeech Commands (Raw)6128BNTrue0.10.0120150010\nSpeech CommandsWe provide details of sweeps run for baseline methods run by us—numbers for all\nothers method are taken from Gu et al.[18].  The best hyperparameters used for S4 are included in Table 11.\nTransformer [44]For MFCC, we swept the number of model layers{2,4}, dropout{0,0.1}and learning rates\n{0.001,0.0005}.  We used 8 attention heads, model dimension 128, prenorm, positional encodings, and trained\nfor 150 epochs with a batch size of 100.  For Raw, the Transformer model’s memory usage made training\nimpossible.\nPerformer [8]For MFCC, we swept the number of model layers{2,4}, dropout{0,0.1}and learning rates\n{0.001,0.0005}.  We used 8 attention heads, model dimension 128, prenorm, positional encodings, and trained\nfor 150 epochs with a batch size of 100.  For Raw, we used a model dimension of 128, 4 attention heads,\nprenorm, and a batch size of 16.  We reduced the number of model layers to 4, so the model would fit on the\nsingle GPU. We trained for 100 epochs with a learning rate of 0.001 and no dropout.\nExpRNN [24]For MFCC, we swept hidden sizes{256,512}and learning rates{0.001,0.002,0.0005}.  Training\nwas run for 200 epochs, with a single layer model using a batch size of 100.  For Raw, we swept hidden sizes\n{32,64}and learning rates{0.001,0.0005}(however, ExpRNN failed to learn).\nLipschitzRNN [13]For MFCC, we swept hidden sizes{256,512}and learning rates{0.001,0.002,0.0005}.\nTraining was run for 150 epochs, with a single layer model using a batch size of 100.  For Raw, we found that\nLipschitzRNN was too slow to train on a single GPU (requiring a full day for 1 epoch of training alone).\nWaveGAN Discriminator [11]The WaveGAN-D in Table 5 is actually our improved version of the discriminator\nnetwork from the recent WaveGAN model for speech [11].  This CNN actually did not work well out-of-the-box,\nand we added several features to help it perform better.  The final model is highly specialized compared to\nour model, and includes:\n•\nDownsampling or pooling between layers, induced by strided convolutions, that decrease the sequence\nlength between layers.\n•\nA global fully-connected output layer; thus the model only works for one input sequence length and\ndoes not work on MFCC features or the frequency-shift setting in Table 5.\n•Batch Normalization is essential, whereas S4 works equally well with either Batch Normalization or\nLayer Normalization.\n•Almost 90×as many parameters as the S4 model (26.3M vs.  0.3M).\nD.3    General Sequence Modeling\nThis  subsection  corresponds  to  the  experiments  in  Section  4.3.   Because  of  the  number  of  experiments\nin  this  section,  we  use  subsubsection  dividers  for  different  tasks  to  make  it  easier  to  follow:  CIFAR-10\ndensity estimation (Appendix D.3.1), WikiText-103 language modeling (Appendix D.3.2), autoregressive\n27",
    "generation (Appendix D.3.3), sequential image classification (Appendix D.3.4), and time-series forecasting\n(Appendix D.3.5).\nD.3.1    CIFAR Density Estimation\nThis task used a different backbone than the rest of our experiments.  We used blocks of alternating S4 layers\nand position-wise feed-forward layers (in the style of Transformer blocks).  Each feed-forward intermediate\ndimension was set to 2×the hidden size of the incoming S4 layer.  Similar to Salimans et al.[39], we used a\nUNet-style backbone consisting ofBidentical blocks followed by a downsampling layer.  The downsampling\nrates were 3,4,4 (the 3 chosen because the sequence consists of RGB pixels).  The base model hadB= 8\nwith starting hidden dimension 128, while the large model hadB= 16 with starting hidden dimension 192.\nWe experimented with both the mixture of logistics from [39] as well as a simpler 256-way categorical loss.  We\nfound they were pretty close and ended up using the simpler softmax loss along with using input embeddings.\nWe used the LAMB optimizer with learning rate 0.005.  The base model had no dropout, while the large\nmodel had dropout 0.1 before the linear layers inside the S4 and FF blocks.\nD.3.2    WikiText-103 Language Modeling\nThe RNN baselines included in Table 8 are the AWD-QRNN [27], an efficient linear gated RNN, and the\nLSTM + Cache + Hebbian + MbPA [33],  the best performing pure RNN in the literature.  The CNN\nbaselines are the CNN with GLU activations [9], the TrellisNet [4], Dynamic Convolutions [49], and TaLK\nConvolutions [26].\nThe Transformer baseline is [2], which uses Adaptive Inputs with a tied Adaptive Softmax.  This model is a\nstandard high-performing Transformer baseline on this benchmark, used for example by Lioutas and Guo\n[26] and many more.\nOur S4 model uses the same Transformer backbone as in [2].  The model consists of 16 blocks of S4 layers\nalternated with position-wise feedforward layers, with a feature dimension of 1024.  Because our S4 layer\nhas around 1/4 the number of parameters as a self-attention layer with the same dimension, we made two\nmodifications to match the parameter count better:  (i) we used a GLU activation after the S4 linear layer\n(Section 3.4) (ii) we used two S4 layers per block.  Blocks use Layer Normalization in the pre-norm position.\nThe embedding and softmax layers were the Adaptive Embedding from [2] with standard cutoffs 20000, 40000,\n200000.\nEvaluation was performed similarly to the basic setting in [2], Table 5, which uses sliding non-overlapping\nwindows.  Other settings are reported in [2] that include more context at training and evaluation time and\nimproves the score.  Because such evaluation protocols are orthogonal to the basic model, we do not consider\nthem and report the base score from [2] Table 5.\nInstead of SGD+Momentum with multiple cosine learning rate annealing cycles, our S4 model was trained\nwith the simpler AdamW optimizer with a single cosine learning rate cycle with a maximum of 800000 steps.\nThe initial learning rate was set to 0.0005.  We used 8 A100 GPUs with a batch size of 1 per gpu and context\nsize 8192.  We used no gradient clipping and a weight decay of 0.1.  Unlike [2] which specified different dropout\nrates for different parameters, we used a constant dropout rate of 0.25 throughout the network, including\nbefore every linear layer and on the residual branches.\nD.3.3    Autoregressive Generation Speed\nProtocol.\nTo account for different model sizes and memory requirements for each method, we benchmark\ngeneration speed by throughput, measured in images per second (Table 7) or tokens per second (Table 8).\nEach model generates images on a singleA100 GPU, maximizing batch size to fit in memory.  (For CIFAR-10\ngeneration we limited memory to 16Gb, to be more comparable to the Transformer and Linear Transformer\nresults reported from [22].)\n28",
    "Table 12:  (Pixel-level image classification.)  Citations refer to the original model; additional citation indicates\nwork from which this baseline is reported.\nModelsMNIST  pMNIST  sCIFAR\nTransformer [42, 44]98.997.962.2\nCKConv [35]99.3298.5463.74\nTrellisNet [4]99.2098.1373.42\nTCN [3]99.097.2-\nLSTM [17, 21]98.995.1163.01\nr-LSTM  [42]98.495.272.2\nDilated GRU [5]99.094.6-\nDilated RNN [5]98.096.1-\nIndRNN [25]99.096.0-\nexpRNN [24]98.796.6-\nUR-LSTM99.2896.9671.00\nUR-GRU [17]99.2796.5174.4\nLMU [45]-97.15-\nHiPPO-RNN [16]98.998.361.1\nUNIcoRNN [38]-98.4-\nLMUFFT [7]-98.49-\nLipschitzRNN [13]99.4\n96.364.2\nS499.6398.7091.13\nBaselines.The Transformer and Linear Transformer baselines reported in Table 7 are the results reported\ndirectly from Katharopoulos et al.[22].  Note that the Transformer number is the one in their Appendix,\nwhich implements the optimized cached implementation of self-attention.\nFor all other baseline models, we used open source implementations of the models to benchmark generation\nspeed.  For the PixelCNN++, we used the fast cached version by Ramachandran et al.[34], which sped\nup generation by orders of magnitude from the naive implementation.  This code was only available in\nTensorFlow, which may have slight differences compared to the rest of the baselines which were implemented\nin PyTorch.\nWe were unable to run the Sparse Transformer [6] model due to issues with their custom CUDA implementation\nof the sparse attention kernel, which we were unable to resolve.\nThe Transformer baseline from Table 8 was run using a modified GPT-2 backbone from the HuggingFace\nrepository,  configured  to  recreate  the  architecture  reported  in  [2].   These  numbers  are  actually  slightly\nfavorable to the baseline, as we did not include the timing of the embedding or softmax layers, whereas the\nnumber reported for S4 is the full model.\nD.3.4    Pixel-Level Sequential Image Classification\nOur models were trained with the AdamW optimizer for up to 200 epochs.  Hyperparameters for the CIFAR-10\nmodel is reported in Table 11.\nFor our comparisons against ResNet-18, the main differences between the base models are that S4 uses\nLayerNorm by default while ResNet uses BatchNorm.  The last ablation in Section 4.3 swaps the normalization\ntype,  using  BatchNorm  for  S4  and  LayerNorm  for  ResNet,  to  ablate  this  architectural  difference.   The\nexperiments with augmentation take the base model and train with mild data augmentation:  horizontal flips\nand random crops (with symmetric padding).\n29",
    "Context\nForecast\nS4\nS4\nS4\nContextForecast\nFigure 5:  Comparison of S4 and specialized time-series models for forecasting tasks.  (Top Left) The forecasting task\ninvolves predicting future values of a time-series given past context.  (Bottom Left) We perform simple forecasting\nusing a sequence model such as S4 as a black box.  (Right) Informer uses an encoder-decoder architecture designed\nspecifically for forecasting problems involving a customized attention module (figure taken from Zhou et al. [50]).\nD.3.5    Time Series Forecasting compared to Informer\nWe include a simple figure (Fig. 5) contrasting the architecture of S4 against that of the Informer [50].\nIn Fig. 5, the goal is to forecast a contiguous range of future predictions (Green, lengthF) given a range of\npast context (Blue, lengthC).  We simply concatenate the entire context with a sequence of masks set to the\nlength of the forecast window.  This input is a single sequence of lengthC+Fthat is run through the same\nsimple deep S4 model used throughout this work, which maps to an output of lengthC+F.  We then use\njust the lastFoutputs as the forecasted predictions.\nTables 13 and 14 contain full results on all 50 settings considered by Zhou et al.[50].  S4 sets the best results\non 40 out of 50 of these settings.\nD.4    Visualizations\nWe visualize the convolutional filter\n ̄\nKlearned by S4 for the Pathfinder and CIFAR-10 tasks in Appendix D.4.\nD.5    Reproduction\nSince the first version of this paper, several experiments have been updated.  Please read the corresponding\nparagraph below before citing LRA or SC results.\nLong Range Arena\nFollow-ups to this paper expanded the theoretical understanding of S4 while improving\nsome results.  The results reported in Table 4 have been updated to results from the papers [19,20].  More\nspecifically, the method S4-LegS in those works refers to thesame modelpresented in this paper, with the\n30",
    "MethodsS4InformerInformer\n†\nLogTransReformerLSTMaDeepARARIMAProphet\nMetricMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAEMSE  MAE\nETTh\n1\n240.061  0.1910.098  0.2470.092  0.2460.103  0.2590.222  0.3890.114  0.2720.107  0.2800.108  0.2840.115  0.275\n480.079  0.2200.158  0.3190.161  0.3220.167  0.3280.284  0.4450.193  0.3580.162  0.3270.175  0.4240.168  0.330\n1680.104  0.2580.183  0.3460.187  0.3550.207  0.3751.522  1.1910.236  0.3920.239  0.4220.396  0.5041.224  0.763\n3360.080  0.2290.222  0.3870.215  0.3690.230  0.3981.860  1.1240.590  0.6980.445  0.5520.468  0.5931.549  1.820\n7200.116  0.2710.269  0.4350.257  0.4210.273  0.4632.112  1.4360.683  0.7680.658  0.7070.659  0.7662.735  3.253\nETTh\n2\n240.095  0.2340.093  0.2400.099  0.2410.102  0.2550.263  0.4370.155  0.3070.098  0.2633.554  0.4450.199  0.381\n480.191  0.3460.155  0.3140.159  0.3170.169  0.3480.458  0.5450.190  0.3480.163  0.3413.190  0.4740.304  0.462\n1680.167  0.3330.232  0.3890.235  0.3900.246  0.4221.029  0.8790.385  0.5140.255  0.4142.800  0.5952.145  1.068\n3360.189  0.3610.263  0.4170.258  0.4230.267  0.4371.668  1.2280.558  0.6060.604  0.6072.753  0.7382.096  2.543\n7200.187  0.3580.277    0.4310.285  0.4420.303  0.4932.030  1.7210.640  0.6810.429  0.5802.878  1.0443.355  4.664\nETTm\n1\n240.024  0.1170.030  0.1370.034  0.1600.065  0.2020.095  0.2280.121  0.2330.091  0.2430.090  0.2060.120  0.290\n480.051  0.1740.069  0.2030.066  0.1940.078  0.2200.249  0.3900.305  0.4110.219  0.3620.179  0.3060.133  0.305\n960.086  0.2290.194  0.3720.187  0.3840.199  0.3860.920  0.7670.287  0.4200.364  0.4960.272  0.3990.194  0.396\n2880.160  0.3270.401  0.5540.409  0.5480.411  0.5721.108  1.2450.524  0.5840.948  0.7950.462  0.5580.452  0.574\n6720.292  0.4660.512  0.6440.519  0.6650.598  0.7021.793  1.5281.064  0.8732.437  1.3520.639  0.6972.747  1.174\nWeather\n240.125  0.2540.117  0.2510.119  0.2560.136  0.2790.231  0.4010.131  0.2540.128  0.2740.219  0.3550.302  0.433\n480.1810.3050.1780.3180.185  0.3160.206  0.3560.328  0.4230.190  0.3340.203  0.3530.273  0.4090.445  0.536\n1680.198  0.3330.266  0.3980.269  0.4040.309  0.4390.654  0.6340.341  0.4480.293  0.4510.503  0.5992.441  1.142\n3360.300  0.4170.297  0.4160.310  0.4220.359  0.4841.792  1.0930.456  0.5540.585  0.6440.728  0.7301.987  2.468\n7200.245  0.3750.359  0.4660.361  0.4710.388  0.4992.087  1.5340.866  0.8090.499  0.5961.062  0.9433.859  1.144\nECL\n480.2220.3500.239  0.3590.238  0.3680.280  0.4290.971  0.8840.493  0.5390.2040.3570.879  0.7640.524  0.595\n1680.3310.4210.447  0.5030.442  0.5140.454  0.5291.671  1.5870.723  0.6550.3150.4361.032  0.8332.725  1.273\n3360.328  0.4220.489  0.5280.501  0.5520.514  0.5633.528  2.1961.212  0.8980.414  0.5191.136  0.8762.246  3.077\n7200.428  0.4940.540  0.5710.543  0.5780.558  0.6094.891  4.0471.511  0.9660.563  0.5951.251  0.9334.243  1.415\n9600.432  0.4970.582  0.6080.594  0.6380.624  0.6457.019  5.1051.545  1.0060.657  0.6831.370  0.9826.901  4.264\nCount2250000200\nTable 13:  Univariate long sequence time-series forecasting results on four datasets (five cases).\n“-LegS” suffix referring to the initialization defined in equation(2).  As such, results from the original Table 4\nhave been directly updated.\nThe updated results have only minor hyperparameter changes compared to the original results.  The original\nresults and hyperparameters are shown in Table 10 (Appendix D.2).  Appendix B of [19] describes the changes\nin hyperparameters, which are also documented from the experiment configuration files in the publically\navailable code athttps://github.com/HazyResearch/state-spaces.\nSpeech CommandsThe Speech Commands (SC) dataset [47] is originally a 35-class dataset of spoken\nEnglish words.  However, this paper was part of a line of work starting with Kidger et al.[23]that has used a\nsmaller 10-class subset of SC [18,23,35,36].In an effort to avoid dataset fragmentation in the literature, we\nhave since moved to the original dataset.We are now calling this 10-class subsetSC10to distinguish it from\nthe full 35-classSCdataset.  To cite S4 as a baseline for Speech Commands, please use Table 11 from [19]\ninstead of Table 5 from this paper.  In addition to using the full SC dataset, it also provides a number of\nmuch stronger baselines than the ones used in this work.\nWikiText-103\nThe original version of this paper used an S4 model with batch size 8, context size 1024\nwhich achieved a validation perplexity of 20.88 and test perplexity of 21.28.  It was later retrained with a\nbatch size of 1 and context size 8192 which achieved a validation perplexity of 19.69 and test perplexity\nof 20.95, and a model checkpoint is available in the public repository.  The rest of the model is essentially\nidentical, so the results from the original table have been updated.\n31",
    "MethodsS4InformerInformer\n†\nLogTransReformerLSTMaLSTnet\nMetricMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAE\nETTh\n1\n240.5250.5420.5770.5490.6200.5770.6860.6040.9910.7540.6500.6241.2930.901\n480.6410.6150.6850.6250.6920.6710.7660.7571.3130.9060.7020.6751.4560.960\n1680.9800.7790.9310.7520.9470.7971.0020.8461.8241.1381.2120.8671.9971.214\n3361.4070.9101.1280.8731.0940.8131.3620.9522.1171.2801.4240.9942.6551.369\n7201.1620.8421.2150.8961.2410.9171.3971.2912.4151.5201.9601.3222.1431.380\nETTh\n2\n240.8710.7360.7200.6650.7530.7270.8280.7501.5311.6131.1430.8132.7421.457\n481.2400.8671.4571.0011.4611.0771.8061.0341.8711.7351.6711.2213.5671.687\n1682.5801.2553.4891.5153.4851.6124.0701.6814.6601.8464.1171.6743.2422.513\n3361.9801.1282.7231.3402.6261.2853.8751.7634.0281.6883.4341.5492.5442.591\n7202.6501.3403.4671.4733.5481.4953.9131.5525.3812.0153.9631.7884.6253.709\nETTm\n1\n240.4260.4870.3230.3690.3060.3710.4190.4120.7240.6070.6210.6291.9681.170\n480.5800.5650.4940.5030.4650.4700.5070.5831.0980.7771.3920.9391.9991.215\n960.6990.6490.6780.6140.6810.6120.7680.7921.4330.9451.3390.9132.7621.542\n2880.8240.6741.0560.7861.1620.8791.4621.3201.8201.0941.7401.1241.2572.076\n6720.8460.7091.1920.9261.2311.1031.6691.4612.1871.2322.7361.5551.9172.941\nWeather\n240.3340.3850.3350.3810.3490.3970.4350.4770.6550.5830.5460.5700.6150.545\n480.4060.4440.3950.4590.3860.4330.4260.4950.7290.6660.8290.6770.6600.589\n1680.5250.5270.6080.5670.6130.5820.7270.6711.3180.8551.0380.8350.7480.647\n3360.5310.5390.7020.6200.7070.6340.7540.6701.9301.1671.6571.0590.7820.683\n7200.5780.5780.8310.7310.8340.7410.8850.7732.7261.5751.5361.1090.8510.757\nECL\n480.2550.3520.3440.3930.3340.3990.3550.4181.4040.9990.4860.5720.3690.445\n1680.2830.3730.3680.4240.3530.4200.3680.4321.5151.0690.5740.6020.3940.476\n3360.2920.3820.3810.4310.3810.4390.3730.4391.6011.1040.8860.7950.4190.477\n7200.2890.3770.4060.4430.3910.4380.4090.4542.0091.1701.6761.0950.5560.565\n9600.2990.3870.4600.5480.4920.5500.4770.5892.1411.3871.5911.1280.6050.599\nCount18560000\nTable 14:  Multivariate long sequence time-series forecasting results on four datasets (five cases).\nFigure 6:  (Convolutional filters on Pathfinder) A random selection of filters learned by S4 in the first layer (top\n2 rows) and last layer (bottom 2 rows) of the best model.\n32"
  ]
}