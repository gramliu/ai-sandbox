{
  "key": "QQNU6BSI",
  "url": "http://arxiv.org/pdf/2309.17453",
  "metadata": {
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "abstract": "  Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na \"sink\" even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n",
    "published": "2023-09-29T17:59:56Z"
  },
  "text": [
    "Published as a conference paper at ICLR 2024\nEFFICIENTSTREAMINGLANGUAGEMODELS\nWITHATTENTIONSINKS\nGuangxuan Xiao\n1∗\nYuandong Tian\n2\nBeidi Chen\n3\nSong Han\n1,4\nMike Lewis\n2\n1\nMassachusetts Institute of Technology\n2\nMeta AI\n3\nCarnegie Mellon University\n4\nNVIDIA\nhttps://github.com/mit-han-lab/streaming-llm\nABSTRACT\nDeploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed but\nposes two major challenges. Firstly, during the decoding stage, caching previous\ntokens’ Key and Value states (KV) consumes extensive memory. Secondly, popular\nLLMs cannot generalize to longer texts than the training sequence length. Window\nattention, where only the most recent KVs are cached, is a natural approach — but\nwe show that it fails when the text length surpasses the cache size.  We observe\nan interesting phenomenon, namelyattention sink, that keeping the KV of initial\ntokens will largely recover the performance of window attention. In this paper, we\nfirst demonstrate that the emergence ofattention sinkis due to the strong attention\nscores towards initial tokens as a “sink” even if they are not semantically important.\nBased on the above analysis, we introduce StreamingLLM, an efficient framework\nthat enables LLMs trained with afinite lengthattention window to generalize to\ninfinite sequence lengthwithout any fine-tuning. We show that StreamingLLM can\nenable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more.  In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training can\nfurther improve streaming deployment.  In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2×speedup.\nCode and datasets are provided in the link.\n1INTRODUCTION\nLarge Language Models (LLMs) (Radford et al., 2018; Brown et al., 2020; Zhang et al., 2022;\nOpenAI, 2023; Touvron et al., 2023a;b) are becoming ubiquitous, powering many natural language\nprocessing applications such as dialog systems (Schulman et al., 2022; Taori et al., 2023; Chiang et al.,\n2023), document summarization (Goyal & Durrett, 2020; Zhang et al., 2023a), code completion (Chen\net al., 2021; Rozière et al., 2023) and question answering (Kamalloo et al., 2023). To unleash the\nfull potential of pretrained LLMs, they should be able to efficiently and accurately perform long\nsequence generation. For example, an ideal ChatBot assistant can stably work over the content of\nrecent day-long conversations.  However, it is very challenging for LLM to generalize to longer\nsequence lengths than they have been pretrained on, e.g., 4K for Llama-2 Touvron et al. (2023b).\nThe reason is that LLMs are constrained by the attention window during pre-training.  Despite\nsubstantial efforts to expand this window size (Chen et al., 2023; kaiokendev, 2023; Peng et al., 2023)\nand improve training (Dao et al., 2022; Dao, 2023) and inference (Pope et al., 2022; Xiao et al., 2023;\nAnagnostidis et al., 2023; Wang et al., 2021; Zhang et al., 2023b) efficiency for lengthy inputs, the\nacceptable sequence length remains intrinsicallyfinite, which doesn’t allow persistent deployments.\nIn this paper, we first introduce the concept of LLM streaming applications and ask the question:\nCan we deploy an LLM for infinite-length inputs without sacrificing efficiency and performance?\n∗\nPart of the work done during an internship at Meta AI.\n1\narXiv:2309.17453v4  [cs.CL]  7 Apr 2024",
    "Published as a conference paper at ICLR 2024\n(a) Dense Attention\n⋯\nT cached tokens\nCurrent Token\n(c) Sliding Window  \nw/ Re-computation\nL re-computed \ntokens\n⋯\nprevious tokens \nare truncated\nO(T\n2\n)O(TL\n2\n)\nPPL: 5641PPL: 5.43\nHas poor efficiency and \nperformance on long text.\n(b) Window Attention\n⋯\nL cached \ntokens\n⋯\nT-L evicted \ntokens\nO(TL)\nPPL: 5158\nBreaks when initial \ntokens are evicted.\nHas to re-compute cache \nfor each incoming token.\n(d) StreamingLLM (ours)\nAttention Sink\n⋯\nL cached \ntokens\n⋯\nevicted \ntokens\nO(TL)\nPPL: 5.40\nCan perform efficient and stable \nlanguage modeling on long texts.\nFigure 1:Illustration of StreamingLLMvs. existing methods.The language model, pre-trained on texts of\nlengthL, predicts theTth token (T≫L). (a) Dense Attention hasO(T\n2\n)time complexity and an increasing\ncache size. Its performance decreases when the text length exceeds the pre-training text length. (b) Window\nAttention caches the most recentLtokens’ KV. While efficient in inference, performance declines sharply once\nthe starting tokens’ keys and values are evicted. (c) Sliding Window with Re-computation rebuilds the KV states\nfrom theLrecent tokens for each new token.  While it performs well on long texts, itsO(T L\n2\n)complexity,\nstemming from quadratic attention in context re-computation, makes it considerably slow. (d) StreamingLLM\nkeeps theattention sink(several initial tokens) for stable attention computation, combined with the recent tokens.\nIt’s efficient and offers stable performance on extended texts. Perplexities are measured using the Llama-2-13B\nmodel on the first book (65K tokens) in the PG-19 test set.\nWhen applying LLMs for infinite input streams, two primary challenges arise:\n1.During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV)\nof all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory\nusage and increasing decoding latency (Pope et al., 2022).\n2.\nExisting  models  have  limited  length  extrapolation  abilities,  i.e.,  their  performance  de-\ngrades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the\nattention window size set during pre-training.\nAn intuitive approach, known as window attention (Beltagy et al., 2020) (Figure 1 b), maintains only\na fixed-size sliding window on the KV states of most recent tokens. Although it ensures constant\nmemory usage and decoding speed after the cache is initially filled, the model collapses once the\nsequence length exceeds the cache size, i.e.,even just evicting the KV of the first token, as illustrated\nin Figure 3. Another strategy is the sliding window with re-computation (shown in Figure 1 c), which\nrebuilds the KV states of recent tokens for each generated token. While it offers strong performance,\nthis approach is significantly slower due to the computation of quadratic attention within its window,\nmaking this method impractical for real-world streaming applications.\nTo understand the failure of window attention, we find an interesting phenomenon of autoregressive\nLLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective\nof their relevance to the language modeling task, as visualized in Figure 2.  We term these tokens\n“attention sinks\". Despite their lack of semantic significance, they collect significant attention scores.\nWe attribute the reason to the Softmax operation, which requires attention scores to sum up to one\nfor all contextual tokens. Thus, even when the current query does not have a strong match in many\nprevious tokens, the model still needs to allocate these unneeded attention values somewhere so it\nsums up to one. The reason behindinitialtokens as sink tokens is intuitive: initial tokens are visible\nto almost all subsequent tokens because of the autoregressive language modeling nature, making\nthem more readily trained to serve as attention sinks.\nBased on the above insights, we propose StreamingLLM, a simple and efficient framework that\nenables LLMs trained with a finite attention window to work on text of infinite length without fine-\ntuning. StreamingLLM exploits the fact that attention sinks have high attention values, and preserving\nthem can maintain the attention score distribution close to normal. Therefore, StreamingLLM simply\nkeeps the attention sink tokens’ KV (with just 4 initial tokens sufficing) together with the sliding\nwindow’s KV to anchor the attention computation and stabilize the model’s performance.  With\nStreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia-\n[2.9,6.9,12]B can reliably model 4 million tokens, and potentially even more. Compared with the only\nviable baseline, sliding window with recomputation, StreamingLLM achieves up to 22.2×speedup,\nrealizing the streaming use of LLMs.\n2",
    "Published as a conference paper at ICLR 2024\nLayer 0 Head 0Layer 1 Head 0Layer 2 Head 0\nLayer 23 Head 0Layer 31 Head 0\nLayer 16 Head 0Layer 9 Head 0\nFigure 2:Visualization of theaverageattention logits in Llama-2-7B over 256 sentences, each with a length\nof 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1) exhibit the \"local\"\npattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends\nto the initial token across all layers and heads.\nFurthermore, we confirm our attention sink hypothesis and demonstrate that language models can\nbe pre-trained to require only a single attention sink token for streaming deployment. Specifically,\nwe suggest that an extra learnable token at the beginning of all training samples can serve as a\ndesignated attention sink. By pre-training 160-million parameter language models from scratch, we\ndemonstrate that adding this single sink token preserves the model’s performance in streaming cases.\nThis stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial\ntokens as attention sinks to achieve the same performance level.\nFinally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within\nthe KV cache without extending the LLMs’ context length. It suits continuous operation needs with\nminimal memory use and past data reliance. Additionally, StreamingLLM can complement context\nextension methods to increase the attendable recent context.\n2RELATEDWORK\nExtensive research has been done on applying LLMs to lengthy texts, with three main areas of focus:\nLength Extrapolation,Context Window Extension, andImproving LLMs’ Utilization of Long\nText. While seemingly related, it’s worth noting that progress in one direction doesn’t necessarily\nlead to progress in the other. For example, extending the context size of LLMs doesn’t improve the\nmodel’s performance beyond the context size, and neither approach ensures effective use of the long\ncontext. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied\nto text significantly exceeding the pre-training window size, potentially even of infinite length. We do\nnot expand the attention window size of LLMs or enhance the model’s memory and usage on long\ntexts. The last two categories are orthogonal to our focus and could be integrated with our techniques.\nLength extrapolationaims to enable language models trained on shorter texts to handle longer\nones during testing. A predominant avenue of research targets the development of relative position\nencoding methods for Transformer models, enabling them to function beyond their training window.\nOne such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021),  which transforms\nthe queries and keys in every attention layer for relative position integration.  Despite its promise,\nsubsequent research (Press et al., 2022; Chen et al., 2023) indicated its underperformance on text\nthat exceeds the training window. Another approach, ALiBi (Press et al., 2022), biases the query-key\nattention scores based on their distance, thereby introducing relative positional information. While\nthis exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the\ntext length was vastly greater than the training length. Current methodologies, however, have yet to\nachieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications.\nContext Window Extensioncenters on expanding the LLMs’ context window, enabling the process-\ning of more tokens in one forward pass.  A primary line of work addresses the training efficiency\nproblem.  Given the attention to computation’s quadratic complexity during training, developing\na long-context LLM is both a computational and memory challenge.  Solutions have ranged from\nsystem-focused optimizations like FlashAttention (Dao et al., 2022; Dao, 2023), which accelerates\nattention computation and reduces memory footprint, to approximate attention methods (Zaheer\net al., 2020b; Beltagy et al., 2020; Wang et al., 2020; Kitaev et al., 2020) that trade model quality for\nefficiency. Recently, there has been a surge of work on extending pre-trained LLMs with RoPE (Chen\net al., 2023; kaiokendev, 2023; bloc97, 2023; Peng et al., 2023), involving position interpolation and\nfine-tuning. However, all the aforementioned techniques only extend LLMs’ context window to a\nlimited extent, which falls short of our paper’s primary concern of handling limitless inputs.\n3",
    "Published as a conference paper at ICLR 2024\nDense AttentionWindow Attention\nSliding Window \n\nw/ Re-computation\nStreamingLLM\nDense AttentionWindow AttentionStreamingLLM\nFigure 3:Language modeling perplexity on texts with 20K tokens across various LLM. Observations reveal\nconsistent trends: (1) Dense attention fails once the input length surpasses the pre-training attention window\nsize.  (2) Window attention collapses once the input length exceeds the cache size, i.e., the initial tokens are\nevicted. (3) StreamingLLM demonstrates stable performance, with its perplexity nearly matching that of the\nsliding window with re-computation baseline.\nImproving LLMs’ Utilization of Long Textoptimizes LLMs to better capture and employ the\ncontent within the context rather than merely taking them as inputs.  As highlighted by Liu et al.\nand Li et al., success in the previously mentioned two directions does not necessarily translate to\ncompetent utilization of lengthy contexts.  Addressing this effective usage of prolonged contexts\nwithin LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,\nenabling the seamless streaming application of LLMs.\n3STREAMINGLLM\n3.1THEFAILURE OFWINDOWATTENTION  ANDATTENTIONSINKS\nWhile the window attention technique offers efficiency during inference, it results in an exceedingly\nhigh language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-\nment in streaming applications. In this section, we use the concept ofattention sinkto explain the\nfailure of window attention, serving as the inspiration behind StreamingLLM.\nIdentifying the Point of Perplexity Surge.Figure 3 shows the perplexity of language modeling on\na 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led\nby the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance\nfrom the predicted tokens, are crucial for maintaining the stability of LLMs.\nWhy do LLMs break when removinginitialtokens’ KV?We visualize attention maps from\nall layers and heads of the Llama-2-7B and models in Figure 2.  We find that, beyond the bottom\ntwo layers, the model consistently focuses on the initial tokens across all layers and heads.  The\nimplication is clear: removing these initial tokens’ KV will remove a considerable portion of the\ndenominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a\nsignificant shift in the distribution of attention scores away from what would be expected in normal\ninference settings.\nSoftMax(x)\ni\n=\ne\nx\ni\ne\nx\n1\n+\nP\nN\nj=2\ne\nx\nj\n,   x\n1\n≫x\nj\n, j∈2, . . . , N(1)\nThere are two possible explanations for the importance of the initial tokens in language modeling:\n(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.\nTo distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four\ntokens are substituted with the linebreak token “\\n\". The observations indicate that the model still\nsignificantly emphasizes these initial linebreak tokens.  Furthermore, reintroducing them restores\nthe language modeling perplexity to levels comparable to having the original initial tokens.  This\nsuggests that the absolute position of the starting tokens, rather than their semantic value, holds\ngreater significance.\nLLMs attend to Initial Tokens as Attention Sinks.To explain why the model disproportionately\nfocuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce\nthe concept of “attention sink\". The nature of the SoftMax function (Equation 1) prevents all attended\ntokens from having zero values. This requires aggregating some information from other tokens across\nall heads in all layers, even if the current embedding has sufficient self-contained information for its\nprediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A\nsimilar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko\net al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.\n4",
    "Published as a conference paper at ICLR 2024\nTable  1:Window  attention  has  poor  per-\nformance  on  long  text.The  perplexity\nis  restored  when  we  reintroduce  the  initial\nfour  tokens  alongside  the  recent  1020  to-\nkens  (4+1020).Substituting  the  original\nfour initial tokens with linebreak tokens “\\n\"\n(4\"\\n\"+1020) achieves comparable perplexity\nrestoration. Cache config x+y denotes adding\nx initial tokens with y recent tokens. Perplex-\nities are measured on the first book (65K to-\nkens) in the PG19 test set.\nLlama-2-13BPPL (↓)\n0 + 1024 (Window)  5158.07\n4 + 10205.40\n4\"\\n\"+10205.60\nTable  2:Effects  of  reintroduced  initial  token  numbers  on\nStreamingLLM. (1) Window attention (0+y) has a drastic in-\ncrease in perplexity.  (2) Introducing one or two initial tokens\ndoesn’t fully restore model perplexity, showing that the model\ndoesn’t solely use the first token as the attention sink. (3) Intro-\nducing four initial tokens generally suffices; further additions\nhave diminishing returns. Cache config x+y denotes adding x\ninitial tokens to y recent tokens. Perplexities are evaluated on\n400K tokens in the concatenated PG19 test set.\nCache Config  0+2048  1+2047  2+2046  4+2044  8+2040\nFalcon-7B17.9012.1212.1212.1212.12\nMPT-7B460.2914.9915.0014.9914.98\nPythia-12B21.6211.9512.0912.0912.02\nCache Config  0+4096  1+4095  2+4094  4+4092  8+4088\nLlama-2-7B3359.95   11.8810.519.599.54\nWhy do various autoregressive LLMs, such as Llama-2, MPT, Falcon, and Pythia, consistently focus\noninitial tokensas their attention sinks, rather than other tokens? Our explanation is straightforward:\nDue to the sequential nature of autoregressive language modeling, initial tokens are visible to all\nsubsequent tokens, while later tokens are only visible to a limited set of subsequent tokens. As a result,\ninitial tokens are more easily trained to serve as attention sinks, capturing unnecessary attention.\nWe’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather\nthan just one.  As illustrated in Figure 2, the introduction of four initial tokens, as attention sinks,\nsuffices to restore the LLM’s performance. In contrast, adding just one or two doesn’t achieve full\nrecovery. We believe this pattern emerges because these models didn’t include a consistent starting\ntoken across all input samples during pre-training. Although Llama-2 does prefix each paragraph\nwith a “<s>\" token, it’s applied before text chunking, resulting in a mostly random token occupying\nthe zeroth position. This lack of a uniform starting token leads the model to use several initial tokens\nas attention sinks. We hypothesize that by incorporating a stable learnable token at the start of all\ntraining samples, it could singularly act as a committed attention sink, eliminating the need for\nmultiple initial tokens to ensure consistent streaming. We will validate this hypothesis in Section 3.3.\n3.2ROLLINGKV CACHE WITHATTENTIONSINKS\n01234567\nGenerating \nToken 13\nGenerating \nToken 14\nGenerating \nToken 15\nAttention Sinks\nRolling KV Cache\n01231011121314\n0123101112131415\nEvicted Tokens\n01234567\nGenerating \nToken 7\n012345678\nGenerating \nToken 8\n0123456789\nEvicted TokensRolling KV Cache\nGenerating \nToken 9\nFigure 4:The KV cache of StreamingLLM.\nTo enable LLM streaming in already trained LLMs, we\npropose a straightforward method that can recover win-\ndow attention’s perplexity without any model finetuning.\nAlongside the current sliding window tokens, we reintro-\nduce a few starting tokens’ KV in the attention computa-\ntion.  The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated\nin Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling\nKV Cache retains the most recent tokens, crucial for language modeling. StreamingLLM’ design is\nversatile and can be seamlessly incorporated into any autoregressive language model that employs\nrelative positional encoding, such as RoPE (Su et al., 2021) and ALiBi (Press et al., 2022).\nWhen determining the relative distance and adding positional information to tokens, StreamingLLM\nfocuses on positionswithin the cacherather than thosein the original text. This distinction is crucial\nfor StreamingLLM’s performance. For instance, if the current cache (Figure 4) has tokens [0, 1, 2, 3,\n6, 7, 8] and is in the process of decoding the 9th token, the positions assigned are [0, 1, 2, 3, 4, 5, 6,\n7], rather than the positions in the original text, which would be [0, 1, 2, 3, 6, 7, 8, 9].\nFor encoding like RoPE, we cache the Keys of tokensprior tointroducing the rotary transformation.\nThen, we apply position transformation to the keys in the rolling cache at each decoding phase. On the\nother hand, integrating with ALiBi is more direct. Here, the contiguous linear bias is applied instead\nof a ’jumping’ bias to the attention scores. This method of assigning positional embedding within the\ncache is crucial to StreamingLLM’s functionality, ensuring that the model operates efficiently even\nbeyond its pre-training attention window size.\n5",
    "Published as a conference paper at ICLR 2024\n3.3PRE-TRAININGLLMS WITHATTENTIONSINKS\nTable 3:Comparison of vanilla attention with prepend-\ning a zero token and a learnable sink token during pre-\ntraining.   To  ensure  stable  streaming  perplexity,  the\nvanilla model requires several initial tokens. While Zero\nSink shows a slight improvement, it still needs other ini-\ntial tokens. Conversely, the model trained with a learn-\nable Sink Token shows stable streaming perplexity with\nonly the sink token added.  Cache configx+ydenotes\naddingxinitial tokens withyrecent tokens. Perplexity\nis evaluated on the first sample in the PG19 test set.\nCache Config0+1024  1+1023  2+1022  4+1020\nVanilla27.8718.4918.0518.05\nZero Sink2921419.9018.2718.01\nLearnable Sink    123518.0118.0118.02\nAs elaborated in Section 3.1, a significant rea-\nson for the model’s excessive attention to multi-\nple initial tokens is the absence of a designated\nsink token to offload excessive attention scores.\nDue to this, the model inadvertently uses glob-\nally visible tokens,  primarily the initial ones,\nas attention sinks.  A potential remedy can be\nthe intentional inclusion of a global trainable\nattention sink token, denoted as a “Sink Token”,\nwhich would serve as a repository for unneces-\nsary attention scores.  Alternatively, replacing\nthe conventional SoftMax function with a vari-\nant like SoftMax-off-by-One (Miller, 2023),\nSoftMax\n1\n(x)\ni\n=\ne\nx\ni\n1 +\nP\nN\nj=1\ne\nx\nj\n,(2)\nwhich does not require the attention scores on all contextual tokens to sum up to one, may also be\neffective. Note thatSoftMax\n1\nis equivalent to prepending a token with an all-zero Key and Value\nfeatures in the attention computation. We denote this method as “Zero Sink” to fit our framework.\nFor validation, we pre-train three language models with 160 million parameters from scratch under\nidentical settings.  The first model utilizes the standard SoftMax attention (Vanilla), the second\nreplaced the regular attention mechanism withSoftMax\n1\n(Zero Sink), and one prepending a learnable\nplaceholder token (Sink Token) in all training samples.  As shown in Table 3, while the zero sink\nalleviates the attention sink problem to some extent, the model still relies on other initial tokens as\nattention sinks. Introducing a sink token is highly effective in stabilizing the attention mechanism.\nSimply pairing this sink token with recent tokens sufficiently anchors the model’s performance, and\nthe resulting evaluation perplexity is even marginally improved. Given these findings, we recommend\ntraining future LLMs with a sink token in all samples to optimize streaming deployment.\n4EXPERIMENTS\nWe evaluate StreamingLLM using four prominent recent model families: Llama-2 (Touvron et al.,\n2023b), MPT (Team, 2023), PyThia (Biderman et al., 2023), and Falcon (Almazrouei et al., 2023).\nNotably, Llama-2, Falcon, and Pythia incorporate RoPE (Su et al., 2021), whereas MPT employs\nALiBi (Press et al., 2022) — two of the most influential position encoding techniques in recent\nresearch. Our diverse model selection ensures the validity and robustness of our findings. We bench-\nmark StreamingLLM against established baselines such as dense attention, window attention, and the\nsliding window approach with re-computation. In all subsequent experiments with StreamingLLM,\nwe default to using four initial tokens as attention sinks unless stated otherwise.\n4.1LANGUAGEMODELING ONLONGTEXTSACROSSLLM FAMILIES ANDSCALES\nWe firstly evaluate StreamingLLM’s language modeling perplexity using the concatenated PG19 (Rae\net al., 2020) test set, which contains 100 long books. For Llama-2 models, the cache size is set at\n2048, while for Falcon, Pythia, and MPT models, it’s set at 1024. This is half the pre-training window\nsize chosen to enhance visualization clarity.\nFigure 3 illustrates that StreamingLLM can match the oracle baseline (sliding window with re-\ncomputation) in terms of perplexity on texts spanning 20K tokens. Meanwhile, the dense attention\ntechnique fails when the input length exceeds its pre-training window, and the window attention\ntechnique struggles when the input length surpasses the cache size, leading to the eviction of the initial\ntokens. In Figure 5, we further substantiate that StreamingLLM can reliably handle exceptionally\nextended texts, encompassing more than 4 million tokens, across a spectrum of model families and\nscales. This includes Llama-2-[7,13,70]B, Falcon-[7,40]B, Pythia-[2.8,6.9,12]B, and MPT-[7,30]B.\n6",
    "Published as a conference paper at ICLR 2024\nDense AttentionWindow Attention\nSliding Window \n\nw/ Re-computation\nStreamingLLM\nDense AttentionWindow AttentionStreamingLLM\nFigure 5:Language modeling perplexity of StreamingLLM on super long texts with 4 million tokens across\nvarious LLM families and scales. The perplexity remains stable throughout. We use the concatenated test set of\nPG19 (100 books) to perform language modeling, with perplexity fluctuations due to book transitions.\nPre-Trained without Sink TokenPre-Trained with Sink Token\nLayer 0 Head 0Layer 2 Head 0Layer 10 Head 0\nLayer 0 Head 0Layer 2 Head 0Layer 10 Head 0\nFigure 7:Visualization of average attention logits over 256 sentences, each 16 tokens long, comparing models\npre-trained without (left) and with (right) a sink token.  Both maps show the same layers and heads.  Key\nobservations:  (1) Without a sink token, models show local attention in lower layers and increased attention\nto initial tokens in deeper layers. (2) With a sink token, there is clear attention directed at it across all layers,\neffectively collecting redundant attention. (3) With the presence of the sink token, less attention is given to other\ninitial tokens, supporting the benefit of designating the sink token to enhance the streaming performance.\n4.2RESULTS OFPRE-TRAINING WITH ASINKTOKEN\nTo validate our suggestion that introducing a sink token to all pre-training samples improves stream-\ning LLMs, we trained two language models, each with 160 million parameters, under identical\nconditions. While one model adhered to the original training settings, the other incorporated a sink\ntoken at the start of every training sample.  Our experiments employed the Pythia-160M (Bider-\nman et al., 2023) codebase and followed its training recipe.  We train the models on an 8xA6000\nNVIDIA GPU server using the deduplicated Pile (Gao et al., 2020) dataset.  Apart from reducing\nthe training batch size to 256, we retained all Pythia training configurations, including learning rate\nschedules, model initialization, and dataset permutations. Both models were trained for 143,000 steps.\nFigure   6:Pre-training   loss\ncurves of models w/ and w/o sink\ntokens. Two models have a simi-\nlar convergence trend.\nTable 4:Zero-shot accuracy (in %) across 7 NLP benchmarks, including\nARC-[Challenge, Easy], HellaSwag, LAMBADA, OpenbookQA, PIQA,\nand Winogrande. The inclusion of a sink token during pre-training doesn’t\nharm the model performance.\nMethodsARC-c   ARC-e    HS    LBD   OBQA   PIQA   WG\nVanilla18.645.229.4   39.616.062.2    50.1\n+Sink Token19.6    45.6  29.8  39.9   16.6   62.6  50.8\nConvergence and Normal Model Performance.Including a sink token during pre-training has no\nnegative impact on model convergence and subsequent performance on a range of NLP benchmarks.\nAs depicted in Figure 6, models trained with a sink token exhibit similar convergence dynamics\ncompared to their vanilla counterparts. We evaluate the two models on seven diverse NLP bench-\nmarks, including ARC-[Challenge, Easy] (Clark et al., 2018), HellaSwag (Zellers et al., 2019),\nLAMBADA (Paperno et al., 2016), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020),\nand Winogrande (Sakaguchi et al., 2019). As shown in Table 4, the model pre-trained with a sink\ntoken performs similarly to that trained using the vanilla approach.\nStreaming Performance.As illustrated in Table 3,  the streaming perplexities differ between\nmodels trained using traditional methods and those augmented with a sink token.  Remarkably,\nthe vanilla model requires the addition of multiple tokens as attention sinks to maintain stable\nstreaming perplexity. In contrast, the model trained with a sink token achieves satisfactory streaming\nperformance using just the sink token.\n7",
    "Published as a conference paper at ICLR 2024\nTable 5:Accuracy (in %) on the ARC-[Easy, Challenge] datasets. Questions were concatenated and answered\nin a streaming manner to mimic a real-world chat setting.  The dense baseline fails due to Out-of-Memory\n(OOM) errors. Window attention has poor accuracy. StreamingLLM has comparable results with the one-shot\nsample-by-sample baseline. Window attention and StreamingLLM use cache sizes of 1024.\nModelLlama-2-7B-Chat  Llama-2-13B-Chat  Llama-2-70B-Chat\nDatasetArc-EArc-CArc-EArc-CArc-EArc-C\nOne-shot71.2553.1678.1663.3191.2978.50\nDenseOOM\nWindow3.581.390.250.340.120.32\nStreamingLLM  71.3455.0380.8965.6191.3780.20\nDense AttentionWindow Attention\nSliding Window \n\nw/ Re-computation\nStreamingLLM\nDense AttentionWindow AttentionStreamingLLM\nFigure 9:Performance on the StreamEval benchmark. Accuracies are averaged over 100 samples.\nBelow is a record of lines I want you to remember. \nThe REGISTER_CONTENT in line 0 is <8806> \n[omitting 9 lines...] \nThe REGISTER_CONTENT in line 10 is <24879> \n[omitting 8 lines...] \nThe REGISTER_CONTENT in line 20 is <45603> \nQuery: The REGISTER_CONTENT in line 0 is \nThe REGISTER_CONTENT in line 21 is <29189> \n[omitting 8 lines...] \nThe REGISTER_CONTENT in line 30 is <1668> \nQuery: The REGISTER_CONTENT in line 10 is \nThe REGISTER_CONTENT in line 31 is <42569> \n[omitting 8 lines...] \nThe REGISTER_CONTENT in line 40 is <34579> \nQuery: The REGISTER_CONTENT in line 20 is \n[omitting remaining 5467 lines...]\nInput Content\nDesired Output\n[“<8806>”, “<24879>”, “<45603>”, ...]\nFigure 8:The first sample in StreamEval.\nAttention Visualization.Figure  7  contrasts  attention\nmaps for models pre-trained with and without a sink token.\nThe model without the sink token, similar to Llama-2-7B\n(Figure 2), shows early-layer local attention and deeper-layer\nfocus on initial tokens.  In contrast, models trained with a\nsink token consistently concentrate on the sink across layers\nand heads, indicating an effective attention offloading mech-\nanism. This strong focus on the sink, with reduced attention\nto other initial tokens, explains the sink token’s efficacy in\nenhancing model’s streaming performance.\n4.3RESULTS ONSTREAMINGQUESTION\nANSWERING  WITHINSTRUCTION-TUNEDMODELS\nTo show StreamingLLM’s real-world applicability, we emulate multi-round question-answering using\ninstruction-tuned LLMs, commonly used in real-world scenarios.\nWe first concatenate all question-answer pairs from the ARC-[Challenge, Easy] datasets, feed the\ncontinuous stream to Llama-2-[7,13,70]B-Chat models, and assess model completions at each answer\nposition using an exact match criterion. As table 5 indicates, dense attention results in Out-of-Memory\n(OOM) errors, showing it unsuitable for this setting.  While the window attention method works\nefficiently, it exhibits low accuracy due to random outputs when the input length exceeds the cache\nsize. Conversely, StreamingLLM excels by efficiently handling the streaming format, aligning with\nthe one-shot, sample-by-sample baseline accuracy.\nHighlighting a more fitting scenario for StreamingLLM, we introduce a dataset, StreamEval, inspired\nby the LongEval (Li et al., 2023) benchmark. As depicted in Figure 8, diverging from LongEval’s\nsingle query over a long-span setup, we query the model every 10 lines of new information. Each\nquery’s answer is consistently 20 lines prior, reflecting real-world instances where questions typically\npertain to recent information. As illustrated in Figure 9, LLMs employing StreamingLLM maintain\nreasonable accuracy even as input lengths approach 120K tokens. In contrast, both dense and window\nattention fail at the pre-training text length and the KV cache size, respectively.  Additionally, we\nutilize two context-extended models, LongChat-7b-v1.5-32k (Li et al., 2023) and Llama-2-7B-32K-\nInstruct (Together, 2023), to show that StreamingLLM can complement context extension techniques.\nWithin StreamingLLM, context extension means broadening the maximum cache size of streaming\nLLMs, enabling the capture of broader local information.\n8",
    "Published as a conference paper at ICLR 2024\nLatency (ms)\n0\n400\n800\n1200\n1600\n256512102420484096\n65\n45\n35\n31\n31\n1411\n523\n223\n103\n63\nSliding Window with Re-computationStreamingLLM\nMemory (GB)\n0\n7\n14\n21\n256512102420484096\n19\n16\n14\n13\n13\n21\n16\n14\n13\n13\nLatency (ms)\n0\n750\n1500\n2250\n3000\n256512102420484096\n106\n75\n60\n52\n48\n2355\n860\n361\n169\n99\nMemory (GB)\n0\n10\n19\n29\n38\n256512102420484096\n34\n29\n27\n26\n25\n36\n29\n26\n25\n25\nLlama-2-7BLlama-2-13B\nFigure 10:Comparison of per-token decoding latency and memory usage between the sliding window approach\nwith re-computation baseline and StreamingLLM, plotted against the cache size (attention window size) on the\nX-axis. StreamingLLM delivers a remarkable speedup of up to 22.2×per token and retains a memory footprint\nsimilar to the re-computation baseline.\n4.4ABLATIONSTUDIES\nNumbers of Initial Tokens.In Table 2, we ablate the effect of adding varying numbers of initial\ntokens with recent tokens on the streaming perplexity. The results show the insufficiency of introduc-\ning merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with\nsubsequent additions contributing marginal effects. This result justifies our choice of introducing 4\ninitial tokens as attention sinks in StreamingLLM.\nCache Sizes.In  Table  6,   we  evaluate  cache  size’s  impact  on  StreamingLLM’s  perplex-\nity.Contrary  to  intuition,   increasing  the  cache  size  doesn’t  consistently  lower  the  lan-\nguage  modeling  perplexity.   This  inconsistency  shows  a  potential  limitation  where  these  mod-\nels  might  not  maximize  the  utility  of  the  entire  context  they  receive.Future  research  ef-\nforts  should  target  enhancing  these  models’  capabilities  to  utilize  extensive  contexts  better.\nTable 6:Effects of cache size on StreamingLLM’s\nperformance.Increasing   the   cache   size   in\nStreamingLLM  doesn’t  consistently  yield  a  de-\ncrease in perplexity, showing these models may\nnot fully utilize the provided context. Cache config\nx+ydenotes addingxinitial tokens withyrecent\ntokens. Perplexity is evaluated on 400K tokens in\nthe concatenated PG19 test set.\nCache4+252   4+508   4+1020  4+2044\nFalcon-7B13.6112.8412.3412.84\nMPT-7B14.1214.2514.3314.99\nPythia-12B    13.1712.5212.0812.09\nCache4+508  4+1020  4+2044  4+4092\nLlama-2-7B   9.739.329.089.59\n4.5EFFICENCYRESULTS\nWe benchmark StreamingLLM’s decoding latency\nand memory usage against the sliding window with\nre-computation, which is the only baseline with ac-\nceptable  quality.   Both  methods  are  implemented\nusing the Huggingface Transformers library (Wolf\net al., 2020) and tested on a single NVIDIA A6000\nGPU using the Llama-2-7B and Llama-2-13B models.\nAs shown in Figure 10, as the cache size increases,\nStreamingLLM’s decoding speed has a linear growth.\nThe sliding window with re-computation baseline\nhas  a  quadratic  rise  in  decoding  latency.Thus,\nStreamingLLM  achieves  an  impressive  speedup,\nreaching up to 22.2×per token. Despite its reduced\nlatency, StreamingLLM sustains a memory footprint\nconsistent with the re-computation baseline.\n5CONCLUSION\nDeploying LLMs in streaming applications is urgently needed but comes with challenges due to\nefficiency limitations and reduced performance with longer texts. Window attention provides a partial\nsolution, but its performance plummets when initial tokens are excluded. Recognizing the role of\nthese tokens as “attention sinks\", we introduced StreamingLLM —a simple and efficient framework\nthat enables LLMs to handle unlimited texts without fine-tuning.  By adding attention sinks with\nrecent tokens, StreamingLLM can efficiently model texts of up to 4 million tokens.  We further\nshow that pre-training models with a dedicated sink token can improve the streaming performance.\nStreamingLLM firstly decouples the LLM’s pre-training window size and its actual text generation\nlength, paving the way for the streaming deployment of LLMs.\n9",
    "Published as a conference paper at ICLR 2024\nREPRODUCIBILITYSTATEMENT\nAll findings presented in this paper can be reproduced. We have made our code and datasets available\nin this github repo. The models used in this paper are all openly available, and we provide references\nto access them. Details regarding our experiments, including hyperparameters, training protocols,\nand evaluation methods, can be found in the Experiments section (Section 4). We are confident that\nwith the provided resources, readers can reproduce the entirety of our presented results.\nIMPACTSTATEMENT\nStreamingLLM has been widely adopted by various LLM serving solutions including NVIDIA\nTensorRT-LLM, Intel Extension for Transformers, HuggingFace Transformers, MLC LLM, etc.\nACKNOWLEDGEMENTS\nThis work is supported by MIT-IBM Watson AI Lab, Amazon and MIT Science Hub, National\nScience Foundation. We thank Angela Li for writing suggestions and demo making, Jingwei Zuo for\nproofreading, and Xiuyu Li for the suggestion on notations.\nREFERENCES\nJoshua Ainslie,  Santiago Ontanon,  Chris Alberti,  Vaclav Cvicek,  Zachary Fisher,  Philip Pham,\nAnirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang.  Etc: Encoding long and structured\ninputs in transformers, 2020.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\njocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language\nmodel with state-of-the-art performance. 2023.\nSotiris Anagnostidis,  Dario Pavllo,  Luca Biggio,  Lorenzo Noci,  Aurelien Lucchi,  and Thomas\nHofmann.  Dynamic context pruning for efficient and interpretable autoregressive transformers,\n2023.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding.arXiv preprint arXiv:2308.14508, 2023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.  Longformer: The long-document transformer,\n2020. arXiv:2004.05150.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,\nLintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models\nacross training and scaling, 2023.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.  Piqa:  Reasoning\nabout physical commonsense in natural language. InThirty-Fourth AAAI Conference on Artificial\nIntelligence, 2020.\nbloc97.   NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size\nwithout any fine-tuning and minimal perplexity degradation., 2023. URL https://www.reddit.com/\nr/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/.\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing\noutliers by helping attention heads do nothing, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners.Advances in neural information processing systems, 33:1877–1901, 2020.\n10",
    "Published as a conference paper at ICLR 2024\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas  Tezak,  Jie  Tang,  Igor  Babuschkin,  Suchir  Balaji,  Shantanu  Jain,  William  Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code, 2021.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation, 2023. arXiv: 2306.15595.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.  Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.  URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv:1803.05457v1, 2018.\nTri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.  FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness, 2022. arXiv:2205.14135.\nTimothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need\nregisters, 2023.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of\ninformation-seeking questions and answers anchored in research papers, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. InNorth American Chapter of the Association\nfor Computational Linguistics, 2019. URL https://api.semanticscholar.org/CorpusID:52967399.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale,\n2021.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale\nmulti-document summarization dataset and abstractive hierarchical model. InProceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, 2019.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb\ndataset of diverse text for language modeling.arXiv preprint arXiv:2101.00027, 2020.\nTanya Goyal and Greg Durrett. Evaluating factuality in generation with dependency-level entailment.\nInFindings  of  the  Association  for  Computational  Linguistics:  EMNLP  2020,  Online,  2020.\nAssociation for Computational Linguistics.\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LM-Infinite: Simple\non-the-fly length generalization for large language models, 2023.\n11",
    "Published as a conference paper at ICLR 2024\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.  Constructing a multi-\nhop QA dataset for comprehensive evaluation of reasoning steps.  InProceedings of the 28th\nInternational Conference on Computational Linguistics, December 2020.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long\ndocument summarization, 2021.\nkaiokendev. Things I’m learning while training superhot., 2023. URL https://kaiokendev.github.io/\ntil#extending-context-to-8k.\nEhsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, and Davood Rafiei. Evaluating open-domain\nquestion answering in the era of large language models, 2023.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.  Reformer: The efficient transformer.  In8th\nInternational Conference on Learning Representations, ICLR 2020. OpenReview.net, April 2020.\nTomáš Ko\nˇ\nciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,\nand Edward Grefenstette. The narrativeqa reading comprehension challenge, 2017.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, , and Hao Zhang. How long can open-source llms truly promise on context length?,\nJune 2023. URL https://lmsys.org/blog/2023-06-29-longchat.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts, 2023.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.  Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. InEMNLP, 2018.\nEvan Miller. Attention is off by one, 2023. URL https://www.evanmiller.org/attention-is-off-by-one.\nhtml.\nOpenAI. Gpt-4 technical report, 2023.\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset:\nWord prediction requiring a broad discourse context. InProceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525–1534, Berlin,\nGermany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144.\nURL https://aclanthology.org/P16-1144.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models, 2023.\nReiner Pope,  Sholto Douglas,  Aakanksha Chowdhery,  Jacob Devlin,  James Bradbury,  Anselm\nLevskaya,  Jonathan Heek,  Kefan Xiao,  Shivani Agrawal,  and Jeff Dean.   Efficiently scaling\ntransformer inference.arXiv preprint arXiv:2211.05102, 2022.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\ninput length extrapolation. InInternational Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=R8sQPpGCv0.\nAlec  Radford,  Karthik  Narasimhan,  Tim  Salimans,  Ilya  Sutskever,  et  al.   Improving  language\nunderstanding by generative pre-training. 2018.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\nCompressive transformers for long-range sequence modelling.  InInternational Conference on\nLearning Representations, 2020.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade\nCopet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel\nSynnaeve. Code Llama: Open foundation models for code, 2023.\n12",
    "Published as a conference paper at ICLR 2024\nKeisuke  Sakaguchi,  Ronan  Le  Bras,  Chandra  Bhagavatula,  and  Yejin  Choi.   Winogrande:  An\nadversarial winograd schema challenge at scale.arXiv preprint arXiv:1907.10641, 2019.\nJohn Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Fe-\nlipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language\nmodels for dialogue.OpenAI blog, 2022.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding.arXiv preprint arXiv:2104.09864, 2021.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto.   Stanford alpaca:  An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.ACM\nComputing Surveys, 55(6), dec 2022. ISSN 0360-0300.\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\nTogether.  Llama-2-7b-32k-instruct — and fine-tuning for llama-2 models with together api, June\n2023. URL https://together.ai/blog/llama-2-7b-32k-instruct.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.  Llama:  Open and\nefficient foundation language models.arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023b.\nHanrui Wang, Zhekai Zhang, and Song Han.  Spatten: Efficient sparse attention architecture with\ncascade token and head pruning.HPCA, 2021.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. 2020.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush.  Huggingface’s transformers: State-of-the-art\nnatural language processing, 2020.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant:\nAccurate and efficient post-training quantization for large language models. InProceedings of the\n40th International Conference on Machine Learning, 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. InConference on Empirical Methods in Natural Language Processing (EMNLP), 2018.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon,  Philip Pham,  Anirudh Ravula,  Qifan Wang,  Li Yang,  and Amr Ahmed.   Big Bird:\nTransformers for longer sequences. InProc. of NeurIPS, volume 33, 2020a.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntañón,  Philip Pham,  Anirudh Ravula,  Qifan Wang,  Li Yang,  and Amr Ahmed.   Big bird:\nTransformers for longer sequences.  In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin (eds.),Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020.\nCurran Associates, Inc., 2020b.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence?CoRR, abs/1905.07830, 2019. URL http://arxiv.org/abs/1905.07830.\n13",
    "Published as a conference paper at ICLR 2024\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOpt: Open pre-trained transformer language models, 2022.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B.\nHashimoto. Benchmarking large language models for news summarization, 2023a.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,\nYuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. H\n2\no: Heavy-\nhitter oracle for efficient generative inference of large language models, 2023b.\n14",
    "Published as a conference paper at ICLR 2024\nADISCUSSIONS\nApplications.StreamingLLM is particularly suited for streaming applications, such as multi-round\ndialogues, where continuous operation without heavy reliance on extensive memory or historical data\nis crucial. For instance, in a daily assistant application based on LLMs, StreamingLLM enables the\nmodel to function seamlessly over extended periods. It bases its responses on recent interactions, thus\navoiding the need for frequent cache refreshes. Traditional methods might require resetting the cache\nwhen the conversation length surpasses the training length, leading to a loss of recent context, or they\nmight need to recompute key-value (KV) states from recent text history, which can be inefficient.\nLimitations.While StreamingLLM improves the efficiency of LLMs in streaming contexts, it\ndoes not extend the models’ context window or enhance their long-term memory capabilities. As\ndetailed in Section C, the model is limited to operating within the confines of its current cache.\nConsequently, StreamingLLM is not suitable for tasks that demand long-term memory and extensive\ndata dependency, such as long document question-answering (QA) and summarization. However, it\nexcels in scenarios only requiring short-term memory, like daily conversations and short document\nQA, where its strength lies in generating coherent text from recent context without the need for cache\nrefreshment.\nBroader Societal Impacts.StreamingLLM significantly enhances the efficiency and accessibility of\nLLMs, democratizing their use across various sectors. By enabling nonstop and rapid interactions\nin applications like conversational agents, StreamingLLM improves user experiences, especially in\nscenarios requiring fixed-length models. This advancement allows for more seamless and contextually\naware dialogues, potentially benefiting sectors like education, healthcare, and customer service.\nAdditionally, StreamingLLM’s efficiency in processing reduces the computational load, aligning with\nthe need for environmentally sustainable AI technologies. This aspect is crucial in making advanced\nAI tools more accessible in regions with limited technological resources.  However, the potential\nnegative impacts of StreamingLLM mirror those associated with general language models, such as\nmisinformation and biased content generation risks. It’s essential to address these risks with robust\nethical guidelines and safeguards. In summary, while StreamingLLM shares some risks common to\nlanguage models, its positive contributions towards enhancing user experience, democratizing AI\naccess, and promoting sustainability are noteworthy. These benefits underscore the importance of\nresponsible deployment and ethical use of this technology.\nBADDITIONALRELATEDWORKS\nSparse Transformers.The literature on efficient Transformer models primarily focuses on reducing\nthe computational and memory complexity of the self-attention mechanism.  A relevant line of\nwork involves sparsifying the attention matrix by restricting the field of view to fixed, predefined\npatterns, such as local windows or block patterns with fixed strides (Tay et al., 2022). Sparse Trans-\nformer (Child et al., 2019) introduces sparse factorizations of the attention matrix, reducing the\ncomputational complexity of attention toO(n\n√\nn).  LongFormer (Beltagy et al., 2020) combines\ndilated local windowed attention with task-motivated global attention. Extended Transformer Con-\nstruction (ETC) Ainslie et al. (2020) presents a novel global-local attention mechanism, incorporating\nfour types of attention patterns: global-to-global, local-to-local, local-to-global, and global-to-local.\nBuilding on ETC, BigBird (Zaheer et al., 2020a) proposes another linear complexity attention alter-\nnative, utilizing global tokens, local sliding window attentions, and random attention. However, these\nmethods have several limitations. First, Sparse Transformer and ETC require custom GPU kernels\nfor a specific block-sparse variant of matrix-matrix multiplication. Second, LongFormer, ETC, and\nBigBird all rely on a global attention pattern, which is unsuitable for autoregressive language models.\nThird, these methods are incompatible with pre-trained models, necessitating retraining from scratch.\nIn contrast, our method offers ease of implementation using standard GPU kernels and is compatible\nwith pre-trained autoregressive language models using dense attention, which are prevalent in the\nNLP community. This compatibility provides a significant advantage, allowing for the leveraging of\nexisting pre-trained models without any fine-tuning.\nConcurrent Works.Our research coincides with the work of Han et al., who conducted a theoretical\nstudy on the length generalization failure of language models, identifying three out-of-distribution\nfactors. Their approach, inspired by this analysis, involves employing a “Λ\"-shaped attention pattern\n15",
    "Published as a conference paper at ICLR 2024\nTable 7:Accuracy (in %) on StreamEval with increasing query-answer distance.  Each line in StreamEval\ncontains 23 tokens. Accuracies are averaged over 100 samples, and each sample contains 100 queries.\nLlama-2-7B-32K-InstructCache Config\nLine DistancesToken Distances4+20444+40924+81884+16380\n2046085.8084.6081.1577.65\n4092080.3583.8081.2577.50\n60138079.1582.8081.5078.50\n80184075.3077.1576.4073.80\n10023000.0061.6050.1040.50\n15034500.0068.2058.3038.45\n20046000.000.0062.7546.90\n40092000.000.000.0045.70\n600138000.000.000.0028.50\n800184000.000.000.000.00\n1000230000.000.000.000.00\nand reconfiguring position encoding distances to enhance length generalization in LLMs.  This\napproach bears a resemblance to our methodology. However, our work uncovers the “attention sink\"\nphenomenon, wherein Transformer models tend to assign high attention scores to initial tokens\nwith small semantics. This phenomenon extends beyond the scope of length generalization failure,\nindicating a more pervasive issue in Transformer models. We observe this “attention sink\" behavior\nnot only in auto-regressive language models but also in encoder Transformers such as BERT (see\nSection H), and Vision Transformers (ViTs) (Darcet et al., 2023), suggesting its broader prevalence in\nTransformer architectures. To mitigate the “attention sink\" phenomenon, we propose the introduction\nof a learnable sink token during pre-training, and we support our findings with extensive ablation\nstudies.\nIn parallel, Darcet et al. observed similar attention concentration on random background patch tokens\nin Vision Transformers, termed as \"registers.\" These registers act as repositories for global image\ninformation. Their solution, adding dedicated \"register\" tokens, aims to balance attention distribution.\nOur finding of \"attention sinks\" parallels this concept. In our paper, the “attention sinks\" are initial\ntokens that disproportionately attract attention from subsequent tokens. Introducing a dedicated sink\ntoken during pre-training prevents the model from inappropriately using content tokens as attention\nsinks, leading to more effective attention distribution. However, a key difference exists: \"registers\" in\nVision Transformers function as global information holders within intermediate layers, whereas our\n\"attention sinks\" are positioned as initial tokens in autoregressive models. This positional variance\nsuggests that the softmax function in attention computation might play a more fundamental role in\nthe emergence of attention sinks.\nCACCURACY  ONSTREAMEVAL WITHINCREASINGQUERY-ANSWERLINE\nDISTANCE\nTo assess StreamingLLM’s handling of extended inputs, we evaluated the Llama-2-7B-32K-Instruct\nmodel on StreamEval, focusing on different query-answer line distances under various cache con-\nfigurations.  In StreamEval, each line consists of 23 tokens, making the line distances equivalent\nto token distances of23×line distances.  Accuracy was calculated by averaging results over 100\nsamples, with each sample comprising 100 queries. Table 7 illustrates that StreamingLLM retains\naccuracy when the token distance between the query and answer is within the cache size. However,\naccuracy diminishes as this distance increases and eventually drops to zero when it surpasses the\ncache capacity.\nThese results demonstrate that while StreamingLLM is effective in generating coherent text based on\nrecent context, it cannot extend the context length of language models. These results also emphasize\na broader challenge in current language models: their inability to fully utilize context information\nwithin the cache, a finding that aligns with the observations made by Liu et al..\n16",
    "Published as a conference paper at ICLR 2024\nTable 8:Performance comparison of StreamingLLM against the default truncation baseline in LongBench (Bai\net al., 2023). The baseline truncates inputs to 1750 initial and 1750 final tokens. StreamingLLM 4+3496 uses 4\nattention sink tokens and 3496 recent tokens, while StreamingLLM 1750+1750 uses 1750 tokens for both initial\nand recent segments.\nLlama2-7B-chat\nSingle-Document QAMulti-Document QASummarization\nNarrativeQA   Qasper   HotpotQA   2WikiMQA   GovReport   MultiNews\nTruncation 1750+175018.719.225.432.827.325.8\nStreamingLLM 4+349611.616.921.628.223.925.5\nStreamingLLM 1750+175018.219.724.932.026.325.9\nDLONG-RANGEBENCHMARKEVALUATION\nWe evaluated StreamingLLM using the Llama-2-7B-chat model (max context length 4k) on Long-\nBench (Bai et al., 2023), which encompasses three key NLP tasks:  single-document QA (Narra-\ntiveQA (Ko\nˇ\nciský et al., 2017) and Qasper (Dasigi et al., 2021)), multi-document QA (HotpotQA (Yang\net al., 2018) and 2WikiMQA Ho et al. (2020)), and summarization (GovReport (Huang et al., 2021),\nMultiNews (Fabbri et al., 2019)). LongBench sets a default max sequence length of 3,500 tokens for\nthe Llama-2-7B-chat model, truncating from the middle to preserve beginning and end information\n(1,750 tokens each). Table 8 shows that StreamingLLM with a 4+3496 cache configuration under-\nperforms compared to the truncation baseline, likely due to the loss of crucial initial input prompt\ninformation. However, aligning the attention sink number to 1750 restores performance to the level\nof the text truncation baseline. These results corroborate the findings in Section C, demonstrating\nthat StreamingLLM’s effectiveness is contingent on the information within its cache, with in-cache\nperformance comparable to the text truncation baseline.\nELLAMA-2-7B ATTENTIONVISUALIZATION ONLONGERSEQUENCES\n(a) Layer 0 Head 0(b) Layer 6 Head 0(c) Layer 12 Head 0\n(d) Layer 18 Head 0(e) Layer 24 Head 0(f) Layer 30 Head 0\nFigure 11:Visualization of theaverageattention logits in Llama-2-7B over 256 sentences, each with a length\nof 128.\n17",
    "Published as a conference paper at ICLR 2024\nFigure 12: Visualization of attention scores (after SoftMax) on the first token across layers in Llama-\n2-7B. Attention Scores are the 4096th token’s attention towards the first token in each layer.  The\nerror bars are the standard deviation of the first token’s attention scores across different heads in one\nlayer. Results are averaged over 256 sentences, each having a length of 4096 tokens.\nFigure 2 visualizes the attention map of Llama-2-7B using short sequences (length of 16) for clarity.\nWe further visualize the attention of Llama-2-7B on longer sequences (length of 128) in Figure 11.\nWe find the observations on short sequences also hold on longer sequences, where the attention scores\nof the initial tokens are much higher than the rest of the tokens in most layers, regardless of the\ndistance between the initial tokens and the tokens in the rest of the sequence. Because the longer the\nsequence, the thinner the attention sinks’ scores are visualized on the heatmap. We further analyze\nthe attention distribution on longer sequences (length of 4096) using a different method in Section F.\nFQUATITATIVEANALYSIS OFATTENTIONSINKS INLONGINPUTS\nFigures 2 and 13 illustrate the attention sink phenomenon using short sequences for clarity. Extending\nthis analysis, Figure 12 demonstrates the distribution of attention scores (after SoftMax) towards\nthe first token in lengthy inputs (sequence length of 4096). We average attention scores across 256\nsequences, with each sequence comprising 4096 tokens.  The plotted data represent the attention\nallocated by the 4096th token to the initial token in every layer.  Notably, the attention scores for\nthe first token are significantly high, often exceeding half of the total attention, except for the two\nbottom layers. This observation empirically substantiates the preferential focus on the first token by\nthe majority of layers and heads, irrespective of other tokens’ distances within the sequence. Such\na trend underscores the critical role of the initial tokens in a sequence, as their removal has a huge\nimpact on language model performance due to a large portion of the denominator in the SoftMax\nfunction being removed.\nGLLAMA-2-70B ATTENTIONVISUALIZATION\nFigure 2 shows the attention visualization of Llama-2-7B, we further visualize the attention of\nLlama-2-70B in Figure 13.  We find the observation on Llama-2-7B also holds on Llama-2-70B,\n18",
    "Published as a conference paper at ICLR 2024\n(a) Layer 0 Head 0(b) Layer 0 Head 1(c) Layer 8 Head 0(d) Layer 8 Head 1\n(e) Layer 16 Head 0(f) Layer 16 Head 1(g) Layer 24 Head 0(h) Layer 24 Head 1\n(i) Layer 32 Head 0(j) Layer 32 Head 1(k) Layer 40 Head 0(l) Layer 40 Head 1\n(m) Layer 48 Head 0(n) Layer 48 Head 1(o) Layer 56 Head 0(p) Layer 56 Head 1\n(q) Layer 64 Head 0(r) Layer 64 Head 1(s) Layer 72 Head 0(t) Layer 72 Head 1\nFigure 13:Visualization of theaverageattention logits in Llama-2-70B over 256 sentences, each with a length\nof 16.\n19",
    "Published as a conference paper at ICLR 2024\nwhere the attention scores of the initial tokens are much higher than the rest of the tokens in most\nlayers.\nHATTENTIONSINKS INENCODERTRANSFORMERS\nFigure 14:Visualization of attention maps for sentence“StreamingLLM can work on infinite-length texts\nwithout compromising efficiency and performance.”in BERT-base-uncased.\nIn this paper, we mainly explore the attention sink phenomenon observed in autoregressive, decoder-\nonly language models like GPT and Llama. Building upon the insights from Section 3.1, we propose\nthat this phenomenon likely extends to other Transformer architectures, including encoder models\nsuch as BERT (Devlin et al., 2019) and ViT (Dosovitskiy et al., 2021).  This assumption stems\nfrom the fact that these models share a similar Transformer structure and utilize SoftMax attention\nmechanisms. To substantiate our hypothesis, we analyze the attention patterns of BERT-base-uncased,\n20",
    "Published as a conference paper at ICLR 2024\nTable 10: Comparison of vanilla attention with prepending a zero token and a learnable sink token\nduring pre-training. Cache configx+ydenotes addingxinitial tokens withyrecent tokens. Perplexity\nis evaluated on the first sample in the PG19 test set.\nCache Config0+1024   1+1023   2+1022   4+1020\nVanilla27.8718.4918.0518.05\n+ 1 Sink Token123518.0118.0118.02\n+ 2 Sink Tokens126225.7318.0518.05\nas depicted in Figure 14.  Our findings reveal that BERT-base-uncased exhibits the attention sink\nphenomenon, characterized by disproportionately high attention scores assigned to the[SEP]token\nin most layers. This indicates that the model consistently relies on the omnipresent[SEP]token as a\nfocal point for attention. Furthermore, concurrent research by Darcet et al. identifies similar attention\nspikes in Vision Transformers, attributed to random background patch tokens acting as \"registers\"\nfor global image information. We contend that these \"registers\" are analogous to the attention sink\nphenomenon we observed, suggesting that this is a universal characteristic across all Transformer\nmodels.\nIUSINGMORESINKTOKENS IN THEPRE-TRAININGSTAGE\nSection 3.3 illustrated that incorporating a single dedicated sink token in the pre-training stage doesn’t\naffect model performance but enhances streaming performance by centralizing attention sinks to\none token. This section delves into whether adding additional sink tokens during pre-training could\nfurther optimize the performance of pre-trained language models.\nAs depicted in Figure 15, our experiments show that incorporating either one or two sink tokens\nduring pre-training results in pre-training loss curves that closely resemble those of the baseline\n(vanilla) model. However, as detailed in Table 9, the introduction of a second sink token does not\nyield substantial improvements in performance across most benchmark tasks.\nFurther analysis, as shown in Table 10, reveals that the inclusion of additional sink tokens does\nnot enhance streaming performance.  Interestingly, the model appears to rely on both sink tokens\nto maintain stable streaming performance. These findings suggest that while a single sink token is\nadequate for improving streaming performance, adding more sink tokens does not lead to further\nenhancements  in  overall  language  model  performance.   This  contrasts  with  findings  in  Vision\nTransformers (ViT) (Darcet et al., 2023), where multiple \"registers\" have been found to be beneficial.\nFigure  15:Pre-training  loss\ncurves of models with 0, 1, and\n2 sink tokens.\nTable 9:  Zero-shot accuracy (in %) across 7 NLP benchmarks,\nincluding ARC-[Challenge, Easy], HellaSwag, LAMBADA, Open-\nbookQA, PIQA, and Winogrande.\nMethodsARC-c   ARC-e    HS    LBD   OBQA   PIQA   WG\nVanilla18.645.229.4   39.616.062.2    50.1\n+ 1 Sink Token19.6    45.6  29.8  39.9   16.662.650.8\n+ 2 Sink Tokens18.745.629.6   37.515.864.350.4\n21"
  ]
}