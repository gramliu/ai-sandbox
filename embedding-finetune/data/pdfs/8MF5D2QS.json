{
  "key": "8MF5D2QS",
  "url": "http://arxiv.org/pdf/2401.15884",
  "metadata": {
    "title": "Corrective Retrieval Augmented Generation",
    "abstract": "  Large language models (LLMs) inevitably exhibit hallucinations since the\naccuracy of generated texts cannot be secured solely by the parametric\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\ndocuments, raising concerns about how the model behaves if retrieval goes\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the overall quality of retrieved\ndocuments for a query, returning a confidence degree based on which different\nknowledge retrieval actions can be triggered. Since retrieval from static and\nlimited corpora can only return sub-optimal documents, large-scale web searches\nare utilized as an extension for augmenting the retrieval results. Besides, a\ndecompose-then-recompose algorithm is designed for retrieved documents to\nselectively focus on key information and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\nRAG-based approaches. Experiments on four datasets covering short- and\nlong-form generation tasks show that CRAG can significantly improve the\nperformance of RAG-based approaches.\n",
    "published": "2024-01-29T04:36:39Z"
  },
  "text": [
    "Corrective Retrieval Augmented Generation\nShi-Qi Yan\n1\n*\n, Jia-Chen Gu\n2\n*\n, Yun Zhu\n3\n, Zhen-Hua Ling\n1\n1\nNational Engineering Research Center of Speech and Language Information Processing,\nUniversity of Science and Technology of China, Hefei, China\n2\nDepartment of Computer Science, University of California, Los Angeles\n3\nGoogle Research\nyansiki@mail.ustc.edu.cn,gujc@ucla.edu,yunzhu@google.com,zhling@ustc.edu.cn\nAbstract\nLarge  language  models  (LLMs)  inevitably\nexhibit  hallucinations  since  the  accuracy  of\ngenerated  texts  cannot  be  secured  solely  by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily  on  the  relevance  of  retrieved  docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose theCorrectiveRetrievalAugmented\nGeneration (CRAG) to improve the robustness\nof  generation.Specifically,  a  lightweight\nretrieval  evaluator  is  designed  to  assess  the\noverall  quality  of  retrieved  documents  for  a\nquery,  returning  a  confidence  degree  based\non  which  different  knowledge  retrieval  ac-\ntions can be triggered.   Since retrieval from\nstatic and limited corpora can only return sub-\noptimal documents, large-scale web searches\nare utilized as an extension for augmenting the\nretrieval results.  Besides, a decompose-then-\nrecompose algorithm is designed for retrieved\ndocuments to selectively focus on key infor-\nmation and filter out irrelevant information in\nthem.CRAGis  plug-and-play  and  can  be\nseamlessly coupled with various RAG-based\napproaches.Experiments  on  four  datasets\ncovering short- and long-form generation tasks\nshow thatCRAGcan significantly improve the\nperformance of RAG-based approaches.\n1\n1    Introduction\nLarge  language  models  (LLMs)  have  attracted\nincreasing attention and exhibited impressive abili-\nties to understand instructions and generate fluent\nlanguage texts (Brown et al., 2020; Ouyang et al.,\n2022; Touvron et al., 2023a). Nevertheless, LLMs\ninevitably manifest hallucinations (Ji et al., 2023)\ndue to their struggle with factual errors (Mallen\net  al.,  2023;  Min  et  al.,  2023)  and  inability  to\nsecure the accuracy of generated texts solely by\n*\nEqual contribution.\n1\nThe code is available at github.com/HuskyInSalt/CRAG\nQ: What is Henry \nFeilden's occupation?\nHenry Feilden \n(Conservative politician):\nHenry Master Feilden \nwas an Conservative \nParty politician...\nPolitician.\n✓\nQ: Who was the screenwriter \nfor Death of a Batman?\nBatman (1989 film): \nof the murder of Bruce \nWayne's parents. When \nHamm'sscript was \nrewritten, ...\nRetriever\n✗\nHamm.\nRetrieved\nDocuments\nGenerator\nAccurate Documents\nInaccurate Documents\nGenerator\nFigure 1: The examples show that a low-quality retriever\nis prone to introducing a substantial amount of irrelevant\ninformation, impeding the generators from acquiring\naccurate knowledge and potentially misleading them.\nthe parametric knowledge they encapsulate (Zhang\net al., 2023b; Muhlgay et al., 2023).\nPrior  research  has  introduced  retrieval  tech-\nniques  to  incorporate  relevant  knowledge  and\naugment generation, as exemplified by retrieval-\naugmented generation (RAG) (Lewis et al., 2020).\nIn  this  framework,  the  input  to  models  is  aug-\nmented by prepending relevant documents that are\nretrieved from an external knowledge corpus (Guu\net al., 2020).  While RAG serves as a practicable\ncomplement  to  LLMs,  its  effectiveness  is  con-\ntingent  upon  the  relevance  and  accuracy  of  the\nretrieved  documents  (Li  et  al.,  2022;  Tan  et  al.,\n2022).  The heavy reliance of generation on the\nretrieved  knowledge  raises  significant  concerns\nabout  the  model’s  behavior  and  performance  in\nscenarios where retrieval may fail or return inaccu-\nrate results (Shi et al., 2023).  As Figure 1 shows\nthat a low-quality retriever is prone to introducing\na  substantial  amount  of  irrelevant  information,\narXiv:2401.15884v2  [cs.CL]  16 Feb 2024",
    "impeding  the  models  from  acquiring  accurate\nknowledge and potentially misleading them, result-\ning in issues such as hallucinations (Zhang et al.,\n2023b).   However,  most  conventional  RAG  ap-\nproaches indiscriminately incorporate the retrieved\ndocuments, regardless of whether these documents\nare relevant or not (Rony et al., 2022). Furthermore,\ncurrent methods mostly treat complete documents\nas reference knowledge both during retrieval and\nutilization. But a considerable portion of the text\nwithin  these  retrieved  documents  is  often  non-\nessential  for  generation,  which  should  not  have\nbeen equally referred to and involved in RAG.\nOn  account  of  the  above  issues,  this  paper\nparticularlystudiesthescenarioswhere\nthe   retriever   returns   inaccurate   results.A\nmethod namedCorrectiveRetrieval-Augmented\nGeneration (CRAG) is proposed to self-correct\nthe results of retriever and improve the utilization\nof  documents  for  augmenting  generation.A\nlightweight  retrieval  evaluator  is  designed  to\nassess the overall quality of retrieved documents\nfor a query.  This serves as a crucial component\nin  RAG,  contributing  to  informative  generation\nby   reviewing   and   evaluating   the   relevance\nand  reliability  of  the  retrieved  documents.A\nconfidence degree  is quantified based on  which\ndifferent knowledge retrieval actions of {Correct,\nIncorrect,Ambiguous} can be triggered. For the\nlatter two actions, large-scale web searches (Piktus\net al., 2021; Komeili et al., 2022) are integrated as\na  strategic  extension,  since  retrieval  from  static\nand limited corpora can only return sub-optimal\ndocuments in terms of scope and diversity.  This\naugmentation  is  implemented  to  broaden  the\nspectrum  of  retrieved  information,   harnessing\nthe  expansive  and  dynamic  nature  of  the  web\nto complement and enrich the initially obtained\ndocuments.  Furthermore, to eliminate redundant\ncontexts contained in retrieved documents that are\nunhelpful for RAG, a decompose-then-recompose\nalgorithm is meticulously crafted throughout the\nretrieval and utilization process.  This algorithm\nensures  the  refinement  of  retrieved  information,\noptimizing  the  extraction  of  key  insights  and\nminimizing the inclusion of non-essential elements,\nthereby enhancing the utilization of retrieved data.\nCRAGis  plug-and-play  and  experimentally\nimplemented into RAG (Lewis et al., 2020) and\nSelf-RAG (Asai et al., 2023) for demonstrating its\nadaptability to RAG-based approaches. Results on\nfour datasets of PopQA (Mallen et al., 2023), Biog-\nraphy (Min et al., 2023), Pub Health (Zhang et al.,\n2023a), and Arc-Challenge (Bhakthavatsalam et al.,\n2021) show thatCRAGcan significantly improve\nthe performance of standard RAG and state-of-the-\nart Self-RAG, demonstrating its generalizability\nacross both short- and long-form generation tasks.\nTo facilitate others to reproduce our results, we will\npublish all source code later.\nIn summary, our contributions in this paper are\nthree-fold:   1)  This  paper  studies  the  scenarios\nwhere the retriever returns inaccurate results and,\nto  the  best  of  our  knowledge,  makes  the  first\nattempt to design corrective strategies for RAG to\nimprove its robustness. 2) A plug-and-play method\nnamedCRAGis proposed to improve the ability of\nautomatic self-correction and efficient utilization\nof retrieved documents.  3) Experimental results\nextensively demonstrateCRAG’s adaptability to\nRAG-based  approaches  and  its  generalizability\nacross short- and long-form generation tasks.\n2    Related Work\nHallucinations of LLMsAlthough LLMs have\nexhibited impressive abilities to understand instruc-\ntions and generate fluent language texts (Bang et al.,\n2023; Qin et al., 2023; Zhong et al., 2023), one\nof  the  most  severe  issues  that  LLMs  have  still\nbeen struggling with is hallucinations.  As many\nstudies found (Zhang et al., 2023b; Shuster et al.,\n2021),  either  outdated  information  or  incorrect\nknowledge that is activated would seriously result\nin hallucinations. Large-scale unregulated training\ndata collection, low proportion of high-quality sam-\npling data, imperfection of data allocation in the\ninput space, and many other realistic factors could\nimpact  the  LLMs  and  exacerbate  the  problems.\nThus, it is obvious that the lack of accurate and\nspecific knowledge can lead to misleading or even\ninaccurate generation, which will severely hurt the\nexperience of users in most practical applications.\nRetrieval-Augmented GenerationRAG (Lewis\net  al.,  2020;  Guu  et  al.,  2020)  is  regarded  as  a\nuseful method to address the issues above, which\nenhances the input questions of generative LMs\nwith retrieved documents.  It usually provides an\nextra  knowledge  source  from  a  specific  corpus,\ni.e.,  Wikipedia,  which greatly improves the per-\nformance of LMs in a variety of tasks, especially\nin the knowledge-intensive ones.   The proposed\nmethods generally leverage information retrieval to\nsupply documents containing relevant knowledge\nfor generative LLMs. Earlier studies adopt either",
    "sparse or dense retrievers at the front end of a pre-\ntrained language model that specializes in response\ngeneration. Despite this, the methods above usually\nignore a question,what if the retrieval goes wrong?\nSince the purpose of introducing a retrieval is to\nsecure that generative LMs can obtain relevant and\naccurate knowledge.  If retrieved documents are\nirrelevant, the retrieval system can even exacerbate\nthe factual error that LMs make.\nAdvanced  RAGMany  advanced  approaches\nhave  been  developed  from  the  original  RAG  in\nrecent years.  Considering that retrieval is some-\ntimes unnecessary for some queries,  conversely,\nresponses without retrieval are even more accurate\nin many situations.  Self-RAG (Asai et al., 2023)\nis proposed to selectively retrieve knowledge and\nintroduce  a  critic  model  to  decide  whether  to\nretrieve. Yoran et al. (2023) designed an NLI model\nto  identify  the  irrelevant  context  and  improve\nrobustness.  SAIL (Luo et al., 2023) is tuned on\ninstructions to insert retrieved documents before in-\nstructions. While Toolformer (Schick et al., 2023)\nis pre-trained for calling APIs such as Wikipedia.\nIn  addition,  in  some  long-text  generation  tasks,\nexternal knowledge is needed more than once, and\nwhen to retrieve should be concerned. Jiang et al.\n(2023) actively anticipate future content and decide\nwhen and what to retrieve in long-form generation.\nCompared  with  recent  studies  (Schick  et  al.,\n2023; Luo et al., 2023; Asai et al., 2023) that are\nthe most relevant to our work, a main difference\nshould be highlighted.   These approaches target\non exploiting retrieval as a useful tool to augment\ngeneration or whether retrieval is necessary, while\nthis study particularly studies the scenarios where\nthe retriever returns inaccurate results. To the best\nof our knowledge, this paper makes the first attempt\nto explore and design corrective strategies for RAG\nto improve its robustness of generation.\n3    Task Formulation\nFollowing previous work (Lewis et al., 2020; Asai\net  al.,  2023),  given  inputXand  an  accessible\ncorpus containing a large amount of knowledge\ndocumentsC={d\n1\n, ..., d\nN\n},  the  system  is  ex-\npected  to  generate  the  outputY.The  entire\nframework is usually divided into a retrieverR\nand a generatorG. The retrieverRaims to retrieve\nthe top-KdocumentsD={d\nr\n1\n, ..., d\nr\nk\n}that are\nrelevant to the inputXfrom the corpusC. Based\non  the  inputXand  the  retrieved  resultsD,  the\ngeneratorGis responsible for generating the output\nY. This framework can be formulated as:\nP(Y|X) =P(D|X)P(Y,D|X).(1)\nIt shows that the retriever and generator are seam-\nlessly coupled, exhibiting low risk tolerance. Any\nunsuccessful retrieval can result in an unsatisfac-\ntory response, regardless of the impressive abilities\nof the generator.  This is exactly the focus of this\npaper to improve the robustness of generation.\n4CRAG\n4.1    Overview of Model Inference\nFigure  2  and  Algorithm  1  present  an  overview\nofCRAGat inference, which designs corrective\nstrategies to improve the robustness of generation.\nGiven an input query and the retrieved documents\nfrom any retriever, a lightweight retrieval evaluator\nis  constructed  to  estimate  the  relevance  score\nof  retrieved  documents  to  the  input  query  (Sec-\ntion 4.2). The relevance score is quantified into a\ntotal of three confidence degrees and then triggered\nthe corresponding actions: {Correct,Incorrect,\nAmbiguous} (Section 4.3).  If the actionCorrect\nis triggered,  the retrieved documents will be re-\nfined  into  more  precise  knowledge  strips.   This\nrefinement operation involves knowledge decom-\nposition,  filter,  and recomposition (Section 4.4).\nIf the actionIncorrectis triggered, the retrieved\ndocuments will be discarded. Instead, web searches\nare  resorted  to  and  regarded  as  complementary\nknowledge sources for corrections (Section 4.5).\nEventually,  when  it  cannot  confidently  make  a\ncorrect or incorrect judgment, a soft and balanced\nactionAmbiguouswhich combines both of them is\ntriggered. After optimizing the retrieval results, an\narbitrary generative model can be adopted.\n4.2    Retrieval Evaluator\nIt is natural to wonder whether the retrieved docu-\nments are accurate or not before using them, which\nis significant since irrelevant or misleading mes-\nsages can be identified in this way. The accuracy\nof the retrieval evaluator undeniably plays a pivotal\nrole in shaping the overall system performance, as\nit influences the outcomes of subsequent processes.\nOur objective is to correct the retrieved documents\nif they are irrelevant. Specifically, T5-large (Raffel\net al., 2020) is adopted for initializing the retrieval\nevaluator and fine-tuned.   The relevance signals\nfor fine-tuning the evaluator can be collected from\nthe existing datasets. More details about this fine-\ntuning step can be referred to in Appendix B.2. For",
    "x: Who was the screenwriter for Death of a Batman?\nd\n1\nd\n2\nRetrieval\nInput\nRetrieved Documents\nAsk: If retrieved \ndocuments are \ncorrect to x?\nCorrect\nRetrieval\nEvaluator\nAsk: If retrieved \ndocuments are \ncorrect to x?\nAmbiguous\nIncorrect\nKnowledge Refinement\nd\n1\nd\n2\nstrip\n1\nstrip\n2\nstrip\nk\nDecompose\n...\nFilter\nstrip\n1\nstrip\nk\nRecompose\nk\nin\nKnowledge Searching\nx\nRewrite\nq:Death of a Batman; \nscreenwriter; Wikipedia\nWeb\nSearch\nk\nex\nk\n1\nk\nn\nk\n2\n...\nSelect\nKnowledge\nCorrection\nGeneration\nCorrectAmbiguous\nIncorrect\nx\nk\nin\n+\nx\nk\nin\n+\nGenerator\nk\nex\n+\nx\nk\nex\n+\nFigure 2: An overview ofCRAGat inference. A retrieval evaluator is constructed to evaluate the relevance of the\nretrieved documents to the input, and estimate a confidence degree based on which different knowledge retrieval\nactions of {Correct,Incorrect,Ambiguous} can be triggered.\nevery question, there are generally 10 documents\nretrieved. The question is concatenated with each\nsingle document as the input,  and the evaluator\npredicts  the  relevance  score  for  each  question-\ndocument pair individually. We also tried to prompt\nChatGPT  to  identify  the  retrieval  relevance  for\ncomparison, but it underperforms as elaborated in\nSection 5.5.  Based on these calculated relevance\nscores,  a  final  judgment  is  made  as  to  whether\nthe retrieval is correct or not associated with the\naction  trigger.   Compared  with  the  critic  model\nof Self-RAG (Asai et al., 2023) that instruction-\ntuned LLaMA-2 (7B), the evaluator designed in\nCRAGdemonstrates the advantages of being quite\nlightweight (0.77B).\n4.3    Action Trigger\nTo correct the irrelevant documents and refine the\ntarget documents as needed, actions should be exe-\ncuted discriminately. Based on the aforementioned\nconfidence score for each retrieved document, three\ntypes of actions are designed and triggered accord-\ningly where the upper and lower thresholds are set.\nIf the confidence score is higher than the upper\nthreshold, the retrieved document is identified as\nCorrect, while identified asIncorrectif below\nthe  lower  threshold.    Otherwise,Ambiguousis\nexecuted.  Each retrieved document is conducted\nindividually and integrated eventually.\nCorrectHere,  a retrieval is assumedCorrect\nwhen the confidence score ofat least one retrieved\ndocumentis higher than the upper threshold. If so,\nit means that there are relevant documents in the\nretrieved results. Even if a relevant document can\nbe found, there is inevitably some noisy knowledge\nstrips  in  this  document.To  extract  the  most\ncritical knowledge strips within this document, a\nknowledge refinement method is further designed\nwhich will be elaborated in Section 4.4.\nIncorrectBesides,    a   retrieval   is   assumed\nIncorrectwhen  the  confidence  scores  ofall\nretrieved documentsare below the lower threshold.\nThis  indicates  that  all  retrieved  documents  are\nconsidered  irrelevant,  which  are  unhelpful  for\ngeneration.Therefore,  we  need  to  seek  new\nsources of knowledge for correction.  Here, web\nsearch is introduced to search from the Internet as",
    "Algorithm 1:CRAG Inference\nRequire :E(Retrieval Evaluator),W(Query Rewriter),G(Generator)\nInput:x(Input question),D={d\n1\n, d\n2\n, ..., d\nk\n}(Retrieved documents)\nOutput  :y(Generated response)\n1score\ni\n=Eevaluates the relevance of each pair (x,d\ni\n),d\ni\n∈D\n2Confidence= Calculate and give a final judgment based on {score\n1\n, score\n2\n, ...score\nk\n}\n//Confidencehas 3 optional values: [CORRECT], [INCORRECT] or [AMBIGUOUS]\n3ifConfidence ==[CORRECT]then\n4Internal_Knowledge = Knowledge_Refine(x,D)\n5k= Internal_Knowledge\n6else ifConfidence ==[INCORRECT]then\n7External_Knowledge = Web_Search(WRewritesxfor searching)\n8k= External_Knowledge\n9else ifConfidence ==[AMBIGUOUS]then\n10Internal_Knowledge = Knowledge_Refine(x,D)\n11External_Knowledge = Web_Search(WRewritesxfor searching)\n12k= Internal_Knowledge + External_Knowledge\n13end\n14Gpredictsygivenxandk\nelaborated in Section 4.5.  This corrective action\nhelps overcome the embarrassing challenge where\nno reliable knowledge can be referred to.\nAmbiguousExcept for the above two situations,\nthe remaining will be assigned to an intermediate\naction ofAmbiguous. Since the retrieval evaluator\nis  not  confident  in  its  judgment,  both  types  of\nprocessed knowledge inCorrectandIncorrect\nare combined to complement each other.  Imple-\nmenting such a moderating and soft strategy can\nsignificantly contribute to strengthening the robust-\nness and resilience of the system, fostering a more\nadaptable framework for optimal performance.\n4.4    Knowledge Refinement\nGiven a retrieved relevant document, a decompose-\nthen-recompose knowledge refinement method is\ndesigned to further extract the most critical knowl-\nedge strips in it.  First, each retrieved document\nis segmented into fine-grained knowledge strips\nthrough heuristic rules, more details are available\nin Appendix B.2. Then, the retrieval evaluator fine-\ntuned in Section 4.2 is employed to calculate the\nrelevance score of each knowledge strip.  Based\non these scores,  irrelevant knowledge strips  are\nfiltered out, while relevant ones are recomposed via\nconcatenation in order, namely internal knowledge.\n4.5    Web Search\nIt is extremely important to seek complementary\nexternal knowledge if the retrieved results are all\nassumed  irrelevant.   Since  retrieval  from  static\nand limited corpora can only return sub-optimal\ndocuments in terms of scope and diversity, large-\nscale web searches (Piktus et al., 2021; Komeili\net al., 2022) are integrated as a strategic extension\nof RAG. Specifically, the inputs are rewritten into\nqueries  composed  of  keywords  by  ChatGPT  to\nmimic  the  daily  usage  of  search  engine.    The\nprompt for rewriting is shown in Appendix A. In\nCRAG, a public and accessible commercial web\nsearch API is adopted to generate a series of URL\nlinks for every query.\n2\nMoreover, we utilize the\nURL links to navigate web pages, transcribe their\ncontent, and employ the same knowledge refine-\nment method as Section 4.4 to derive the relevant\nweb knowledge, namely external knowledge.\n5    Experiments\nWe conducted experiments to extensively demon-\nstrateCRAG’s  adaptability  to  RAG-based  ap-\nproaches and its generalizability across both short-\nand long-form generation tasks.\n5.1    Tasks, Datasets and Metrics\nCRAGwas evaluated on four datasets, including\nPopQA(Mallen et al., 2023) (short-form gener-\nation),Biography(Min et al., 2023) (long-form\ngeneration),PubHealth(Zhang et al., 2023a) (true-\nor-falsequestion), andArc-Challenge(Bhaktha-\nvatsalam et al., 2021) (multiple-choicequestion).\n2\nIn this study, Google Search API is utilized for searching.",
    "PopQABioPubARC\nMethod(Accuracy)(FactScore)(Accuracy)(Accuracy)\nLMs trained with propriety data\nLLaMA2-c\n13B\n20.055.949.438.4\nRet-LLaMA2-c\n13B\n51.879.952.137.9\nChatGPT29.371.870.175.3\nRet-ChatGPT50.8-54.775.3\nPerplexity.ai-71.2--\nBaselines without retrieval\nLLaMA2\n7B\n14.744.534.221.8\nAlpaca\n7B\n23.645.849.845.0\nLLaMA2\n13B\n14.753.429.429.4\nAlpaca\n13B\n24.450.255.554.9\nCoVE\n65B\n-71.2--\nBaselines with retrieval\nLLaMA2\n7B\n38.278.030.048.0\nAlpaca\n7B\n46.776.640.248.0\nSAIL--69.248.4\nLLaMA2\n13B\n45.777.530.226.0\nAlpaca\n13B\n46.177.751.157.6\nLLaMA2-hf-7b\nRAG37.744.99.123.8\nCRAG39.847.79.125.8\nSelf-RAG*29.032.20.723.9\nSelf-CRAG49.069.10.627.9\nSelfRAG-LLaMA2-7b\nRAG40.359.239.046.7\nCRAG59.374.175.654.8\nSelf-RAG54.981.272.467.3\nSelf-CRAG61.886.274.867.2\nTable 1: Overall evaluation results on the test sets of four datasets. Results are separated based on the generation\nLLMs.Boldnumbers indicate the best performance among all methods and LLMs.Gray-coloredbold scores\nindicate the best performance using a specific LLM. * indicates the results reproduced by us, otherwise results\nexcept ours are cited from their original papers.\nFollowing previous work, accuracy was adopted\nas the evaluation metric for PopQA, PubHealth,\nand Arc-Challenge.  FactScore (Min et al., 2023)\nwas adopted as the evaluation metric for Biography.\nReaders can refer to Appendix B.1 for more details.\n5.2    Baselines\nWe  primarily  comparedCRAGwith  both  ap-\nproaches  without  and  with  retrieval,  where  the\nlatter consists of standard RAG and advanced RAG.\nBaselines without retrieval.We evaluated some\npublic LLMs, LLaMA2-7B,13B (Touvron et al.,\n2023b), instruction-tuned models, Alpaca-7B,13B\n(Dubois et al., 2023), and CoVE\n65B\n(Dhuliawala\net al., 2023) which introduces iterative engineering\nto  improve  the  factuality  of  LLM  generations.\nPropriety  LLMs  such  as  LLaMA2-chat\n13B\nand\nChatGPT are also included.\nStandard  RAG.We  evaluated  the  standard\nRAG (Lewis et al., 2020) where an LM generates\noutput  given  the  query  prepended  with  the  top\nretrieved documents using the same retriever as\nin  our  system.    Here  we  adopted  several  pub-\nlic instruction-tuned LLMs, including LLaMA2-\n7B, 13B (Touvron et al., 2023b), Alpaca-7B,13B\n(Dubois  et  al.,  2023),  as  well  as  LLaMA2-7B\ninstruction-tuned in Self-RAG (Asai et al., 2023).\nAdvanced RAG.(1) SAIL (Luo et al., 2023) that\ninstruction-tuned an LM on the Alpaca instruction-\ntuning data with top retrieved documents inserted\nbefore  instructions.   (2)  Self-RAG  (Asai  et  al.,\n2023) that tuned the LLaMA2 on the instruction-\ntuning data comtaining several sets of reflection\ntokens  which  were  labeled  by  GPT-4  (OpenAI,\n2023).  (3) Following Asai et al. (2023), we also",
    "cited the results of retrieval-augmented baselines\ntrained with private data:  Ret-ChatGPT and Ret-\nLLaMA-chat, which deploy the same augmenta-\ntion technique above, as well as perplexity.ai, an\nInstructGPT-based production search system.\n5.3    Results\nTable 1 presents the results on four datasets. The\nmodel coupling the proposed method with standard\nRAG is namedCRAGand that coupling with Self-\nRAG is named Self-CRAG. Readers can refer to\nAppendix B.2 for more implementation details of\nour proposed methods. From these results, we can\nconclude the following findings:\nFirst,  the  proposed  method  can  significantly\nimprove the performance of RAG and Self-RAG.\nSpecifically,CRAGoutperformed   RAG   by\nmargins  of  19.0%  accuracy  on  PopQA,  14.9%\nFactScore  on  Biography,   36.6%  accuracy  on\nPubHealth, and 8.1% accuracy on Arc-Challenge\nwhen  based  onSelfRAG-LLaMA2-7b,  as  well\nas  by  margins  of  2.1%  accuracy  on  PopQA,\n2.8%  FactScore  on  Biography,   and  2.0%  on\nArc-Challenge  when  based  onLLaMA2-hf-7b.\nCompared  with  the  current  state-of-the-art  Self-\nRAG, Self-CRAG outperformed it by margins of\n20.0% accuracy on PopQA, 36.9% FactScore on\nBiography, and 4.0% accuracy on Arc-Challenge\nwhen  based  onLLaMA2-hf-7b,  as  well  as  by\nmargins  of  6.9%  accuracy  on  PopQA,  5.0%\nFactScore on Biography,  and 2.4% accuracy on\nPubHealth, when based onSelfRAG-LLaMA2-7b.\nThese   results   demonstrated   the   adaptability\nofCRAGwhich  is  plug-and-play  and  can  be\nimplemented into RAG-based approaches.\nSecond,  the  proposed  method  demonstrated\ngreat  generalizability  across  a  variety  of  gen-\neration  tasks.In  particular,  these  benchmarks\nreported in Table 1 respectively represent different\npractical  scenarios  including  short-form  entity\ngeneration  (PopQA),  long-form  generation  (Bi-\nography),  and closed-set tasks (PubHealth,  Arc-\nChallenge).  These results verified the consistent\neffectiveness ofCRAG. Its versatility across a spec-\ntrum of tasks underscores its robust capabilities and\ngeneralizability across diverse scenarios.\nThird,  the proposed method exhibited greater\nflexibility in replacing the underlying LLM gen-\nerator.It can be seen thatCRAGstill showed\ncompetitive  performance  when  the  underlying\nLLMs  was  changed  fromSelfRAG-LLaMA2-7b\ntoLLaMA2-hf-7b, while the performance of Self-\nLLaMA2-hf-7b  SelfRAG-LLaMA2-7b\nCRAG47.359.3\nw/o.Correct44.558.1\nw/o.Incorrect46.858.6\nw/o.Ambiguous45.758.5\nSelf-CRAG49.061.8\nw/o.Correct43.659.6\nw/o.Incorrect47.760.8\nw/o.Ambiguous48.161.5\nTable 2: Ablation study for removing each single action\non the PopQA dataset in terms of accuracy.\nRAG dropped significantly, even underperforming\nthe standard RAG on several benchmarks.   The\nreason for these results is that Self-RAG needs to be\ninstruction-tuned using human or LLM annotated\ndata  to  learn  to  output  special  critic  tokens  as\nneeded, while this ability is not learned in common\nLLMs.CRAGdoes not have any requirements\nfor this ability.  As you can imagine, when more\nadvanced LLMs are available in the future, they\ncan be coupled withCRAGeasily, while additional\ninstruction tuning is still necessary for Self-RAG.\n5.4    Ablation Study\nThe impact of each triggered action.To fur-\nther verify the effectiveness of triggered actions\ndesigned in the retrieval evaluator, ablation tests\nfor removing each single action in the proposed\nmethod  were  conducted  as  shown  in  Table  2.\nEvaluations on the PopQA dataset were conducted\nto demonstrate the performance change in terms of\naccuracy.  Specifically, when the actionCorrect\norIncorrectwas removed, it was merged with\nAmbiguousso that the proportion that originally\ntriggeredCorrectorIncorrectwould  trigger\nAmbiguous.  On the other hand, when the action\nAmbiguouswas  removed,  there  was  only  one\nthreshold against which all input queries clearly\ntriggeredCorrectorIncorrect.    From  these\nresults, it can be seen that there was a performance\ndrop no matter which action was removed, illustrat-\ning that each action contributed to improving the\nrobustness of generation.\nThe impact of each knowledge utilization oper-\nation.Table 3 illustrated how the performance\nchanged if a key knowledge utilization operation\nwas ablated. Evaluations on the PopQA dataset in\nterms of accuracy were conducted by individually\nremoving the knowledge utilization operations of\ndocument refinement, search query rewriting, and",
    "LLaMA2-hf-7b  SelfRAG-LLaMA2-7b\nCRAG47.359.3\nw/o. refinement38.947.0\nw/o. rewriting44.856.6\nw/o. selection44.053.8\nSelf-CRAG49.061.8\nw/o. refinement35.952.2\nw/o. rewriting37.258.4\nw/o. selection24.957.9\nTable 3: Ablation study for removing each knowledge\nutilization operation on the PopQA in terms of accuracy.\nAccuracy\nOur Retrieval Evaluator (T5-based)84.3\nChatGPT58.0\nChatGPT-CoT62.4\nChatGPT-few-shot64.7\nTable  4:   Evaluation  of  our  retrieval  evaluator  and\nChatGPT for the retrieval results on the PopQA dataset.\nexternal knowledge selection. Removing document\nrefinement denoted that the original retrieved docu-\nments were directly fed to the following generator,\nas in most existing works. Additionally, removing\nsearch query rewriting denoted that questions were\nnot rewritten into queries consisting of keywords\nduring knowledge searching. Eventually, removing\nknowledge selection denoted that all searched con-\ntent of web pages was all regarded as the external\nknowledge without selection.  These results help\nderive  the  findings  that  the  performance  of  the\nfinal system degraded no matter which knowledge\nutilization operation was removed, revealing that\neach knowledge utilization operation contributed\nto improving the utilization of knowledge.\n5.5    Accuracy of the Retrieval Evaluator\nThe quality of the retrieval evaluator significantly\ndetermined the performance of the entire system.\nGiven the document retrieval results, we assessed\nwhether  the  retrieval  evaluator  can  accurately\ndetermine the overall quality of these results. The\nassessment  accuracy  on  the  PopQA  dataset  of\nour retrieval evaluator and the commercial LLM\nChatGPT on the document retrieval results was\nshown  in  Table  4.    The  prompts  ofChatGPT,\nChatGPT-CoT, andChatGPT-few-shotused in our\nexperiments  can  be  referred  to  in  Appendix  A.\nResults  reveal  that  the  lightweight  T5-based  re-\ntrieval  evaluator  significantly  outperformed  the\ncompetitive ChatGPT in all settings.\n69.8\n(Actual)\n605040302010\nAccuracy of retrieval\n20\n30\n40\n50\n60\n70\nAccuracy of generation\nno retrieval\nSelf-RAGSelf-CRAG\nFigure 3:  The generation performance of Self-RAG,\nSelf-CRAG, RAG, andCRAGgiven different retrieval\nperformance  on  the  PopQA  dataset  with  SelfRAG-\nLLaMA-7b.  The lower horizontal line demonstrates\nthe performance of the generator without retrieval.\n5.6    Robustness to Retrieval Performance\nTo further verify the robustness of the proposed\nmethod to retrieval performance, we studied how\nthe generation performance changed given different\nretrieval performance. A part of accurate retrieval\nresults  were  deliberately  removed  at  random  to\nimitate a low-quality retriever and evaluate how\nthe performance changed. Figure 3 demonstrated\nthe  performance  change  of  Self-RAG  and  Self-\nCRAG  on  the  PopQA  dataset.   It  can  be  seen\nthat the generation performance of Self-RAG and\nSelf-CRAG dropped as the retrieval performance\ndropped, indicating that the generator relied heavily\non the quality of the retriever.   Furthermore,  as\nthe retrieval performance dropped, the generation\nperformance of Self-CRAG dropped more slightly\nthan  that  of  Self-RAG.  These  results  imply  the\nsuperiority of Self-CRAG over Self-RAG on en-\nhancing the robustness to retrieval performance.\n6    Conclusion\nThis paper studies the problem where RAG-based\napproaches are challenged if retrieval goes wrong,\nthereby exposing inaccurate and misleading knowl-\nedge  to  generative  LMs.    Corrective  Retrieval\nAugmented Generation is proposed to improve the\nrobustness of generation. Essentially, a lightweight\nretrieval evaluator is to estimate and trigger three\nknowledge retrieval actions discriminately.  With\nthe further leverage of web search and optimized\nknowledge utilization,CRAGhas significantly im-\nproved the ability of automatic self-correction and\nefficient utilization of retrieved documents. Exper-\niments extensively demonstrate its adaptability to\nRAG-based approaches as well as generalizability\nacross short- and long-form generation tasks.",
    "Limitations\nWhile we primarily proposed to improve the RAG\nframework from a corrective perspective, how to\ndetect  and  correct  the  wrong  knowledge  more\naccurately  and  effectively  still  requires  further\nstudy. AlthoughCRAGcan be seamlessly coupled\nwith various RAG-based approaches, fine-tuning\na  retrieval  evaluator  is  inevitable.    In  addition,\npotential bias introduced by web searches is also\nworth  concern.   The  quality  of  internet  sources\ncan vary significantly, and incorporating such data\nwithout enough consideration may introduce noise\nor misleading information to the generated outputs.\nFuture work will further explore a more stable and\nreliable method of retrieval augmentation.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson,  Dmitry  Lepikhin,  Alexandre  Passos,  Siamak\nShakeri,  Emanuel  Taropa,  Paige  Bailey,  Zhifeng\nChen,  Eric  Chu,  Jonathan  H.  Clark,  Laurent  El\nShafey,  Yanping  Huang,  Kathy  Meier-Hellstern,\nGaurav  Mishra,  Erica  Moreira,  Mark  Omernick,\nKevin Robinson, Sebastian Ruder, et al. 2023. PaLM\n2 technical report.CoRR, abs/2305.10403.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023.  Self-rag:  Learning to\nretrieve, generate, and critique through self-reflection.\nCoRR, abs/2310.11511.\nYejin   Bang,   Samuel   Cahyawijaya,   Nayeon   Lee,\nWenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,\nZiwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do,\nYan  Xu,  and  Pascale  Fung.  2023.A  multitask,\nmultilingual, multimodal evaluation of chatgpt on\nreasoning, hallucination, and interactivity.CoRR,\nabs/2302.04023.\nSumithra Bhakthavatsalam, Daniel Khashabi, Tushar\nKhot,  Bhavana  Dalvi  Mishra,  Kyle  Richardson,\nAshish   Sabharwal,   Carissa   Schoenick,   Oyvind\nTafjord,  and  Peter  Clark.  2021.   Think  you  have\nsolved direct-answer question answering? try arc-da,\nthe direct-answer AI2 reasoning challenge.CoRR,\nabs/2102.03315.\nTom  B  Brown,  Benjamin  Mann,  Nick  Ryder,  et  al.\n2020.  Language models are few-shot learners.  In\nAdvances in neural information processing systems,\npages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten  Bosma,  Gaurav  Mishra,  Adam  Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian  Gehrmann,  Parker  Schuh,  Kensen  Shi,\nSasha  Tsvyashchenko,  Joshua  Maynez,  Abhishek\nRao,   Parker   Barnes,   Yi   Tay,   Noam   Shazeer,\nVinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson,  Reiner Pope,  James Bradbury,  Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke,  Anselm Levskaya,  Sanjay Ghemawat,\nSunipa Dev,  Henryk Michalewski,  Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid  Dohan,  Shivani  Agrawal,  Mark  Omernick,\nAndrew  M.  Dai,  Thanumalayan  Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon  Child,  Oleksandr  Polozov,  Katherine  Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand  Noah  Fiedel.  2023.   Palm:  Scaling  language\nmodeling  with  pathways.J.  Mach.  Learn.  Res.,\n24:240:1–240:113.\nShehzaad  Dhuliawala,   Mojtaba  Komeili,   Jing  Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. 2023.  Chain-of-verification reduces\nhallucination  in  large  language  models.CoRR,\nabs/2309.11495.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023.  Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback.CoRR, abs/2305.14387.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020.  Retrieval augmented\nlanguage model pre-training. InProceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 ofProceedings of Machine Learning Research,\npages 3929–3938. PMLR.\nGautier  Izacard,   Mathilde  Caron,   Lucas  Hosseini,\nSebastian Riedel, Piotr Bojanowski, Armand Joulin,\nand  Edouard  Grave.  2022.Unsupervised  dense\ninformation retrieval with contrastive learning.Trans.\nMach. Learn. Res., 2022.\nZiwei  Ji,  Nayeon  Lee,  Rita  Frieske,  Tiezheng  Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto,   and  Pascale  Fung.  2023.Survey  of\nhallucination in natural language generation.ACM\nComput. Surv., 55(12):248:1–248:38.\nZhengbao  Jiang,  Frank  F.  Xu,  Luyu  Gao,  Zhiqing\nSun,  Qian  Liu,  Jane  Dwivedi-Yu,  Yiming  Yang,\nJamie Callan,  and Graham Neubig. 2023.   Active\nretrieval  augmented  generation.InProceedings\nof  the  2023  Conference  on  Empirical  Methods\nin  Natural  Language  Processing,  EMNLP  2023,\nSingapore, December 6-10, 2023, pages 7969–7992.\nAssociation for Computational Linguistics.\nMojtaba  Komeili,  Kurt  Shuster,  and  Jason  Weston.\n2022.  Internet-augmented dialogue generation.  In\nProceedings  of  the  60th  Annual  Meeting  of  the\nAssociation for Computational Linguistics (Volume\n1:  Long Papers), ACL 2022, Dublin, Ireland, May\n22-27,  2022,  pages  8460–8478.  Association  for\nComputational Linguistics.",
    "Patrick   S.   H.   Lewis,    Ethan   Perez,    Aleksandra\nPiktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim  Rocktäschel,  Sebastian  Riedel,  and  Douwe\nKiela.  2020.   Retrieval-augmented  generation  for\nknowledge-intensive  NLP  tasks.    InAdvances  in\nNeural Information Processing Systems 33: Annual\nConference   on   Neural   Information   Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nHuayang  Li,  Yixuan  Su,  Deng  Cai,  Yan  Wang,  and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation.CoRR, abs/2202.01110.\nHongyin  Luo,  Tianhua  Zhang,  Yung-Sung  Chuang,\nYuan Gong, Yoon Kim, Xixin Wu, Helen Meng, and\nJames R. Glass. 2023. Search augmented instruction\nlearning.InFindings   of   the   Association   for\nComputational Linguistics: EMNLP 2023, Singapore,\nDecember 6-10, 2023, pages 3717–3729. Association\nfor Computational Linguistics.\nAlex  Mallen,  Akari  Asai,  Victor  Zhong,  Rajarshi\nDas,  Daniel  Khashabi,  and  Hannaneh  Hajishirzi.\n2023.When   not   to   trust   language   models:\nInvestigating effectiveness of parametric and non-\nparametric memories.   InProceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics  (Volume  1:  Long  Papers),  ACL  2023,\nToronto, Canada, July 9-14, 2023, pages 9802–9822.\nAssociation for Computational Linguistics.\nSewon  Min,   Kalpesh  Krishna,   Xinxi  Lyu,   Mike\nLewis,  Wen-tau Yih,  Pang Wei Koh,  Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision   in   long   form   text   generation.In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2023, Singapore, December 6-10, 2023, pages 12076–\n12100. Association for Computational Linguistics.\nDor  Muhlgay,  Ori  Ram,  Inbal  Magar,  Yoav  Levine,\nNir Ratner, Yonatan Belinkov, Omri Abend, Kevin\nLeyton-Brown, Amnon Shashua, and Yoav Shoham.\n2023. Generating benchmarks for factuality evalua-\ntion of language models.CoRR, abs/2307.06908.\nOpenAI.  2023.GPT-4  technical  report.CoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll  L.  Wainwright,  Pamela  Mishkin,  Chong\nZhang,  Sandhini  Agarwal,  Katarina  Slama,  Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter\nWelinder, Paul F. Christiano, Jan Leike, and Ryan\nLowe. 2022.   Training language models to follow\ninstructions with human feedback. InNeurIPS.\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick S. H. Lewis, Barlas Oguz, Edouard Grave,\nWen-tau Yih, and Sebastian Riedel. 2021. The web\nis your oyster - knowledge-intensive NLP against a\nvery large web corpus.CoRR, abs/2112.09924.\nChengwei   Qin,   Aston   Zhang,   Zhuosheng   Zhang,\nJiaao  Chen,  Michihiro  Yasunaga,  and  Diyi  Yang.\n2023. Is chatgpt a general-purpose natural language\nprocessing  task  solver?InProceedings  of  the\n2023 Conference on Empirical Methods in Natural\nLanguage  Processing,  EMNLP  2023,  Singapore,\nDecember 6-10, 2023, pages 1339–1384. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei  Li,  and  Peter  J.  Liu.  2020.Exploring  the\nlimits of transfer learning with a unified text-to-text\ntransformer.J. Mach. Learn. Res., 21:140:1–140:67.\nMd.  Rashad  Al  Hasan  Rony,  Ricardo  Usbeck,  and\nJens Lehmann. 2022. Dialokg: Knowledge-structure\naware task-oriented dialogue generation. InFindings\nof  the  Association  for  Computational  Linguistics:\nNAACL  2022,   Seattle,   WA,  United  States,   July\n10-15,  2022,  pages  2557–2571.  Association  for\nComputational Linguistics.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR, abs/2302.04761.\nFreda  Shi,  Xinyun  Chen,  Kanishka  Misra,  Nathan\nScales, David Dohan, Ed H. Chi, Nathanael Schärli,\nand  Denny  Zhou.  2023.   Large  language  models\ncan  be  easily  distracted  by  irrelevant  context.   In\nProceedings of the 40th International Conference\non Machine Learning, volume 202 ofProceedings\nof Machine Learning Research, pages 31210–31227.\nPMLR.\nKurt  Shuster,   Spencer  Poff,   Moya  Chen,   Douwe\nKiela,    and   Jason   Weston.   2021.Retrieval\naugmentation reduces hallucination in conversation.\nInFindings of the Association for Computational\nLinguistics:  EMNLP  2021,  Virtual  Event  /  Punta\nCana, Dominican Republic, 16-20 November, 2021,\npages  3784–3803.  Association  for  Computational\nLinguistics.\nChao-Hong Tan, Jia-Chen Gu, Chongyang Tao, Zhen-\nHua  Ling,   Can  Xu,   Huang  Hu,   Xiubo  Geng,\nand  Daxin  Jiang.  2022.Tegtok:    Augmenting\ntext  generation  via  task-specific  and  open-world\nknowledge.InFindings  of  the  Association  for\nComputational  Linguistics:    ACL  2022,   Dublin,\nIreland,   May   22-27,   2022,   pages   1597–1609.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.CoRR,\nabs/2302.13971.",
    "Hugo  Touvron,  Louis  Martin,  Kevin  Stone,  Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale,  Dan  Bikel,  Lukas  Blecher,  et  al.  2023b.\nLlama  2:   Open  foundation  and  fine-tuned  chat\nmodels.CoRR, abs/2307.09288.\nOri  Yoran,  Tomer  Wolfson,  Ori  Ram,  and  Jonathan\nBerant. 2023. Making retrieval-augmented language\nmodels   robust   to   irrelevant   context.CoRR,\nabs/2310.01558.\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\nDanny Fox, Helen Meng, and James R. Glass. 2023a.\nInterpretable  unified  language  checking.CoRR,\nabs/2304.03728.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\nBi, Freda Shi, and Shuming Shi. 2023b. Siren’s song\nin the AI ocean: A survey on hallucination in large\nlanguage models.CoRR, abs/2309.01219.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too? A\ncomparative study on chatgpt and fine-tuned BERT.\nCoRR, abs/2302.10198.",
    "A    Task Prompts\nThe prompts for generating knowledge keywords\nas web search queries were illustrated in Table 5.\nExtract at most three keywords separated by comma from\nthe following dialogues and questions as queries for the\nweb search, including topic background within dialogues\nand main intent within questions.\nquestion: What is Henry Feilden’s occupation?\nquery: Henry Feilden, occupation\nquestion: In what city was Billy Carlson born?\nquery: city, Billy Carlson, born\nquestion: What is the religion of John Gwynn?\nquery: religion of John Gwynn\nquestion:What   sport   does   Kiribati   men’s   national\nbasketball team play?\nquery: sport, Kiribati men’s national basketball team play\nquestion: [question]\nquery:\nTable 5:  The few-shot prompt to GPT-3.5 Turbo for\ngenerating knowledge keywords as web search queries.\nThe prompts to instruct ChatGPT as the evalua-\ntor were illustrated in Table 6, Table 7, and Table 8\nrespectively.\nGiven a question, does the following document have exact\ninformation to answer the question?  Answer yes or no\nonly.\nQuestion: [question]\nDocument: [document]\nTable 6:  The direct prompt to GPT-3.5 Turbo as the\nevaluator.\nGiven a question, does the following document have exact\ninformation to answer the question?\nQuestion: [question]\nDocument: [document]\nThink Step by step, and answer with yes or no only.\nTable 7: The prompt to GPT-3.5 Turbo with Chain-of-\nThought as the evaluator.\nGiven a question, does the following document have exact\ninformation to answer the question?  Answer yes or no\nonly.\nQuestion: In what city was Abraham Raimbach born?\nDocument:  Bancroft  was  born  on  November  25,  1839\nin New Ipswich, New Hampshire to James Bancroft and\nSarah Kimball.  At an early age he was cared for by Mr.\nand Mrs. Patch of Ashby, Massachusetts, the neighboring\ntown.  While not legally adopted, they named him Cecil\nFranklin Patch Bancroft, adding Franklin Patch after the\nson  Mr.   and  Mrs.   Patch  had  who  recently  died.   He\nattended public schools in Ashby as well as the Appleton\nAcademy in New Ipswich. He entered Dartmouth College\nin 1856 at the age of sixteen and graduated in 1860 near\nthe top of his class. Bancroft continued his education as he\nbegan his career in teaching. He took classes at the Union\nTheological Seminary in New York City during the 1864-\n65 academic year.  While there he was a member of the\nUnited States Christian Commission, traveling to support\nsoldiers during the Civil War.  He then transferred to the\nAndover Theological Seminary where he would graduate\nin 1867.\nAnswer: No.\nQuestion:   In  what  country  is  Wilcza  Jama,  Sokółka\nCounty?\nDocument: Wilcza Jama is a village in the administrative\ndistrict   of   Gmina   Sokółka,   within   Sokółka   County,\nPodlaskie Voivodeship, in north-eastern Poland, close to\nthe border with Belarus.\nAnswer: Yes.\nQuestion:   What  sport  does  2004  Legg  Mason  Tennis\nClassic play?\nDocument: The 2004 Legg Mason Tenis Classic was the\n36th  edition  of  this  tennis  tournament  and  was  played\non outdoor hard courts.  The tournament was part of the\nInternational Series of the 2004 ATP Tour. It was held at\nthe William H.G. FitzGerald Tennis Center in Washington,\nD.C. from August 16 through August 22, 2004.\nAnswer: Yes.\nQuestion: Who is the author of Skin?\nDocument: The Skin We’re In: A Year of Black Resistance\nand  Power  is  a  book  by  Desmond  Cole  published  by\nDoubleday Canada in 2020. The Skin We’re In describes\nthe struggle against racism in Canada during the year 2017,\nchronicling Cole’s role as an anti-racist activist and the\nimpact of systemic racism in Canadian society.  Among\nthe events it discusses are the aftermath of the assault of\nDafonte Miller in late 2016 and Canada 150.  The work\nargues that Canada is not immune to the anti-Black racism\nthat characterizes American society. Due to an error by the\npublisher, the initial printing of the book’s cover did not\ninclude word\n ̈\nBlackïn the subtitle. The mistake was later\ncorrected. The book won the Toronto Book Award for 2020.\nIn 2021,  the book was nominated for the Shaughnessy\nCohen Prize for Political Writing.\nAnswer: No.\nQuestion: [question]\nDocument: [document]\nAnswer:\nTable 8: The few-shot prompt to GPT-3.5 Turbo as the\nevaluator.",
    "B    Experiments\nB.1    Tasks, Datasets and Metrics\nCRAGwas evaluated on four datasets, which are in\npublic domain and licensed for research purposes,\nincluding:\nPopQA(Mallen et al., 2023) is ashort-form\ngeneration  task.   Generally,  only  one  entity  of\nfactual knowledge is expected to be answered for\neach  single  question.    In  our  experiments,  we\nexactly  followed  the  setting  in  Self-RAG  (Asai\net al., 2023) which evaluated methods on a long-tail\nsubset consisting of 1,399 rare entity queries whose\nmonthly Wikipedia page views are less than 100.\nAccuracy was adopted as the evaluation metric.\nBiography(Min  et  al.,  2023)  is  along-form\ngeneration task that is tasked to generate a detailed\nbiography about a certain entity. Following previ-\nous work, FactScore (Min et al., 2023) was adopted\nto evaluate the generated biographies.\nPubHealth(Zhang  et  al.,  2023a)  is  a  task\nin health care domain consisting of true-or-false\nquestions.   Claims  are  represented  about  health\nwith factual information, and the model is tasked\nto verify the authenticity and give the judgment.\nAccuracy was adopted as the evaluation metric.\nArc-Challenge(Bhakthavatsalam et al., 2021)\nis  a  multiple-choice  question  task  about  some\ndaily commonsense science phenomena.   Given\na scientific event that occurs in daily life, the model\nis required to select the correct description among\n3 or 4 optional choices. Accuracy was adopted as\nthe evaluation metric as well.\nB.2    Implementation Details\nRetrieval Evaluator:We fine-tuned the retrieval\nevaluator based on the lightweight T5-large (Raffel\net al., 2020) pre-trained model. Its parameter size\nis much smaller than the most current LLMs (Tou-\nvron et al., 2023a,b; Chowdhery et al., 2023; Anil\net  al.,  2023;  Brown  et  al.,  2020;  Ouyang  et  al.,\n2022; OpenAI, 2023). To ensure all experimental\nresults were comparable with Self-RAG (Asai et al.,\n2023),  the  same  retrieval  results  through  Con-\ntriever (Izacard et al., 2022) were provided by Self-\nRAG and were also adopted in our experiments.\nThe relevance signals for fine-tuning the evaluator\ncan be collected from the existing datasets.  For\nexample,  PopQA (Mallen et al., 2023) provides\nthe golden subject wiki title from wikipedia for\neach  question.   We  can  use  that  to  track  a  not\n100% relevant but rather high-quality passage. We\nutilized that as the relevance labels for fine-tuning\nthe retrieval evaluator.\n3\nOn the other hand,  the\nnegative samples were randomly sampled and we\nused  the  version  provided  by  Self-RAG  (Asai\net  al.,  2023).   Specifically,  the  original  PopQA\ndataset consists of 14k samples, 1,399 of which\nwere used for testing following Self-RAG (Asai\net  al.,  2023),  and  the  remaining  were  used  for\nfine-tuning to avoid information leakage. Besides,\nthe fine-tuned evaluator was transferred and also\nutilized on the Bio, Pub and ARC datasets during\ninference.   The label of positive samples was 1,\nwhile that of negative ones was -1.  At inference,\nthe evaluator scored the relevance from -1 to 1 for\neach document.   The two confidence thresholds\nfor  triggering  one  of  the  three  actions  were  set\nempirically.  Specifically, they were set as (0.59,\n-0.99) in PopQA, (0.5, -0.91) in PubQA and Arc-\nChallenge, as well as (0.95, -0.91) in Biography.\nInternal Knowledge:To obtain fine-grained\nretrieval results, we segmented the retrieved results\ninto internal strips. If a retrieved result is as short as\none or two sentences, it is regarded as an individual\nstrip, otherwise, retrieval documents are required to\nbe split into smaller units which generally consist\nof a few sentences according to the total length.\nThe scale is assumed to include an independent\npiece of information, and the filtering is based on\nthe segments.  We directly adopted the evaluator\nagain for knowledge strips filtering, and the top-k\nis set to 5, filter threshold as -0.5.\nExternal  Knowledge:Google  Search  API\nwas  adopted  to  search  for  the  relevant  URLs,\ntop-k  is  set  to  5,  and  pages  from  Wikipedia\nwill be added preferentially.   The searched web\npages are generally in the form of HTML files,\nwhere  content  is  split  with  special  tokens  like\n<p> and </p>.   Thus an extra segmentation like\nthe knowledge refinement is not required, related\nknowledge paragraphs can be directly selected with\nthe evaluator similar to internal knowledge.\nGenerator:AsCRAGis  a  plug-and-play\nmethod,  all  generation  models  that  can  be  uti-\nlized  in  RAG  fit  our  approach  as  well.To\nbe consistent with baselines for comparison, we\nadopted LLaMA2 (Touvron et al., 2023b) for the\ngeneration.  We first introduced theLLaMA2-hf-\n7bfrom huggingface to generate responses. Since\nSelf-RAG (Asai et al., 2023) fine-tuned LLaMA2\nand  reached  a  new  state-of-the-art  performance\n3\nhttps://huggingface.co/datasets/akariasai/PopQA",
    "on several tasks, we further utilized the launched\nmodel,SelfRAG-LLaMA2-7b, as a new generator to\nbe consistent with their work and study the specific\nimprovement of our method.\nSelf-CRAG:To demonstrate that our plug-and-\nplay approach can be utilized in other concurrent\nstudies,  we  specifically  designed  to  insert  our\nCRAGinto  the  Self-RAG  (Asai  et  al.,  2023)\nframework and named it Self-CRAG. Self-RAG\nis an advanced RAG approach that introduces a\ncritic model to decide whether to retrieve and which\nretrieved document to be referred for generation. It\nmeets our demand for deciding which action to be\ntriggered, thus we replaced the retrieved items in\nSelf-RAG with our processed internal knowledge\nforCorrect, external knowledge forIncorrect,\nand combined knowledge forAmbiguous.\nB.3    Results on PubHealth and Arc-Challenge\nIt  is  worth  mentioning  that  the  performance  on\nPubHealth  based  onLLaMA2-hf-7bwas  much\nworse  than  others.   We  studied  these  cases  and\nfound  thatLLaMA2-hf-7bis  relatively  weak  in\ninstruction  comprehension.    Most  of  the  cases\nfail to generateTrueorFalsein such a binary-\nquestion task, resulting in a low accuracy during\nthe evaluation. This situation somewhat happens in\nArc-Challenge as well, when the model is tasked\nto generate the index of a candidate."
  ]
}