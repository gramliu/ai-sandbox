{
  "key": "7ZSCH5PA",
  "url": "http://arxiv.org/pdf/2212.09741",
  "metadata": {
    "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
    "abstract": "  We introduce INSTRUCTOR, a new method for computing text embeddings given\ntask instructions: every text input is embedded together with instructions\nexplaining the use case (e.g., task and domain descriptions). Unlike encoders\nfrom prior work that are more specialized, INSTRUCTOR is a single embedder that\ncan generate text embeddings tailored to different downstream tasks and\ndomains, without any further training. We first annotate instructions for 330\ndiverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive\nloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are\nunseen during training), ranging from classification and information retrieval\nto semantic textual similarity and text generation evaluation. INSTRUCTOR,\nwhile having an order of magnitude fewer parameters than the previous best\nmodel, achieves state-of-the-art performance, with an average improvement of\n3.4% compared to the previous best results on the 70 diverse datasets. Our\nanalysis suggests that INSTRUCTOR is robust to changes in instructions, and\nthat instruction finetuning mitigates the challenge of training a single model\non diverse datasets. Our model, code, and data are available at\nhttps://instructor-embedding.github.io.\n",
    "published": "2022-12-19T18:57:05Z"
  },
  "text": [
    "One Embedder, Any Task: Instruction-Finetuned Text Embeddings\nHongjin Su\n♠∗\nWeijia Shi\n♣∗\nJungo Kasai\n♣\nYizhong Wang\n♣\nYushi Hu\n♣\nMari Ostendorf\n♣\nWen-tau Yih\n♢\nNoah A. Smith\n♣♡\nLuke Zettlemoyer\n♣♢\nTao Yu\n♠\n♠\nThe University of Hong Kong\n♣\nUniversity of Washington\n♢\nMeta AI\n♡\nAllen Institute for AI\n{hjsu,tyu}@cs.hku.hk, {yushihu,ostendor}@uw.edu  scottyih@meta.com\n{swj0419,jkasai,yizhongw,nasmith,lsz}@cs.washington.edu\nAbstract\nWe  introduceINSTRUCTOR,  a  new  method\nfor computing text embeddings given task in-\nstructions:  every  text  input  is  embedded  to-\ngether with instructions explaining the use case\n(e.g., task and domain descriptions).  Unlike\nencoders from prior work that are more special-\nized,INSTRUCTORis a single embedder that\ncan generate text embeddings tailored to differ-\nent downstream tasks and domains,withoutany\nfurther training. We first annotate instructions\nfor 330 diverse tasks and trainINSTRUCTOR\non this multitask mixture with a contrastive loss.\nWe evaluateINSTRUCTORon 70 embedding\nevaluation tasks (66 of which areunseenduring\ntraining), ranging from classification and infor-\nmation retrieval to semantic textual similarity\nand text generation evaluation.INSTRUCTOR,\nwhile having an order of magnitude fewer pa-\nrameters than the previous best model, achieves\nstate-of-the-art performance, with an average\nimprovement  of  3.4%  compared  to  the  pre-\nvious best results on the 70 diverse datasets.\nOur  analysis  suggests  thatINSTRUCTORis\nrobust to changes in instructions, and that in-\nstruction finetuning mitigates the challenge of\ntraining a single model on diverse datasets. Our\nmodel, code, and data are available athttps:\n//instructor-embedding.github.io.\n1    Introduction\nText embeddings represent discrete text inputs (e.g.,\nsentences,  documents,  and  code)  as  fixed-sized\nvectors  that  can  be  used  in  many  downstream\ntasks.  These tasks include semantic textual sim-\nilarity (Agirre et al., 2012; Marelli et al., 2014; Cer\net al., 2017; Lin et al., 2018), information retrieval\n(Mitra et al., 2017; Karpukhin et al., 2020; Izac-\nard et al., 2022), automatic text evaluation (Zhang\net al., 2020; Sellam et al., 2020; Hessel et al., 2021),\nprompt retrieval for in-context learning (Liu et al.,\n2022; Rubin et al., 2022; Su et al., 2022), and be-\nyond. Recently, we have seen dramatic advances\n∗\nEqual contribution.\nFigure 1: At execution time,INSTRUCTORgenerates\nembeddings based on both the text input and the task\ninstruction. The same input (e.g.,Who sings the song\n“Love Story”?)  will be encoded into different embed-\ndings, depending on the end task (e.g.,duplicateques-\ntiondetection,informationretrieval, andtopicclassifi-\ncation).\nin learning text embeddings (Kiros et al., 2015;\nConneau et al., 2017; Logeswaran and Lee, 2018;\nReimers and Gurevych, 2019; Gao et al., 2021;\nNi et al., 2021, 2022) that perform well on their\nintended tasks or datasets.\nHowever, most existing embeddings can have\nsignificantly   degraded   performance   when   ap-\nplied  to  new  tasks  or  domains  (Thakur  et  al.,\n2021;  Muennighoff  et  al.,  2022).   For  example,\nDPR (Karpukhin et al., 2020) is stronger for re-\ntrieval than text similarity tasks, and vice versa for\nSimCSE (Gao et al., 2021). Moreover, existing em-\nbeddings usually perform poorly when applied to\nthe same type of task but in different domains such\nas medicine and finance.  A common method to\naddress this issue is to further finetune the embed-\ndings on datasets in downstream tasks and domains,\nwhich often requires a lot of annotated data (Guru-\narXiv:2212.09741v3  [cs.CL]  30 May 2023",
    "Text Similiarty\nWhy do rockets look white?\nQuestion Answering\nRetrieve documents that can help answer \nthe question: \nFind documents that can help verify the fact: \nThe Ten Commandments is an epic film.\nFact Checking\nSentiment Analysis\nClassify the sentiment of the sentence:\nYou should see their decadent dessert menu\n...\nMeasuring the similarity between sentences:\nHow can I be a good geologist?\n\nWhat should I do to be a great geologist?\nText Similarity\n330 datasets\n12 datasets\nClassification\n15 datasets\nRetrieval\nClustering\n11 datasets\n4 datasets\nReranking\nSemantic Similarity\n10 datasets\nPair Classification\n3 datasets\n(       ,       ) \n(       ,       ) \nTrain\nEval\n11 datasets\nPrompt Retrieval\nText Evaluation\n4 datasets\nFigure 2:INSTRUCTORtraining and evaluation pipeline.INSTRUCTORis a single embedding model that takes not\nonly text inputs but also task instructions, thereby creating task- and domain-aware embeddings. It is trained on a\nmultitask mixture of 330 diverse datasets with human-written task instructions (MEDI dataset, §2.3). After training\non MEDI (left),INSTRUCTORis evaluated on a variety of 70 embedding datasets (66 of which are not seen during\ntraining), spanning various downstream applications (right).INSTRUCTORoutperforms the prior best model by an\naverage of 3.4% over the 70 diverse datasets.\nrangan et al., 2020). In this paper, we hypothesize\nthat text embeddings (even for thesametext input)\ncan be adjusted to different downstream applica-\ntions using task and domain descriptions,without\nfurther task- or domain-specific finetuning.\nWe introduceINSTRUCTOR(Instruction-based\nOmnifariousRepresentations), a single multitask\nmodel that generates task- and domain-aware em-\nbeddings given a text input and its task instructions.\nIt achieves state-of-the-art performance on mas-\nsively many downstream embedding tasks with-\nout any training.  At the core of our approach is\ninstruction-based finetuning (Zhong et al., 2021;\nMin et al., 2022; Sanh et al., 2022; Wei et al., 2022):\nwe embed every input together with its end task\nand domain instruction, departing from prior ap-\nproaches to embeddings that only take text input.\nINSTRUCTORembeds the same input into differ-\nent vectors for different end goals (e.g.,Who sings\nthe song “Love Story”?is embedded into three\ndifferent vectors for different tasks in Fig. 1). As\nshown in Fig. 2,INSTRUCTORis trained on MEDI,\nour new collection of 330 text embedding datasets\nnewly annotated with human-written task instruc-\ntions (§2.3).  We trainINSTRUCTORwith a con-\ntrastive loss over all datasets that maximizes the\nsimilarity between semantically related text pairs\nwhile minimizing unrelated pairs.\nWe extensively evaluateINSTRUCTORon di-\nverse domains (e.g., finance, medicine, and news)\nand a variety of downstream applications (a total of\n70 embedding evaluation datasets, including 66not\nseen during training), spanning classification, se-\nmantic textual similarity, information retrieval, text\ngeneration evaluation, and prompt retrieval for in-\ncontext learning.INSTRUCTORsignificantly out-\nperforms prior state-of-the-art embedding models\nby an average of 3.4% over the 70 diverse datasets.\nINSTRUCTORalso outperforms a variant that is\ntrainedwithouttask instructions (§4), demonstrat-\ning the importance of instructions to create task-\naware embeddings. Our analysis shows that instruc-\ntion finetuning addresses the challenge of training\nasinglemodel ondiversedatasets (§4.1). Further,\nwe demonstrate that the task diversity of MEDI\nmakes the performance ofINSTRUCTORparticu-\nlarly robust to paraphrases in instructions (§4.2).\nOverall, these results strongly suggest that instruc-\ntion finetuning should be adopted broadly for text\nembeddings, which we support by sharing all of\nour models and code.\n2INSTRUCTOR\nINSTRUCTORencodes inputs together with task\ninstructions, thereby providing task-specific repre-\nsentations that can be used for many downstream\nlanguage  tasks,withoutany  additional  training.\nHere we introduce the architecture ofINSTRUC-\nTOR(§2.1),  present  how  we  perform  multitask\ninstruction-based finetuning (§2.2), and describe",
    "how we collect and annotate the MEDI training\ndata (§2.3). By default, we refer \"task\" to a dataset,\nand use them interchangeably throughout the paper,\nwhile a \"task category\", such as Retrieval, includes\nmany tasks.\n2.1    Embedding Architecture\nWe buildINSTRUCTOR, based on the single en-\ncoder architecture (Izacard and Grave, 2021; Ni\net al., 2021, 2022). Following prior work (Ni et al.,\n2021, 2022), we use GTR models as the backbone\nencoder (GTR-Base forINSTRUCTOR-Base, GTR-\nLarge forINSTRUCTOR, GTR-XL forINSTRUC-\nTOR-XL). The GTR models are initialized from T5\nmodels, pretrained on a web corpus, and finetuned\non information search datasets.  The availability\nof different sizes in the GTR model family allows\nus to explore the scaling behaviors of instruction-\nfinetuned embedding models. Given an input text\nxand a task instructionI\nx\n, INSTRUCTORencodes\ntheir concatenationI\nx\n⊕x.  We then generate a\nfixed-sized, task-specific embeddingE\nI\n(I\nx\n,x)by\napplying mean pooling to the last hidden represen-\ntations over the tokens inx.\n2.2    Training Objective\nINSTRUCTORis  trained  by formulating  a wide\nvariety of tasks as a text-to-text problem of distin-\nguishing good/bad candidate outputsy∈{y\n+\n,y\n−\ni\n}\ngiven an inputx, where a training sample corre-\nsponds to the tuple(x,I\nx\n,y,I\ny\n), withI\nx\nandI\ny\nbeing instructions associated withxandy, respec-\ntively. For example, in a retrieval task,xis a query,\nand good/badyis a relevant/irrelevant document\nfrom some document collection. For a textual simi-\nlarity task, the input and output have a similar form\nand typically come from the same source collection.\nFor a classification task, training samples can be\nformed by choosingyas text sequences associated\nwith the same vs. different classes for good vs. bad\nexamples (Details about pair construction are in\n§2.3). The input and output instructions depend on\nthe task. Forsymmetrictasks such as textual sim-\nilarity, where the input and output have the same\nform and encoding objective, the instructions are\nthe same. Forasymmetrictasks such as retrieval,\nwhere the input is a single sentence query and the\noutput is a document, the instructions reflect that\ndifference.\nThe goodness of candidateyfor inputxis given\nby  similaritys(x,y)that  is  the  cosine  between\ntheir INSTRUCTOR embeddings:\ns(x,y)=cos(E\nI\n(I\nx\n⊕x),E\nI\n(I\ny\n⊕y))\nFollowing Ni et al. (2021), we maximize the simi-\nlarity between positive pairs(x,y\n+\n)and minimize\nnegative pairs{(x,y\n−\ni\n)}\nk\ni=1\n, wherekdenotes the\nnumber of negative pairs per positive pair. Specifi-\ncally, our training objective is:\nL=\ne\ns(x,y\n+\n)/γ\n∑\ny∈B\ne\ns(x,y)/γ\n,\nwhereγis the softmax temperature andBis a union\nof(x,y\n+\n)and{(x,y\n−\ni\n)}\nk\ni=1\n. Further following Ni\net al. (2021),  we compute the same loss withx\nandyswapped and add it to the previous loss (i.e.,\nbidirectional in-batch sampled loss).\n2.3    MEDI: Multitask Embedding Data with\nInstructions\nThere  are  no  existing  datasets  that  consist  of  a\nvariety of tasks for embedding training with in-\nstructions.  We thus construct a collection of330\ndatasets with instructions across diverse task cate-\ngories and domains:MultitaskEmbeddingsData\nwithInstructions (MEDI).\nDataConstructionWebuildMEDI\nbycombining300datasetsfromSuper-\nNaturalInstructions    (super-NI;    Wang    et    al.,\n2022b) with 30 datasets from existing collections\ndesigned for embedding training.\nThe  super-NI  datasets  come  with  natural  lan-\nguage instructions, but positive and negative pairs\nare not provided. We construct these pairs by using\nSentence-T5  embeddings  (Ni  et  al.,  2022),\n1\nde-\nnoted withE(⋅).   For the classification datasets,\nwe  calculate  the  pairwise  cosine  similarity  be-\ntween examples based on input text embeddings\ncos(E(x\ni\n),E(x\nj\n)).   An examplex\ni\nwith a high\nsimilarity tox\nj\nis used to create a positive pair if\nboth examples have the same class label (y\n+\nj\n=y\ni\n),\nand a negative pair if the labels differ (y\n−\nj\n≠y\ni\n).\nFor the remaining tasks where the output labels\nare text sequences, the following scores are first\ncomputed:\ns\npos\n=cos(E(x\ni\n),E(x\nj\n))+cos(E(y\ni\n),E(y\nj\n))\ns\nneg\n=cos(E(x\ni\n),E(x\nj\n))−cos(E(y\ni\n),E(y\nj\n))\n1\nWe do not include instruction for Sentence-T5 as it is not\nfine-tuned with instructions.",
    "Task type# of DatasetsTaskInstruction\nRetrieval15Natural Question (BEIR)\nQuery instruction:Represent the Wikipedia question for re-\ntrieving supporting documents:,Doc instruction:Represent the\nWikipedia document for retrieval:\nReranking4MindSmallReranking\nQuery instruction:Represent the News query for retrieving arti-\ncles:Doc instruction:Represent the News article for retrieval:\nClustering11MedrxivClusteringS2SRepresent the Medicine statement for retrieval:\nPair  Clas-\nsification\n3TwitterSemEval2015Represent the Tweet post for retrieving duplicate comments:\nClassification12ImdbClassification\nRepresent the Review sentence for classifying emotion as posi-\ntive or negative:\nSTS10STS12Represent the statement:\nSummarization1SummEval\nRepresent the Biomedical summary for retrieving duplicate sum-\nmaries:\nText Eval-\nuation\n3MscocoRepresent the caption for retrieving duplicate captions:\nPrompt\nRetrieval\n11GeoQuery\nRepresent the Geography example for retrieving duplicate exam-\nples:\nTable 1: Instruction examples for evaluation datasets. Our embedding evaluation includes 70 diverse datasets in\n9 different downstream applications, ranging from classification and semantic textual similarity to information\nretrieval and text generation evaluation. The first two tasks areasymmetricand require two distinct instructions.\nInstructions for the MEDI training data can be found in Tables 7 and 8 in the appendix.\nWe select example pairs with the highests\npos\nas\npositive pairs and highests\nneg\nas hard negative\npairs. We use one hard negative together with in-\nbatch sampled negatives in the training. Our later\nanalysis shows that the training data from super-NI\nparticularly improve the instruction robustness in\nevaluation due to the diverse task definitions (§4.2).\nThe other 30 embedding training datasets come\nfrom the Sentence Transformers embedding data,\n2\nKILT (Petroni et al., 2021), and MedMCQA (Pal\net  al.,  2022).These  30  datasets  already  con-\ntain  positive  pairs;  a  few  of  them,  such  as  MS-\nMARCO (Bajaj et al., 2016) and Natural Questions\n(Kwiatkowski et al., 2019), also contain hard nega-\ntive pairs. Following Ni et al. (2021), we use four\nnegative pairs (hard or in-batch negatives) during\nthe model finetuning process.  Since all of these\ndatasets  do  not  have  instructions,  we  develop  a\nunified  instruction  template  and  manually  write\na specific prompt for each dataset,  as described\nnext.\n3\nWe release these instructions together with\nour MEDI data.\nInstruction AnnotationEach training instance\nfrom MEDI is a tuple(x,I\nx\n,y,I\ny\n), where the nat-\n2\nhttps://huggingface.co/datasets/\nsentence-transformers/embedding-training-data.\n3\nAll prompts are reviewed by multiple authors indepen-\ndently to make sure they consistently follow our template.\nural language instructionsI\nx\nandI\ny\ndescribe how\nthe embeddings ofxandyare used for the task. For\nexample, in open-domain QA (e.g., Natural Ques-\ntions in Table 1),I\nx\nis “Represent the Wikipedia\nquestion for retrieving supporting documents; In-\nput:  ,” andI\ny\nis “Represent the Wikipedia docu-\nment for retrieval; Input: .”\nTo   make   instructions   consistent   across   all\ndatasets in MEDI, we design a unified instruction\nformat that consists of the following parts (see Ta-\nble 4 in the appendix for instances of each part):\n•Text Typespecifies the type of input text that\nwe encode using the embedding model.  For\nexample, for an open-domain QA task, the\ninput type of the query is a question, while the\ninput type of the target is a document.\n•\nTask Objective (Optional)describes the ob-\njective of how the input text is used in a task.\nFor example, for a classification task, the task\nobjective is to classify the sentence into some\ncategory, while the task objective of the re-\ntrieval is to retrieve a relevant document. Be-\ncause not all sentences are associated with a\nspecific task (e.g., STS targets general encod-\ning), we make this part optional.\n•Domain  (Optional)describes  the  task  do-\nmain. For example, for NewsIR, the domain\nof the task is news. Because not all tasks spec-",
    "BenchmarkMTEBBillboard    PromptAvg.\nTask categoryRetri.Rerank    Cluster    Pair.    Class.    STS    Sum.    Avg.    Text Eval.Retri.\n# datasets154113121015631170\nSmall Models for reference (<500M)\nSimCSE (110M)21.947.533.473.767.379.123.348.729.458.348.2\ncoCondenser (110M)33.051.837.681.764.776.529.552.431.559.651.8\nContriever (110M)41.953.141.182.566.776.530.456.029.057.353.2\nGTR-Large (335M)47.455.441.685.367.178.229.558.331.259.855.1\nINSTRUCTOR (335M)47.657.545.385.973.983.231.861.636.963.258.4\nRelative gain (%)+0.4+4.5+8.9+0.7    +10.1    +6.4+7.8+5.7+18.3+5.7+5.9\nLarge Models for reference(≥500M)\nSent-T5-XXL (4.8B)42.256.443.785.173.482.630.159.533.961.556.5\nGTR-XXL (4.8B)48.156.742.486.167.478.430.658.932.060.855.8\nSGPT-NLI (5.8B)32.352.337.077.070.180.530.453.729.657.951.9\nGTR-XL (1.5B)48.056.041.586.167.177.830.258.432.060.455.5\nINSTRUCTOR-XL (1.5B)49.357.344.786.673.283.132.061.834.168.6\n58.8\nRelative gain (%)+2.7+2.3+7.7+0.6+9.1+6.9+6.0+5.8+6.6+13.6+5.9\nTable 2: Results on the massive text embedding benchmark (MTEB; Muennighoff et al., 2022), Billboard (Kasai\net al., 2022a), and prompt retrieval (Su et al., 2022). The last column averages performance scores over 9 categories\n(7 from MTEB, 1 from Billboard, and 1 from prompt retrieval).  Out of the 70 evaluation datasets, 66 (50 from\nMTEB, 3 from BillBoard, and 11 from prompt retrieval) are unseen tasks during finetuning. Retri., Pair., Class.,\nSum., Text Eval. refer to retrieval, pair classification, classification, summarization, and text evaluation, respectively.\nInstruction finetuning improves performance by 5.9% compared to GTR(335M/1.5B) and achieves 3.4% and 4.1%\nperformance gains over the state-of-the-art model Sent-T5-XXL for INSTRUCTOR (335M/1.5B). The relative gain\n(%) indicates INSTRUCTOR’s improvement relative to the original GTR model of the same size.\nify a domain (e.g.,  STS deals with general\nstatements),this part is also optional.\nThe final instruction takes the following format:\n“REPRESENT THE(DOMAIN)TEXTTYPEFOR\nTASKOBJECTIVE:.\" Appendix 8 shows instruc-\ntions for each dataset in MEDI.\n3    Experiments\nWe trainINSTRUCTORon the MEDI data and eval-\nuate it on a wide range of 70 downstream tasks.\nSpecifically, we use the MTEB benchmark from\nrecent work (Muennighoff et al., 2022), which con-\nsists of 56 datasets over 7 diverse task categories,\nsuch as classification, reranking, and information\nretrieval. We then further applyINSTRUCTORto\nprompt retrieval for in-context learning and text\ngeneration  evaluation.   In  all  three  settings,IN-\nSTRUCTORachieves  the  state-of-the-art  perfor-\nmance. See Appendix §A and §B for our detailed\nsettings.\n3.1    Main Results\nTable 2 presents the results fromINSTRUCTORand\nthe baselines over the three benchmarks: MTEB,\nBillboard, and prompt retrieval. We conduct head-\nto-head comparison betweenINSTRUCTORand\nGTR models with the same size. We also include\nthe performance of other representative models for\nreference, while they are not meant for direct com-\nparison.\nINSTRUCTORachieves  the  best  performance\non all three benchmarks on average.  Compared\nto  GTR-Large  (335M),  from  whichINSTRUC-\nTORis initialized, instruction finetuning enhances\nthe  performance  by  5.7%,  18.3%,  and  5.7%  in\nMTEB,  Billboard,  and  prompt  retrieval  respec-\ntively. Specifically, among all task categories,IN-\nSTRUCTOR(335M) demonstrates large improve-\nments  over  GTR-Large  on  the  text  evaluation\n(18.3%),  classification  (10.1%),  and  clustering\ntasks (8.9%). Particularly noteworthy isINSTRUC-\nTOR’s performance compared to the previous state-\nof-the-art model, Sent-T5-XXL (58.4 vs. 56.5 on\naverage), despite the fact thatINSTRUCTORhas\none order of magnitude fewer parameters (335M\nvs. 4.8B).\nAs expected,  the retrieval-based models (e.g.,\nGTR-XXL) show strong performance on retrieval\nand reranking but significantly lag behind on STS\nand  classification.   Conversely,  similarity-based\nmodels (e.g., Sent-T5-XXL) perform well on STS,\nclassification, and text evaluation, but not on re-\ntrieval. It suggests that these baselines tend to gen-\nerate  specialized  embeddings  that  only  excel  at",
    "certain tasks, whileINSTRUCTORprovides univer-\nsal embeddings that perform well on diverse task\ncategories.\n4    Analysis and Ablations\nWe demonstrateINSTRUCTORenables universal\ntext embeddings for many diverse tasks. Here we\nanalyze our results from various perspectives: the\nimportance of instructions (§4.1), instruction ro-\nbustness (§4.2) and complexity (§4.3), model sizes\n(§4.4), domain shifts (§4.5), and qualitative anal-\nysis (§4.6).  By default, we report average perfor-\nmance across all categories.\nw/o instruction w/ instruction\n53\n54\n55\n56\n57\n58\n59\nAverage score\nGTR-large\nTuned on Sym.\nTuned on Asym.\nTuned on Sym.& Asym. (MEDI)\nFigure 3:  Average (by category) performance ofIN-\nSTRUCTOR(with and without instructions) over three\ntypes of training data: symmetric data, asymmetric data,\nor both (entire MEDI). The model finetuned with in-\nstructions on both data is the originalINSTRUCTOR\nmodel. The diverse training data with both typeshurt\nthe performance when finetuned without instructions\nbutimprovewhen instructions are used.\n4.1    Instructions Enable Diverse Training\nHere we analyze the importance of instructions\nwhen training data are diverse. We first split MEDI\ninto symmetric (e.g., text similarity) and asymmet-\nric groups (e.g., open-domain QA), as defined in\n§2.3 (see Table §5 in the appendix for details about\nthe symmetric and asymmetric groups).  We then\ntrainINSTRUCTORwithorwithoutinstructions on\neach group separately.\nAs  shown  in  Fig.  3,INSTRUCTORfinetuned\nwithoutinstructions  yields  performance  similar\nto or better than the original GTR model (dotted\nline), if the data are symmetric or asymmetriconly.\nHowever,INSTRUCTORsuffers if finetuned with-\nout task instructions on the combination of both\ntypes of data (entire MEDI). In contrast, finetun-\ning with instructions enables the model to benefit\nfrom the combination of symmetric and asymmet-\nric data (see that the rightmost bar gets additive\nperformance gains from the asymmetric and sym-\nmetric tasks). This result demonstrates the impor-\ntance of instruction finetuning when diverse data\nare used for embedding training.  Note that train-\ning on symmetric tasks only without instructions\nis similar to Sent-T5. Similarly, training on asym-\nmetric tasks only without instructions is similar to\nGTR, which is also trained on asymmetric open-\ndomain QA datasets.  Departing from these prior\nmethods, instruction-based finetuning enables di-\nverse training on both types.\n4.2    Instruction Robustness\nw/o super-NIw/ super-NI\n52\n53\n54\n55\n56\n57\n58\n59\nworst-performing instruction \nbest-performing instruction\nFigure 4: Comparison of the model performance across\nfive paraphrased instructions. W/o super-NI (w/ super-\nNI) refers to the inclusion (exclusion) of the 300 datasets\nfrom Super-NaturalInstructions in MEDI. These diverse\ndatasets with task instructions improve the robustness of\nINSTRUCTORto instruction paraphrases (i.e., smaller\nperformance gaps between best- and worst-performing\ninstructions).\nPrevious work (Sanh et al., 2022; Zhou et al.,\n2022) shows that instruction-finetuned language\nmodels are not robust to paraphrased instructions.\nHere we measureINSTRUCTOR’s robustness to\nvariation in human-written instructions.\nSpecifically, we write five paraphrased instruc-\ntions  for  all  evaluation  datasets  (Table  6  in  Ap-\npendix) and measureINSTRUCTOR’s performance\ngap between the best-performing and the worst-\nperforming instructions. Fig. 4 shows that inclusion\nof 300 super-NI datasets is critical to the robustness\nofINSTRUCTOR. Removing these datasets from\ntraining (w/o super-NI) substantially increases the\nperformance  gap  between  the  best-  and  worst-",
    "performing instructions, suggesting that super-NI’s\ndiverse instructions help the model handle different\nformats and styles.\n4.3    Complexity of Instructions\nN/Atagsimpledetail\nComplexity of instruction\n53\n54\n55\n56\n57\n58\n59\nAverage score\nINSTRUCTOR\nGTR-large\nFigure 5: Average performance over varying degrees of\ninstruction details. As the instructions become more de-\ntailed, the performance improves. N/A: no instructions\nare given; tag:  dataset names are prepended; simple:\none word or two for the task domain are given (e.g.,\nWikipedia);  and  detailed:  our  proposed  instructions\n(§2.3).\nHere we further analyze the role of instructions\nover varying degrees of their complexity. Specifi-\ncally, we consider four levels of instruction com-\nplexity: N/A (no instructions), dataset tags, simple\ninstructions, and detailed instructions (the original\ninstruction format, §2.3). In the dataset tag setup,\neach example is prepended with its dataset name.\nFor instance, on the Natural Questions dataset, the\nquery is formatted as\"Natural Questions; Input:\nwho sings the song Love Story\").  In the simple\ninstruction setup, we use one or two words to de-\nscribe the domain (e.g., for Natural Questions, the\ninput query isWikipedia Questions; Input: who\nsings the song Love Story).   Fig.  5  shows  their\naverage performances across all  task categories.\nEven with trivial dataset tags,INSTRUCTORout-\nperforms the original GTR model, illustrating the\neffectiveness of instructions for diverse training.\nAs more information is provided in the instruction\n(from tag to simple and from simple to detail), we\nobserve consistent improvements.\n4.4    Model Sizes and Instruction Finetuning\nFig. 6 studies the influence of model sizes. Specifi-\ncally, we use GTR-Base (0.1B), GTR-Large (0.3B),\nand GTR-XL (1.5B). They are pretrained on the\nsame corpus and differ only in the encoder size\n0.1B0.3B1.5B\nModel Size (# parameters)\n54\n55\n56\n57\n58\n59\nAverage score\nINSTRUCTOR\nGTR\nFigure 6: Average performance comparisons with vary-\ning sizes of models.INSTRUCTORbenefits more from\nscaling up, perhaps because instructions require addi-\ntional computations.\n(the embedding sizes are the same). We compare\nmodels of various sizes and report the average per-\nformance across all the categories. As the encoder\ntransformer model scales up, the performance con-\ntinues to increase for both GTR andINSTRUCTOR.\nNonetheless, the improvement inINSTRUCTORis\nmore pronounced,  perhaps because embeddings\nwith  instructions  benefit  from  larger  capacities.\nThis implies that large models are more generaliz-\nable to compute texts in various domains and task\ntypes, providing embeddings for general purposes.\nFurther scale-ups are left to future work.\n4.5    Instructions Mitigate Domain Shifts\nOne advantage of instruction-based finetuning is\nthat it improves models’ ability to generalize to\nunseen domains and tasks.   To demonstrate this\neffectiveness, we found three unseen domains that\nINSTRUCTORwas not trained on: geography, bi-\nology, and civil comments.  As shown in Table 3,\nINSTRUCTORlargely improves (above the aver-\nage improvement) GTR-Large’s performance on\nall three domains, indicating that instructions can\nhelp more when applying models to unseen or un-\ncommon domains.\n4.6    Qualititive Analysis\nIn this qualitative analysis, we use T-SNE (van der\nMaaten and Hinton, 2008) to visualize two example\nof classification with and without instructions. The\ndesired outcome is, for pairs with the same senti-\nment to be closer together, and pairs with different\nsentiment to be farther apart. As shown in Fig. 7,\nwithout instructions, the green dot pairs (different",
    "ModelGeographyBiologyCivil\nGTR-Large53.425.771.8\nINSTRUCTOR64.231.377.2\nRelative gain (%)+20.2+21.8+7.5\nTable 3: Results of GTR-Large andINSTRUCTORon\nunseen  domains:  geography,  biology  and  civil  com-\nments.   Domain-specific datasets benefit particularly\nfrom instruction finetuning. More results can be found\nin Tables 9 and 10 in the appendix; they also show simi-\nlar trends.\nFigure 7: Visualization of pair classification examples\nwithout (dot) and with instruction (dot with a solid bor-\nder).  The red dot pairs that have the same sentiment\nshould be closer together, while the green dot pairs with\ndifferent sentiment should be farther apart. When em-\nbedded with the instruction, the distance between the\ngreen dot pair becomes farther.\nsentiment) are closer together in the embedding\nspace,  while  the  red  dot  pairs  (same  sentiment)\nare farther apart. However, with instructions, our\nmethod (INSTRUCTOR) successfully encodes the\nred dot pairs into close embeddings and correctly\nclassifies the pairs. The distance between the green\ndot pairs with different sentiment is also larger in\nthe embedding space with instructions.\n5    Related Work\nText  EmbeddingsText  embeddings  are  use-\nful in many applications such as information re-\ntrieval (Thakur et al., 2021), text similarity (Gao\net al., 2021), prompt retrieval for in-context learn-\ning  (Su et al., 2022), classification (Reimers and\nGurevych, 2019), and beyond.  Much prior work\ndevelops different embedding models for different\napplications. For example, SBERT (Reimers and\nGurevych, 2019) and SimCSE (Gao et al., 2021)\nare applied solely to text similarity and classifi-\ncation tasks, while DPR (Karpukhin et al., 2020)\nand Contriever (Izacard et al., 2022) focus on in-\nformation retrieval.  Different from Sentence-T5\ntrained only on symmetric data or GTR trained\nonly on asymmetric data, we combine both groups\nof datasets and build MEDI, which is then used\nto trainINSTRUCTORwith instructions.  Muen-\nnighoff et al. (2022) introduced the massive text\nembedding benchmark (MTEB), which can be used\nto evaluate embedding models on a variety of em-\nbedding tasks, spanning reranking, classification,\ninformation retrieval, bitext mining, pair classifica-\ntion, STS, and summarization.  Their benchmark\nshows that models performing well on one task\nmay not perform well on other tasks.  The poor\nzero-shot transfer abilities of existing embedding\nmodels make it difficult to use them in applications\nwhere only few labeled data are available. This mo-\ntivates us to develop a single embedding model that\nis applicable to a variety of tasks and has better gen-\neralization to unseen tasks. Wang et al. (2022a) re-\ncently proposed E5, weakly-supervised contrastive\npre-trained text embeddings, which achieve strong\nperformance across various tasks on the MTEB\nbenchmark, employing a larger embedding dimen-\nsion compared to INSTRUCTOR.\nInstruction  FinetuningRecent  work  demon-\nstrated that instruction-finetuned language models\ncould perform new tasks given a natural language\ninstruction (Mishra et al., 2022; Zhong et al., 2021;\nMin et al., 2022; Sanh et al., 2022; Wei et al., 2022;\nWang et al., 2022b; Ouyang et al., 2022). Nonethe-\nless, instruction finetuning has yet to be studied in\nthe context of broadly-applicable embeddings. In\nthis work, we explore finetuning embedding mod-\nels to follow human instructions where the instruc-\ntion specifies eventual use cases. Concurrent work\ndemonstrated that instructions could facilitate in-\nformation retrieval (Asai et al., 2022),  which is\nrelated to ourINSTRUCTORdesign.  They used\ninstructions to build a task-aware retrieval system\nand conducted evaluations on the retrieval task; we\nbuild a general-purpose embedding model with in-\nstructions that can be applied to 8 tasks categories\n(Fig. 2), including retrieval, text similarity, cluster-\ning, and text evaluation.\n6    Conclusion\nWe introducedINSTRUCTOR, a single model that\ncreates broadly-applicable text embeddings using\nnatural  language  instructions.    We  constructed\nMEDI, a collection of diverse datasets, to finetune\nINSTRUCTORwith  instructions.   Our  extensive",
    "experiments showed thatINSTRUCTORachieves\nstate-of-the-art  performance  on  text  embedding\nbenchmarks, as well as prompt retrieval for few-\nshot in-context learning. We hope that researchers\nand practitioners will benefit from our embeddings\nor our datasets for tasks of their interest.\n7    Limitations\nAlthoughINSTRUCTORsignificantly improves the\nbaseline  GTR  performance,  we  were  only  able\nto use four negative examples during the model\nfinetuning process due to computation constraints.\nHowever,  negative  examples  have  been  shown\nto  play  an  important  role  in  contrastive  learn-\ning (Robinson et al., 2021).  We hope that future\nwork will scale up the number of negatives used\nduring finetuning and investigate various methods\nfor mining hard negatives. Additionally, we do not\nhave enough computation resources to apply mul-\ntitask instruction finetuning to GTR-XXL (4.8B\nparameters), which is also an area for future explo-\nration.\nAt the core ofINSTRUCTORis the instruction\ndesign.  While our current unified instruction for-\nmat has demonstrated effectiveness, future research\ncan  explore  other  instructional  elements  to  fur-\nther improve performance. For example, previous\nwork (Wang et al., 2022b) have shown that incor-\nporating demonstration examples and explanations\ncan be beneficial for instruction-finetuned language\nmodels.\nAcknowledgements\nWe thank Akari Asai, Jack Lin, Minghan Li, and\nthe ARK group at UW for their helpful feedback\non this work.\nReferences\nEneko  Agirre,  Daniel  Cer,  Mona  Diab,  and  Aitor\nGonzalez-Agirre. 2012.   SemEval-2012 task 6:  A\npilot on semantic textual similarity.  InProc. of Se-\nmEval.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier  Izacard,  Sebastian  Riedel,  Hannaneh  Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016.  MS MARCO: A human generated machine\nreading comprehension dataset. InProc. of CoCo.\nLoïc  Barrault,   Magdalena  Biesialska,   Ond\nˇ\nrej  Bo-\njar,  Marta  R.  Costa-jussà,  Christian  Federmann,\nYvette Graham, Roman Grundkiewicz, Barry Had-\ndow,   Matthias  Huck,   Eric  Joanis,   Tom  Kocmi,\nPhilipp Koehn, Chi-kiu Lo, Nikola Ljubeši\n ́\nc, Christof\nMonz, Makoto Morishita, Masaaki Nagata, Toshi-\naki bnghvtcf Nakazawa, Santanu Pal, Matt Post, and\nMarcos Zampieri. 2020. Findings of the 2020 confer-\nence on machine translation (WMT20). InProc. of\nWMT.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference.  In\nProc. of EMNLP.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017 task\n1: Semantic textual similarity-multilingual and cross-\nlingual focused evaluation. InProc. of SemEval.\nArman  Cohan,  Sergey  Feldman,  Iz  Beltagy,  Doug\nDowney,  and  Daniel  S.  Weld.  2020.   SPECTER:\nDocument-level    representation    learning    using\ncitation-informed transformers. InProc. of ACL.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. InProc. of LREC.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault,  and  Antoine  Bordes.  2017.   Supervised\nlearning of universal sentence representations from\nnatural language inference data. InProc. of EMNLP.\nWilliam Coster and David Kauchak. 2011. Simple en-\nglish Wikipedia:  a new text simplification task.  In\nProc. of ACL.\nEmily  Dinan,  Stephen  Roller,  Kurt  Shuster,  Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof Wikipedia:  Knowledge-powered conversational\nagents. InProc. of ICLR.\nHady  Elsahar,  Pavlos  Vougiouklis,  Arslen  Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018.  T-rex:  A large scale\nalignment of natural language with knowledge base\ntriples. InProc. of LREC.\nAlexander R Fabbri, Wojciech Kry\n ́\nsci\n ́\nnski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. SummEval: Re-evaluating summariza-\ntion evaluation.TACL.\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni.\n2014.   Open question answering over curated and\nextracted knowledge bases. InProc. of KDD.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019.  ELI5:\nlong form question answering. InProc. of ACL.\nMarkus Freitag, George Foster, David Grangier, Viresh\nRatnakar, Qijun Tan, and Wolfgang Macherey. 2021.\nExperts, errors, and context: A large-scale study of\nhuman evaluation for machine translation.TACL.",
    "Luyu Gao and Jamie Callan. 2022.  Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. InProc. of ACL.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE:  Simple  contrastive  learning  of  sentence\nembeddings. InProc. of EMNLP.\nMansi  Gupta,  Nitish  Kulkarni,  Raghuveer  Chanda,\nAnirudha  Rayasam,  and  Zachary  C  Lipton.  2019.\nAmazonQA: A review-based question answering task.\nInProc. of IJCAI.\nSuchinGururangan,AnaMarasovi\n ́\nc,Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020.  Don’t stop pretraining:\nAdapt language models to domains and tasks.   In\nProc. of ACL.\nFelix Hamborg, Norman Meuschke, Corinna Breitinger,\nand Bela Gipp. 2017. news-please: A generic news\ncrawler and extractor. InProc. of ISI.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative filtering. InProc. of WWW.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le\nBras, and Yejin Choi. 2021. CLIPScore: A reference-\nfree evaluation metric for image captioning. InProc.\nof EMNLP.\nDoris Hoogeveen, Karin M Verspoor, and Timothy Bald-\nwin. 2015. CQADupStack: A benchmark data set for\ncommunity question-answering research. InProc. of\nADCS.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis,  and Marc Brockschmidt. 2019.   Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel,  Piotr Bojanowski,  Armand Joulin,\nand Edouard Grave. 2022. Unsupervised dense infor-\nmation retrieval with contrastive learning.TMLR.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. InProc. of EACL.\nMandar Joshi,  Eunsol Choi,  Daniel Weld,  and Luke\nZettlemoyer. 2017.  TriviaQA: A Large Scale Dis-\ntantly  Supervised  Challenge  Dataset  for  Reading\nComprehension. InProc. of ACL.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. InProc. of EMNLP.\nJungo  Kasai,  Keisuke  Sakaguchi,  Ronan  Le  Bras,\nLavinia Dunagan, Jacob Morrison, Alexander R. Fab-\nbri, Yejin Choi, and Noah A. Smith. 2022a. Bidimen-\nsional leaderboards: Generate and evaluate language\nhand in hand. InProc. of NAACL.\nJungo  Kasai,  Keisuke  Sakaguchi,  Lavinia  Dunagan,\nJacob Morrison,  Ronan Le Bras,  Yejin Choi,  and\nNoah A. Smith. 2022b.  Transparent human evalua-\ntion for image captioning. InProc. of NAACL.\nDaniel Khashabi, Amos Ng, Tushar Khot, Ashish Sab-\nharwal,  Hannaneh  Hajishirzi,  and  Chris  Callison-\nBurch.  2021.   GooAQ:  Open  question  answering\nwith diverse answer types.  InFindings of the ACL:\nEMNLP 2021.\nDaniel Khashabi, Gabriel Stanovsky, Jonathan Bragg,\nNicholas Lourie, Jungo Kasai, Yejin Choi, Noah A.\nSmith, and Daniel S. Weld. 2022.  GENIE: Toward\nreproducible and standardized human evaluation for\ntext generation. InProc. of EMNLP.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard\nZemel, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler.  2015.    Skip-thought  vectors.    InProc. of\nNeurIPS.\nMahnaz Koupaee and William Yang Wang. 2018. Wiki-\nHow: A large scale text summarization dataset.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research.TACL.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettle-\nmoyer. 2017. Zero-shot relation extraction via read-\ning comprehension. InProc. of CoNLL.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Min-\nervini, Heinrich Küttler, Aleksandra Piktus, Pontus\nStenetorp, and Sebastian Riedel. 2021. PAQ: 65 mil-\nlion probably-asked questions and what you can do\nwith them.TACL.\nShuyang Li. 2020.  INTERVIEW: NPR media dialog\ntranscripts.\nLucy H. Lin, Scott B. Miles, and Noah A. Smith. 2018.\nSemantic matching against a corpus: New methods\nand applications.\nTsung-Yi  Lin,   Michael  Maire,   Serge  J.  Belongie,\nLubomir D. Bourdev, Ross B. Girshick, James Hays,\nPietro  Perona,  Deva  Ramanan,  Piotr  Dollár,  and\nC. Lawrence Zitnick. 2014. Microsoft COCO: com-\nmon objects in context. InProc. of ECCV.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence  Carin,  and  Weizhu  Chen.  2022.   What\nmakes good in-context examples for GPT-3?In\nProc. of DeeLIO 2022.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021.  Pre-\ntrain, prompt, and predict:  A systematic survey of\nprompting methods in natural language processing.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. InProc. of ACL.",
    "Lajanugen Logeswaran and Honglak Lee. 2018.  An\nefficient framework for learning sentence representa-\ntions. InProc. of ICLR.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\nelli. 2014. A SICK cure for the evaluation of compo-\nsitional distributional semantic models.  InProc. of\nLREC.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. InProc. of NAACL.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022.  Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nInProceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487.\nBhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017.\nLearning to match using local and distributed repre-\nsentations of text for web search. InProc. of WWW.\nNiklas Muennighoff. 2022.  SGPT: GPT sentence em-\nbeddings for semantic search.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and\nNils Reimers. 2022. MTEB: Massive text embedding\nbenchmark.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018.  Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. InProc. of EMNLP.\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant,\nJi Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.\nSentence-T5: Scalable sentence encoders from pre-\ntrained text-to-text models. InFindings of the ACL:\nACL 2022.\nJianmo  Ni,  Chen  Qu,  Jing  Lu,  Zhuyun  Dai,  Gus-\ntavo  Hernández  Ábrego,  Ji  Ma,  Vincent  Y.  Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021.  Large dual encoders are generalizable\nretrievers. InProc. of EMNLP.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.  Training language models to follow instruc-\ntions with human feedback. InProc. of NeurIPS.\nAnkit  Pal,  Logesh  Kumar  Umapathi,  and  Malaikan-\nnan Sankarasubbu. 2022. MedMCQA: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. InProc. of CHIL.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021.   KILT: a benchmark for knowledge\nintensive language tasks. InProc. of NAACL.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. InProc. of EMNLP.\nNils  Reimers  and  Iryna  Gurevych.  2019.   Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. InProc. of EMNLP.\nJoshua David Robinson, Ching-Yao Chuang, Suvrit Sra,\nand Stefanie Jegelka. 2021.  Contrastive learning with\nhard negative samples. InInternational Conference\non Learning Representations.\nAndrew  Rosenberg  and  Julia  Hirschberg.  2007.   V-\nmeasure: A conditional entropy-based external clus-\nter evaluation measure. InProc. of EMNLP.\nOhad  Rubin,  Jonathan  Herzig,  and  Jonathan  Berant.\n2022.  Learning to retrieve prompts for in-context\nlearning. InProc. of NAACL.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach,  Lintang  Sutawika,  Zaid  Alyafeai,  Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan  Dey,  M  Saiful  Bari,  Canwen  Xu,  Urmish\nThakker, Shanya Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala  Neeraj,  Jos  Rozen,  Abheesht  Sharma,  An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Stella Rose Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. InProc. of ICLR.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\n2020.   BLEURT: Learning robust metrics for text\ngeneration. InProc. of ACL.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise\nGetoor, Brian Galligher, and Tina Eliassi-Rad. 2008.\nCollective classification in network data.AI maga-\nzine.\nRodrigo  FG  Silva,  Klérisson  Paixão,  and  Marcelo\nde Almeida Maia. 2018.  Duplicate question detec-\ntion in stack overflow:  A reproducibility study.  In\nProc. of SANER.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022.\nSelective annotation makes language models better\nfew-shot learners.\nNandan Thakur,  Nils Reimers,  Andreas Rücklé,  Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. InProc. of NeurIPS.\nJamesThorne,AndreasVlachos,Christos\nChristodoulopoulos,and    Arpit    Mittal.    2018.\nFever:  a large-scale dataset for fact extraction and\nverification. InProc. of NAACL.",
    "Laurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE.JMLR.\nBen  Wang  and  Aran  Komatsuzaki.  2021.GPT-J-\n6B:  A  6  Billion  Parameter  Autoregressive  Lan-\nguage Model.https://github.com/kingoflolz/\nmesh-transformer-jax.\nLiang  Wang,  Nan  Yang,  Xiaolong  Huang,  Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022a.  Text embeddings by weakly-\nsupervised contrastive pre-training.\nYizhong  Wang,   Swaroop  Mishra,   Pegah  Alipoor-\nmolabashi,   Yeganeh   Kordi,   Amirreza   Mirzaei,\nAnjana  Arunkumar,   Arjun  Ashok,   Arut  Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia,  Krima Doshi,  Maitreya Patel,  Kuntal Ku-\nmar  Pal,  Mehrad  Moradshahi,  Mihir  Parmar,  Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza,\nPulkit Verma, Ravsehaj Singh Puri, Rushang Karia,\nShailaja  Keyur  Sampat,  Savan  Doshi,  Siddhartha\nMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,\nXudong Shen, Chitta Baral, Yejin Choi, Hannaneh\nHajishirzi,  Noah  A.  Smith,  and  Daniel  Khashabi.\n2022b.   Super-NaturalInstructions:  Generalization\nvia declarative instructions on 1600+ NLP tasks. In\nProc. of EMNLP.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai,  and  Quoc  V.  Le.  2022.   Finetuned  language\nmodels are zero-shot learners. InProc. of ICLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018.  A broad-coverage challenge corpus for sen-\ntence understanding through inference.  InProc. of\nNAACL.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018.  HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. InProc. of EMNLP.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014.  From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions.TACL.\nTianyi  Zhang,  Varsha  Kishore,  Felix  Wu,  Kilian  Q.\nWeinberger,  and  Yoav  Artzi.  2020.    BERTScore:\nEvaluating text generation with BERT.  InProc. of\nICLR.\nXiang  Zhang,  Junbo  Zhao,  and  Yann  LeCun.  2015.\nCharacter-level convolutional networks for text clas-\nsification. InProc. of NeurIPS.\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.\n2021. Adapting language models for zero-shot learn-\ning by meta-tuning on dataset and prompt collections.\nInFindings of the ACL: EMNLP 2021.\nChunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Prompt con-\nsistency for zero-shot task generalization.\nAppendices\nA    Training Setups\nMinibatch SamplingTraining is performed on\na  combination  of  all  training  datasets  in  MEDI.\nSince the number of examples in each dataset is\ndifferent in orders of magnitude, we downsample\nlarge ones. Details for the downsampled numbers\nof examples on each dataset are shown in Table 5\nin the appendix.  At each step, we first randomly\nselect a dataset and then construct a minibatchonly\nusing the examples from that dataset. In this way,\nwe ensure that in-batch negatives are sampled from\nthe  same  dataset,  thereby  preventing  the  model\nfrom using task differences to predict the negative\nlabel. We use the maximum batch size that fits the\nmachine memory and run all our experiments on\n40GB A100 GPUs.\nTrainingWe initializeINSTRUCTORwith the\nGTR-Large model (Ni et al., 2021, 335M parame-\nters)\n4\nand finetune it on MEDI using the AdamW\noptimizer with learning rate2×10\n−5\nand warmup\nratio 0.1.  We use a softmax temperature of 0.01\nand finetune INSTRUCTOR for 20K steps.\nBaselinesWe use the official MTEB benchmark\nfor  comparisons,  but  here  we  highlight  several\nstrong  baselines  with  the  following  two  types.\nThe first class of baselines is embedding models\nspecializing in information retrieval:Contriever-\nMS(Izacard et al., 2022),GTR(Ni et al., 2021),\nandcoCondenser-MS(Gao  and  Callan,  2022).\nThey are all trained on open-domain QA datasets\nsuch as  MS MARCO (Bajaj et  al., 2016).   The\nsecond class of baselines focuses on semantic tex-\ntual similarity:SimCSE(Gao et al., 2021),Sent-\nT5(Ni et al., 2022), andSGPT-NLI(Muennighoff,\n2022).   They  are  mainly  trained  on  symmetric\nparaphrase datasets such as NLI (Williams et al.,\n2018) and the Quora question pairs.\n5\nAll of these\nbaselines are based on pretrained language models,\nachieving strong performance on the MTEB leader-\nboard. In particular, Sent-T5-XXL and GTR-XXL\n4\nhttps://huggingface.co/sentence-transformers/\ngtr-t5-large.\n5\nhttps://www.quora.com/q/quoradata/.",
    "(both with 4.8B parameters) achieve the first and\nsecond best average performances.\nB    Embedding Evaluations\nHere we provide a high-level summary of the eval-\nuation tasks (Table 1).  Following MTEB (Muen-\nnighoff et al., 2022), Billboard (Kasai et al., 2022a),\nand prompt retrieval (Su et al., 2022), we split 70\nevaluation datasets into 9 categories by task ob-\njectives.   Out of the 70 evaluation tasks,  66 are\nunseen during training (See Table 5 for datasets\nincluded during training), Table 1 for examples and\ninstructions for the evaluation datasets.\nB.1    Massive Text Embedding Benchmark\nMTEB (Muennighoff et al., 2022) is a comprehen-\nsive embedding evaluation benchmark that aims\nto provide a holistic view of current embedding\nmodels’ performance and to discover universal text\nembeddings applicable to a wide range of tasks. It\ncombines several conventional benchmarks (e.g.,\nBEIR, Thakur et al., 2021,  and STS, Cer et al.,\n2017) and spans a wide range of domain-specific\ndatasets, including science, biology, and medicine.\nFollowing Muennighoff et al. (2022), we also re-\nport the average performance over 56 datasets. For\neach task family, we briefly describe the task ob-\njective, evaluation metric, and how embeddings are\nused.\nRetrievalGiven a queryqand a corpusD=\n{p\n1\n,p\n2\n...p\nn\n}, retrieval aims to find the most rel-\nevant documentsp\ni\ninDfor queryq.   The em-\nbedding  model  is  used  to  embedqandp\n1\n...p\nn\ninto  fixed-sized  vectors,  and  then  the  similarity\nbetweenqandp\ni\nis measured by their embedding\ncosine similarity.   There are 14 diverse datasets\n(e.g., Natural Questions, Scifact, and NFCorpus)\ntogether with the community question-answering\n(CQA) benchmark (Hoogeveen et al., 2015).  We\nuse NDCG@10 (Normalized Discounted cumula-\ntive gain at rank position 10) to measure the perfor-\nmance.\nRerankingReranking ranks a list of documents\nbased on their relevance to a query. Given a queryq\nand a list of documentsD={p\n1\n,p\n2\n...p\nn\n}, the em-\nbedding model computes embeddings of both the\nquery and documents, which are then used to rank\nthe documents based on their cosine similarities.\nWe use MAP (mean average precision), a standard\nmetric in reranking, to measure performance.\nClusteringThe goal of clustering is to group sim-\nilar documents into meaningful clusters. Given a\nset of documents, the encoder maps each document\ninto an embedding. The k-means clustering algo-\nrithm is then used to partition the embedded doc-\numents into clusters. The clustering performance\nis measured by the v-measure that is independent\nof the permutations of clustering labels (Rosenberg\nand Hirschberg, 2007).\nPair ClassificationPair classification tasks aim\nto predict a binary label for a pair of texts.  An\nexample of this task is paraphrase identification,\nwhere the goal is to predict whether two sentences\nare paraphrases of each other. Given a sentence pair\n(t\n1\n,t\n2\n), the embedding model encodest\n1\nandt\n2\nseparately. The cosine similarity between the two\nembeddings is then used to predict the label. The\naverage precision score is measured for evaluation.\nClassificationClassification is a popular way to\nevaluate the quality of embeddings (Conneau and\nKiela, 2018). For each example in the classification\ndataset, the embedding of the input text is used as\nfeatures to a classifier. The classifier is trained on\nthe training data while sentence embedings are kept\nfrozen. We report the classification accuracy on the\ntest set as the evaluation metric.\nSTSSemantic textual similarity (STS) tasks eval-\nuate the similarity between two sentences. Given a\nsentence pair(t\n1\n,t\n2\n), the embedding model maps\nt\n1\nandt\n2\ninto embeddings separately, and then the\nsimilarity betweent\n1\nandt\n2\nis measured by their\nembedding cosine similarity. The evaluation met-\nric is Spearman’s rank correlation, which measures\nthe correlation between the similarity scores and\nhuman judgements.\nSummarizationAutomatic summarization eval-\nuation aims to evaluate the quality of a machine-\ngenerated  summary  given  a  reference  summary.\nWhile human evaluations are considered more ac-\ncurate, automatic evaluations allow for fast, inex-\npensive development cycles (Khashabi et al., 2022).\nGiven  a  reference  summaryrand  a  machine-\ngenerated summaryt, the embedding model maps\nthem into embeddings separately, and we compute\nthe cosine similarity betweenrandt. Spearman’s\nrank correlation is reported between human judge-\nments and automatic scores.",
    "Examples\nText type\nquestion, query, answer, summary, sen-\ntence, review, post, comment, statement,\nparagraph, passage, document\nText objective\nclassify the sentence as positive or neg-\native, retrieve a duplicate sentence, re-\ntrieve the supporting document\nDomain\nwikipedia, news, medicine, biology, red-\ndit, stackoverflow, science, quora, coro-\nnavirus, math, physics\nTable 4:  Examples of text types,  objectives,  and do-\nmains.\nB.2    Prompt Retrieval\nLarge language models have demonstrated the abil-\nity of in-context learning,  where the model can\nperform downstream tasks by conditioning genera-\ntion on a few task demonstrations (Liu et al., 2021).\nSu et al. (2022) introduce the prompt retrieval task,\nwhere the goal is to retrieve a few in-context learn-\ning (i.e., demonstration) examples from annotated\nexamples given a test instance.  The embedding\nmodel is used to encode all annotated examples\nand to find the few most similar examples to the\ntest instance based on the cosine similarity.  Fol-\nlowing Su et al. (2022), we use the retrieved exam-\nples for in-context learning on GPT-J (Wang and\nKomatsuzaki, 2021) over 11 diverse downstream\ntasks (e.g., classification, multiple choice, and text-\nto-SQL) that are not included in MEDI (thus zero-\nshot settings).  We compare different embedding\nmethods by measuring the average performance on\nthese downstream tasks.\nB.3    Automatic Evaluation for Generation\nSimilar to summarization evaluation in MTEB, we\nuse the Billboard benchmark (Kasai et al., 2022a)\nto applyINSTRUCTORto automatic evaluations for\nthree additional text generation tasks: MSCOCO\nimage captioning (Lin et al., 2014; Kasai et al.,\n2022b), CNN/DailyMail news summarization (Fab-\nbri et al., 2021), and WMT21 Chinese-to-English\ntranslation  (Barrault  et  al.,  2020;  Freitag  et  al.,\n2021). Following Kasai et al. (2022a), we measure\nthe cosine similarity between the generated text\nand each reference text and take the maximum sim-\nilarity score over all references available (Zhang\net al., 2020). We evaluate all embedding models by\nthe Pearson correlation with the human judgments,\nagain following Kasai et al. (2022a).  We then re-\nport the average correlation scores over the three\ndatasets.  Note that we do not use the English-to-\nGerman dataset in Billboard because our models\nare trained only on English data.\nC    Full instructions\nWe list all instructions for each dataset in MEDI in\nTable 7 and Table 8\nD    Full Results\nWe provide the detailed evaluation scores in MTEB,\nBillboard and prompt retrieval benchmarks in Table\n9 & 10.",
    "DatasetSymmetric/AsymmetricNumber\ngooaq_pairs (Khashabi et al., 2021)Asymmetric25,000\nyahoo_answers_title_answer (Zhang et al., 2015)Asymmetric25,000\nstackexchange (Silva et al., 2018)Symmetric25,000\neli5_question_answer (Fan et al., 2019)Asymmetric25,000\nsquad_pairs (Rajpurkar et al., 2016)Asymmetric25,000\nNQ\n∗\n(Kwiatkowski et al., 2019)Asymmetric50,000\namazon-qa (Gupta et al., 2019)Asymmetric100,000\nWikiAnswers (Fader et al., 2014)Symmetric25,000\nagnews (Zhang et al., 2015)Asymmetric45,000\nAllNLI (Bowman et al., 2015)Symmetric50,000\nnpr (Li, 2020)Asymmetric25,000\nspecter_train_triples (Cohan et al., 2020)Symmetric50,000\nccnews_title_text (Hamborg et al., 2017)Asymmetric25,000\ntriviaqa (Joshi et al., 2017)Asymmetric50,000\nzero_shot_re (Levy et al., 2017)Asymmetric15,000\nflickr30k_captions (Young et al., 2014)Symmetric25,000\nxsum (Narayan et al., 2018)Asymmetric10,000\ncode_search (Husain et al., 2019)Asymmetric15,000\nmsmarco\n∗\n(Bajaj et al., 2016)Asymmetric175,000\nhotpotqa\n∗\n(Yang et al., 2018)Asymmetric40,000\nfever\n∗\n(Thorne et al., 2018)Asymmetric75,000\namazon_review_2018  (He and McAuley, 2016)Asymmetric100,000\nS2ORC_title_abstract (Lo et al., 2020)Asymmetric100,000\nPAQ_pairs (Lewis et al., 2021)Asymmetric25,000\nwow (Dinan et al., 2019)Asymmetric30,000\ntrex (Elsahar et al., 2018)Asymmetric30,000\npubmed (Sen et al., 2008)Asymmetric30,000\nmedmcqa (Pal et al., 2022)Asymmetric30,000\nwikihow (Koupaee and Wang, 2018)Asymmetric5,000\nsimple_wiki (Coster and Kauchak, 2011)Asymmetric5,000\nSuper-NI (300 datasets) (Wang et al., 2022b)Symmetric180,000\nTable 5: Number of training instances in each dataset. The dataset with * indicates that its test-split is included in\nthe evaluation.\nDatasetInstruction\nInstruction 1:Represent the Amazon comment for classifying the sentence as positive or negative:\nAmazon\nInstruction 2:Represent the Amazon review comment for classifying the emotion as positive or negative:\nPolarity\nInstruction 3:Represent the Amazon sentence for classifying its sentiment as positive or negative:\nClassification\nInstruction 4:Represent an Amazon post for classifying its sentiment as positive or negative:\nInstruction 5:Represent the Amazon review for classifying the review sentiment as negative or positive:\nQuery instruction 1:Represent the finance query for retrieving supporting documents:\nDoc instruction 1: Represent the finance document for retrieval:\nQuery instruction 2:Represent the financial question for retrieving supporting documents:\nDoc instruction 2: Represent the financial document for retrieval:\nFIQA2018\nQuery instruction 3:Represent the finance query for retrieving related documents:\nDoc instruction 3: Represent the finance document for retrieval:\nQuery instruction 4:Represent a finance query for retrieving relevant documents:\nDoc instruction 4: Represent the financial document for retrieval:\nQuery instruction 5:Represent the finance query for retrieving supporting passages:\nDoc instruction 5: Represent the finance passage for retrieval:\nTable 6:  Example paraphrased instructions for AmazonPolarityClassification and FIQA2018.  They follow the\nunified template (§2.3) with the same information and only differ in wording choices.",
    "DatasetInstruction\nMSMARCO\nQuery instruction:Represent the [domain] question for retrieving evidence documents:\nDoc instruction: Represent the domain document for retrieval:\ngooaq_pairs\nQuery instruction:Represent the Google question for retrieving answers:\nDoc instruction: Represent the Google answer for retrieval:\nyahoo_answers_title_answer\nQuery instruction:Represent the Yahoo question for retrieving answers:\nDoc instruction:Represent the Yahoo answer for retrieval:\neli5_question_answer\nQuery instruction:Represent the ELI5 question for retrieving answers:\nDoc instruction:Represent the ELI5 answer for retrieval:\nsquad_pairs\nQuery instruction:Represent the Squad question for retrieving evidence documents:\nDoc instruction:Represent the Squad document for retrieval:\nNatural Question\nQuery instruction:Represent the Wikipedia question for retrieving supporting documents:\nDoc instruction:Represent the Wikipedia document for retrieval:\namazon-qa\nQuery instruction:Represent the Amazon question for retrieving answers:\nDoc instruction:Represent the Amazon answer for retrieval:\nagnews\nQuery instruction:Represent the news title for retrieving relevant articles:\nDoc instruction:Represent the news article for retrieval:\nnpr\nQuery instruction:Represent the news title for retrieving relevant articles:\nDoc instruction:Represent the news article for retrieval:\nccnews_title_text\nQuery instruction:Represent the news title for retrieving relevant articles:\nDoc instruction:Represent the news article for retrieval:\ntriviaqa\nQuery instruction:Represent the question for retrieving evidence documents:\nDoc instruction: Represent the evidence document for retrieval:\nzero_shot_re\nQuery instruction:Represent the Wikipedia question for retrieving evidence documents:\nDoc instruction:Represent the Wikipedia document for retrieval:\nxsum\nQuery instruction:Represent the news title for retrieving relevant articles:\nDoc instruction:Represent the news article for retrieval:\ncode_search\nQuery instruction:Represent the comment for retrieving corresponding codes:\nDoc instruction:Represent the code for retrieval:\nhotpotqa\nQuery instruction:Represent the Wikipedia question for retrieving supporting documents:\nDoc instruction:Represent the Wikipedia document for retrieval:\nfever\nQuery instruction:Represent the fact for retrieving supporting evidence:\nDoc instruction:Represent the evidence for retrieval:\namazon_review_2018\nQuery instruction:Represent the Amazon title for retrieving relevant reviews:\nDoc instruction:Represent the Amazon review for retrieval:\nS2ORC_title_abstract\nQuery instruction:Represent the Scientific title for retrieving relevant abstracts:,Doc\ninstruction:Represent the Scientific abstract for retrieval:\nPAQ_pairs\nQuery instruction:Represent the question for retrieving evidence documents:,Doc\ninstruction:Represent the evidence document for retrieval:\nwow\nQuery instruction:Represent the Wikipedia question for retrieving supporting docu-\nments:,Doc instruction:Represent the Wikipedia document for retrieval:\ntrex\nQuery instruction:Represent the Wikipedia question for retrieving supporting docu-\nments:,Doc instruction:Represent the Wikipedia document for retrieval:\npubmed\nQuery instruction:Represent the Medicine sentence for retrieving relevant documents:,\nDoc instruction:Represent the Medicine document for retrieval:\nmedmcqa\nQuery instruction:Represent the Medicine question for retrieving supporting answers:,\nDoc instruction:Represent the Medicine answer for retrieval:\nwikihow\nQuery instruction:Represent the Wikipedia summary for retrieving relevant passages:,\nDoc instruction:Represent the Wikipedia passage for retrieval:\nsimple_wiki\nQuery instruction:Represent the Wikipedia sentence for retrieving simplified sentences:,\nDoc instruction:Represent the Wikipedia sentence for retrieval:\nTable 7: Instructions of asymmetric training dataset. We use Kmeans clustering to put MSMARCO examples into\n30 groups, and label the domain for each group.",
    "DatasetInstruction\nstackexchangeInstruction:Represent the StackExchange question for retrieving duplicate questions:\nWikiAnswers\nInstruction:Represent the Wikipedia question for retrieving duplicate questions:\nAllNLI\nInstruction:Represent the sentence for retrieving duplicate sentences:\nspecter_train_triples\nInstruction:Represent the scientific title for retrieving duplicate titles:\nflickr30k_captions\nInstruction:Represent the caption for retrieving duplicate captions:\nsuper-NIInstruction:Represent the example for the following task: [dataset definition]:\nTable 8: Instructions of symmetric training dataset. We use the task definitions of Super-NaturalInstructions as the\ntask objective.\nCategoryDatasetGTRINSTRUCTORGTRINSTRUCTOR\n335M335M1.5B1.5B\nSciFact63.864.364.264.6\nNFcorpus32.434.133.336.0\nArguAna52.157.152.855.7\nCQADupstackWebmastersRetrieval35.746.436.545.1\nCQADupstackEnglishRetrieval46.850.846.549.3\nCQADupstackGamingRetrieval56.363.155.863.3\nCQADupstackGisRetrieval33.739.534.640.6\nCQADupstackAndroidRetrieval46.155.944.955.0\nCQADupstackTexRetrieval25.130.026.129.1\nCQADupstackUnixRetrieval35.344.736.642.5\nCQADupstackMathematicaRetrieval24.830.727.430.8\nCQADupstackStatsRetrieval30.434.630.135.7\nRetrievalCQADupstackPhysicsRetrieval38.547.839.745.3\nCQADupstackProgrammersRetrieval38.547.539.644.9\nCQADupstackWordpressRetrieval28.234.930.435.5\nClimateFEVER26.927.827.026.5\nFEVER72.772.772.270.0\nFiQA201842.845.544.247.0\nHotpotQA57.955.258.955.9\nMSMARCO42.739.743.541.6\nNQ55.150.156.257.3\nQuoraRetrieval88.588.488.988.9\nSCIDOCS15.518.615.717.4\nDBPedia39.636.739.740.2\nTRECCOVID56.758.160.171.4\nTouche202028.321.625.323.4\nTable 9: All Retrieval results in MTEB benchmark.",
    "CategoryDatasetGTRINSTRUCTORGTRINSTRUCTOR\n335M335M1.5B1.5B\nSummarizationSummEval29.531.830.232.0\nAskUbuntuDupQuestions61.664.363.165.4\nRerankingStackOverflowDupQuestions51.652.252.852.5\nSciDocsRR76.482.076.579.5\nMindSmallReranking31.831.731.531.8\nBiorxivClusteringS2S25.731.326.130.6\nMedrxivClusteringS2S27.432.026.730.8\nTwentyNewsgroupsClustering51.654.151.253.3\nArxivClusteringP2P37.543.237.942.5\nArxivClusteringS2S30.632.630.532.2\nClusteringBiorxivClusteringP2P29.637.630.537.5\nMedrxivClusteringP2P28.734.228.733.2\nRedditClustering61.763.761.363.4\nRedditClusteringP2P61.764.661.165.1\nStackExchangeClustering69.968.870.068.4\nStackExchangeClusteringP2P33.236.132.735.1\nSprintDuplicateQuestions95.193.195.594.9\nPair ClassificationTwitterSemEval201576.077.477.878.0\nTwitterURLCorpus84.987.285.186.9\nSTS1270.376.369.175.3\nSTS1382.288.281.887.4\nSTS1477.281.977.181.9\nSTS1586.389.086.088.9\nSTSSTS1681.985.582.285.4\nSTS1783.990.384.990.5\nSTS2264.367.466.668.6\nBIOSSES84.984.478.984.2\nSICK-R73.481.373.681.7\nSTSBenchmark77.686.977.786.6\nBanking77Classification81.278.582.282.7\nTweetSentimentExtractionClassification54.164.154.861.7\nAmazonReviewsClassification37.247.938.243.0\nEmotionClassification46.352.745.553.2\nAmazonCounterfactualClassification70.088.168.685.1\nClassificationImdbClassification70.988.368.280.1\nMassiveIntentClassification70.168.970.271.5\nMassiveScenarioClassification75.573.475.976.5\nMTOPIntentClassification63.968.065.972.3\nMTOPDomainClassification94.093.993.695.1\nAmazonPolarityClassification73.991.574.686.5\nToxicConversationsClassification68.771.167.670.3\nRTE56.158.856.859.3\nSST-552.453.853.260.1\ncoda19_title_generation21.223.621.427.8\nmultirc_answerability62.563.663.772.6\nMRPC60.365.460.872.9\nPrompt RetrievalHellaSwag61.662.863.472.4\nAmazon36.038.036.048.0\nDbpedia_1491.793.091.794.0\nGeoQuery53.464.253.563.2\nMulti-Woz90.894.491.095.2\nCivilComments71.877.272.688.3\nmscoco32.341.633.239.7\nBillboardcnn summary25.830.326.131.9\nmachine translation35.438.936.630.6\nTable 10: All Prompt retrieval, Billboard, and MTEB English results, cont."
  ]
}