{
  "key": "Q8QXPA9K",
  "url": "http://arxiv.org/pdf/2309.16588",
  "metadata": {
    "title": "Vision Transformers Need Registers",
    "abstract": "  Transformers have recently emerged as a powerful tool for learning visual\nrepresentations. In this paper, we identify and characterize artifacts in\nfeature maps of both supervised and self-supervised ViT networks. The artifacts\ncorrespond to high-norm tokens appearing during inference primarily in\nlow-informative background areas of images, that are repurposed for internal\ncomputations. We propose a simple yet effective solution based on providing\nadditional tokens to the input sequence of the Vision Transformer to fill that\nrole. We show that this solution fixes that problem entirely for both\nsupervised and self-supervised models, sets a new state of the art for\nself-supervised visual models on dense visual prediction tasks, enables object\ndiscovery methods with larger models, and most importantly leads to smoother\nfeature maps and attention maps for downstream visual processing.\n",
    "published": "2023-09-28T16:45:46Z"
  },
  "text": [
    "Published as a conference paper at ICLR 2024\nVISIONTRANSFORMERSNEEDREGISTERS\nTimoth\n ́\nee Darcet\n1,2\nMaxime Oquab\n1\nJulien Mairal\n2\nPiotr Bojanowski\n1\n1\nFAIR, Meta\n2\nUniv. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France\nABSTRACT\nTransformers have recently emerged as a powerful tool for learning visual rep-\nresentations.  In this paper, we identify and characterize artifacts in feature maps\nof both supervised and self-supervised ViT networks. The artifacts correspond to\nhigh-norm tokens appearing during inference primarily in low-informative back-\nground areas of images, that are repurposed for internal computations. We propose\na simple yet effective solution based on providing additional tokens to the input se-\nquence of the Vision Transformer to fill that role. We show that this solution fixes\nthat problem entirely for both supervised and self-supervised models, sets a new\nstate of the art for self-supervised visual models on dense visual prediction tasks,\nenables object discovery methods with larger models, and most importantly leads\nto smoother feature maps and attention maps for downstream visual processing.\nWithout registersWith registers\nInputDeiT-IIIOpenCLIPDINOv2DeiT-IIIOpenCLIPDINOv2\nFigure 1:  Register tokens enable  interpretable attention maps in all vision transformers, similar to\nthe original DINO method (Caron et al., 2021). Attention maps are calculated in high resolution for\nbetter visualisation. More qualitative results are available in appendix H.\n1INTRODUCTION\nEmbedding images into generic features that can serve multiple purposes in computer vision has\nbeen a long-standing problem. First  methods relied on handcrafted principles, such as SIFT (Lowe,\n2004), before the scale of data and deep learning techniques allowed for end-to-end training.  Pur-\nsuing generic feature embeddings is still relevant today, as collecting valuable annotated data for\nmany specific tasks remains difficult.  This difficulty arises because of the required expertise (e.g.,\nmedical data, or remote sensing) or the cost at scale.  Today, it is common to pretrain a model for\na task for which plenty of data is available and extract a subset of the model to use as a feature\nextractor. Multiple approaches offer this possibility; supervised methods, building on classification\nCorrespondence totimdarcet@meta.com\n1\narXiv:2309.16588v2  [cs.CV]  12 Apr 2024",
    "Published as a conference paper at ICLR 2024\nInputDeiT-III-BDeiT-III-LOpenCLIP-B    OpenCLIP-LDINO-BDINOv2-g\nFigure  2:  Illustration  of  artifacts  observed  in  the  attention  maps  of  modern  vision  transformers.\nWe consider ViTs trained with label supervision (DeiT-III), text-supervision (OpenCLIP) or self-\nsupervision (DINO and DINOv2).  Interestingly, all models but DINO exhibit peaky outlier values\nin the attention maps. The goal of this work is to understand and mitigate this phenomenon.\nor text-image alignment, allow training strong feature models to unlock downstream tasks. Alterna-\ntively, self-supervised methods building on the Transformer architecture have attracted significant\nattention due to their high prediction performance on downstream tasks and the intriguing ability of\nsome models to provide unsupervised segmentations (Caron et al., 2021)\nIn particular, the DINO algorithm is shown to produce models that contain explicit information about\nthe semantic layout of an image. Indeed, qualitative results show that the last attention layer naturally\nfocuses on semantically consistent parts of images and often produces interpretable attention maps.\nExploiting these properties, object discovery algorithms such as LOST (Sim\n ́\neoni et al., 2021) build\non top of DINO. Such algorithms can detect objects without supervision by gathering information\nin attention maps. They are effectively unlocking a new frontier in computer vision.\nDINOv2 (Oquab et al., 2023),  a follow-up to DINO, provides features that allow tackling dense\nprediction tasks. DINOv2 features lead to successful monocular depth estimation and semantic seg-\nmentation with a frozen backbone and linear models. Despite the strong performance on dense tasks,\nwe observed that DINOv2 is surprisingly incompatible with LOST. When used to extract features, it\ndelivers disappointing performance, only on par with supervised alternative backbones in this sce-\nnario.  This suggests that DINOv2 behaves differently than DINO. The investigation described in\nthis work notably exposes the presence of artefacts in the feature maps of DINOv2 that were not\npresent in the first version of this model.   These are observable qualitatively using straightforward\nmethods.  Also surprisingly, applying the same observations to supervised vision transformers ex-\nposes similar artifacts, as shown in Fig. 2.  This suggests that DINO is, in fact, an exception, while\nDINOv2 models match the baseline behavior of vision transformers.\nIn this work, we set out to better understand this phenomenon  and develop methods to detect these\nartifacts. We observe that they are tokens with roughly 10x higher norm at the output and correspond\nto a small fraction of the total sequence (around 2%). We also show that these tokens appear around\nthe middle layers of the vision transformer, and that they only appear after a sufficiently long training\nof a sufficiently big transformer.  In particular, we show that these outlier tokens appear in patches\nsimilar to their neighbors, meaning patches that convey little additional information.\nAs  part  of  our  investigation,  we  evaluate  the  outlier  tokens  with  simple  linear  models  to  under-\nstand the information they contain. We observe that, compared to non-outlier tokens, they hold less\ninformation about their original position in the image or the original pixels in their patch.  This ob-\n2",
    "Published as a conference paper at ICLR 2024\ninput imageDINO normsDINOv2 norms\n0\n20\n40\n60\n80\n100\n0200400600\nL\n2\n norm\n10\n5\n10\n3\n10\n1\nDINO norms\n0200400600\nL\n2\n norm\n10\n5\n10\n3\n10\n1\nDINOv2 norms\nFigure 3:  Comparison of local feature norms for DINO ViT-B/16 and DINOv2 ViT-g/14.  We ob-\nserve that DINOv2 has a few outlier patches, whereas DINO does not present these artifacts.  For\nDINOv2, although most patch tokens have a norm between 0 and 100, a small proportion of tokens\nhave a very high norm. We measure the proportion of tokens with norm larger than 150 at 2.37%.\nservation suggests that the model discards the local information contained in these patches during\ninference.   On the other hand,  learning an image classifier on outlier patches yields significantly\nstronger accuracy than doing so on the other patches, suggesting that  they contain global informa-\ntion about the image.  We propose the following interpretation to these elements:  the model learns\nto recognize patches containing little useful information, and recycle the corresponding tokens to\naggregate global image information while discarding spatial information.\nThis interpretation is consistent with an inner mechanism in transformer models that allows per-\nforming computations within a restricted set of tokens.  In order to test this hypothesis, we append\nadditional tokens - that we call registers - to the token sequence, independent of the input image. We\ntrain several models with and without this modification and observe that the outlier tokens disappear\nfrom the sequence entirely. As a result, the performance of the models increases in dense prediction\ntasks, and the resulting feature maps are significantly smoother. These smooth feature maps enable\nobject discovery methods like LOST mentioned above with the updated models.\n2PROBLEMFORMULATION\nAs shown in Fig. 2, most modern vision transformers exhibit artifacts in the attention maps.  The\nunsupervised DINO backbone (Caron et al., 2021) has been previously praised for the quality of\nlocal features and interpretability of attention maps.   Surprisingly,  the outputs of the subsequent\nDINOv2 models have been shown to hold good local information but exhibit undesirable artifacts in\nattention maps. In this section, we propose to studywhyandwhenthese artifacts appear. While this\nwork focuses on alleviating artefacts in all vision transformers, we focus our analysis on DINOv2.\n2.1ARTIFACTS IN THE LOCAL FEATURES OFDINOV2\nArtifacts are high-norm outlier tokens.We want to find a quantitative way of characterizing\nartefacts that appear in the local features. We observe that an important difference between “artifact”\npatches and other patches is the norm of their token embedding at the output of the model. In Fig. 3\n(left),  we compare the norm of local features for a DINO and DINOv2 model given a reference\nimage.   We  clearly  see  that  the  norm  of  artifact  patches  is  much  higher  than  the  norm  of  other\npatches.   We also plot the distribution of feature norms over a small dataset of images in Fig. 3\n(right), which is clearly bimodal, allowing us to choose a simple criterion for the rest of this section:\ntokens with norm higher than 150 will be considered as “high-norm” tokens, and we will study their\nproperties relative to regular tokens.  This hand-picked cutoff value can vary across models.  In the\nrest of this work, we use “high-norm” and “outlier” interchangeably.\nOutliers appear during the training of large models.We make several additional observations\nabout the conditions in which these outlier patches appear during the training of DINOv2.   This\nanalysis is illustrated in Fig. 4. First, these high-norm patches seem to differentiate themselves from\nother patches around layer 15 of this 40-layer ViT (Fig. 4a). Second, when looking at the distribution\nof norms along training of DINOv2, we see that these outliers only appear after one third of training\n(Fig. 4b). Finally, when analyzing more closely models of different size (Tiny, Small, Base, Large,\nHuge and giant), we see that only the three largest models exhibit outliers (Fig. 4c).\n3",
    "Published as a conference paper at ICLR 2024\n(a) Norms along layers.(b) Norms along iterations.(c) Norms across model size.\nFigure 4:  Illustration of several properties of outlier tokens in the 40-layer DINOv2 ViT-g model.\n(a):  Distribution  of  output token  norms  along  layers.(b):  Distribution  of  norms along  training\niterations.(c):  Distribution  of  norms  for  different  model  sizes.   The  outliers  appear  around  the\nmiddle of the model during training; they appear with models larger than and including ViT-Large.\n(a) Cosine similarity to neighbors.\nposition predictionreconstruction\ntop-1 accavg. distance↓L2 error↓\nnormal41.70.7918.38\noutlier22.85.0925.23\n(b) Linear probing for local information.\nFigure 5:(a):  Distribution of cosine similarity between input patches and their 4 neighbors.  We\nplot separately artifact patches (norm of theoutput tokenover 150) and normal patches.(b): Local\ninformation probing on normal and outlier patch tokens.  We train two models:  one for predicting\nposition, and one for reconstructing the input patch. Outlier tokens have much lower scores than the\nother tokens, suggesting they are storing less local patch information.\nHigh-norm tokens appear where patch information is redundant.To verify this, we measure\nthe cosine similarity between   high-norm tokens and their 4 neighbors right after the patch em-\nbedding layer (at the beginning of the vision transformer).   We illustrate the density plot in Fig.\n5a.  We observe that  high-norm tokens appear on patches that are very similar to their neighbors.\nThis suggests that these patches contrain redundant information and that the model could discard\ntheir information without hurting the quality of the image representation.  This matches qualitative\nobservations (see Fig. 2) that they often appear in uniform, background areas.\nHigh-norm tokens hold little local information.In order to better understand the nature of these\ntokens, we propose to probe the patch embeddings for different types of information.  For that we\nconsider two different tasks:  position prediction and pixel reconstruction.  For each of these tasks,\nwe   train  a  linear  model  on  top  of  the  patch  embeddings,  and  measure  the  performance  of  this\nmodel. We  compare the performance achieved with high-norm tokens and with other tokens, to see\nif  high-norm tokens contain different information than “normal” tokens.\n•Position prediction.We train a linear model to predict the position of each patch token in\nthe image, and measure its accuracy.  We note that this position information was injected\nin the tokens before the first ViT layer in the form of absolute position embeddings.  We\nobserve that high-norm tokens have  much lower accuracy than the other tokens (Fig. 5b),\nsuggesting they contain less information about their position in the image.\n•Pixel reconstruction.We train a linear model to predict the pixel values of the image from\nthe patch embeddings,  and measure the accuracy of this model.   We observe again that\nhigh-norm tokens achieve much lower accuracy than  other tokens (Fig. 5b). This suggests\nthat  high-norm tokens contain less information to reconstruct the image than the others.\nArtifacts hold global information.In order to evaluate how much global information is gathered\nin the high-norm tokens, we propose to evaluate them on standard image representation learning\n4",
    "Published as a conference paper at ICLR 2024\nIN1k  P205  Airc.  CF10  CF100  CUB  Cal101  Cars  DTD  Flow.  Food  Pets  SUN  VOC\n[CLS]86.0 66.4 87.3  99.4  94.5  91.396.991.5 85.2  99.7  94.7 96.9 78.689.1\nnormal65.8   53.1   17.1   97.181.318.673.210.8  63.1   59.5   74.2  47.8  37.7   70.8\noutlier69.055.179.199.393.784.997.685.284.999.693.594.178.589.7\nTable 1: Image classification via linear probing on normal and outlier patch tokens. We also report\nthe accuracy of classifiers learnt on the class token.  We see that outlier tokens have a much higher\naccuracy than regular ones, suggesting they are effectively storing global image information.\n[CLS]\n[REG1][REG2]\n...\n[REGN]\nTransformer Model\noutput\ninput patches\nFigure 6: Illustration of the proposed remediation and resulting model. We addNadditional learn-\nable input tokens (depicted in yellow),  that the model can use asregisters.   At the output of the\nmodel, only the patch tokens and[CLS]tokens are used, both during training and inference.\nbenchmarks.  For each image in a classification dataset, we forward it through DINOv2-g and ex-\ntract the patch embeddings.  From those, we choose a single token at random, either high-norm or\nnormal.  This token is then considered as the image representation.  We then train a logistic regres-\nsion classifier to predict the image class from this representation, and measure the accuracy.    We\nobserve that the high-norm tokens have a much higher accuracy than the other tokens (Table 1). This\nsuggests that  outlier tokens contain more global information than  other patch tokens.\n2.2HYPOTHESIS AND REMEDIATION\nHaving made these observations, we make the following hypothesis:large,sufficiently trainedmod-\nels learn to recognizeredundanttokens, and to use them as places tostore,processandretrieve\nglobal information.  Furthermore, we posit that while this behavior is not bad in itself, the fact that\nit happens inside the patch tokens is undesirable.  Indeed, it leads the model to discard local patch\ninformation (Tab. 5b), possibly incurring decreased performance on dense prediction tasks.\nWe therefore propose a simple  fix to this issue: we explicitly add new tokens to the sequence, that\nthe model can learn to use as registers. We add these tokens after the patch embedding layer, with a\nlearnable value, similarly to the[CLS]token. At the end of the vision transformer, these tokens are\ndiscarded, and the[CLS]token and patch tokens are used as image representations, as usual. This\nmechanism  was first proposed in Memory Transformers (Burtsev et al., 2020), improving translation\ntasks in NLP. Interestingly, we show here that this mechanism admits a natural justification for vision\ntransformers, fixing an interpretability and performance issue that was present otherwise.\nWe note that we have not been able to fully determine which aspects of the training led to the appear-\nance of artifacts in different models.  The pretraining paradigm seems to play a role, as OpenCLIP\nand DeiT-III exhibit outliers both at size B and L (Fig.  2).  However, the model size and training\nlength also play important parts, as observed in Fig. 4.\n3EXPERIMENTS\nIn this section,  we validate the proposed solution by training vision transformers with additional\n[reg]register tokens.  We evaluate the effectiveness of our approach by a quantitative and quali-\ntative analysis.  We then ablate the number of registers used for training, to check that they do not\n5",
    "Published as a conference paper at ICLR 2024\nFigure 7: Effect of register tokens on the distribution of output norms on DINOv2, OpenCLIP and\nDeiT-III. Using register tokens effectively removes the norm outliers that were present previously.\ncause a performance regression, evaluate an unsupervised object discovery method atop our features\nand finally provide a qualitative analysis of the patterns learnt by the registers.\n3.1TRAINING ALGORITHMS AND DATA\nAs the proposed solution is a simple architectural change, we can easily apply it to any training pro-\ncedure. We try it on three different state-of-the-art training methods for supervised, text-supervised,\nand unsupervised learning, shortly described below.\nDEIT-III(Touvron et al., 2022) is a simple and robust supervised training recipe for classification\nwith ViTs on ImageNet-1k and ImageNet-22k.   We choose this method as an example of label-\nsupervised  training  as  it  is  simple,  uses  the  base  ViT  architecture,  achieves  strong  classification\nresults, and is easy to reproduce and modify with our improvements.  We run this method on the\nImageNet-22k dataset, using the ViT-B settings, as provided in the official repository\n1\n.\nOpenCLIP(Ilharco et al., 2021) is a strong training method for producing text-image aligned mod-\nels,  following  the  original  CLIP  work.   We  chose  this  method  as  an  example  of  text-supervised\ntraining because it is open-source, uses the base ViT architecture, and is easy to reproduce and mod-\nify with our improvements. We run the OpenCLIP method on a text-image-aligned corpus based on\nShutterstock that includes only licensed image and text data. We use a ViT-B/16 image encoder, as\nproposed in the official repository\n2\n.\nDINOv2(Oquab et al., 2023) is a self-supervised method for learning visual features, following the\nDINO work. We apply our changes to this method as it is the main focus of our study. We run this\nmethod on ImageNet-22k with the ViT-L configuration. We use the official repository\n3\n.\n3.2EVALUATION  OF THE PROPOSED SOLUTION\nAs shown in Fig. 1, we get rid of the artifacts by training models with additional register tokens.\nIn the appendix, we provide additional qualitative results for more images in Fig. 19.  In order to\nquantitatively measure this effect, for each model, we probe the norm of features at the output of\nthe model.  We report these norms for all three algorithms with and without registers in Fig. 7.  We\nsee that when training with registers, models do not exhibit large-norm tokens at the output, which\nconfirms the initial qualitative assessment.\nPerformance regression.In the previous section, we have shown that the proposed approach re-\nmoves artifacts from local feature maps. In this experiment, we want to check that the use of register\ntokens does not affect the representation quality of those features.  We run linear probing on Im-\nageNet classification, ADE20k Segmentation, and NYUd monocular depth estimation.  We follow\nthe experimental protocol outlined in Oquab et al. (2023).  We summarize the performance of the\nmodels described in Sec. 3.1 with and without register tokens in Table 2a.  We see that when us-\ning registers, models do not lose performance and sometimes even work better.  For completeness,\nwe also provided the zero-shot classification performance on ImageNet for OpenCLIP (Table 2b),\nwhich remains unchanged. Please note that the absolute performance of our OpenCLIP reproduction\nis lower due to the data source we used.\n1\nhttps://github.com/facebookresearch/deit\n2\nhttps://github.com/mlfoundations/open_clip\n3\nhttps://github.com/facebookresearch/dinov2\n6",
    "Published as a conference paper at ICLR 2024\nImageNetADE20kNYUd\nTop-1mIoUrmse↓\nDeiT-III84.738.90.511\nDeiT-III+reg84.739.10.512\nOpenCLIP78.226.60.702\nOpenCLIP+reg78.126.70.661\nDINOv284.346.60.378\nDINOv2+reg84.847.90.366\n(a) Linear evaluation with frozen features.\nImageNet\nTop-1\nOpenCLIP59.9\nOpenCLIP+reg60.1\n(b) Zero-shot classification.\nTable 2:  Evaluation of downstream performance of the models that we trained, with and without\nregisters. We consider linear probing of frozen features for all three models, and zero-shot evaluation\nfor the OpenCLIP model. We see that using register not only does not degrade performance, but even\nimproves it by a slight margin in some cases.\nInput0[reg]1[reg]2[reg]4[reg]8[reg]16[reg]\n0481216\nnumber of [reg] tokens\n84.4\n84.5\n84.6\n84.7\n84.8\ntop-1 acc\nImageNet\n0481216\nnumber of [reg] tokens\n66.0\n66.2\n66.4\n66.6\n66.8\nmIoU\nAverage of segmentation tasks\n0481216\nnumber of [reg] tokens\n2.73\n2.76\n2.79\n2.82\n2.85\nrmse\nAverage of depth tasks\nFigure 8: Ablation of the the number of register tokens used with a DINOv2 model.(top): qualita-\ntive visualization of artifacts appearing as a function of number of registers.(bottom): performance\non three tasks (ImageNet, ADE-20k and NYUd) as a function of number of registers used.  While\none register is sufficient to remove artefacts, using more leads to improved downstream performance.\nNumber of register tokens.As described in Sec. 2.2,  we propose alleviating the feature maps’\nartifacts by adding register tokens. In this experiment, we study the influence of the number of such\ntokens on local features and downstream performance. We train DINOv2 ViT-L/14 models with 0, 1,\n2, 4, 8 or 16 registers. In Fig. 8, we report the results of this analysis. In Fig. 8(top), we qualitatively\nstudy the attention maps and observe that the visible artifacts disappear when adding at least one\nregister.  We then examine in Fig. 8(bottom)performance on downstream evaluation benchmarks,\nfollowing the protocol from Oquab et al. (2023). There seems to be an optimal number of registers\nfor dense tasks, and adding one brings most of the benefit.  This optimum is likely explained by\nthe disappearance of artifacts, leading to better local features. On ImageNet, however, performance\nimproves when using more registers. In all our experiments, we kept4register tokens.\n3.3OBJECT DISCOVERY\nRecent unsupervised object discovery methods rely on the quality and smoothness of local feature\nmaps (Sim\n ́\neoni et al., 2021; Wang et al., 2023).  By leveraging DINO Caron et al. (2021),  these\nmethods have significantly surpassed the previous state of the art.  However,  the algorithm leads\nto poor performance when applied to modern backbones such as DINOv2 Oquab et al. (2023) or\nsupervised ones Touvron et al. (2022). We posit that this can be alleviated by the method proposed\nin this work. We run LOST (Sim\n ́\neoni et al., 2021) on features extracted from backbones trained using\nthe algorithms described in Sec.3.1 with and without registers. We run object discovery on PASCAL\n7",
    "Published as a conference paper at ICLR 2024\nVOC 2007VOC 2012COCO 20k\nDeiT-III11.713.110.7\nDeiT-III+reg27.132.725.1\nOpenCLIP38.844.331.0\nOpenCLIP+reg37.142.027.9\nDINOv235.340.226.9\nDINOv2+reg55.460.042.0\nTable 3:  Unsupervised Object Discovery using LOST (Sim\n ́\neoni et al., 2021) on models with and\nwithout registers.  We evaluated three types of models trained with various amounts of supervision\non VOC 2007, 2012 and COCO. We measure performance using corloc.  We observe that adding\nregister tokens makes all models significantly more viable for usage in object discovery.\nInput[CLS]     [reg\n0\n]     [reg\n6\n]     [reg\n8\n]     [reg\n12\n]\nFigure 9:  Comparison of the attention maps of the[CLS]and register tokens.   Register tokens\nsometimes attend to different parts of the feature map, similarly to slot attention (Locatello et al.,\n2020). This behaviour was never required from the model, and emerged naturally from training.\nVOC 2007 and 2012 and COCO 20k. We use values for DeiT and OpenCLIP, and for DINOv2, we\nuse keys.  Because the output features may have different conditioning, we manually add a bias to\nthe gram matrix of features.  The results of this experiment are presented in Table 3.  For DINOv2\nand DeiT-III, adding registers significantly improves the discovery performance. For OpenCLIP, the\nperformance is slighty worse with registers (see Sec. C for analysis). The performance of DINOv2\non VOC2007 still does not match that of DINO as reported by Sim\n ́\neoni et al. (2021) (61.9corloc).\nHowever, the model with registers gets an improvement of20.1corloc (55.4versus35.3).\n3.4QUALITATIVE EVALUATION OF REGISTERS\nIn this final experiment, we qualitatively probe for the behavior of register tokens. We want to verify\nif they all exhibit similar attention patterns or whether a differentiation automatically emerges.  To\nthis end, we plot the attention maps of the class and register tokens to patch tokens.  The result of\nthis visualization is shown in Fig. 9. We see that registers do not have a completely aligned behavior.\nSome selected registers exhibit interesting attention patterns, attending to the different objects in the\nscene. While nothing enforced this behavior, their activations had some natural diversity. We leave\nthe study of the regularization of registers for future work.\n4RELATEDWORK\nFeature extraction with pretrained models.Using pretrained neural network models for extracting\nvisual features  has stood the test of time  since the AlexNet (Krizhevsky et al., 2012) CNN model\npretrained on ImageNet-1k (Russakovsky et al., 2015). More recent models have upgraded the same\nsetup with modern architectures, such as ResNets (used in,e.g., DETR, Carion et al., 2020) or even\nVision Transformers. As Transformers are easily able to handle different modalities during training,\noff-the-shelf backbones are now commonly trained on label supervision (e.g., DeiT-III on ImageNet-\n22k, Touvron et al., 2022) or text supervision (e.g., CLIP (Radford et al., 2021)), providing strong\nvisual foundation models,  scaling well with model sizes,  and enabling excellent performance on\na variety of tasks including detection (Carion et al., 2020) and segmentation (Zheng et al., 2021;\nKirillov et al., 2023).  In this context, supervision relies on annotations in the form of labels or text\nalignment;  the dataset biases (Torralba & Efros, 2011) are not well characterized,  yet they drive\n8",
    "Published as a conference paper at ICLR 2024\nlearning and shape the learned models.  An alternative approach consists of not using supervision\nand letting the models learn from the data via a pretext task that is designed to require understanding\nthe content of images (Doersch et al., 2015). This self-supervised learning paradigm was explored in\nmultiple methods using Vision Transformers: MAE (He et al., 2022) trains a model at reconstructing\npixel values of hidden areas of an image and then applies fine-tuning to address a new task. With a\ndifferent approach, the self-distillation family of methods (He et al., 2020; Caron et al., 2021; Zhou\net al., 2022) showcase strong performance using frozen backbones, allowing for more robustness to\ndomain shifts for task-specific downstream models.  In this work, we focused the analysis on self-\nsupervised learning, and more specifically on the DINOv2 approach (Oquab et al., 2023), which\nhas shown to be particularly effective for learning local features.  We showed that despite excellent\nbenchmark scores, DINOv2 features exhibit undesirable artifacts and that correcting these artifacts\nin the learning process allows for further improvements in the benchmark performances.   These\nphenomenon is even more surprising as DINOv2 builds upon DINO (Caron et al., 2021),  which\ndoes not show signs of artifacts.  We then further showed that the correction techniques hold for\nsupervised paradigms by testing on DeiT-III and OpenCLIP.\nAdditional tokens in transformers.Extending the transformer sequence with special tokens was\npopularized in BERT (Devlin et al., 2019). However, most approaches add new tokens either to pro-\nvide the network with new information as for example[SEP]tokens in BERT, provide opportunity\nto spend more computation on the input as seen with the tape tokens in AdaTape (Xue et al., 2023),\nor to gather information in these tokens, and use their output value as an output of the model:  for\nclassification, as[CLS]tokens in BERT and ViT (Dosovitskiy et al., 2021); for generative learning,\nas[MASK]in BERT and BEiT (Bao et al., 2021); for detection, as object queries in DETR (Carion\net al., 2020), detection tokens in YOLOS (Fang et al., 2021), and ViDT (Song et al., 2021); or for\naccumulating information from possibly multiple modalities before decoding, as latent token arrays\nin Perceivers  (Jaegle et al., 2021; 2022). Different to these works, the tokens we add to the sequence\nadd no information, and their output value is not used for any purpose.  They are simply registers\nwhere the model can learn to store and retrieve information during the forward pass.  The Mem-\nory Transformer (Burtsev et al., 2020), closer to our work, presents a simple approach to improve\ntransformer models using memory tokens added to the token sequence, improving translation perfor-\nmance. In follow-up work, Bulatov et al. (2022) address complex copy-repeat-reverse tasks. Sandler\net al. (2022) extend this line to the vision domain for fine-tuning but observe that such tokens do not\ntransfer well across tasks. In contrast, we do not perform fine-tuning and employ additional tokens\nduring pretraining to improve the features obtained for all tasks downstream. More importantly, our\nstudy contributes the following new insight in Sec. 2: the mechanism implemented through memory\ntokens already appears naturally in Vision Transformers; our study shows thatsuch tokens allow us\nnot to create but to isolate this existing behavior, and thus avoid collateral side-effects.\nAttention maps of vision transformers.Visualising the attention map from[CLS]token to patch\ntokens was popularized in DINO (Caron et al., 2021).  It was shown there that the attention maps\nof DINO were clean of artifacts, as opposed to the attention maps of previous vision transformers.\nOther works have since reported interesting attention maps using various techniques: by modifying\nthe optimisation procedure (Chen et al., 2022), by steering the attention scores towards useful image\nparts (Shi et al., 2023), by modifying the architecture of the transformer layers (Yu et al., 2024), or\nby introducing a learnable pooling to produce the[CLS]token (Psomas et al., 2023).\n5CONCLUSION\nIn  this  work,  we  exposed  artifacts  in  the  feature  maps  of  DINOv2  models,  and  found  this  phe-\nnomenon to be present in multiple existing popular models. We have described a simple method to\ndetect these artifacts by observing that they correspond to tokens with an outlier norm value at the\noutput of the Transformer model.  Studying their location, we have proposed an interpretation that\nmodels naturally recycle tokens from low-informative areas and repurpose them into a different role\nfor inference. Following this interpretation, we have proposed a simple fix, consisting of appending\nadditional tokens to the input sequence that are not used as outputs, and have found that this entirely\nremoves the artifacts, improving the performance in dense prediction and object discovery.  More-\nover, we have shown that the proposed solution also removes the same artifacts present in supervised\nmodels such as DeiT-III and OpenCLIP, confirming the generality of our solution.\n9",
    "Published as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nWe thank Hu Xu, Oriane Sim\n ́\neoni, Mido Assran and Armand Joulin for their insightful discussions\nand help during the course of this work.  We thank Pyrrhus for posing for fig 8.  Julien Mairal was\nsupported by ANR 3IA MIAI@Grenoble Alpes (ANR-19-P3IA-0003) and by ERC grant number\n101087696 (APHELEIA project).\nREFERENCES\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. InICLR, 2021.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev.  Recurrent memory transformer.  InNeurIPS,\n2022.\nMikhail S Burtsev, Yuri Kuratov, Anton Peganov, and Grigory V Sapunov.  Memory transformer.\narXiv preprint arXiv:2006.11527, 2020.\nNicolas  Carion,  Francisco  Massa,  Gabriel  Synnaeve,  Nicolas  Usunier,  Alexander  Kirillov,  and\nSergey Zagoruyko. End-to-end object detection with transformers. InECCV, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\n ́\ne J\n ́\negou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. InICCV, 2021.\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong.  When vision transformers outperform resnets\nwithout pre-training or strong data augmentations. InICLR, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.  Bert:  Pre-training of deep\nbidirectional transformers for language understanding.NAACL, 2019.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros.  Unsupervised visual representation learning by\ncontext prediction. InICCV, 2015.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner,  Mostafa  Dehghani,  Matthias  Minderer,  Georg  Heigold,  Sylvain  Gelly,  et  al.   An\nimage is worth 16x16 words: Transformers for image recognition at scale. InICLR, 2021.\nYuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and\nWenyu Liu.   You only look at one sequence:  Rethinking transformer in vision through object\ndetection. InNeurIPS, 2021.\nKaiming  He,  Haoqi  Fan,  Yuxin  Wu,  Saining  Xie,  and  Ross  Girshick.   Momentum  contrast  for\nunsupervised visual representation learning. InCVPR, 2020.\nKaiming  He,  Xinlei  Chen,  Saining  Xie,  Yanghao  Li,  Piotr  Doll\n ́\nar,  and  Ross  Girshick.   Masked\nautoencoders are scalable vision learners. InCVPR, 2022.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal  Dave,  Vaishaal  Shankar,  Hongseok  Namkoong,  John  Miller,  Hannaneh  Hajishirzi,  Ali\nFarhadi, and Ludwig Schmidt. Openclip. 2021.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.\nPerceiver: General perception with iterative attention. InICML, 2021.\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David\nDing,  Skanda  Koppula,  Andrew  Brock,  Evan  Shelhamer,  Olivier  J.  H’enaff,  Matthew  M.\nBotvinick,  Andrew  Zisserman,  Oriol  Vinyals,  and  Jo\n ̃\nao  Carreira.   Perceiver  io:  A  general  ar-\nchitecture for structured inputs & outputs. InICLR, 2022.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao,  Spencer  Whitehead,  Alexander  C  Berg,  Wan-Yen  Lo,  et  al.   Segment  anything.arXiv\npreprint arXiv:2304.02643, 2023.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.  Imagenet classification with deep convo-\nlutional neural networks. InNeurIPS, 2012.\n10",
    "Published as a conference paper at ICLR 2024\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf.  Object-centric learning with slot atten-\ntion. InNeurIPS, 2020.\nDavid G Lowe. Distinctive image features from scale-invariant keypoints.IJCV, 2004.\nMaxime Oquab,  Timoth\n ́\nee Darcet,  Th\n ́\neo Moutakanni,  Huy Vo,  Marc Szafraniec,  Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning\nrobust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023.\nBill Psomas, Ioannis Kakogeorgiou, Konstantinos Karantzalos, and Yannis Avrithis.  Keep it sim-\npool: Who said supervised transformers suffer from attention deficit?  InICCV, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.  Learning transferable visual\nmodels from natural language supervision. InICML, 2021.\nOlga  Russakovsky,  Jia  Deng,  Hao  Su,  Jonathan  Krause,  Sanjeev  Satheesh,  Sean  Ma,  Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei.\nImagenet large scale visual recognition challenge.IJCV, 2015.\nMark  Sandler,  Andrey  Zhmoginov,  Max  Vladymyrov,  and  Andrew  Jackson.   Fine-tuning  image\ntransformers using learnable memory. InCVPR, 2022.\nBaifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Toast: Transfer learning via attention steering,\n2023.\nOriane Sim\n ́\neoni,  Gilles Puy,  Huy V Vo,  Simon Roburin,  Spyros Gidaris,  Andrei Bursuc,  Patrick\nP\n ́\nerez, Renaud Marlet, and Jean Ponce.  Localizing objects with self-supervised transformers and\nno labels. InBMVC, 2021.\nHwanjun  Song,  Deqing  Sun,  Sanghyuk  Chun,  Varun  Jampani,  Dongyoon  Han,  Byeongho  Heo,\nWonjae Kim,  and Ming-Hsuan Yang.   Vidt:  An efficient and effective fully transformer-based\nobject detector. InICLR, 2021.\nAntonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. InCVPR, 2011.\nHugo Touvron, Matthieu Cord, and Herv\n ́\ne J\n ́\negou. Deit iii: Revenge of the vit. InECCV, 2022.\nXudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra.  Cut and learn for unsupervised object\ndetection and instance segmentation. InCVPR, 2023.\nFuzhao Xue, Valerii Likhosherstov, Anurag Arnab, Neil Houlsby, Mostafa Dehghani, and Yang You.\nAdaptive computation with elastic input sequence. InICML, 2023.\nYaodong Yu, Tianzhe Chu, Shengbang Tong, Ziyang Wu, Druv Pai, Sam Buchanan, and Yi Ma.\nEmergence of segmentation with minimalistic white-box transformers. InCPAL, 2024.\nSixiao  Zheng,  Jiachen  Lu,  Hengshuang  Zhao,  Xiatian  Zhu,  Zekun  Luo,  Yabiao  Wang,  Yanwei\nFu,  Jianfeng Feng,  Tao Xiang,  Philip HS Torr,  et al.   Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers. InCVPR, 2021.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.  ibot:\nImage bert pre-training with online tokenizer. InICLR, 2022.\n11",
    "Published as a conference paper at ICLR 2024\nFigure 10:  Feature norms along locations:  proportion of tokens with norm larger than the cutoff\nvalue at a given location.  Left:  official DINOv2 model (no antialiasing), right:  our models (with\nantialiasing). At some positions, more than 20% of tokens have a high norm.\nFigure 11:  Propagating unit gradients through a bicubic interpolation (16×16→7×7) without\nantialiasing. We observe a striping pattern similar to the one of Fig. 10 (left).\nAINTERPOLATION ARTIFACTS AND  OUTLIER POSITION DISTRIBUTION\nWe plot in Figure 10 (left) the proportion of outlier tokens, characterized by a norm larger than the\ncutoff value defined manually, following the distribution of norms shown in Fig. 3 (main text). We\nmake two observations:\nFirst, the distribution has a vertical-striped pattern. We investigate this phenomenon and notice that\nin the original DINOv2 implementation, during training the position embeddings are interpolated\nfrom a16×16map into a7×7map,  without antialiasing.  Propagating unit gradients through\nsuch an interpolation function (bicubic resize) leads to the following gradients, shown in Fig.  11.\nIn this work, when producing results with DINOv2 (especially for the results in Tables 2a,3), we\nalways apply antialiasing in the interpolation operator, removing the striping pattern, which gives an\nupdated distribution of outlier positions as shown in Fig. 10 (right).\nSecond, the outliers tend to appear in areas closer to the border of the feature map rather than in the\ncenter. Our interpretation is that the base model tends to recycle tokens in low-informative areas to\nuse as registers:  pictures produced by people tend to be object-centric, and in this case the border\nareas often correspond to background, which contains less information than the center.\nBCOMPLEXITY ANALYSIS\nSince our proposed fix introduces new tokens, it also increases the number of learnable parameters\nand the FLOP count of the model. We show in Fig. 12 the relationship between number of registers\nand increase in model FLOP count and parameter count.  We observe that adding registers induces\na negligible change in number of parameters, and a slight change in FLOP count.  Still, forn= 4\nregisters, the increase in FLOPs stays below 2%.\n12",
    "Published as a conference paper at ICLR 2024\nFigure 12: Increase in model parameter and FLOP count when adding different numbers of registers.\nAdding registers can increase model FLOP count by up to 6% for 16 registers. However, in the more\ncommon case of using 4 registers, that we use in most of our experiments, this increase is below 2%.\nIn all cases, the increase in model parameters is negligible.\nDINOv2\nOpenCLIP\nDeiT-III\nw/ REGw/o REGw/ REGw/o REGw/ REGw/o REG\nLOST\nscore\ndot prod.\nw/ seed\nseed\nexpansion\nFigure 13:    Illustration of the intermediate computations in the LOST algorithm for all models.\nAdding registers drastically improves the look of all intermediate steps for DeiT-III and DINOv2.\nThe difference is less striking for the OpenCLIP model.\nCANALYSIS OFLOSTPERFORMANCE\nThe results presented in Sec. 3.3 show that adding registers allows us to obtain better object dis-\ncovery performance with DINOv2 models.  The conclusions for the two other models studied in\nthis work could be more crisp.  In order to understand why this is so,  we qualitatively study the\nimpact of removing artifacts on the intermediate computations in the LOST algorithm. We show the\nintermediate outputs of LOST for all models on a given input image in Fig. 13.\nAdding registers improves the scores and the resulting seed expansion for DeiT-III and DINOv2.\nThis observation is coherent with the improved numbers reported in Table 3.  For OpenCLIP, how-\never, the LOST algorithm seems robust to the type of outliers observed in the local features. Adding\nregisters does remove artifacts (as clearly shown in Fig. 20) but does not have much impact on the\nLOST score. It is also worth noting that OpenCLIP, with or without registers, provides comparable\nperformance to DINOv2 without registers and DeiT-III with registers. The qualitative assessment is\ncoherent with the numbers reported in Table 3.\nA surprising observation is that despite the existence of high-norm patches in the output of Open-\nCLIP models without registers (as seen in Fig. 7), the seed expansion score in Fig. 13 looks smooth.\nIn the LOST experiment with OpenCLIP models, we do not use the features directly, but the values\nfrom the computation of attention maps.  In Fig. 14, we show the seed expansion score for Open-\nCLIP models with and without registers for keys, queries and values. We see that artifacts are clearly\nvisible as spots in the background for keys and queries, for the model without registers. As soon as\nregisters are used, the LOST score is focusing on the object, with a smoother score for values.  We\nqualitatively observe that for the OpenCLIP model, the value projection filters out the outliers even\nwithout registers. This means that the outliers appear to live in the null space of the value projection\nlayer; the investigation for this phenomenon is left for future work.\n13",
    "Published as a conference paper at ICLR 2024\nvalues\nw/ REG\nw/o REG\nqueries\nkeys\nFigure  14:  Illustration  of  the  seed  expansion  score  in  LOST  for  an  OpenCLIP  model  with  and\nwithout registers for the three types of features considered:  keys, queries, and values.  The score\nis qualitatively improved across all features, with fewer artifacts appearing.  Interestingly, the seed\nexpansion map computed using values does not exhibit artifacts with nor without registers.\nDBEHAVIOR  OF MODELS TRAINED  WITH  REGISTERS\nIn order to better understand the phenomenon at hand, we examine the question of to what extent\ndid the register tokens ”replace” the high-norm tokens and took on the same role.\nD.1NORMS\n(a) DINOv2 - no register(b) DINOv2 - 4 registers\nFigure 15: Distribution of token norms for a DINOv2 model without (left) and with (right) 4 regis-\nters. Introducing registers entirely negates the high-norm outliers among the patch tokens.\nIn Fig.  15 we compare the distribution of token norms for a model with or without registers.  This\nfigure  is  similar  to  Fig.   7  but  with  a  finer  granularity,  as  we  also  plot  the  norm  distribution  of\nindividual register tokens and[CLS]tokens.  We observe the following:  with registers, the norms\nof patch tokens do not contain outliers anymore, and the high-norm tokens are entirely contained in\nthe set of registers.  As a result, we conclude that the behavior leading to high-norm outliers in the\nmodel is effectively absorbed in the registers.\nAn additional interesting observation is that the norms of the registers appear to be quantized, com-\npared to the previous outliers; we leave the investigation of this phenomenon for future work.\nD.2INFORMATION HELD BY TOKENS\nWe report on table 4 the linear probing performance of models trained with and without registers,\nwhen using different tokens as representations.  We evaluate on the aircrafts dataset, as it showed\nclear conclusions in the similar table 1.  We observe that adding a register does not significantly\nmodify  the  scores  obtained  with  the[CLS]or  patch  tokens.   However,  the  outlier  patches  are\nremoved, and their behavior is transferred to the newly added register.\n14",
    "Published as a conference paper at ICLR 2024\ntop-1 accuracy\n#registers[CLS]normal patchoutlier patchregister\n084.615.573.3N/A\n185.214.5N/A71.1\nTable 4: Linear probing of models with and without registers on the Aircraft dataset, using various\ntokens as representation.   We observe that the behavior of the outlier tokens,  aggregating global\ninformation, is absorbed into the register.\nWe further conduct an evaluation of the local information contained in the patch tokens of a model\ntrained with and without registers (table 5). We observe that the non-outliers patches, in both cases,\nhold similar local information, confirming that the registers only remove the outlier behavior, with-\nout significantly modifying the information held by the other patches.\nposition predictionreconstruction\n#registerspatches consideredtop-1 accL2 error↓\n0non-outliers66.315.9\n4non-outliers (ie all)65.816.0\nTable  5:  Linear  probing  for  local  information  on  the  patch  tokens  of  models  trained  without  or\nwith registers.  We only consider patches considered ”normal”, i.e.  not the high-norm outliers.  We\nobserve that adding registers does not significantly modify the scores of these patches.\nD.3POSITIONAL FOCUS\n(a)[CLS](b) reg\n0\n(c) reg\n1\n(d) reg\n2\n(e) reg\n3\n(f) patch\nFigure 16:  Average attention map of registers and[CLS]token.  There is a variability observed,\nwith register 3 of this model focusing more on border areas.  We also include the average attention\nmap of a patch for comparison. The patch has a much more focused average attention.\nIn Fig.  16 we display the positional focus for the class token and the 4 registers of a DINOv2+reg\nmodel. We produce these plots by running the model on a random subset of ImageNet-22k, and av-\neraging the attention maps for the corresponding tokens at the last layer. We note that ImageNet-22k\ncontains mostly object-centric images rather than scenes, which explains why the average attention\nmaps correspond to centered blobs.\nWe make several observations. First, the attention maps for registers can be different of each other;\nfor example, register 3 tends to focus on border areas, while the other registers tend to focus on more\ncentered areas. Register 2 tends to focus slightly more on the upper areas of images that others. This\nis consistent with Fig.  9, where we show registers focusing on different large areas of the image,\nsuggesting some level of specialization.\nSecond, by comparing the register maps to the[CLS]token map and to a patch token map, we\nobserve that registers produce maps with a large support area, very similarly to the[CLS]token,\nand very different of a typical patch token which is more localized. As the[CLS]token is known to\ncarry global information (as proven by the linear probing classification performance): this suggests\nthat registers also carry global information.\n15",
    "Published as a conference paper at ICLR 2024\nEMASKED  AUTOENCODERS\nMasked Autoencoding (He et al., 2022) is another common way of pretraining self-supervised mod-\nels. We observe in Fig. 17 that there are no artifacts in the maps produced by MAE: our hypothesis\nis that the absence of artifacts is due to the training procedure using only a local loss on the patch\ntokens, rather than an objective involving global aggregation of information. However, we also note\nthat the performance of MAE models is very low for self-supervised representation learning (75%\nlinear probing performance on ImageNet classification for ViT-Large), preventing it from being used\nas is, and making fine-tuning a requirement.\nFigure 17:  First three principal components of the output feature map of a ViT-Large Masked Au-\ntoencoder.\nFBEHAVIOR  PER ATTENTION HEAD\nIn this section, we investigate whether the artifacts appear only on the attention maps for specific\nheads of the last vision transformer block, or for all of them.  We show in Fig. 18 the input image\nalong with the attention maps for different heads. We observe that the artifacts appear for all atten-\ntion heads, despite heads focusing on different areas of the object. We still observe that some heads\nfocus more on artifacts than others.\nGVARIANCE ON  TOKEN INFORMATION PROBING\nThe  results  presented  in  table  1  are  obtained  by  taking  a  random  patch  token,  either  normal  or\noutlier.  However, the choice of this token adds a significant source of variance in the evaluation.\nFor thoroughness, we report in table 6 the standard deviation of the scores obtained relative to this\nchoice.\n16",
    "Published as a conference paper at ICLR 2024\ninput\nFigure  18:  Attention  maps  of  the[CLS]token  to  the  patch  tokens,  shown  here  separately  per\nattention head. We produce these maps with a DINOv2-L model trained without registers.\ndatasetAirc.CF10CF100CUBCal101CarsDTD\ntoken\nnormal17.1±0.597.1±0.181.3±0.318.6±0.673.2±1.310.8±0.363.1±0.8\noutlier79.1±0.599.3±0.093.7±0.384.9±2.197.6±0.785.2±0.984.9±0.9\n[CLS]87.399.494.591.396.991.585.2\ndatasetFlow.FoodIN1kP205PetsSUNVOC\ntoken\nnormal59.5±1.274.2±0.365.8±0.153.1±0.347.8±0.537.7±0.370.8±0.5\noutlier99.6±0.093.5±0.269.0±0.755.1±1.094.1±0.278.5±0.289.7±0.1\n[CLS]99.794.786.066.496.978.689.1\nTable 6: Image classification via linear probing on normal and outlier patch tokens. As we select the\npatch tokens randomly among the set of eligible tokens, this adds a source of variability. We report\nthe standard deviation of this variability in grey along with the scores. This table is a detailed view\nof table 1.\nHQUALITATIVERESULTS\nWe trained three popular models:  DeiT-III, OpenCLIP, DINOv2 with and without the introduction\nof register tokens.  We observe in Fig.  19 the attention maps in the last layer of the Vision Trans-\nformer, for all three cases.  We see that our approach provides much cleaner attention maps, with\nconsiderably fewer artifacts, explaining the improvement on the downstream object discovery task\nmentioned in Sec.  3.3.  The feature maps are also visibly improved, as shown in Fig.  20.  Finally,\n17",
    "Published as a conference paper at ICLR 2024\nwe also show the norm of the patch tokens in Fig.  21, and confirm that in all three models, artifact\npatches correspond to norm outliers.\n18",
    "Published as a conference paper at ICLR 2024\nWithout registersWith registers\nInputDeiT-IIIOpenCLIPDINOv2DeiT-IIIOpenCLIPDINOv2\nFigure 19: Attention maps of models trained without and with registers on various images.\n19",
    "Published as a conference paper at ICLR 2024\nWithout registersWith registers\nInputDeiT-IIIOpenCLIPDINOv2DeiT-IIIOpenCLIPDINOv2\nFigure 20:  First principal component of the feature maps output by models trained without and\nwith registers on various images. The components are whitened and the colormap covers the range\n[−3σ,+3σ].\n20",
    "Published as a conference paper at ICLR 2024\nWithout registersWith registers\nInputDeiT-IIIOpenCLIPDINOv2DeiT-IIIOpenCLIPDINOv2\nFigure 21:  Maps of token norms for models trained without and with registers on various images.\nThe norm outliers are very visible for models trained without registers.\n21"
  ]
}